Of course, the best way to answer the question about the quality of the approximation of the characteristics of the posterior forecast distribution is to compare it with the characteristics of the posterior forecast distribution itself. Unfortunately, for most simulation models, and in particular for ABM, which is the focus of this paper, constructing an exact posterior distribution of forecasts is impossible, and approximations of it based on MCMC algorithms (see Andrieu et al. [2010]) or sequential Monte Carlo algorithms (SMC; see Chopin et al. [2012]) require computationally complex algorithms with a particle filter (see Gordon et al. [1993]) or variations of it. 

**Forecast error standardization.**  In research on probabilistic time series forecasting, 7 the forecasts 7 See Clark [2011] are often tested by interval calibration or probability integral transforms. These methods cannot be applied directly to a case in which only the mean and the standard deviation of the forecast are estimated, since the probability distribution of the forecasts is not fully specified. However, it can be noted that, similarly to the probability integral transforms, which must match the normal distribution and have zero autocorrelation, the standardized forecast errors (with the mean removed and divided by the standard deviation) should have the same properties (zero mean and unit standard deviation), other than the form of distribution, 8 for 8 The distribution need not be normal. a well-trained model. Thus, we look at the mean and the standard deviation of the standardized forecast errors and the mean and the standard deviation of the product of standardized errors separated by $k$ periods to test the quality. The latter, in fact, is equivalent to testing for autocorrelation, but it does not require the adjustment of the asymptotic distribution due to the finite length of the time series. Thus, there is no worry that the distribution of the correlation estimates will have a mean and standard deviation different from zero and one. 

There are two points to note about this quality metric. First, the results of the test described above should be considered one stage of the verification of the quality of the model. Not passing it should serve as a signal of problems with the quality of the neural network. The reverse situation, in which the test is passed, is not a guarantee that the model works well. For example, if the conditional forecasting model is poorly trained and does not take the presence of a scenario into account in any way, but produces only an unconditional forecast, it will pass the test. Second, we cannot apply the standard formal hypothesis testing about the mean and standard deviation, since the drift of the neural network coefficients at non-zero learning rates (see Mandt et al. [2017]) makes a comparable contribution to the distribution of the mean and standard deviation estimates. 9 9 It is probable that the estimates could be improved by applying ensembles of models based on several runs of the training procedure, or by using averaging at different iterations within the same training procedure, but this issue is beyond the scope of this paper. Therefore, below, we look at these quantities without formal hypothesis testing. 

**Comparison with the benchmark model.**  As mentioned above, passing the test on standardized forecast errors is only an indirect confirmation of the quality of the neural network, since, among other things, it can be passed by models that do not take all relevant information into account. To see how well the neural network takes historical information into account, we estimate the lower bound of forecast quality on a test dataset. 

Such an estimate can be made with a benchmark model. Note that after optimization, the neural network should have the smallest mean square forecast error (hereinafter, MSFE) for each variable and for each horizon by construction of the loss function. Moreover, the mean log predictive scores (hereinafter, LPS) should be the largest in the class of normal distributions. These two facts allow us to conclude that, no matter which forecasting model we build on the test data, it should not outperform the neural network in terms of MSFE and LPS. 

The benchmark model should be chosen based on a balance of flexibility and training time. On the one hand, the more flexible the model, the tighter the lower bound will be. On the other hand, estimating the lower bound should take adequate time. A representative test dataset often contains thousands or tens of thousands of time series sets, so it is necessary to train the model hundreds of thousands of times (the number of test sets multiplied by the number of periods in which forecasts are made) to test one forecast horizon for one variable. 

In this paper, vector autoregression (VAR) with ridge regularization is chosen as a benchmark model. It is computationally easy enough to estimate and allows the calculation of both conditional and unconditional 