# Natural Language is All a Graph Needs **Ruosong Ye** **Caiqi Zhang** **Runhui Wang** Rutgers University University of Cambridge Rutgers University 

ruosong.ye@rutgers.edu cz391@cam.ac.uk runhui.wang@rutgers.edu 

**Shuyuan Xu** **Yongfeng Zhang** Rutgers University Rutgers University 

shuyuan.xu@rutgers.edu yongfeng.zhang@rutgers.edu 

##### Abstract
The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose ** InstructGLM** ( **Instruct** ion-finetuned ** G** raph ** L** anguage ** M** odel), systematically design highly scalable prompts based on natural language instructions, and use natural language to describe the geometric structure and node features of the graph for instruction tuning an LLM to perform learning and inference on graphs in a generative manner. Our method exceeds all competitive GNN baselines on ogbn-arxiv, Cora and PubMed datasets, which demonstrates the effectiveness of our method and sheds light on generative large language models as the foundation model for graph machine learning. 

## Introduction Before the advent of Transformers [ 1 ], various artificial intelligence domains with different inductive biases had diverse foundational model architectures. For instance, CNNs [ 2 ,  3 ] were designed with considerations for spatial invariance in images, leading to superior performance in computer vision tasks [ 4 ,  5 ]. Memory-enhanced models like RNNs [ 6 ] and LSTM [ 7 ,  8 ] were widely used for handling sequential data such as natural language [ 9 ] and audio [ 10 ]. Graph Neural Networks (GNNs) excel in capturing topological information by employing message passing and aggregation mechanisms, making them a preferred choice in the field of graph learning for a long time [ 11 – 13 ]. 

In recent years, the AI community has witnessed the emergence of numerous powerful pre-trained Large Language Models (LLMs) [ 14 – 18 ], which are driving huge advancements and lead to the pursuit of possible Artificial General Intelligence (AGI) [ 19 ]. Under this background, there is a trend towards unification in model architectures across different domains. Specifically, pre-trained Transformers have demonstrated remarkable performance on various modalities, such as images [ 20 ] and videos [ 21 ] in computer vision, text in natural language processing [ 22 ], structured data in graph machine learning [ 23 ], decision sequences in reinforcement learning [ 24 ], and visual-text pairs in multimodal tasks [ 25 ]. There has even been Transformers capable of handling twelve modalities [ 26 ]. 

Besides model architecture, the unification of processing method in handling multimodal data is also a significant trend worth attention. T5 [ 15 ] established a text-to-text framework, unifying all NLP 