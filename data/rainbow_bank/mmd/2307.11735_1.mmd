the presynaptic and postsynaptic neuron activities have low correlation their connection are likely to be removed. The latter process is called synaptic pruning and it is considered essential for optimizing activity propagation and memory capacity(Chklovskii, Mel, and Svoboda, 2004; Knoblauch _ et al._ , 2014; Knoblauch and Sommer, 2016). Furthermore, it is commonly believed that synaptic pruning and rewiring dysfunction are one of the neural correlate of developmental disorders such as autism or schizophrenia (Bourgeron, 2009;
Moyer, Shelton, and Sweet,
 

2015), leading to, respectively, an higher or lower synaptic density with respect to neurotypical subjects(Hutsler and Zhang, 2010; Pagani _ et al._ , 2021; Glantz and Lewis, 2000). In the last decades computational neuroscience has investigated brain dynamics at different scales, from cellular (Markram _ et al._ , 2015) to mesoscopic and macroscopic through mean-field approaches (Wilson and Cowan, 1972; Amit and Brunel, 1997; Hopfield, 1984; Renart, Brunel, 

2020). Regarding synaptic plasticity, computational models were mostly focused on plasticity mechanisms that involve strengthening or weakening of existing synapses, like short-term plasticity (STP) (Tsodyks, Pawelzik, and Markram, 1998) or spike timing-dependent plasticity (STDP) (G¨ utig _ et al._ , 2003) and on their role in short-term, long-term, working memory and learning 

qiang Bi and ming Poo, 2001; Golosio _ et al._ , 2021; Capone _ et al._ , 2022). Only in recent times computational models of structural plasticity and connectivity rearrangements during learning were developed, showing intriguing results. Knoblauch _ et al._  (2014) and Knoblauch and Sommer (2016) describe a model of structural plasticity based on ”effectual connectivity”, defined in these works as the fraction of synapses able to represent a memory stored in a network. By structural plasticity, effectual connectivity is improved, since synapses that do not code for the memory are moved in order to optimize network’s connectivity. Their model defines synapses using a Markov model of three states: potential (i.e. not instantiated), instantiated but silent or instantiated and consolidated. Structural plasticity is thus related to the passage of the synapses from a potential state to an instantiated state (and vice versa), whereas changes only related to the synaptic weight are described by the consolidation of the instantiated synapses. With such a model, it is possible to show that networks with structural plasticity have higher or comparable memory capacity to networks with dense connectivity and it is possible to explain some cognitive mechanism such as the spacing effect (Knoblauch _ et al._ , 2014). Spiess _ et al._  (2016) simulated a spiking neural network with structural plasticity and STDP, showing that structural plasticity reduces the amount of noise of the network after a learning process, thus making the network able to have a clearer output. Furthermore, such a network with structural plasticity shows higher learning speed than the same network with only STDP implemented. Some new insights about the importance of synaptic pruning are also shown in Navlakha, Barth, and Bar-Joseph (2015), in which different pruning rates were studied suggesting that a slowly decreasing rate of pruning over time leads to more efficient network architectures. As discussed above, the biochemical and biophysical mechanisms underlying structural plasticity are extremely complex and only partially understood to date. For this reason, rather than attempting to build a biologically detailed model, this work exploits a relatively simple phenomenological model, including both the activity-driven and the homeostatic contributions; despite the lower complexity, this model accounts for the effects of structural plasticity in terms of the consolidation of synaptic connections between neurons with a high activity correlation as well as those of pruning and rewiring the connections for which this correlation is lower. This approach is also justified by the requirement for a simple and effective computational model suitable for simulating networks with a relatively large number of neurons and connections and for representing learning processes with sizable numbers of training and validation patterns. This model will then serve as the foundation for the creation of a mean-field-based theoretical framework for learning through synaptic plasticity capable of accounting for a variety of biological network properties. This framework will be used in a training and validation procedure to characterize learning and memory capacity of plastic neuronal networks as the number of training patterns and other model parameters vary. The results will then be compared with those obtained through simulations based on firing-rate-based neuronal networks. The model consid- 