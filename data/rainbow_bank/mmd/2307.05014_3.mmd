be e ff ective for support vector machines (SVM) [ 79 ] and large language models [ 25 ]. Another line of work called transductive learning uses test data to add constraints to the margin of SVMs [ 31 ,  11 ,  66 ]. The principle of transduction, as stated by Vapnik, also emphasizes locality [ 18 ,  67 ]: "Try to get the answer that you really need but not a more general one." 

In computer vision, the idea of training at test time has been well explored for specific applica tions [ 30 ,  57 ,  46 ,  73 ], especially depth estimation [ 62 ,  63 ,  82 ,  84 ,  43 ]. Our paper extends TTT-MAE [ 19 ], detailed in Section  3 . TTT-MAE, in turn, is inspired by the work Sun et al. [ 61 ], which proposed the general framework for test-time training with self-supervision, regardless of application. The particular self-supervised task used in [ 61 ] is rotation prediction [ 21 ]. Many other papers have followed this framework since then [ 24 ,  60 ,  40 ,  77 ], including [ 69 ] on videos discussed in Section  1 , and [ 5 ] which we discuss next. 

In [ 5 ], each video is treated as a dataset of unordered frames instead of a stream. In particular, there is no concept of past vs. future frames. The same model is used on the entire video. In contrast, our paper emphasizes locality. We have access to only the current and past frames, and our model keeps learning over time. In addition, all of our results are on real world videos, while [ 5 ] experiment on videos with artificial corruptions. These corruptions are also i.i.d. across frames. 

Our paper is very much inspired by [ 45 ]. To make video segmentation more e ffi cient, [ 45 ] makes predictions frame-by-frame using a small student model. If the student is not confident, it queries an expensive teacher model, and then trains the student to fit the prediction from the teacher online. Thanks to temporal smoothness, the student can generalize confidently across many frames without querying the teacher, so learning and predicting combined is still faster than naively using the teacher at every frame. Our method only consists of one model, which learns from a self-supervised task instead of a teacher model. Rather than focusing on computational e ffi ciency as in [ 45 ], the main goal of our paper is to improve inference quality. Behind their particular algorithm, however, we see the shared idea of locality, regardless of the form of supervision. 

## Background: TTT-MAE Our paper extends the work of _ Test-Time Training with Masked Autoencoders_  (TTT-MAE) [ 19 ], and uses TTT-MAE as the inner loop when updating the model for each frame. This section briefly describes TTT-MAE, as background for our extension. Figure  3  illustrates the process of TTT-MAE. 

The architecture for TTT with self-supervision [ 61 ] is Y-shaped with a stem and two heads: a prediction head \(g\) for the self-supervised task, a prediction head \(h\) for the main task, and a feature extractor \(f\) as the stem. The output features of \(f\) are shared between \(g\) and \(h\) as input. For TTT-MAE, the self-supervised task is masked image reconstruction [ 27 ]. Following standard terminology for autoencoders, \(f\) is also called the encoder, and \(g\) the decoder. 

Each input image \(x\) is first split into many non-overlapping patches. To produce the autoencoder input \(\tilde{x}\)\(\tilde{x}\), we mask out majority, e.g. 80%, of the patches in \(x\) at random. The self-supervised objective \(\ell_{s}(g\circ f(\tilde{x}),x)\)computes
 compares the reconstructed patches from
 \(g\circ f(\tilde{x})\)to the masked patches in
 \(x\), and computes the pixel-wise mean squared error. For the main task, e.g. segmentation, all patches in the original \(x\) are given as input to \(h\circ f\) , during both training and testing. 

### Training-Time Training There are three widely accepted ways to optimize the model components ( \(f\) , \(g\), \(h\)) at training time: joint training, probing, and fine-tuning. Fine-tuning is unsuitable for TTT, because it makes \(h\) rely too much on features that are used by the main task. Our paper uses joint training, described in Section  4 . In contrast, [ 19 ] uses probing, which we describe next for completeness. 

To prepare for probing, the common practice is to first train \(f\) and \(g\) with \(\ell_{s}\) on the training set without ground truth. This preparation stage is also called self-supervised pre-training. TTT- 