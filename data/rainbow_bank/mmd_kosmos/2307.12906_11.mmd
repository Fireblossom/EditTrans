controlled Y-axis rotations into simpler circuits. The quantum state preparation process was defined using quantum gates such as \(R_{Y}\) (rotation around the y-axis), controlled-NOT (CNOT), and \(Pauli-X\) gates. The primary quantum circuit incorporates the state preparation process, applying multiple rotations layers based on the given weights. Then a function applies rotation gates on qubits 0 and 1 and performs a CNOT operation between them. The quantum circuit was evaluated on a test input by applying the state preparation process and estimating the expectation value of the \(Pauli-Z\) operator on qubit 0. 

#### Classical NN+Encoder+QNN 

As suggested in 25 , this hybrid model is made up of a classical NN, an encoder circuit, and a QNN. There are two qumodes that make up the quantum circuit. Each vector entry was used as the parameter of available quantum gates to encode classical data into quantum states. Two 10-neuron hidden layers, each with an ’ELU’ activation function and a 14-neuron output layer, comprise the classical NN. Then, 14 entries of the classical NN’s output vectors are sent into squeezer, interferometers, displacement gates, and Kerr gates as input parameters. Kerr gates, Interferometer-1, interferometer-2, squeezers, and displacement gates were employed in the QNN’s four-step sequence. Using the \(Pauli-X\) gate’s \(\langle\phi_{k}|X|\phi_{k}\rangle\)expectation value, for the final state \(|\phi_{k}\rangle\)of each qumode, a two-element vector [ \(\langle\phi_{0}|X|\phi_{0}\rangle\), \(\langle\phi_{1}|X|\phi_{1}\rangle\)] was constructed. The ROC value of this model is 71.09%, and the closest threshold to optimal ROC is 54%. 

### Deep Reinforcement Learning Model 

We used TensorFlow 2.3+ 32 and TF Agents 0.6+ 15 to implement Double Deep Q-Network (DDQN) 28 . By treating the classification problem as an Imbalanced Classification Markov Decision Process, DDQN predicts that the episode will end when the agent misclassifies a sample from the minority-class but not a majority-class sample. The training process involved 100,000 episodes, and a replay memory was used with a length matching the number of warmup steps. Mini-batch training was performed with a batch size of 32, and the Q-network was updated using 2,000 steps of data collected during each episode. The policy was updated every 500 steps, and a soft update strategy was employed with a blending factor of 1 to update the target Q-network every 800 steps. The model architecture consisted of three dense layers with 256 units and ReLU activation, followed by dropout layers with a rate of 0.2. The final layer directly outputted the Q-values. Adam optimization was applied with a learning rate of 0.00025, and future rewards were not discounted. The exploration rate decayed from 1.0 to min_epsilon over \(\frac{1}{10}th\) of the total episodes, and the minimum and final chance of choosing a random action was set to 0.5. 

### Proposed QAmplifyNet Model 

The provided Figure  3  presents an overview of our proposed methodological framework. The first phase in the framework is gathering baseline information, which may include supplier efficiency, lead times, inventory levels, and product sales. Information on sales, supplier efficiency, inventory levels, and lead times for suppliers is gathered from a wide variety of data sources. These data are then combined and grouped into weekly time intervals for orders. The dataset is subsequently divided into training and testing sets. The collected data undergoes preprocessing using our suggested ‘Log transformation+Standard Scaling+VIF treatment’ method to address the common anomalies found in manufacturing industrial sensor data. This involves eliminating inconsistent data points, managing null values, and scaling and normalizing the data within a specified range. We applied PCA on both the train and test datasets to prepare the input for our 2-qubit Amplitude Encoder, resulting in 4 features. This dimensionality choice aligns with the model’s requirements, as it operates on \(\log_{2}{4}\), which yields a 2-dimensional classical data input. The aggregated data is then prepared for predictive analytics, employing a hybrid Q-CNN named QAmplifyNet as the core component of the proposed framework. The classical layers process the input data, while the quantum layer performs quantum computations on the encoded data. This comprehensive framework enables us to effectively leverage the collected data and utilize the hybrid model for analysis and prediction purposes. 

In our implementation, we leveraged the capabilities of PennyLane 3 to convert QNodes into Keras layers. This integration allowed us to combine these quantum layers with a diverse set of classical layers available in Keras, enabling the creation of genuinely hybrid models. Figure  5  explains the proposed architecture and summary of QAmplifyNet, which consists of a Keras Sequential model consisting of an input layer, three classical hidden layers, one quantum layer, and a classical output layer. Here is an explanation of each layer: 


1._ Input Layer:_  The input layer accepts inputs from 4 PC features and comprises 4 neurons. 

**Figure 5.**  Model architecture of QAmplifyNet model. 