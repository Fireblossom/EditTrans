<table>
<thead>
<tr>
<th style="text-align: center;">Epoches</th>
<th style="text-align: center;">Optimizer</th>
<th style="text-align: center;"><table>
<tbody>
<tr>
<td style="text-align: center;">Batch</td>
</tr>
<tr>
<td style="text-align: center;">size</td>
</tr>
</tbody>
</table></th>
<th style="text-align: center;"><table>
<tbody>
<tr>
<td style="text-align: center;">Learning</td>
</tr>
<tr>
<td style="text-align: center;">rate</td>
</tr>
</tbody>
</table></th>
<th style="text-align: center;"><table>
<tbody>
<tr>
<td style="text-align: center;">LR</td>
</tr>
<tr>
<td style="text-align: center;">decay</td>
</tr>
</tbody>
</table></th>
<th style="text-align: center;"><table>
<tbody>
<tr>
<td style="text-align: center;">Weight</td>
</tr>
<tr>
<td style="text-align: center;">decay</td>
</tr>
</tbody>
</table></th>
<th style="text-align: center;"><table>
<tbody>
<tr>
<td style="text-align: center;">Drop</td>
</tr>
<tr>
<td style="text-align: center;">path</td>
</tr>
</tbody>
</table></th>
<th style="text-align: center;"><table>
<tbody>
<tr>
<td style="text-align: center;">Embedding</td>
</tr>
<tr>
<td style="text-align: center;">dim</td>
</tr>
</tbody>
</table></th>
<th style="text-align: center;">Head</th>
<th style="text-align: center;"><span
class="math inline"><em>L</em></span></th>
<th style="text-align: center;"><table>
<tbody>
<tr>
<td style="text-align: center;">Window</td>
</tr>
<tr>
<td style="text-align: center;">size</td>
</tr>
</tbody>
</table></th>
<th style="text-align: center;"><table>
<tbody>
<tr>
<td style="text-align: center;">Patch</td>
</tr>
<tr>
<td style="text-align: center;">size</td>
</tr>
</tbody>
</table></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">300</td>
<td style="text-align: center;">Adam[]</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">4e-4</td>
<td style="text-align: center;">cosine</td>
<td style="text-align: center;">1e-7</td>
<td style="text-align: center;"><span
class="math inline">0.1</span></td>
<td style="text-align: center;"><span class="math inline">15</span></td>
<td style="text-align: center;"><span class="math inline">5</span></td>
<td style="text-align: center;"><span class="math inline">2</span></td>
<td style="text-align: center;"><span class="math inline">16</span></td>
<td style="text-align: center;">512</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Epoches</th>
<th style="text-align: center;">Optimizer</th>
<th style="text-align: center;"><table>
<tbody>
<tr>
<td style="text-align: center;">Batch</td>
</tr>
<tr>
<td style="text-align: center;">size</td>
</tr>
</tbody>
</table></th>
<th style="text-align: center;"><table>
<tbody>
<tr>
<td style="text-align: center;">Learning</td>
</tr>
<tr>
<td style="text-align: center;">rate</td>
</tr>
</tbody>
</table></th>
<th style="text-align: center;"><table>
<tbody>
<tr>
<td style="text-align: center;">LR</td>
</tr>
<tr>
<td style="text-align: center;">decay</td>
</tr>
</tbody>
</table></th>
<th style="text-align: center;"><table>
<tbody>
<tr>
<td style="text-align: center;">Weight</td>
</tr>
<tr>
<td style="text-align: center;">decay</td>
</tr>
</tbody>
</table></th>
<th style="text-align: center;"><table>
<tbody>
<tr>
<td style="text-align: center;">Drop</td>
</tr>
<tr>
<td style="text-align: center;">path</td>
</tr>
</tbody>
</table></th>
<th style="text-align: center;"><table>
<tbody>
<tr>
<td style="text-align: center;">Embedding</td>
</tr>
<tr>
<td style="text-align: center;">dim</td>
</tr>
</tbody>
</table></th>
<th style="text-align: center;">Head</th>
<th style="text-align: center;"><span
class="math inline"><em>L</em></span></th>
<th style="text-align: center;"><table>
<tbody>
<tr>
<td style="text-align: center;">Window</td>
</tr>
<tr>
<td style="text-align: center;">size</td>
</tr>
</tbody>
</table></th>
<th style="text-align: center;"><table>
<tbody>
<tr>
<td style="text-align: center;">Patch</td>
</tr>
<tr>
<td style="text-align: center;">size</td>
</tr>
</tbody>
</table></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">300</td>
<td style="text-align: center;">Adam[]</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">4e-4</td>
<td style="text-align: center;">cosine</td>
<td style="text-align: center;">1e-7</td>
<td style="text-align: center;"><span
class="math inline">0.1</span></td>
<td style="text-align: center;"><span class="math inline">15</span></td>
<td style="text-align: center;"><span class="math inline">5</span></td>
<td style="text-align: center;"><span class="math inline">2</span></td>
<td style="text-align: center;"><span class="math inline">16</span></td>
<td style="text-align: center;">512</td>
</tr>
</tbody>
</table>


Table 1. Default training and network hyper-parameters used in our method, unless stated otherwise. 

**COMO-ViT.** Given the input feature \(\mathbf{F}_{l-1}\in\mathbb{R}^{H\times W\times c}\)

of COMO-ViT, we conduct two branches of operations. In the first branch, we uniformly split it into \(n\) non-overlapping windows \(\mathcal{P}=[\mathbf{P}^{1},\mathbf{P}^{2},\cdots,\mathbf{P}^{n}]\in\mathbb{R} ^{n\times w\times w\times c}\), where \((w,w)\) is the window resolution. SNR [ 43 ] and STAR [ 52 ] downsample images, losing local structures and some important pixel-level information. Instead, the proposed COMO-ViT completely models the dependencies among all pixels of an image via a local-to-global hierarchical selfattention. Locally, each pixel in a window \(\mathbf{P}^{i}\)is regarded as an individual, we thus reshape \(\mathbf{P}^{i}\)as follows: 

\[\mathbf{P}^{i}\to[p^{i,1},p^{i,2},\cdots,p^{i,m}],\] (10)  

where \(p^{i,j}\in\mathbb{R}^{1\times 1\times c}\), \(m=w^{2}\)is the number of pixels in \(\mathbf{P}^{i}\). With a linear projection, we then transform the pixels into a sequence of pixel embeddings \(\mathbf{X}^{i}=[x^{i,1},x^{i,2},\cdots,x^{i,m}]\), where \(x^{i,j}\in\mathbb{R}^{c1}\)is the \(j\)-th pixel embedding, \(c1\) is the embedding dimension. For \(\mathbf{X}^{i}\), we utilize a local Transformer module to extract deep features as follows: 

\[{\mathbf{Y}^{{}^{\prime}}}^{i} =\mathbf{X}^{i}+\operatorname{MSA}(\operatorname{LN}(\mathbf{X}^{ i})),\] (11) \[\mathbf{Y}^{i} ={\mathbf{Y}^{{}^{\prime}}}^{i}+\operatorname{MLP}(\operatorname{ LN}({\mathbf{Y}^{{}^{\prime}}}^{i})),\]  

where \(\mathbf{Y}^{i}\)is the feature learned by the local Transformer module, \(\operatorname{MSA}(\cdot)\) is the Multi-head Self-Attention [ 35 ], \(\operatorname{LN}(\cdot)\) is layer normalization [ 1 ] for stable training and faster convergence, \(\operatorname{MLP}(\cdot)\) is multi-layer perceptron for feature transformation at channel dimension and nonlinearity. In such a process, we adopt 1D learnable location embedding to encode the spatial information of pixels. 

To complement the non-overlapping window attention, in the second branch which is parallel with local attention, we use a CNN module to model local pixel dependencies in \(\mathbf{F}_{l-1}\) via an overlapped sliding kernel to recover image details, in which a SE block [ 11 ] is used to explore channel relationship to boost representative power: 

\[\mathbf{F}^{{}^{\prime}}=\operatorname{Conv}(\operatorname{LN}( \mathbf{F}_{l-1})),\quad\mathbf{F}_{conv}=\mathbf{F}^{{}^{\prime}}\odot \operatorname{SE}(\mathbf{F}^{{}^{\prime}}).\] (12)  

\(\mathbf{F}_{conv}\) is then split into \(n\) non-overlapping windows \(\mathcal{Q}=[\mathbf{Q}^{1},\mathbf{Q}^{2},\cdots,\mathbf{Q}^{n}]\in\mathbb{R} ^{n\times w\times w\times c}\), and each \(\mathbf{Q}^{i}\)is reshaped: 

\[\mathbf{Q}^{i}\to[q^{i,1},q^{i,2},\cdots,q^{i,m}].\] (13)  

We combine the features from both branches as: 

\[\mathcal{C}=[\mathbf{C}^{1},\mathbf{C}^{2},\cdots,\mathbf{C}^{n}] ,\quad\mathbf{C}^{i}=\mathbf{Q}^{i}+\mathbf{Y}^{i}\] (14)  

Global pixel dependencies are explored by calculating window attention via a global attention module. Firstly, \(\mathcal{C}\)is transformed into a sequence of window embedding: 

\[\mathbf{U}=[u^{1},u^{2},\cdots,u^{n}],\quad u^{i}=\operatorname{ FC}(\operatorname{Vec}(\mathbf{C}^{i})),\] (15)  

where \(\operatorname{Vec}(\cdot)\) is vectorization operation. Then, we utilize a global Transformer module to explore inter-window dependencies, obtaining the feature \(\mathbf{F}_{l}\in\mathbb{R}^{H\times W\times c}\), where \(l\in\{1,2,\cdots,L\}\), and \(L\) is the COMO-ViT number. When \(l=1\), \(\mathbf{F}_{l-1}\) is the fused feature \(\mathbf{F}_{f}\)in Eq. . 

The result of the second stage is obtained by decoding \(\mathbf{F}_{L}\) with a convolutional layer ( \(\mathcal{D}(\cdot)\)): 

Figure 4. We observe that 

\[\mathbf{R}_{s2}=\operatorname{Conv}(\mathbf{F}_{L}).\] (16)  \(\mathbf{R}_{s3}\) keeps higher illumination than \(\mathbf{R}_{s2}\), illustrating the effectiveness of our LGCM. \(\mathbf{\Gamma}_{l}\) is corresponding pixel-wise local gamma map. 

We visually show \(\mathbf{R}_{s2}\) in Fig.  4 , observing that the illumination is enhanced and image details are also recovered. Especially, the noise is well removed by our COMO-ViT. 

**LGCM.** LGCM takes \(\mathbf{R}_{s2}\) as input, and learns local deep features to perceive illumination gap between \(\mathbf{R}_{s2}\) and ground truth and elaborately enhances illumination to reduce local color deviation: 

\[\mathbf{\Gamma}_{l}=\varphi(\operatorname{Conv}_{3}(\mathbf{R}_{s 2})),\quad\mathbf{R}_{s3}=\mathbf{R}_{s2}^{\mathbf{\Gamma}_{l}}.\] (17)  