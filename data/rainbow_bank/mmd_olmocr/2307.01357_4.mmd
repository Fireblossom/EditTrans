decomposition, we can define the projection matrix onto the subspace spanned by the top \(k\) right singular vectors as \(\mathbf{P}_{k}\in\mathbb{R}^{d\times d}\)given by \(\mathbf{P}_{k}:=\mathbf{V}_{k}\mathbf{V}_{k}^{T}\). 

For \(n\geq 1\), \(a\in[A]\), and \(\mathbf{Z}_{n}(a)\) as defined above, we write the \(k\)-truncated singular value decomposition of \(\mathbf{Z}_{n}(a)\) as \(\mathbf{Z}_{n,k}(a)=\widehat{\mathbf{U}}_{n,k}(a)\mathrm{diag}(\sigma_{1}( \mathbf{Z}_{n}(a)),\dots,\sigma_{k\wedge n\wedge d}(\mathbf{Z}_{n}(a)))\widehat{ \mathbf{V}}^{T}_{n,k}(a)\), and the corresponding projection onto the top \(k\) right singular vectors of \(\mathbf{Z}_{n}(a)\) as \(\widehat{\mathbf{P}}_{n,k}(a)\)\(\widehat{\mathbf{P}}_{n,k}(a)\). When \(k=r\), we leverage the simplified notation \(\widehat{\mathbf{P}}_{n}(a):=\widehat{\mathbf{P}}_{n,r}(a)\)\(\widehat{\mathbf{P}}_{n}(a):=\widehat{\mathbf{P}}_{n,r}(a)\). (Recall \(r=\dim(W^{*})\).) By \(\mathbf{P}\), we denote the projection matrix onto the true, underlying subspace \(W^{\ast}\). While \(\mathbf{P}\) is never known, our results leverage the fact that \(\widehat{\mathbf{P}}_{n}(a)\)\(\widehat{\mathbf{P}}_{n}(a)\) converges to \(\mathbf{P}\) nicely over time. We define the projected noisy covariate matrix matrix to be   \(\widehat{\mathbf{Z}}_{n}(a):=\mathbf{Z}_{n}(a)\widehat{\mathbf{P}}_{n}(a)\)\(\widehat{\mathbf{Z}}_{n}(a):=\mathbf{Z}_{n}(a)\widehat{\mathbf{P}}_{n}(a)\), and define \(\widehat{\mathbf{X}}_{n}(a),\widehat{\mathcal{E}}_{n}(a)\)\(\widehat{\mathbf{X}}_{n}(a),\widehat{\mathcal{E}}_{n}(a)\) similarly. Any quantity with a “ \(\widecheck{\cdot}\)\(\widecheck{\cdot}\) ” is defined equivalently to quantities with “ \(\widehat{\cdot}\)\(\widehat{\cdot}\) ”, except with \(\mathbf{P}\) in place of \(\widehat{\mathbf{P}}_{n}(a)\)\(\widehat{\mathbf{P}}_{n}(a)\). We are now ready to introduce our procedure for estimating \(\theta(a)\) for \(a\in[A]\), called 

**Definition 3.4**  ( **Adaptive Principal Component Regression** ) **.** _ Given_  regularization parameter \(\rho\geq 0\)_ and_  truncation level \(k\in\mathbb{N}\)_, for_ \(a\in[A]\)_ and_ \(n\geq 1\)_ let_ \(\widehat{\mathbf{Z}}_{n}(a):=\mathbf{Z}_{n}(a)\widehat{\mathbf{P}}_{n,k}(a)\)\(\widehat{\mathbf{Z}}_{n}(a):=\mathbf{Z}_{n}(a)\widehat{\mathbf{P}}_{n,k}(a)\)_ and_ \(\widehat{\mathcal{V}}_{n}(a):=\widehat{\mathbf{Z}}_{n}(a)^{T}\widehat{\mathbf{ Z}}_{n}(a)+\rho\widehat{\mathbf{P}}_{n,k}(a)\)\(\widehat{\mathcal{V}}_{n}(a):=\widehat{\mathbf{Z}}_{n}(a)^{T}\widehat{\mathbf{ Z}}_{n}(a)+\rho\widehat{\mathbf{P}}_{n,k}(a)\)_. Regularized principal component regression estimates_ \(\theta(a)\)_ as_ 

\[\widehat{\theta}_{n}(a):=\widehat{\mathcal{V}}_{n}(a)^{-1}\widehat{\mathbf{Z}} _{n}(a)\mathbf{Y}_{n}(a).\]  \[\widehat{\theta}_{n}(a):=\widehat{\mathcal{V}}_{n}(a)^{-1}\widehat{\mathbf{Z}} _{n}(a)\mathbf{Y}_{n}(a).\]  

Setting \(\rho=0\) recovers the version of PCR used in Agarwal et al.  [ 7 ] . In words, (unregularized) \(k\)-truncation, before estimating \(\theta(a)\) via linear regression using the projected covariates. We the sequel, we only consider adaptive PCR with truncation level \(k=r\). 

### Signal to noise ratio 

(this is the “signal” of the problem, measured through \(\sigma_{r}(\mathbf{X}_{n}(a))\)) stand out sequentially with respect to the “noise” induced by \(\mathcal{E}_{n}\) (which we will measure through the relevant high probability bounds on \(\|\mathcal{E}_{n}\|_{op}\)).-1 

**Definition 3.5**  ( **Signal to Noise Ratio** ) **.** _ We define the signal to noise ratio associated with_ _an action_ \(a\in[A]\)_ at round_ \(n\)_ as_ 

\[\mathrm{snr}_{n}(a):=\frac{\sigma_{r}(\mathbf{X}_{n}(a))}{U_{n}},\]  _where_ \((U_{n})_{n\geq 1}\)_ is a noise-dependent sequence growing as_ 

\(U_{n}=O\left(\sqrt{n}+\sqrt{d}+\sqrt{\log\left(\frac{1}{\delta}\log(n)\right)}\right)\)

noise ratio, \(\mathrm{snr}_{n}(a)\). While one may imagine defining \(\mathrm{snr}_{n}(a)\) as the ratio between \(\sigma_{r}(\mathbf{X}_{n}(a))\)and \(\|\mathcal{E}_{n}(a)\|_{op}\), bounding \(\|\mathcal{E}_{n}(a)\|_{op}\) is a nontrivial task as the rows of \(\mathcal{E}_{n}(a)\) may be strongly correlated. To circumvent this, we apply the trivial bound \(\|\mathcal{E}_{n}(a)\|_{op}\leq\|\mathcal{E}_{n}\|_{op}\). Thus, _ the price of_ _adaptivity in our setting is that the signal from covariates associated with an action_ \(a\)_ must stand_ _out with respect to the_ _total_ _ covariate noise by time_ \(n\). The growth condition on \(U_{n}\) presented in Definition  3.5  is motivated as follows: w.h.p \(\mathbb{E}\left\|\mathcal{E}_{n}\right\|_{op}\approx\sqrt{d}+\sqrt{n}\), and the extra additive \(\sqrt{\log\left(\frac{1}{\delta}\log(n)\right)}\)factor is the price we pay for having high probability control of \(\|\mathcal{E}_{n}\|_{op}\) uniformly 