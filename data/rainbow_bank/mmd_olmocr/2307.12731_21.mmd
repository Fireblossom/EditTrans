denoted by \(b\) in ( 27 ) are numerically equal to the corresponding coefficients denoted by \(b^{\prime}\)in ( 28 ). 21 

With regard to the question posed in section  2 , Frisch and Waugh provide the same answer as Yule: 21 For a proof see appendix  D coefficients are the same whether they are estimated from the multiple or from partial regressions. There . In his presentation of the Frisch-Waugh theorem,  Chipman  ( 1998 are both similarities and differences between , p. 84–86) argues as if Frisch  Yule  ( 1907 ) and  Frisch and Waugh  ( and Waugh had used projection matrices in their proof. That is not correct. 1933 ). First, whereas in Yule  Frisch and Waugh  (  ( 1907 1933 ) did not use projection ), only one variable could be included in matrices in their proof. \(W_{1}\) (the subset of covariates that was of interest to the researcher), in  Frisch and Waugh  ( 1933 ) only one random variable could be included in \(W_{2}\) (the subset of covariates that was _ not_  of interest to the researcher). Second, much like  Yule  ( 1907 ) before them,  Frisch and Waugh  ( 1933 ) did not investigate the relationship among estimated variances of the parameters of multiple and partial regressions. The question that is relevant for statistical inference, i.e. standard errors, had still not been posed. 

### Lovell extends the OLS analysis 

Lovell  ( 1963 ) extended the reach of the theorem significantly and addressed both questions that had been left unanswered by  Yule  ( 1907 ) and  Frisch and Waugh  ( 1933 ). On the one hand,  Lovell  ( 1963 ) partitioned the set of regressors, \(W\), into two subsets without any restrictions on the number of variables in each subset; on the other, he laid the groundwork for thinking about the _ estimated_  covariance matrices of the coefficient vectors. 

In the context of OLS estimation,  Lovell  ( 1963 ) demonstrated two important results: (a) the coefficient vectors are numerically the same whether they are estimated from the multiple or the partial regressions, and (b) the vector of residuals from the multiple and partial regressions are numerically the same. 22 22 The first I omit the proofs because they are just special cases of theorem result completed the YFWL so far as the estimate of the coefficient is concerned because the partitioning of  2  in this paper. the set of regressors was completely general; the second result laid the groundwork for comparing estimated variances of the coefficient vectors from multiple and partial regressions. 23 23 It is straightforward to extend the YFWL from ordinary least squares to generalized least squares estimation, as Lovell had noted when commenting on autocorrelated errors ( 

### D. Giles extends the theorem to IV estimation Lovell ,  1963 , p. 1004). Other scholars have worked on variations of Lovell’s 

So far, the YFWL theorem was always posed in the context of least squares, ordinary or generalized, results; see, for instance,  Fiebig and Bartels  ( 1996 ); estimation.  Krishnakumar  ( 2006 To the best of my knowledge, ).  Giles  ( 1984 ) was the first scholar to extend the theorem to 