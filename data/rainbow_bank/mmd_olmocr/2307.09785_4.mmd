distribution to Eq. ( 1 ), we introduce two patterns of internal parameters and distributions: 

\[P({v},{h}|\theta,\beta_{vh},\beta_{v},\beta_{h})=\frac{\exp{\left(\beta_{vh} \sum_{i,j}w_{ij}v_{i}h_{j}+\beta_{v}\sum_{i}b_{i}v_{i}+\beta_{h}\sum_{j}c_{j}h _{j}\right)}}{Z_{\beta_{vh,v,h}}},\] (12)  

and 

\[P({v},{h}|\theta,\beta_{vh},\beta_{v1},\ldots,\beta_{vn},\beta_{h1},\ldots, \beta_{hm})=\frac{\exp{\left(\beta_{vh}\sum_{i,j}w_{ij}v_{i}h_{j}+\sum_{i} \beta_{vi}b_{i}v_{i}+\sum_{j}\beta_{hj}c_{j}h_{j}\right)}}{Z_{\beta_{vh,vi,hi} }}.\] (13)  

We can derive the update rule for each internal parameter from Eq. ( 9 ) similar to Eq. ( 10 ) as 

\[\begin{split}&\delta{\beta_{vh}}=\eta\left(\left<\sum_{i,j}w_{ij}v_{i}h_{j} \right>_{S}-\left<\sum_{i,j}w_{ij}v_{i}h_{j}\right>_{\chi}\right),\\ &\delta{\beta_{v}}=\eta\left(\left<\sum_{i}b_{i}v_{i}\right>_{S}-\left<\sum_{i }b_{i}v_{i}\right>_{\chi}\right),\\ &\delta{\beta_{h}}=\eta\left(\left<\sum_{j}c_{j}h_{j}\right>_{S}-\left<\sum_{j }c_{j}h_{j}\right>_{\chi}\right),\\ &\delta{\beta_{vi}}=\eta\left(\left<b_{i}v_{i}\right>_{S}-\left<b_{i}v_{i} \right>_{\chi}\right),\\ &\delta{\beta_{hj}}=\eta\left(\left<c_{j}h_{j}\right>_{S}-\left<c_{j}h_{j} \right>_{\chi}\right).\\ \end{split}\] (14)  

The same algorithm can be applied by replacing line 8 in ESTIMATE_BETA of Fig.  2  with corresponding update rules in Eq.  14 . These patterns of internal parameters are hereinafter referred as one-parameter, three-parameter, and one-and-all-bias, respectively. The estimation of many internal parameters requires a large number of samples. Because the proposed method is an online estimation scheme, an increase in internal parameters leads to an increase in estimation time. If we introduce a calibration parameter for each cross term \(v_{i}h_{j}\), the increase in parameters is the product of \(n\) and \(m\), which may significantly increase estimation time. We, therefore, focus on the above three patterns of internal parameters. 

  Number of samples   Gibbs sampling   One-parameter   Three-parameter   One-and-all-bias
  ------------------- ---------------- --------------- ----------------- ------------------
  100000              3.49             3.88            3.70              3.64
  1000000             1.87             2.46            2.31              2.13


**Table 1.**  KL divergence between empirical distributions of samples and the original RBM. 

## Experiments 

First, we validated the proposed estimation scheme for the internal parameters without training. The CD algorithm trained an RBM comprising 32 visible nodes and 8 hidden nodes in advance using the coarse-grained MNIST training dataset, similar to prior studies 42 . The same RBM was embedded in 12 locations of D-Wave 2000Q without faulty qubits to gather samples efficiently. We compared samples generated by Gibbs sampling and quantum annealing with those calibrated by the proposed scheme. Table  1  shows the results of KL divergence. The increase in the size of samples decreased KL divergence because the empirical distributions approach the original distribution. In theory, Gibbs sampling can accurately obtain samples following the Boltzmann distribution. Therefore, results close to those obtained by Gibbs sampling can be considered more accurate. We found that the increased number of internal parameters improved sample quality. To determine the reason, Fig.  3  presents histograms of the samples’ energies. Although one parameter can adjust the total energy \(E\), the samples’ distributions of \(E_{vh}\) and \(E_{v}\) exhibit differences between Gibbs sampling and quantum annealing. \(E_{v}\) of the three-parameter case and \(E_{vh}\) of one-and-all-bias case seem to fit well with Gibbs sampling. Quantum annealers have not only temperature differences, but imperfections of qubits. We consider many internal parameters that can be calibrated to improve sample quality. 

Next, we conducted RBM training with the simultaneous estimation of internal parameters. The RBM size and training data were identical to the previous experiment. The number of times for updating \(\beta\) in each epoch was set to five. The increase 