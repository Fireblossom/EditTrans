FIG. 4: Scatter plots of estimated values at several number of steps \(n\) when \(\theta_{*}=(\theta_{*}^{1},\theta_{*}^{2})=(0,0.3)\). Each blue dot represents an estimate and the red dot represents the true value of the parameter. 

Since we want to check the asymptotic behavior of the estimates, the grid points are set finer around the true value of the parameter. 

We calculated a sequence of estimates \(\hat{\theta}_{1},\hat{\theta}_{2},\ldots,\)\(\hat{\theta}_{1},\hat{\theta}_{2},\ldots,\)\(\hat{\theta}_{8000}\in\Theta\)\(\hat{\theta}_{8000}\in\Theta\)in each run of AQSE, and repeated such runs 1000 times, to obtain 1000 samples of the sequence of estimates. 

First, we check the consistency (21). Fig. 4 plots the estimates  \(\hat{\theta}_{n}\)\(\hat{\theta}_{n}\) for \(n=1000\), 2000, 4000, and 8000. In each figure, the horizontal axis is \(\theta^{1}\)and the vertical axis is \(\theta^{2}\), with the blue dots representing estimates and the red dot representing the true value. The estimates are initially widely scattered around the true value, but as the number of steps increases, the estimates get closer to the true value. 

Next, we check the asymptotic normality (22). We performed goodness-of-fit tests on 1000 samples of  \(\hat{\theta}_{8000}\)\(\hat{\theta}_{8000}\)under the null hypothesis that they follow a multivariate normal distribution. The Anderson–Darling test in the mvnTest  package of R yielded a p-value of 0.9349, and the Cram´er–von Mises test in the same package yielded a p-value of 0.9434. The null hypothesis was accepted with a very high p-value for both tests. 

Finally, we check how the sample covariance matrix \(V[\hat{\theta}_{n}]\)evolves with the number of steps. Fig. 5a shows the weighted trace of the sample covariance matrix \(\operatorname{Tr}K_{\bar{\theta}}V[\hat{\theta}_{n}]\), where \(K_{\bar{\theta}}\) is the quantum Fisher information matrix at the sample mean  \(\bar{\theta}\)\(\bar{\theta}\) of the estimates at each step. For the sake of comparison, Fig. 5b also shows the result for the case where the true value is \(\theta_{*}=(\theta_{*}^{1},\theta_{*}^{2})=(0,0.1)\). Note that the values of the weighted trace are multiplied by \(n\), since the sample covariance matrix decreases by \(1/n\). The dashed lines indicate the ultimate limits of estimation precision displayed in Fig. 1. In each case, the solid curve approaches the dashed line as the number of steps increases. This means that if the number of steps is large enough, we can estimate the centroid \(\theta^{1}\)and the separation \(\theta^{2}\)simultaneously with the best accuracy theoretically possible. 

### Trapping phenomena near  θ 2 = 0 

It is noteworthy that, as can be seen from Figs. 5a and 5b, the convergence becomes much slower as \(\theta_{*}^{2}\)gets closer to zero. We also find from the heatmaps in Fig. 6 that, when \(\theta_{*}^{2}\)is small, a significant number of MLEs are trapped near the boundary \(\theta^{2}=0\)for a long time. These observations prompt us to envisage the following scenario: when \(\theta_{*}^{2}\)is small, a good number of estimates are located in the boundary region \(\theta^{2}\approx 0\)at an early stage of AQSE because of the large sample dispersion, and are kept trapped in that region for a long time, yielding a notable slowdown of the convergence of the sample covariance matrix. 

Let us examine the validity of this “boundary effect” scenario by means of the following tentative evaluation: because of the nature of convergence in distribution, each contour of the probability en sity function would converge to \(\theta_{*}\)in the rate \(\sim 1/\sqrt{n}\), so that the time \(\tau\) for a certain contour to pass through the “trapping wall”, i.e. the grid line closest to the axis \(\theta^{2}=0\), at a distance \(d\)from the true parameter \(\theta_{*}\)may be evaluated as 

\[d\sim\frac{C}{\sqrt{\tau}}\quad\Longleftrightarrow\quad\tau\sim\frac{C^{2}}{d^ {2}},\] (24)  

where \(C\) is a certain constant corresponding to the contour that characterizes the trapping effect. Assume further that, after getting out of the influence of the trapping wall, the time \(t_{0}\) required for the estimates to converge in distribution is independent of \(\theta_{*}\). Then the total time \(T=\tau+t_{0}\) of convergence in distribution would be 