[{"edit": ["The datasets used in this study are collected by NYCTLC2, which include both trip-level and aggregated shift-level information. The trip-level data contain the daily average yellow/green taxi trips, average trip duration, vendor ID, pickup and drop-off timestamps and locations (zonal level), trip distance, number of passengers, trip fare, tip, extra surcharges3 in rush hour or overnight, improvement surcharge 4, tax, and tolls. Each valid trip is defined as at least one passenger in the car with the value of trip duration, trip distance, and trip fare to be a positive number.\n\nFootnote 2: In 2009, NYCTLC initiated the Taxi Passenger Enhancement Project, which mandated the use of upgraded metering and information technology in all New York medallion cabs.\n\nFootnote 3: These are miscellaneous extras and surcharges, and currently include $0.50 and $1 rush hour and overnight charges.\n\nFootnote 4: The improvement surcharge began being levied in 2015, and $0.30 improvement surcharge assessed trips at the flag drop.\n\nIn the lack of individual shift-level data, we use the aggregated shift-level data in the study. The aggregated shift-level data are monthly based and are processed from approximately 8.32 billion yellow taxi trips, 0.73 billion green taxi trips, and 6.55 billion TNC trips over six years (January 2013 to December 2018). The data are obtained at the average individual-level and market-level. The average individual-level data cover daily average work hours per yellow/green taxi driver, daily average income per yellow/green driver, daily average yellow/green taxi medallions, monthly average active days per medallion. The average market-level data include the daily average fare of yellow/green taxis (include tips from credit card), monthly yellow/green taxi drivers, monthly yellow/green taxi medallions, total monthly work hours of all yellow/green taxi medallions and taxi drivers. We calculate the monthly income per driver by dividing the total fare per month by the monthly number of drivers. The data applied in our analysis can be seen in Table 1.\n\n## 4 Methodology\n\n### Hypotheses\n\nThe rapid growth of TNCs has significantly altered the landscape of the mobility-on-demand market and leads to a great loss in the taxi industry. To quantify how much do TNCs impact the overall labor supply and revenue of the taxi market, we propose the first hypothesis that _the rise of "], "nougat": ["The datasets used in this study are collected by NYCTLC2, which include both trip-level and aggregated shift-level information. The trip-level data contain the daily average yellow/green taxi trips, average trip duration, vendor ID, pickup and drop-off timestamps and locations (zonal level), trip distance, number of passengers, trip fare, tip, extra surcharges 3 These are miscellaneous extras and surcharges, and currently include $0.50 and $1 rush hour and overnight, and tolls. Each valid trip is defined as at least one passenger in the car with the value of trip duration, trip distance, and trip fare to be a positive number.\n\nFootnote 2: In 2015, and $0.30 improvement surcharge assessed trips at the flag drop.\n\nFootnote 3: In 2016, the 2017, and $0.30 improvement surcharge assessed trips at the flag drop.\n\nIn the lack of individual shift-level data, we use the aggregated shift-level data in the study. The aggregated shift-level data are monthly based and are processed from approximately 8.32 billion yellow taxi trips, 0.73 billion green taxi trips, and 6.55 billion TNC trips over six years (January 2013 to December 2018). The data are obtained at the average individual-level and market-level. The average individual-level data cover daily average work hours per yellow/green taxi driver, daily average income per yellow/green driver, daily average yellow/green taxi medallions, monthly average active days per medallion. The average market-level data include the daily average fare of yellow/green taxis (include tips from credit card), monthly yellow/green taxi drivers, monthly yellow/green taxi medallions, total monthly work hours of all yellow/green taxi medallions and taxi drivers. We calculate the monthly income per driver by dividing the total fare per month by the monthly number of drivers. The data applied in our analysis can be seen in Table 1.\n\n## 4 Methodology\n\n### Hypotheses\n\nThe rapid growth of TNCs has significantly altered the landscape of the mobility-on-demand market and leads to a great loss in the taxi industry. To quantify how much do TNCs impact the overall labor supply and revenue of the taxi market, we propose the first hypothesis that _the rise of "]}, {"edit": ["Now specialise to linear utility \\(\\ell(x)=\\ell x\\) and \\(h(x)=hx\\), for some given (positive) constants \\(\\ell\\) and \\(h\\). In this case we can derive a closed-form solution to the HJB equation in (4.10), and then derive the optimal strategy \\(\\nu_{*}(t)\\) (in terms of \\(\\widetilde{P}_{\\beta}(t)\\)). Let\n\n\\[\\Psi(t):=\\frac{1}{N(t)}\\left(he^{-\\beta T}N(T)+\\ell\\int_{t}^{T}e^{-\\beta s}N(s) ds\\right). \\tag{4.11}\\]\n\nThe following corollary classifies all possible optimal strategies (of the miner).\n\n**Corollary 4.3**.: [36] _Let \\(\\ell(x)=\\ell x\\) and \\(h(x)=hx\\) with \\(\\ell,h>0\\), and \\(N(t)\\) satisfy Assumption 4.1 (i). Assume that \\(\\widetilde{P}_{\\beta}(t)\\) satisfies the Lipschitz condition in \\((\\ref{Lipschitz})\\), and that \\(\\overline{\\nu}\\) satisfies:_\n\n\\[\\overline{\\nu}\\int_{0}^{T}\\frac{dt}{N(t)}\\leqslant\\frac{x}{N}\\wedge\\frac{N-x}{ N}. \\tag{4.12}\\]\n\n_Then, the following results hold:_\n\n1. _Suppose_ \\(\\widetilde{P}_{\\beta}(t)\\) _stays constant, i.e., for all_ \\(t\\in[0,T]\\)_,_ \\(\\widetilde{P}(t)=\\widetilde{P}(0)=P(0)\\)_._ 1. _If_ \\(P(0)\\geqslant\\Psi(0)\\)_, then_ \\(\\nu_{*}(t)=-\\overline{\\nu}\\) _for all_ \\(0\\leqslant t\\leqslant T\\)_._ 2. _If_ \\(P(0)\\leqslant\\Psi(T)\\)_, then_ \\(\\nu_{*}(t)=\\overline{\\nu}\\)_._ 3. _If_ \\(\\Psi(T)<P(0)<\\Psi(0)\\)_, then_ \\(\\nu_{*}(t)=\\overline{\\nu}\\) _for_ \\(t\\leqslant t_{0}\\)_, and_ \\(-\\overline{\\nu}\\) _for_ \\(t>t_{0}\\)_, where_ \\(t_{0}\\) _is the unique point in_ \\([0,T]\\) _such that_ \\(P(0)=\\Psi(t_{0})\\) _with_ \\(\\Psi(t)\\) _defined in_ \\((\\ref{Lipschitz})\\)_._\n2. _Suppose that_ \\(\\widetilde{P}_{\\beta}(t)\\) _is increasing in_ \\(t\\in[0,T]\\)_._ 1. _If_ \\(P(0)\\geqslant\\Psi(0)\\)_, then_ \\(\\nu_{*}(t)=-\\overline{\\nu}\\) _for all_ \\(0\\leqslant t\\leqslant T\\)_._ 2. _If_ \\(\\widetilde{P}_{\\beta}(T)\\leqslant\\Psi(T)\\)_, then_ \\(\\nu_{*}(t)=\\overline{\\nu}\\)_._ 3. _If_ \\(P(0)<\\Psi( "], "nougat": ["Now specialise to linear utility \\(\\ell(x)=\\ell x\\) and \\(h(x)=hx\\), for some given (positive) constants \\(\\ell\\) and \\(h\\). In this case we can derive a closed-form solution to the HJB equation in (4.10), and then derive the optimal strategy \\(\\nu_{*}(t)\\) (in terms of \\(\\widetilde{P}_{\\beta}(t)\\)). Let\n\n\\[\\Psi(t):=\\frac{1}{N(t)}\\left(he^{-\\beta T}N(T)+\\ell\\int_{t}^{T}e^{-\\beta s}N(s) ds\\right). \\tag{4.11}\\]\n\nThe following corollary classifies all possible optimal strategies (of the miner).\n\n**Corollary 4.1 (i). Assume that \\(\\widetilde{P}_{\\beta}(t)\\) satisfies the Lipschitz condition in (4.9), and that \\(\\overline{\\nu}\\) satisfies:\n\n\\[\\overline{\\nu}\\int_{0}^{T}\\frac{dt}{N(t)}\\leqslant\\frac{x}{N}\\wedge\\frac{N-x}{ N}. \\tag{4.12}\\]\n\nThen, the following results hold:\n\n1. Suppose \\(\\widetilde{P}_{\\beta}(t)\\) stays constant, i.e., for all \\(t\\in[0,T]\\), \\(\\widetilde{P}(t)=\\widetilde{P}(0)=P(0)\\). 1. If \\(P(0)\\geqslant\\Psi(0)\\), then \\(\\nu_{*}(t)=-\\overline{\\nu}\\) for all \\(0\\leqslant t\\leqslant T\\). 2. If \\(P(0)\\leqslant\\Psi(T)\\), then \\(\\nu_{*}(t)=\\overline{\\nu}\\). 3. If \\(\\Psi(T)<P(0)<\\Psi(0)\\), then \\(\\nu_{*}(t)=\\overline{\\nu}\\) for \\(t\\leqslant t_{0}\\), and \\(-\\overline{\\nu}\\) for \\(t>t_{0}\\), where \\(t_{0}\\) is the unique point in \\([0,T]\\) such that \\(P(0)=\\Psi(t_{0})\\) with \\(\\Psi(t)\\) defined in (4.11).\n2. Suppose that \\(\\widetilde{P}_{\\beta}(t)\\) is increasing in \\(t\\in[0,T]\\). 1. If \\(P(0)\\geqslant\\Psi(0)\\), then \\(\\nu_{*}(t)=-\\overline{\\nu}\\) for all \\(0\\leqslant t\\leqslant T\\). 2. If \\(\\widetilde{P}_{\\beta}(T)\\leqslant\\Psi(T)\\), then \\(\\nu_{*}(t)=\\overline{\\nu}\\). 3. If \\(P(0)<\\Psi(0)\\) and \\(\\widetilde{P}_{\\beta}(T)>\\Psi(T)\\), then \\(\\nu_{*}(t)=\\overline{\\nu}\\) for \\(t\\leqslant t_{0}\\), and \\(-\\overline{\\nu}\\) for \\(t>t_{0}\\), where \\(t_{0}\\) is the unique point of intersection of \\(\\widetilde{P}_{\\beta}(t)\\) and \\(\\Psi(t)\\) on \\([0,T]\\).\n3. Suppose that \\(\\widetilde{P}\n\n "]}, {"edit": ["_ii) \\(M^{\\star}_{T,i}>M^{\\star\\star}_{T,i}(\\gamma)>0\\) for all \\(i\\) and \\(\\gamma\\in\\Gamma\\), where \\(M^{\\star\\star}_{T,i}(\\gamma)=E_{\\mathscr{D}}\\left(T^{-1}X^{\\prime}_{i}M_{F}X_{i} (\\gamma)\\right)\\); iii) \\(M^{\\star}_{NT}>M^{\\star}_{NT}(\\gamma^{1},\\gamma^{2})>0\\) for all \\(\\gamma^{1},\\gamma^{2}\\in\\Gamma\\), where \\(M^{\\star}_{NT}=E_{\\mathscr{D}}\\Big{(}(NT)^{-1}\\sum_{i=1}^{N}X^{\\prime}_{i}M_{F} X_{i}\\Big{)}\\), and \\(M^{\\star}_{NT}(\\gamma^{1},\\gamma^{2})=E_{\\mathscr{D}}\\left((NT)^{-1}\\sum_{i=1}^{N }X_{i}(\\gamma^{1})^{\\prime}M_{F}X_{i}(\\gamma^{2})\\right)\\). If \\(\\gamma^{1}=\\gamma^{2}=\\gamma\\), then \\(M^{\\star}_{NT}(\\gamma)=E_{\\mathscr{D}}\\left((NT)^{-1}\\sum_{i=1}^{N}X_{i}(\\gamma )^{\\prime}M_{F}X_{i}(\\gamma)\\right)\\); iv) \\(M^{\\star}_{NT}>M^{\\star\\star}_{NT}(\\gamma)>0\\) for all \\(\\gamma\\in\\Gamma\\), where \\(M^{\\star\\star}_{NT}(\\gamma)=E_{\\mathscr{D}}\\Big{(}(NT)^{-1}\\sum_{i=1}^{N}X^{ \\prime}_{i}M_{F}X_{i}(\\gamma)\\Big{)}\\)._\n\n**Assumption A.9**.: _[Identification Condition] Consider the cross-product terms \\(\\mathcal{S}_{1i}(\\gamma)=\\tilde{Z}_{i}(\\gamma)^{\\prime}\\tilde{Z}_{i}(\\gamma)\\) with \\(\\mathcal{S}_{1i}(\\gamma)>0\\), \\(\\mathcal{S}_{2i}(\\gamma)=\\tilde{Z}_{i}(\\gamma)^{\\prime}\\tilde{W}_{i}(\\gamma^{0 },\\gamma)\\), \\(\\mathcal{S}_{3i}(\\gamma)=\\tilde{W}_{i}(\\gamma^{0},\\gamma)^{\\prime}\\tilde{W}_{i} (\\gamma^{0},\\gamma)\\), and \\(\\tilde{W}_{i}(\\gamma^{1},\\gamma^{2})=\\tilde{W}_{i}(\\gamma^{1})-\\tilde{W}_{i}( \\gamma^{2})\\) for any \\(\\gamma^{1},\\gamma^{2}\\in\\Gamma\\). Then, i) \\(N^{-1}\\sum_{i=1}^{N}||\\delta_{i}^{0}||^{2}>0\\), including \\(N\\to\\infty\\); ii) There exists some constant \\(\\tau>0\\) "], "nougat": ["_ii) \\(M^{\\star}_{T,i}>M^{\\star\\star}_{T,i}(\\gamma)>0\\) for all \\(i\\) and \\(\\gamma\\in\\Gamma\\), where \\(M^{\\star\\star}_{T,i}(\\gamma)=E_{\\mathscr{D}}\\left(T^{-1}X^{\\prime}_{i}M_{F}X_{i} (\\gamma)\\right)\\); iii) \\(M^{\\star}_{NT}>M^{\\star}_{NT}(\\gamma^{1},\\gamma^{2})>0\\) for all \\(\\gamma^{1},\\gamma^{2}\\in\\Gamma\\), where \\(M^{\\star}_{NT}=E_{\\mathscr{D}}\\Big{(}(NT)^{-1}\\sum_{i=1}^{N}X^{\\prime}_{i}M_{F} X_{i}\\Big{)}\\), and \\(M^{\\star}_{NT}(\\gamma^{1},\\gamma^{2})=E_{\\mathscr{D}}\\left((NT)^{-1}\\sum_{i=1}^{N }X_{i}(\\gamma^{1})^{\\prime}M_{F}X_{i}(\\gamma^{2})\\right)\\). If \\(\\gamma^{1}=\\gamma^{2}=\\gamma\\), then \\(M^{\\star}_{NT}(\\gamma)=E_{\\mathscr{D}}\\left((NT)^{-1}\\sum_{i=1}^{N}X_{i}(\\gamma )^{\\prime}M_{F}X_{i}(\\gamma)\\right)\\); iv) \\(M^{\\star}_{NT}>M^{\\star\\star}_{NT}(\\gamma)>0\\) for all \\(\\gamma\\in\\Gamma\\), where \\(M^{\\star\\star}_{NT}(\\gamma)=E_{\\mathscr{D}}\\Big{(}(NT)^{-1}\\sum_{i=1}^{N}X^{ \\prime}_{i}M_{F}X_{i}(\\gamma)\\Big{)}\\)._\n\n**Assumption A.9**.: _[Identification Condition] Consider the cross-product terms \\(\\mathcal{S}_{1i}(\\gamma)=\\tilde{Z}_{i}(\\gamma)^{\\prime}\\tilde{Z}_{i}(\\gamma)\\) with \\(\\mathcal{S}_{1i}(\\gamma)>0\\), \\(\\mathcal{S}_{2i}(\\gamma)=\\tilde{Z}_{i}(\\gamma)^{\\prime}\\tilde{W}_{i}(\\gamma^{0} ,\\gamma)\\), \\(\\mathcal{S}_{3i}(\\gamma)=\\tilde{W}_{i}(\\gamma^{0},\\gamma)^{\\prime}\\tilde{W}_{i} (\\gamma^{0},\\gamma)\\), and \\(\\tilde{W}_{i}(\\gamma^{1},\\gamma^{2})=\\tilde{W}_{i}(\\gamma^{1})-\\tilde{W}_{i}( \\gamma^{2})\\) for any \\(\\gamma^{1},\\gamma^{2}\\in\\Gamma\\). Then, i) \\(N^{-1}\\sum_{i=1}^{N}||\\delta_{i}^{0}||^{2}>0\\), including \\(N\\to\\infty\\); ii) There exists some constant \\(\\tau>0\\) "]}, {"edit": ["denoted by \\(b\\) in (27) are numerically equal to the corresponding coefficients denoted by \\(b^{\\prime}\\) in (28).21\n\nFootnote 21: For a proof see appendix D. In his presentation of the Frisch-Waugh theorem, Chipman (1998, p. 84\u201386) argues as if Frisch and Waugh had used projection matrices in their proof. That is not correct. Frisch and Waugh (1933) did not use projection matrices in their proof.\n\nWith regard to the question posed in section 2, Frisch and Waugh provide the same answer as Yule: coefficients are the same whether they are estimated from the multiple or from partial regressions. There are both similarities and differences between Yule (1907) and Frisch and Waugh (1933). First, whereas in Yule (1907), only one variable could be included in \\(W_{1}\\) (the subset of covariates that was of interest to the researcher), in Frisch and Waugh (1933) only one random variable could be included in \\(W_{2}\\) (the subset of covariates that was _not_ of interest to the researcher). Second, much like Yule (1907) before them, Frisch and Waugh (1933) did not investigate the relationship among estimated variances of the parameters of multiple and partial regressions. The question that is relevant for statistical inference, i.e. standard errors, had still not been posed.\n\n### Lovell extends the OLS analysis\n\nLovell (1963) extended the reach of the theorem significantly and addressed both questions that had been left unanswered by Yule (1907) and Frisch and Waugh (1933). On the one hand, Lovell (1963) partitioned the set of regressors, \\(W\\), into two subsets without any restrictions on the number of variables in each subset; on the other, he laid the groundwork for thinking about the _estimated_ covariance matrices of the coefficient vectors.\n\nIn the context of OLS estimation, Lovell (1963) demonstrated two important results: (a) the coefficient vectors are numerically the same whether they are estimated from the multiple or the partial regressions, and (b) the vector of residuals from the multiple and partial regressions are numerically the same.22 The first result completed the YFWL so far as the estimate of the coefficient is concerned because the partitioning of the set of regressors was completely general; the second result laid the groundwork for comparing estimated variances of the coefficient vectors from multiple and partial regressions.23\n\nFootnote 22: I omit the proofs because they are just special cases of theorem 2 in this paper.\n\nFootnote 23: It is straightforward to extend the YFWL from ordinary least squares to generalized least squares estimation, as Lovell had noted when commenting on autocorrelated errors (Lovell, 1963, p. 1004). Other scholars have worked on variations of Lovell\u2019s results; see, for instance, Fiebig and Bartels (1996); Krishnakumar (2006).\n\n### D. Giles extends the theorem to IV estimation\n\nSo far, the YFWL theorem was always posed in the context of least squares, ordinary or generalized, estimation. To the best of my knowledge, Giles (1984) was the first scholar to extend the theorem to "], "nougat": ["denoted by \\(b\\) in ( 27 ) are numerically equal to the corresponding coefficients denoted by \\(b^{\\prime}\\) in ( 28 ).21\n\nFootnote 21: For a proof see appendix D. In his presentation of the Frisch-Waugh theorem, Chipman (1998, p. 84\u201386) argues as if Frisch and Waugh provide the same answer as Yule: coefficients are the same whether they are estimated from the multiple or from partial regressions. There are both similarities and differences between Yule (1907) and Frisch and Waugh (1933). First, whereas in Yule (1907), only one variable could be included in \\(W_{1}\\) (the subset of covariates that was of interest to the researcher), in Frisch and Waugh (1933) only one random variable could be included in \\(W_{2}\\) (the subset of covariates that was _not_ of interest to the researcher). Second, much like Yule (1907) before them, Frisch and Waugh (1933) did not investigate the relationship among estimated variances of the parameters of multiple and partial regressions. The question that is relevant for statistical inference, i.e. standard errors, had still not been posed.\n\n### Lovell extends the OLS analysis\n\nLovell (1963) extended the reach of the theorem significantly and addressed both questions that had been left unanswered by Yule (1907) and Frisch and Waugh (1933). On the one hand, Lovell (1963) partitioned the set of regressors, \\(W\\), into two subsets without any restrictions on the number of variables in each subset; on the other, he laid the groundwork for thinking about the _estimated_ covariance matrices of the coefficient vectors.\n\nIn the context of OLS estimation, Lovell (1963) demonstrated two important results: (a) the coefficient vectors are numerically the same whether they are estimated from the multiple or the partial regressions, and (b) the vector of residuals from the multiple and partial regressions are numerically the same.22 The first result completed the YFWL so far as the estimate of the coefficient is concerned because the partitioning of the set of regressors was completely general; the second result laid the groundwork for comparing estimated variances of the coefficient vectors from multiple and partial regressions.23\n\nFootnote 22: I omit the proofs because they are just special cases of theorem 2 in this paper.\n\nFootnote 23: It is straightforward to extend the YFWL from ordinary least squares to generalized least squares estimation, as Lovell had noted when commenting on autocorrelated errors (Lovell, 1963, p. 1004). Other scholars have worked on variations of Lovell\u2019s results; see, for instance, Fiebig and Bartels (1996); Krishnakumar (2006).\n\n### D. Giles extends the theorem to IV estimation\n\nSo far, the YFWL theorem was always posed in the context of least squares, ordinary or generalized, estimation. To the best of my knowledge, Giles (1984) was the first scholar to extend the theorem to "]}, {"edit": ["(\\(P_{2}=0.521\\)), followed by paths 1 and 5 (\\(P_{1}=P_{5}=0.192\\)), path 4 (\\(P_{4}=0.070\\)) and path 3 (0.026). Note that the results in cases 1 and 2 are consistent with those of the non-adaptive and path-based multinomial logit (MNL) model.\n\nIn contrast, in case 3, travelers could not obtain information about the additional travel times before the trip, and they had to adjust their paths locally. Because they globally perceived only \\(x_{1}\\) when they departed from the origin \\(o\\), they originally planned their paths according to the same path probabilities as case 1, resulting in more than half of travelers taking an action to travel link \\(a_{2}\\) that is the first elemental link of path 3 (and paths 4 and 5). However, at the sink node of link \\(a_{2}\\), the travelers locally perceived the additional time of link \\(a_{5}\\) and switched their paths, moving to link \\(a_{3}\\). This local adaption was also observed when travelers move from \\(a_{4}\\) to \\(a_{6}\\), instead of \\(a_{7}\\). As a result, path 5 got the highest probability (\\(P_{5}=0.434\\)), followed by path 2 (\\(P_{2}=0.268\\)), path 4 (\\(P_{4}=0.160\\)), path 1 (\\(P_{4}=0.099\\)) and path 3 (\\(P_{4}=0.040\\)). This result implies that travelers cannot take the globally optimal path when some attributes are only locally perceived by them. As such, our model can describe both global preferences for and local responses to network attributes through the specifications of decomposed global and local utility functions.\n\nNext, to discuss the effect of the size of the scale parameter \\(\\mu_{G}\\), we computed the path probabilities with different values of \\(\\mu_{G}\\) where \\(\\mu\\) and \\(\\gamma\\) are both fixed to one, thereby \\(\\gamma\\mu/\\mu_{G}=1/\\mu_{G}\\). The results in Figure 3(a) show that the probabilities gradually converge to certain values as \\(\\mu_{G}\\) grows. This is because, when \\(\\mu_{G}\\) goes to a sufficiently large value, the global path choice becomes deterministic, and the value function can be approximated by \\(V^{d}(k)\\approx\\max_{u\\in A(k)}\\{v_{G}(a|k)+V^{d}(a)\\}\\) describing the deterministic maximum path utility from link \\(k\\) to \\(d\\). This change in the value function is displayed in Figure 3(b). Also, the difference between the values \\(V^{d}(a_{1})\\) and \\(V^{d}(a_{2})\\) of links \\(a_{1}\\) and \\(a_{2}\\) gets larger according to the increase in \\(\\mu_{G}\\). This explains that the probabilities of paths 1 and 2 whose first link is \\(a_{1}\\) decrease, whereas those of paths 3-5 whose first link is \\(a_{2}\\) increase. Note that although the certainty of travelers' perception of the global utility increases, they still locally maximize their utilities whose uncertainty is characterized by the scale \\(\\mu\\).\n\n## 4 Learning preferences from observed network paths\n\nIn this section, we present the estimation of the proposed global-local path choice model based on maximum likelihood. Unlike most of the previous adaptive (plan-action) path choice models, the proposed model can be estimated only with observations of paths that travelers actually took, without the information of _plans_ "], "nougat": ["(\\(P_{2}=0.521\\)), followed by paths 1 and 5 (\\(P_{1}=P_{5}=0.192\\)), path 4 (\\(P_{4}=0.070\\)) and path 3 (0.026). Note that the results in cases 1 and 2 are consistent with those of the non-adaptive and path-based multinomial logit (MNL) model.\n\nIn contrast, in case 3, travelers could not obtain information about the additional travel times before the trip, and they had to adjust their paths locally. Because they globally perceived only \\(x_{1}\\) when they departed from the origin \\(o\\), they originally planned their paths according to the same path probabilities as case 1, resulting in more than half of travelers taking an action to travel link \\(a_{2}\\) that is the first elemental link of path 3 (and paths 4 and 5). However, at the sink node of link \\(a_{2}\\), the travelers locally perceived the additional time of link \\(a_{5}\\) and switched their paths, moving to link \\(a_{3}\\). This local adaption was also observed when travelers move from \\(a_{4}\\) to \\(a_{6}\\), instead of \\(a_{7}\\). As a result, path 5 got the highest probability (\\(P_{5}=0.434\\)), followed by path 2 (\\(P_{2}=0.268\\)), path 4 (\\(P_{4}=0.160\\)), path 1 (\\(P_{4}=0.099\\)) and path 3 (\\(P_{4}=0.040\\)). This result implies that travelers cannot take the globally optimal path when some attributes are only locally perceived by them. As such, our model can describe both global preferences for and local responses to network attributes through the specifications of decomposed global and local utility functions.\n\nNext, to discuss the effect of the size of the scale parameter \\(\\mu_{G}\\), we computed the path probabilities with different values of \\(\\mu_{G}\\) where \\(\\mu\\) and \\(\\gamma\\) are both fixed to one, thereby \\(\\gamma\\mu/\\mu_{G}=1/\\mu_{G}\\). The results in Figure 3 (a) show that the probabilities gradually converge to certain values as \\(\\mu_{G}\\) grows. This is because, when \\(\\mu_{G}\\) goes to a sufficiently large value, the global path choice becomes deterministic, and the value function can be approximated by \\(V^{d}(k)\\approx\\max_{a\\in A(k)}\\{v_{G}(a|k)+V^{d}(a)\\}\\) describing the deterministic maximum path utility from link \\(k\\) to \\(d\\). This change in the value function is displayed in Figure 3 (b). Also, the difference between the values \\(V^{d}(a_{1})\\) and \\(V^{d}(a_{2})\\) of links \\(a_{1}\\) and \\(a_{2}\\) gets larger according to the increase in \\(\\mu_{G}\\). This explains that the probabilities of paths 1 and 2 whose first link is \\(a_{1}\\) decrease, whereas those of paths 3-5 whose first link is \\(a_{2}\\) increase. Note that although the certainty of travelers\u2019 perception of the global utility increases, they still locally maximize their utilities whose uncertainty is characterized by the scale \\(\\mu\\).\n\n## 4 Learning preferences from observed network paths\n\nIn this section, we present the estimation of the proposed global-local path choice model based on maximum likelihood. Unlike most of the previous adaptive (plan-action) path choice models, the proposed model can be estimated only with observations of paths that travelers actually took, without the information of _pl "]}, {"edit": ["and therefore an unbiased estimator of the outstanding claims is the difference between the estimated total and the currently paid amount,\n\n\\[\\hat{L}^{O}(\\tau)=\\hat{L}(\\tau)-L^{P}(\\tau)=\\sum_{j=1}^{N^{P}(\\tau)}\\frac{Y_{j}}{ \\pi_{j}(\\tau)}-\\sum_{j=1}^{N^{P}(\\tau)}Y_{j}=\\sum_{j=1}^{N^{P}(\\tau)}\\frac{1- \\pi_{j}(\\tau)}{\\pi_{j}(\\tau)}Y_{j}. \\tag{3}\\]\n\nThe intuition behind the HT estimator lies in the fact that only a portion of all payments \\(Y_{j}\\) is reported, proportionally to \\(\\pi_{j}(\\tau)\\), and so each payment in the sample is \"augmented\" by a factor of \\(1/\\pi_{j}(\\tau)\\) to approximate the actual total amount. It is important to note that the HT estimator is non-parametric, meaning that it leads to an estimation of the reserve that does not require any assumptions on the underlying distribution of the number of claims (frequency) or the distribution of claim sizes (severity), which is a remarkable property. Additionally, we emphasize the fact that even though the estimator is based on the population level (i.e a macro-level scale), the inclusion probabilities are dependent on the individual attributes of policyholders, claims, and payments. Therefore the estimator incorporates granular information as part of the estimation.\n\nThe HT estimator is widely recognized as one of the most influential estimators in the statistics literature, having been extensively studied for over 70 years in the field of population sampling (e.g Arnab (2017)). Consequently, the HT estimator has a solid theoretical foundation and possesses numerous desirable properties that directly inherit to the claim-reserving problem, including consistency, unbiasedness, sufficiency, among others. More recently, it has also been applied in inverse probability weighting (IPW) methods for estimation in causal inference (Yao et al. (2021)), including applications in fairness in insurance. The terminology \"IPW estimator\" is more extended in and outside the statistics literature, and so We will mostly refer to the estimator of the reserve as the IPW estimator, and reserve the naming of HT estimator when referring to the general concept.\n\nA specific case of interest arises when we set \\(Y_{i}=1\\). In this scenario, all the sums above simplifies to a count of the number of payments, allowing us to obtain an unbiased estimator for the number of payments yet to make, as\n\n\\[\\hat{N}^{O}(\\tau):=\\hat{N}(\\tau)-N^{P}(\\tau)=\\sum_{j=1}^{N^{P}(\\tau)}\\frac{1}{ \\pi_{j}(\\tau)}-\\sum_{j=1}^{N^{P}(\\tau)}1=\\sum_{j=1}^{N^{P}(\\tau)}\\frac{1-\\pi_{ j}(\\tau)}{\\pi_{j}(\\tau)}. \\tag{4}\\]\n\nIt is worth noting that this particular expression coincides with the one utilized by Fung et al. (2022) for the specific case of the number of incurred but not reported (IBNR) claims. In their work, they derived this expression under the assumption that the number of unreported claims follows a geometric distribution and demonstrated its unbiasedness when the number of claims is driven by a Poisson process. However, it is important to emphasize that within the framework of the HT estimator, this result is immediate and does not require of the assumption of the geometric distributions.\n\n#### A \"Micro-level\" Chain-Ladder method\n\nFrom an actuarial standpoint, the IPW estimator for the ultimate can be perceived as an individual-level adaptation of the Chain-Ladder method. By expressing the estimator in Equation (2) as\n\n\\[\\hat{L}(\\tau)=\\sum_{j=1}^{N^{P}(\\tau)}f_{j}(\\tau)Y_{j "], "nougat": ["and therefore an unbiased estimator of the outstanding claims is the difference between the estimated total and the currently paid amount,\n\n\\[\\hat{L}^{O}(\\tau)=\\hat{L}(\\tau)-L^{P}(\\tau)=\\sum_{j=1}^{N^{P}(\\tau)}\\frac{Y_{j}}{ \\pi_{j}(\\tau)}-\\sum_{j=1}^{N^{P}(\\tau)}Y_{j}=\\sum_{j=1}^{N^{P}(\\tau)}\\frac{1- \\pi_{j}(\\tau)}{\\pi_{j}(\\tau)}Y_{j}. \\tag{3}\\]\n\nThe intuition behind the HT estimator lies in the fact that only a portion of all payments \\(Y_{j}\\) is reported, proportionally to \\(\\pi_{j}(\\tau)\\), and so each payment in the sample is \u201caugmented\u201d by a factor of \\(1/\\pi_{j}(\\tau)\\) to approximate the actual total amount. It is important to note that the HT estimator is non-parametric, meaning that it leads to an estimation of the reserve that does not require any assumptions on the underlying distribution of the number of claims (frequency) or the distribution of claim sizes (severity), which is a remarkable property. Additionally, we emphasize the fact that even though the estimator is based on the population level (i.e a macro-level scale), the inclusion probabilities are dependent on the individual attributes of policyholders, claims, and payments. Therefore the estimator incorporates granular information as part of the estimation.\n\nThe HT estimator is widely recognized as one of the most influential estimators in the statistics literature, having been extensively studied for over 70 years in the field of population sampling (e.g Arnab (2017)). Consequently, the HT estimator has a solid theoretical foundation and possesses numerous desirable properties that directly inherit to the claim-reserving problem, including consistency, unbiasedness, sufficiency, among others. More recently, it has also been applied in inverse probability weighting (IPW) methods for estimation in causal inference (Yao et al. (2021)), including applications in fairness in insurance. The terminology \u201cIPW estimator\u201d is more extended in and outside the statistics literature, and so We will mostly refer to the estimator of the reserve as the IPW estimator, and reserve the naming of HT estimator when referring to the general concept.\n\nA specific case of interest arises when we set \\(Y_{i}=1\\). In this scenario, all the sums above simplifies to a count of the number of payments, allowing us to obtain an unbiased estimator for the number of payments yet to make, as\n\n\\[\\hat{N}^{O}(\\tau):=\\hat{N}(\\tau)-N^{P}(\\tau)=\\sum_{j=1}^{N^{P}(\\tau)}\\frac{1}{ \\pi_{j}(\\tau)}-\\sum_{j=1}^{N^{P}(\\tau)}1=\\sum_{j=1}^{N^{P}(\\tau)}\\frac{1-\\pi_{ j}(\\tau)}{\\pi_{j}(\\tau)}. \\tag{4}\\]\n\nIt is worth noting that this particular expression coincides with the one utilized by Fung et al. (2022) for the specific case of the number of incurred but not reported (IBNR) claims. In their work, they derived this expression under the assumption that the number of unreported claims follows a geometric distribution and demonstrated its unbiasedness when the number of claims is driven by a Poisson process. However, it is important to emphasize that within the framework of the HT estimator, this result is immediate and does not require of the assumption of the geometric distributions.\n\n#### A \"Micro-level\u201d Chain-Ladder method\n\nFrom an actuarial standpoint, the IPW estimator for the ultimate can be perceived as an individuallevel adaptation of the Chain-Ladder method. By expressing the estimator in Equation (2) as\n\n\\[\\hat{L}(\\tau)=\\sum_{j=1}^{N^{P}(\\tau)}f_{j}(\\tau)Y_{j},\\ "]}, {"edit": ["1. _Externalities from production are negative,_ \\(\\frac{\\partial v(y^{-i})}{\\partial y^{-i}}<0\\)__\n2. _For each_ \\(e^{\\prime},e\\in\\mathcal{E}\\)_, action_ \\(e^{\\prime}\\) _is more informative than_ \\(e\\) _if and only if_ \\(e^{\\prime}<e\\)_._\n\nWe omit the formal proofs for the claim, as such proofs would follow very closely the ones reported in the appendix. Instead, we provide the intuition for the one-technology case (the multiple-technology case follows the same intuition). Let the alternative assumptions of claim 1 hold, and consider a disagreeing team. Note that the player with view \\(\\mathcal{H}\\) will be hurt from a change in mind of the \\(\\mathcal{L}\\)-view player, due to the negative production externalities of point (i). This force pushes the optimistic player to reduce information arrival in the first period, in order to decrease the (subjective) probability of a co-worker's change in mind. However, when the relation between effort and informativeness is inverse - as of point (ii) - the way to reduce information arrival is by working harder. Hence, even under these alternative assumptions, disagreement pushes optimists to work harder and, if negative externalities are strong enough, a disagreeing team will produce on average more than any like-minded team.35 Regardless of this equivalence, we believe that our original specification - with positive externalities and more information arriving the more a team works on a project - is particularly realistic and captures well the zest of most of our motivating examples.\n\nFootnote 35: A few details of the propositions will indeed need to be intuitively modified for the result to hold under the assumptions of claim 1. In particular, the second part of proposition 3 (ii) holds for \\(\\Delta\\)_negative_ enough; and the requirement for inequalities to be strict becomes that \\(\\varphi_{m^{i}}^{m^{-i}}(\\mathbf{e},\\mathbf{k}|m^{i})\\) is strictly _decreasing_ in effort.\n\nWe conclude the discussion with a word on the welfare implications of disagreement. A welfare analysis would be particularly complex in our setup, which does not impose strong assumptions on the true process \\(Q\\). Without such assumptions, it is hard to tell, for instance, whether the effort levels that maximize team members' joint expected payoffs (or are Pareto efficient in terms of expected utility) are above or below \\(e^{\\mathcal{H}}\\) and \\(e^{\\mathcal{L}}\\). What we can say with certainty is that if the assumptions of proposition 5 part (i) hold and if, additionally, \\(Q_{x}=Q_{y}=\\mathcal{H}\\), any Pareto efficient stage effort must be above \\(e^{\\mathcal{H}}\\). Hence the boost in both players' effort generated by horizontal disagreement can - as in the example presented in section 1.1 - lead to a Pareto improvement. Not much can be concluded, however, in general.\n\nThis final observation leaves us with a word of caution: we have shown that disagreement can increase effort. We can expect this to boost innovation and output. Increasing innovation "], "nougat": ["1. _Externalities from production are negative,_ \\(\\frac{\\partial v(y^{-i})}{\\partial y^{-i}}<0\\)__\n2. _For each_ \\(e^{\\prime},e\\in\\mathcal{E}\\)_, action_ \\(e^{\\prime}\\) _is more informative than_ \\(e\\) _if and only if_ \\(e^{\\prime}<e\\)_._\n\nWe omit the formal proofs for the claim, as such proofs would follow very closely the ones reported in the appendix. Instead, we provide the intuition for the one-technology case (the multiple-technology case follows the same intuition). Let the alternative assumptions of claim 1 hold, and consider a disagreeing team. Note that the player with view \\(\\mathcal{H}\\) will be hurt from a change in mind of the \\(\\mathcal{L}\\)-view player, due to the negative production externalities of point (i). This force pushes the optimistic player to reduce information arrival in the first period, in order to decrease the (subjective) probability of a co-worker's change in mind. However, when the relation between effort and informativeness is inverse - as of point (ii) For each (ii) - the way to reduce information arrival is by working harder. Hence, even under these alternative assumptions, disagreement pushes optimists to work harder and, if negative externalities are strong enough, a disagreeing team. Note that the player with view \\(\\mathcal{H}\\) will be hurt from a change in mind of the \\(\\mathcal{L}\\)-view player, due to the negative production externalities of point (i). This force pushes the optimistic player to reduce information arrival in the first period, in order to decrease the (subjective) probability of a co-worker\u2019s change in mind. However, when the relation between effort and informativeness is inverse \u2013 as of point (ii) \u2013 the way to reduce information arrival is by working harder. Hence, even under these alternative assumptions, disagreement pushes optimists to work harder and, if negative externalities are strong enough, a disagreeing team will produce on average more than any like-minded team. 35 A few details of the propositions will indeed need to be intuitively modified for the result to hold under the assumptions of claim 1. In particular, the second part of proposition 3 (ii) holds for \\(\\Delta\\)_negative_ enough; and the requirement for inequalities to be strict becomes that \\(\\varphi_{m^{i}}^{m^{-i}}(\\mathbf{e},\\mathbf{k}|m^{i})\\) is strictly _decreasing_ in effort. Regardless of this equivalence, we believe that our original specification \u2013 with positive externalities and more information arriving the more a team works on a project \u2013 is particularly realistic and captures well the zest of most of our motivating examples.\n\nFootnote 35: A few details of the propositions will indeed need to be intuitively modified for the result to hold under the assumptions of claim 1. In particular, the second part of proposition 3 (ii) holds for \\(\\Delta\\)_negative_ enough; and the requirement for inequalities to be strict becomes that \\(\\varphi_{m^{i}}^{m^{-i}}(\\mathbf{e},\\mathbf{k}|m^{i})\\) is strictly _decreasing_ in effort.\n\nWe conclude the discussion with a word on the welfare implications of disagreement. A welfare analysis would be particularly complex in our setup, which does not impose strong assumptions on the true process \\(Q\\). Without such assumptions, it is hard to tell, for instance, whether the effort levels that maximize team members\u2019 joint expected payoffs (or are Pareto efficient in terms of expected utility) are above or below \\(e^{\\mathcal{H}}\\) and \\(e^{\\mathcal{L}}\\). What we can say with certainty is that if the assumptions of proposition 5 part (i) hold and if, additionally, \\(Q_{x}=Q_{y}=\\mathcal{H}\\), any Pareto efficient stage effort must be above \\(e^{\\mathcal{H}}\\). Hence the "]}, {"edit": ["\n\n### Discussion of Model Assumptions\n\nPayoffsTwo characteristics of the payoff function must be noted. First, the utility is additively separable in each players' output, ruling out static strategic complementarities between players' actions, a modelling choice that allows us to isolate disagreement as a driving force of our results.27 Second, we allow players utility to depend on each other's output. We interpret this as a reduced-form way of capturing how the overall team performance in a project can feedback positively on each team member. In particular, the positive externalities imply that players benefit from increasing each others' production and breakthroughs. In section 6, we discuss how our results are impacted under different assumptions regarding the sign of production externalities.\n\nAction InformativenessIn both example 1 and example 2, the technology views have the property that higher effort is more informative than lower effort, as we show in the appendix. Intuitively, in the bandit problem higher effort increases \\(F(e)\\) the probability of a return, making players subjectively more likely to parse between \\(\\mathcal{H}\\) and \\(\\mathcal{L}\\) at the end of the period. In the log-concave noise example, an increase in effort makes the two alternative views' predictions diverge - so that high output realization becomes stronger evidence in favor of \\(\\mathcal{H}\\) while low output realizations become stronger evidence in favor of \\(\\mathcal{L}\\). While 3 is in practice satisfied by many applications (e.g., Keller et al., 2005; Heidhues et al., 2018; Dong, 2018; Ba, 2022), in section 6 we briefly discuss the implications of other relations between the productivity and informativeness of actions, providing an example of a technology that violates assumption 3.\n\nDifferent Models and Model ShiftsThree non-standard assumptions about model and model shifts are worth discussing. First, in our model players can agree to disagree on the true model of the world. When they hold different models, Ann sees Bob's model as misspecified - for instance because based on the wrong hypotheses. Hence, she does not regard it as informative of the true process \\(Q\\). Second, for \\(\\alpha\\) low, the model switch rule LR means that"], "nougat": ["\n\n### Discussion of Model Assumptions\n\nPayoffsTwo characteristics of the payoff function must be noted. First, the utility is additively separable in each players\u2019 output, ruling out static strategic complementarities between players\u2019 actions, a modelling choice that allows us to isolate disagreement as a driving force of our results. 27 Second, we allow players utility to depend on each other's output. We interpret this as a reduced-form way of capturing how the overall team performance in a project can feedback positively on each team member. In particular, the positive externalities imply that players benefit from increasing each others' production and breakthroughs. In section 6, we discuss how our results are impacted under different assumptions regarding the sign of production externalities.\n\nAction InformativenessIn both example 1 and example 2, the technology views have the property that higher effort is more informative than lower effort, as we show in the appendix. Intuitively, in the bandit problem higher effort increases \\(F(e)\\) the probability of a return, making players subjectively more likely to parse between \\(\\mathcal{H}\\) and \\(\\mathcal{L}\\) at the end of the period. In the log-concave noise example, an increase in effort makes the two alternative views' predictions diverge - so that high output realization becomes stronger evidence in favor of \\(\\mathcal{H}\\) while low output realizations become stronger evidence in favor of \\(\\mathcal{L}\\). While 3 is in practice satisfied by many applications (e.g., Keller et al., 2005; Heidhues et al., 2018; Dong, 2018; Ba, 2022), in section 6 we briefly discuss the implications of other relations between the productivity and informativeness of actions, providing an example of a technology that violates assumption 3.\n\nDifferent Models and Model ShiftsThree non-standard assumptions about model and model shifts are worth discussing. First, in our model players can agree to disagree on the true model of the world. When they hold different models, Ann sees Bob's model as misspecified - for instance because based on the wrong hypotheses. Hence, she does not regard it as informative of the true process \\(Q\\). Second, for \\(\\alpha\\) low, the model switch rule LR means that"]}, {"edit": ["better estimation of the total liability and, consequently, a more accurate estimation of the IBNR reserve.\n\nIn this case, within the two-stage sampling mechanism described in Section 2, we can assume that only the first stage is performed. This means that either a claim is reported or not, and the ultimate value of the claim as based on the case estimates, which we will denote as \\(\\hat{Y}_{i}\\), is observed. Alternatively, this can be viewed as if there was only one payment per claim, where that payment represents the full amount of the claim.\n\nFollowing this analogy, with this \"single payment\" per claim, we can estimate the total outstanding claims and the IBNR reserve using the IPW framework as follows:\n\n\\[\\hat{L}(\\tau)=\\sum_{j=1}^{N^{P}(\\tau)}\\frac{\\hat{Y}_{j}}{\\pi_{j}^{U}(\\tau)}\\]\n\n\\[\\hat{L}^{IBNR}(\\tau)=\\sum_{j=1}^{N^{P}(\\tau)}\\frac{1-\\pi_{j}^{U}(\\tau)}{\\pi_{j }^{U}(\\tau)}\\hat{Y}_{j}\\]\n\nwhere in this case \\(N^{P}(\\tau)\\) denotes the number of these \"single payments\" made by valuation time \\(\\tau\\), which coincides with just the number of reported claims.\n\nThe resulting estimator for the IBNR provides an intuitive interpretation of the estimator of the previous section. Indeed, we can informally interpret the quantity \\(Y_{j}/\\pi_{j}^{V}(\\tau)\\) in the equation above as an HT estimator of the ultimate amount of the claim \\(\\hat{Y}_{j}\\), so leading to the estimator in the previous equation.\n\n\\[\\hat{L}(\\tau)=\\sum_{j=1}^{M^{I}(\\tau)}\\frac{\\hat{Y}_{j}}{\\pi_{j}^{U}(\\tau)}\\]\n\n\\[\\hat{L}^{IBNR}(\\tau)=\\hat{L}(\\tau)-L^{I}(\\tau)=\\sum_{j=1}^{M^{I}(\\tau)}\\frac{1 -\\pi_{j}^{U}(\\tau)}{\\pi_{j}^{U}(\\tau)}\\hat{Y}_{j}\\]\n\n## 4 Estimation of the model\n\nIn order to implement the IPW estimator, the key input required is the unknown inclusion probabilities \\(\\pi_{i}(\\tau)\\). These probabilities are associated with the evolution of a claim, including reporting and settlement delays and depend on various attributes of the payment, the claim, and the policyholder, denoted as \\(X_{i}\\), as well as the claim amount \\(Y_{i}\\) itself. In this section, we outline a data-driven approach to estimating these values. As explained in Section 2, the inclusion probabilities consist of two separate components: the probability of reporting and the probability of settlement. Each one is estimated separately, so we discuss different strategies in Sections 4.1 and 4.2.\n\n### Estimation of the reporting delay times probabilities \\(P(U_{j}\\tau-T_{j})\\)\n\nTo estimate probabilities, it is common to assume that the reporting delay times, conditioned on claim attributes \\(X_{i}\\), follow a common distribution function (see, for example, Verrall and Wuthrich "], "nougat": ["better estimation of the total liability and, consequently, a more accurate estimation of the IBNR reserve.\n\nIn this case, within the two-stage sampling mechanism described in Section 2, we can assume that only the first stage is performed. This means that either a claim is reported or not, and the ultimate value of the claim as based on the case estimates, which we will denote as \\(\\hat{Y}_{i}\\), is observed. Alternatively, this can be viewed as if there was only one payment per claim, where that payment represents the full amount of the claim.\n\nFollowing this analogy, with this \u201dsingle payment\u201d per claim, we can estimate the total outstanding claims and the IBNR reserve using the IPW framework as follows:\n\n\\[\\hat{L}(\\tau)=\\sum_{j=1}^{N^{P}(\\tau)}\\frac{\\hat{Y}_{j}}{\\pi_{j}^{U}(\\tau)}\\]\n\n\\[\\hat{L}^{IBNR}(\\tau)=\\sum_{j=1}^{N^{P}(\\tau)}\\frac{1-\\pi_{j}^{U}(\\tau)}{\\pi_{j }^{U}(\\tau)}\\hat{Y}_{j}\\]\n\nwhere in this case \\(N^{P}(\\tau)\\) denotes the number of these \u201dsingle payments\u201d made by valuation time \\(\\tau\\), which coincides with just the number of reported claims.\n\nThe resulting estimator for the IBNR provides an intuitive interpretation of the estimator of the previous section. Indeed, we can informally interpret the quantity \\(Y_{j}/\\pi_{j}^{V}(\\tau)\\) in the equation above as an HT estimator of the ultimate amount of the claim \\(\\hat{Y}_{j}\\), so leading to the estimator in the previous equation.\n\n\\[\\hat{L}(\\tau)=\\sum_{j=1}^{M^{I}(\\tau)}\\frac{\\hat{Y}_{j}}{\\pi_{j}^{U}(\\tau)}\\]\n\n\\[\\hat{L}^{IBNR}(\\tau)=\\hat{L}(\\tau)-L^{I}(\\tau)=\\sum_{j=1}^{M^{I}(\\tau)}\\frac{1 -\\pi_{j}^{U}(\\tau)}{\\pi_{j}^{U}(\\tau)}\\hat{Y}_{j}\\]\n\n## 4 Estimation of the model\n\nIn order to implement the IPW estimator, the key input required is the unknown inclusion probabilities \\(\\pi_{i}(\\tau)\\). These probabilities are associated with the evolution of a claim, including reporting and settlement delays and depend on various attributes of the payment, the claim, and the policyholder, denoted as \\(X_{i}\\), as well as the claim amount \\(Y_{i}\\) itself. In this section, we outline a data-driven approach to estimating these values. As explained in Section 2 , the inclusion probabilities consist of two separate components: the probability of reporting and the probability of settlement. Each one is estimated separately, so we discuss different strategies in Sections 4.1 and 4.2 .\n\n### Estimation of the reporting delay times probabilities \\(P(U_{j}\\tau-T_{j})\\)\n\nTo estimate probabilities, it is common to assume that the reporting delay times, conditioned on claim attributes \\(X_{i}\\), follow a common distribution function (see, for example, Verrall and W \u0308uthrich "]}, {"edit": ["and those with a one standard deviation increase in advertising expenditures, holding all other variables at their mean levels. We also compute these results for different levels of lockdowns and for each period.\n\nWe find that an average increase in government lockdowns is associated with a 7.8 percentage point increase in the probability of closure across our full sample. Figure 1 shows the estimated differences in the probability of closure for different baseline levels of government lockdowns. We find that the impact of these lockdowns was strongest at lower baseline levels and diminishes as these levels increase. This suggests that businesses had the most difficulty surviving the initial shock of the lockdowns, while subsequent increases in the severity of these lockdowns had less of an impact. Figure 2 shows the estimated difference in the probability of closure for each period of our study. We find that the impact of lockdowns was greater in March and May or later, with a slight decrease in April. This result broadly corresponds to our findings in Figure 1, in which lockdowns were at or close to zero in most countries before the pandemic was declared in mid-March, increased substantially in April, and decreased in May before leveling off afterward.\n\nWe also find that a one standard deviation increase in advertising is associated with a 2.8 percentage point decrease on average in the probability of closure across our full sample. Figure 3 shows the estimated differences in the probability of closure for different levels of lockdowns. We find small changes in this estimate across different level of lockdowns. Although the estimated differences in the probability of closure increase slightly as lockdowns become more severe, they stay close to its mean of 2.8. Figure 4 shows the estimated difference in the probability of closure for each period of our study. Again, we find small differences in this estimate across periods, though still larger in March and May or later, which corresponds to the pattern observed in Figure 2. Overall, we find evidence that both government lockdowns and advertising expenditures had an impact on business closures during the pandemic. The results thus far, however, are based on our full sample. In the following subsections, we investigate potential heterogeneous responses that may exist between key subpopulations, including analyses by country, industry, and business size.\n\n### Results by Country and Region\n\nWe use the results in the third column of Table 2 to calculate the estimated effect of government lockdowns across countries. Similar to our previous analysis, we compute the difference in prob "], "nougat": ["and those with a one standard deviation increase in advertising expenditures, holding all other variables at their mean levels. We also compute these results for different levels of lockdowns and for each period.\n\nWe find that an average increase in government lockdowns is associated with a 7.8 percentage point increase in the probability of closure across our full sample. Figure 1 shows the estimated differences in the probability of closure for different baseline levels of government lockdowns. We find that the impact of these lockdowns was strongest at lower baseline levels and diminishes as these levels increase. This suggests that businesses had the most difficulty surviving the initial shock of the lockdowns, while subsequent increases in the severity of these lockdowns had less of an impact. Figure 2 shows the estimated difference in the probability of closure for each period of our study. We find that the impact of lockdowns was greater in March and May or later, with a slight decrease in April. This result broadly corresponds to our findings in Figure 1, in which lockdowns were at or close to zero in most countries before the pandemic was declared in mid-March, increased substantially in April, and decreased in May before leveling off afterward.\n\nWe also find that a one standard deviation increase in advertising is associated with a 2.8 percentage point decrease on average in the probability of closure across our full sample. Figure 3 shows the estimated differences in the probability of closure for different levels of lockdowns. We find small changes in this estimate across different level of lockdowns. Although the estimated differences in the probability of closure increase slightly as lockdowns become more severe, they stay close to its mean of 2.8. Figure 4 shows the estimated difference in the probability of closure for each period of our study. Again, we find small differences in this estimate across periods, though still larger in March and May or later, which corresponds to the pattern observed in Figure 2. Overall, we find evidence that both government lockdowns and advertising expenditures had an impact on business closures during the pandemic. The results thus far, however, are based on our full sample. In the following subsections, we investigate potential heterogeneous responses that may exist between key subpopulations, including analyses by country, industry, and business size.\n\n### Results by Country and Region\n\nWe use the results in the third column of Table 2 to calculate the estimated effect of government lockdowns across countries. Similar to our previous analysis, we compute the difference in prob- "]}, {"edit": ["used as contingencies for payments. Despite the importance of this question and its relevance for analysis of contracting decisions, this part of the contracting procedure is less explored.\n\nOur paper has two broad implications. First, our methodology of thinking about likelihood ratios can be applied to other settings in which considerations of communication off the equilibrium path are important. In our setup, unlike other models of communication and information design, the design of information structure for the equilibrium level of effort affects off-path communication and the ability of the principal and the agent to capture surplus. This can arise in other settings with strategic information transmission and our method can be useful for that.\n\nSecond, our paper ties the choice of indicators in contracting to the bargaining power of the parties. In the textbook moral hazard problem, principal makes a take-it-or-leave it offer. As a result, a version of the informativeness principle often holds; the principal wishes to use all the information available, if possible, as contingency for payments. In contrast, in our model, the agent has different incentives for choice of indicators. There are some casual observations that are in line with this explanation. For example, contracts in NFL are often extremely detailed and payments to football players are highly contingent on various measures of individual and team outcomes. In contrast, contracts found in the English Premier League are not as detailed. They are often contingent on very coarse personal outcomes such as the number of goals scored reaching a particular threshold. In light of our theory, the level of competition in English/European soccer (in the form of increased player bargaining power) compared to a lack thereof in NFL could be behind this observation. Future work can hopefully shed light on the importance of this channel in the data.\n\n "], "nougat": ["used as contingencies for payments. Despite the importance of this question and its relevance for analysis of contracting decisions, this part of the contracting procedure is less explored.\n\nOur paper has two broad implications. First, our methodology of thinking about likelihood ratios can be applied to other settings in which considerations of communication off the equilibrium path are important. In our setup, unlike other models of communication and information design, the design of information structure for the equilibrium level of effort affects off-path communication and the ability of the principal and the agent to capture surplus. This can arise in other settings with strategic information transmission and our method can be useful for that.\n\nSecond, our paper ties the choice of indicators in contracting to the bargaining power of the parties. In the textbook moral hazard problem, principal makes a take-it-or-leave it offer. As a result, a version of the informativeness principle often holds; the principal wishes to use all the information available, if possible, as contingency for payments. In contrast, in our model, the agent has different incentives for choice of indicators. There are some casual observations that are in line with this explanation. For example, contracts in NFL are often extremely detailed and payments to football players are highly contingent on various measures of individual and team outcomes. In contrast, contracts found in the English Premier League are not as detailed. They are often contingent on very coarse personal outcomes such as the number of goals scored reaching a particular threshold. In light of our theory, the level of competition in English/European soccer (in the form of increased player bargaining power) compared to a lack thereof in NFL could be behind this observation. Future work can hopefully shed light on the importance of this channel in the data.\n\n "]}, {"edit": ["We use a penalty function that accounts for differences in the variance of the regressors which has not been studied in the literature before for least squares estimators, to our knowledge, and which is borrowed from the penalized quantile regression literature, see Belloni and Chernozhukov (2011). Although accounting for the variance is an appealing property on its own, we also prefer this penalty function as it means the penalization function of the coefficients in the VaR and ES estimation steps is equivalent. Indeed, we rely on the quantile regression estimator of Belloni et al. (2023) (from hereon BCMPW) to obtain the VaR predictions in the first step, although our results apply to any quantile estimator that satisfies the conditions.\n\nAs financial data is often dependent over time and heavy-tailed, we derive our results under conditions that allow for these properties. Specifically, we allow the data to be \\(\\beta\\)-mixing sequences with finite moments of a certain order. To obtain our results, we develop a Fuk-Nagaev inequality for \\(\\beta\\)-mixing sequences, which may be of independent interest. Using the Fuk-Nagaev inequality we obtain rates on the parameters and penalty parameter that are directly comparable to those for the penalized quantile regression estimator in BCMPW.\n\nWe apply our method in a systemic risk analysis. In this analysis we generate predictions of the Conditional Expected Shortfall (CoES) to measure the risk spillover from the financial sector to the entire stock market. The CoES measure was introduced in Adrian and Brunnermeier (2016) as an extension of CoVaR, an alternative systemic risk measure which is commonly used but is less prudent than CoES from a similar argument to the one that favors ES over VaR. The estimation of CoES relies on three stages of quantile regression and one stage of ES regression. Like Adrian and Brunnermeier (2016) we condition on seven lagged fundamental variables, but we extend their set of fundamental variables by also considering nonlinear transformations. Specifically, we use each the Chebyshev polynomials to transform each of the fundamental variables, where the degree of polynomials used may increase with the sample size. Our results show that the penalized VaR and ES estimators outperform the unpenalized benchmark estimator that strictly uses the untransformed set of fundamental variables by a considerable margin in terms of out-of-sample prediction error. We observe that a moderate degree of Chebyshev polynomials is optimal in our sample and generates CoES measurements that are more conservative than the benchmark and more responsive to news. Finally, we observe that the inclusion of nonlinear transformations of the fundamental variables results in a kind of leverage effect: risk predictions are less impacted by positive returns.\n\nIn related literature, there are several papers that study the properties of LASSO estimators "], "nougat": ["We use a penalty function that accounts for differences in the variance of the regressors which has not been studied in the literature before for least squares estimators, to our knowledge, and which is borrowed from the penalized quantile regression literature, see Belloni and Chernozhukov (2011). Although accounting for the variance is an appealing property on its own, we also prefer this penalty function as it means the penalization function of the coefficients in the VaR and ES estimation steps is equivalent. Indeed, we rely on the quantile regression estimator of Belloni et al. (2023) (from hereon BCMPW) to obtain the VaR predictions in the first step, although our results apply to any quantile estimator that satisfies the conditions.\n\nAs financial data is often dependent over time and heavy-tailed, we derive our results under conditions that allow for these properties. Specifically, we allow the data to be \\(\\beta\\)-mixing sequences with finite moments of a certain order. To obtain our results, we develop a Fuk-Nagaev inequality for \\(\\beta\\)-mixing sequences, which may be of independent interest. Using the Fuk-Nagaev inequality we obtain rates on the parameters and penalty parameter that are directly comparable to those for the penalized quantile regression estimator in BCMPW.\n\nWe apply our method in a systemic risk analysis. In this analysis we generate predictions of the Conditional Expected Shortfall (CoES) to measure the risk spillover from the financial sector to the entire stock market. The CoES measure was introduced in Adrian and Brunnermeier (2016) as an extension of CoVaR, an alternative systemic risk measure which is commonly used but is less prudent than CoES from a similar argument to the one that favors ES over VaR. The estimation of CoES relies on three stages of quantile regression and one stage of ES regression. Like Adrian and Brunnermeier (2016) we condition on seven lagged fundamental variables, but we extend their set of fundamental variables by also considering nonlinear transformations. Specifically, we use each the Chebyshev polynomials to transform each of the fundamental variables, where the degree of polynomials used may increase with the sample size. Our results show that the penalized VaR and ES estimators outperform the unpenalized benchmark estimator that strictly uses the untransformed set of fundamental variables by a considerable margin in terms of out-of-sample prediction error. We observe that a moderate degree of Chebyshev polynomials is optimal in our sample and generates CoES measurements that are more conservative than the benchmark and more responsive to news. Finally, we observe that the inclusion of nonlinear transformations of the fundamental variables results in a kind of leverage effect: risk predictions are less impacted by positive returns.\n\nIn related literature, there are several papers that study the properties of LASSO estimators "]}, {"edit": ["In our analysis, we consider the following order of events:\n\n1. Firms engage in a data-sharing mechanism \\(M=(M_{B},r)\\).\n2. Firm \\(A\\) announces uniform price \\(p_{A}\\).4 Footnote 4: Note that firm \\(B\\) knows all consumers\u2019 locations, and so personalizes prices to each. It therefore need not post a uniform price.\n3. A consumer arrives, and all firms who know the consumer's location \\(\\theta\\) simultaneously offer that consumer a personalized price, \\(p_{A}(\\theta)\\) and \\(p_{B}(\\theta)\\).\n4. The consumer chooses a firm from which to buy, and payoffs are realized.\n\nNote that firms share data, and firm \\(A\\) announces its uniform prices, before consumers arrive. After a consumer arrives to the market, the firms who know the consumer's specific location simultaneously offer personalized prices. If firm \\(A\\) offers a consumer a personalized price, this offer subsumes the firm's original uniform price. Thus, the uniform price \\(p_{A}\\) will apply only to those consumers who will not subsequently be offered a personalized price by firm \\(A\\).\n\nImportantly, when firms set personalized prices, they know the uniform price set by firm \\(A\\) in the previous stage. This is the standard timing considered in the literature (see, e.g., [34, 11, 10, 9, 27, 9]).5\n\nFootnote 5: An alternative model that we do not analyze is one in which firms set uniform and personalized prices simultaneously, for each consumer. [27] show that, in this case, a (pure) equilibrium may fail to exist.\n\nFor any fixed mechanism \\(M\\), we will consider the pure subgame perfect equilibria of the game that starts with data-sharing mechanism \\(M\\). Such equilibria always exists, and consist of a uniform price for firm \\(A\\) followed by personalized prices for both firms. Once the uniform price is fixed, the equilibrium personalized prices for each consumer \\(\\theta\\) are uniquely fixed. We will be interested in designing mechanisms \\(M\\) that lead to equilibria with high firm-profits and high consumer-welfare.\n\nOne important desideratum of data-sharing mechanisms (with corresponding equilibria) is that they be _individually rational (IR)_: That the expected utility of each firm with data sharing be at least as high as without data sharing. A data-sharing mechanism should be IR if we expect firms to participate.\n\nOur main focus will be on mechanisms that are not only IR, but also _Pareto-improving_: when sharing takes place, (i) the expected utility of each firm and _every_ consumer be at least as high as without data sharing, and that (ii) either firm \\(A\\)'s profits, firm \\(B\\)'s profits, or total consumer welfare be strictly higher.\n\nWe note that many of our results make no assumptions about the distribution of consumers. In such a general setting there may be multiple equilibria, even with no data-sharing, each with different uniform prices. Hence, we will often describe mechanisms as being IR or Pareto-improving _relative to_ a particular no-sharing equilibrium.\n\n## 3 No Data-Sharing\n\nWe begin by analyzing equilibria under no data-sharing. To this end, define \\(\\mu(p_{A})=\\frac{1}{2}-\\frac{p_{A}}{2t}\\). If firm \\(A\\) charges uniform price \\(p_{A}\\), then the consumer located at \\(\\mu(p_{A})\\) is indifferent between purchasing from firm \\(A\\) at that price and purchasing from firm \\(B\\) at price \\(0\\). All consumers located to the left of \\(\\mu(p_{A})\\) will thus strictly prefer purchasing from firm \\(A\\) at price \\(p_{A}\\) than from firm \\(B\\) at any nonnegative price. In contrast, for every consumer located to the right of \\(\\mu(p_{A})\\) there exists a nonnegative price of firm \\(B\\) such that "], "nougat": ["In our analysis, we consider the following order of events:\n\n1. Firms engage in a data-sharing mechanism \\(M=(M_{B},r)\\).\n2. Firm \\(A\\) announces uniform price \\(p_{A}\\).4 Footnote 4: Note that firm \\(B\\) knows all consumers\u2019 locations, and so personalizes prices to each. It therefore need not post a uniform 3. A consumer arrives, and all firms who know the consumer\u2019s location \\(\\theta\\) simultaneously offer that consumer a personalized price, \\(p_{A}(\\theta)\\) and \\(p_{B}(\\theta)\\).\n3. The consumer chooses a firm from which to buy, and payoffs are realized.\n\nNote that firms share data, and firm \\(A\\) announces its uniform prices, before consumers arrive. After a consumer arrives to the market, the firms who know the consumer\u2019s specific location simultaneously offer personalized prices. If firm \\(A\\) offers a consumer a personalized price, this offer subsumes the firm\u2019s original uniform price. Thus, the uniform price \\(p_{A}\\) will apply only to those consumers who will not subsequently be offered a personalized price by firm \\(A\\).\n\nImportantly, when firms set personalized prices, they know the uniform price set by firm \\(A\\) in the previous stage. This is the standard timing considered in the literature (see, e.g., [33, 10, 9, 26, 8]). 5\n\nFootnote 5: An alternative model that we do not analyze is one in which firms set uniform and personalized prices simultaneously, for each consumer. [26] show that, in this case, a (pure) equilibrium may fail to exist.\n\nFor any fixed mechanism \\(M\\), we will consider the pure subgame perfect equilibria of the game that starts with data-sharing mechanism \\(M\\). Such equilibria always exists, and consist of a uniform price for firm \\(A\\) followed by personalized prices for both firms. Once the uniform price is fixed, the equilibrium personalized prices for each consumer \\(\\theta\\) are uniquely fixed. We will be interested in designing mechanisms \\(M\\) that lead to equilibria with high firm-profits and high consumer-welfare.\n\nOne important desideratum of data-sharing mechanisms (with corresponding equilibria) is that they be _individually rational (IR)_: That the expected utility of each firm with data sharing be at least as high as without data sharing. A data-sharing mechanism should be IR if we expect firms to participate.\n\nOur main focus will be on mechanisms that are not only IR, but also _Pareto-improving_: when sharing takes place, (i) the expected utility of each firm and _every_ consumer be at least as high as without data sharing, and that (ii) either firm \\(A\\)'s profits, or total consumer welfare be strictly higher.\n\nWe note that many of our results make no assumptions about the distribution of consumers. In such a general setting there may be multiple equilibria, even with no data-sharing, each with different uniform prices. Hence, we will often describe mechanisms as being IR or Pareto-improving _relative to_ a particular no-sharing equilibrium.\n\n## 3 No Data-Sharing\n\nWe begin by analyzing equilibria under no data-sharing. To this end, define \\(\\mu(p_{A})=\\frac{1}{2}-\\frac{p_{A}}{2t}\\). If firm \\(A\\) charges uniform price \\(p_{A}\\), then the consumer located at \\(\\mu(p_{A})\\) is indifferent between purchasing from firm \\(A\\) at that price and purchasing from firm \\(B\\) at price 0. All consumers located to the left of \\(\\mu(p_{A})\\) will thus strictly prefer purchasing from firm \\(A\\) at price \\(p_{A}\\) than from firm \\(B\\) at any nonnegative price. In contrast, for every consumer located to the right of \\(\\mu(p_{A})\\) there exists a nonnegative price of firm \\(B\\) such that that consumer will prefer to purchase from \\(B\\) than from \\(A\\ "]}, {"edit": ["higher-order beliefs. The construction--which is based on the set-up in Heifetz (1993)--shows that this set of hierarchies identifies in a natural way a type structure, which we call it \"canonical.\" Next, we show how each profile of types in a type structure can be associated with an element of the constructed set of hierarchies. This part is standard (cf. Heifetz and Samet 1998).\n\n#### 4.2.1 From hierarchies to types\n\nTo construct the set of hierarchies of conditional beliefs, we define recursively, for each player, two sequences of sets as well as a sequence of conditioning events. The first sequence, \\(\\left(\\Theta_{i}^{n}\\right)_{n\\geq 0}\\), represents player \\(i\\)'s \\((n+1)\\)-order domain of uncertainty, for each \\(n\\geq 0\\). The second sequence, \\(\\left(H_{i}^{n}\\right)_{n\\geq 1}\\), consists of player \\(i\\)'s \\(n\\)-tuples of _coherent_ conditional beliefs over these space. The notion of coherence, formally defined below, says that, conditional on any relevant hypothesis, beliefs at different order do not contradict one another.\n\nFormally, for each player \\(i\\in I\\), let\n\n\\[\\Theta_{i}^{0} : =S,\\] \\[\\mathcal{B}_{i}^{0} : =\\mathcal{B}_{i},\\] \\[H_{i}^{1} : =\\Delta^{\\mathcal{B}_{i}^{0}}\\left(\\Theta_{i}^{0}\\right).\\]\n\nThe set \\(\\Theta_{i}^{0}\\) is player \\(i\\)'s 1-order (primitive) domain of uncertainty, and a first-order belief, viz. \\(\\mu_{i}^{1}\\), is an element of the set \\(H_{i}^{1}\\).\n\nFor \\(n\\geq 1\\), assume that \\(\\left(\\Theta_{i}^{m}\\right)_{m=0,\\ldots,n-1}\\), \\(\\left(\\mathcal{B}_{i}^{m}\\right)_{m=0,\\ldots,n-1}\\) and \\(\\left(H_{i}^{m}\\right)_{m=1,\\ldots,n}\\) have been defined for each player \\(i\\in I\\). Then, for each \\(i\\in I\\), let\n\n\\[\\Theta_{i}^{n}:=\\Theta_{i}^{0}\\times H_{j}^{n}.\\]\n\nThat is, \\(\\Theta_{i}^{n}\\) is player \\(i\\)'s \\((n+1)\\)-order domain of uncertainty: it consists of the space of primitive uncertainty and what player \\(j\\neq i\\) believes about the space of primitive uncertainty, what player \\(j\\) believes about what player \\(i\\) believes about the space of primitive uncertainty,..., and so on, up to level \\(n\\). For each \\(i\\in I\\) and \\(n\\geq 1\\), let \\(\\pi_{i}^{n,n+1}:H_{i}^{n+1}\\to H_{i}^{n}\\) and \\(\\rho_{i}^{n-1,n}:\\Theta_{i}^{n}\\to\\Theta_{i}^{n-1}\\) denote the coordinate projections. By construction, these maps satisfy the following property:\n\n\\[\\forall i\\in I,\\forall n\\geq 2,\\rho_{i}^{n-1,n}=\\left(\\mathrm{Id}_{\\Theta_{i}^{0} },\\pi_{j}^{n-1 "], "nougat": ["higher-order beliefs. The construction\u2014which is based on the set-up in Heifetz (1993)\u2014shows that this set of hierarchies identifies in a natural way a type structure, which we call it \u201ccanonical.\u201d Next, we show how each profile of types in a type structure can be associated with an element of the constructed set of hierarchies. This part is standard (cf. Heifetz and Samet 1998).\n\n#### 4.2.1 From hierarchies to types\n\nTo construct the set of hierarchies of conditional beliefs, we define recursively, for each player, two sequences of sets as well as a sequence of conditioning events. The first sequence, \\(\\left(\\Theta_{i}^{n}\\right)_{n\\geq 0}\\), represents player \\(i\\)'s \\((n+1)\\)-order domain of uncertainty, for each \\(n\\geq 0\\). The second sequence, \\(\\left(H_{i}^{n}\\right)_{n\\geq 1}\\), consists of player \\(i\\)'s \\(n\\)-tuples of _coherent_ conditional beliefs over these space. The notion of coherence, formally defined below, says that, conditional on any relevant hypothesis, beliefs at different order do not contradict one another.\n\nFormally, for each player \\(i\\in I\\), let\n\n\\[\\Theta_{i}^{0} : =S,\\] \\[\\mathcal{B}_{i}^{0} : =\\mathcal{B}_{i},\\] \\[H_{i}^{1} : =\\Delta^{\\mathcal{B}_{i}^{0}}\\left(\\Theta_{i}^{0}\\right).\\]\n\nThe set \\(\\Theta_{i}^{0}\\) is player \\(i\\)'s 1-order (primitive) domain of uncertainty, and a first-order belief, viz. \\(\\mu_{i}^{1}\\), is an element of the set \\(H_{i}^{1}\\).\n\nFor \\(n\\geq 1\\), assume that \\(\\left(\\Theta_{i}^{m}\\right)_{m=0,\\ldots,n-1}\\), \\(\\left(\\mathcal{B}_{i}^{m}\\right)_{m=0,\\ldots,n-1}\\) and \\(\\left(H_{i}^{m}\\right)_{m=1,\\ldots,n}\\) have been defined for each player \\(i\\in I\\). Then, for each \\(i\\in I\\), let\n\n\\[\\Theta_{i}^{n}:=\\Theta_{i}^{0}\\times H_{j}^{n}.\\]\n\nThat is, \\(\\Theta_{i}^{n}\\) is player \\(i\\)'s \\((n+1)\\)-order domain of uncertainty: it consists of the space of primitive uncertainty and what player \\(j\\neq i\\) believes about the space of primitive uncertainty, what player \\(j\\) believes about what player \\(i\\) believes about the space of primitive uncertainty,..., and so on, up to level \\(n\\). For each \\(i\\in I\\) and \\(n\\geq 1\\), let \\(\\pi_{i}^{n,n+1}:H_{i}^{n+1}\\to H_{i}^{n}\\) and \\(\\rho_{i}^{n-1,n}:\\Theta_{i}^{n}\\to\\Theta_{i}^{n-1}\\) denote the coordinate projections. By construction, these maps satisfy the following property:\n\n\\[\\forall i\\in I,\\forall n\\geq 2,\\rho_{i}^{n-1,n}=\\left(\\mathrm{Id}_{\\Theta_{i}^{0} },\\pi_{j}^{n-1, "]}, {"edit": ["\\(s\\in S\\cup\\left\\{\\emptyset\\right\\}\\) to \\(s^{\\prime}\\in S\\cup\\left\\{\\emptyset\\right\\}\\) and \\(\\emptyset\\) represents her/his best outside option. If \\(sP_{i}\\emptyset\\), then school \\(s\\) is said to be **acceptable** for \\(i\\). Further, \\(sR_{i}s^{\\prime}\\) implies \\(sP_{i}s^{\\prime}\\) or \\(s=s^{\\prime}\\). Let \\(\\mathbb{P}\\) be all possible total orders on \\(S\\cup\\left\\{\\emptyset\\right\\}\\).\n\nEach school \\(s\\) has a capacity constraint represented by \\(q_{s}\\in\\mathbb{Z}_{++}\\) and moreover, \\(q=\\left(q_{s}\\right)_{s\\in S}.\\) Let \\(\\succ_{s}\\) be an asymmetric and irreflexive binary relation on \\(I\\) representing the priority for school \\(s\\), where \\(\\left(i,j\\right)\\in\\succ_{s}\\) means that \\(i\\) has a higher priority than \\(j\\) for school \\(s\\). Let \\(\\mathcal{B}\\) be the set of all possible asymmetric and irreflexive binary relations on \\(I\\). Moreover, let \\(\\mathcal{P}\\), \\(\\mathcal{W}\\) and \\(\\mathcal{T}\\) be the set of all possible partial orders, weak orders and total orders on \\(I\\), respectively. Hereafter, we call \\(\\succ_{s}\\in\\mathcal{B}\\) a **priority order**.4 Let \\(\\succ=\\left(\\succ_{s}\\right)_{s\\in S}\\in\\mathcal{B}^{\\left|S\\right|}\\) and \\(\\succ^{\\prime}=\\left(\\succ_{s}^{\\prime}\\right)_{s\\in S}\\in\\mathcal{B}^{\\left| S\\right|}\\) be typical profiles of priority orders on \\(I\\).\n\nFootnote 4: To put it precisely, \\(\\succ_{s}\\in\\mathcal{B}\\) may be improper to call a priority \u201corder\u201d, because it may not be transitive. However, with few exceptions, we assume that \\(\\succ_{s}\\) is transitive and thus we call it a priority order.\n\nFor \\(\\succ_{s}\\in\\mathcal{B}\\), let \\(\\succ_{s}^{*}\\) be an (total order) **extension** of \\(\\succ_{s}\\) if \\(\\succ_{s}^{*}\\in\\mathcal{T}\\) and \\(\\succ_{s}\\subseteq\\succ_{s}^{*}\\). The following result is provided by Kitahara and Okumura (2023, Proposition 1).\n\n**Remark 1**: _There is an extension of \\(\\succ_{s}\\in\\mathcal{B}\\) if and only if \\(\\succ_{s}\\) is P-acyclic._\n\nTherefore, for any \\(\\succ_{s}\\in\\mathcal{P}\\), there is an extension of \\(\\succ_{s}\\). Note that there may exist multiple extensions for \\(\\succ_{s}\\). Let \\(\\succ^{*}\\) be an **extension profile of \\(\\succ\\)** if \\(\\succ_{s}^{*}\\) is an extension of \\(\\succ_{s}\\) for all \\(s\\in S\\).\n\nWe let\n\n\\[G_{S}=\\left(I,S,P,\\succ,q\\right)\\]\n\nbe a **school choice problem with "], "nougat": ["\\(s\\in S\\cup\\left\\{\\emptyset\\right\\}\\) to \\(s^{\\prime}\\in S\\cup\\left\\{\\emptyset\\right\\}\\) and \\(\\emptyset\\) represents her/his best outside option. If \\(sP_{i}\\emptyset\\), then school \\(s\\) is said to be **acceptable** for \\(i\\). Further, \\(sR_{i}s^{\\prime}\\) implies \\(sP_{i}s^{\\prime}\\) or \\(s=s^{\\prime}\\). Let \\(\\mathbb{P}\\) be all possible total orders on \\(S\\cup\\left\\{\\emptyset\\right\\}\\).\n\nEach school \\(s\\) has a capacity constraint represented by \\(q_{s}\\in\\mathbb{Z}_{++}\\) and moreover, \\(q=\\left(q_{s}\\right)_{s\\in S}.\\) Let \\(\\succ_{s}\\) be an asymmetric and irreflexive binary relation on \\(I\\) representing the priority for school \\(s\\), where \\(\\left(i,j\\right)\\in\\succ_{s}\\) means that \\(i\\) has a higher priority than \\(j\\) for school \\(s\\). Let \\(\\mathcal{B}\\) be the set of all possible asymmetric and irreflexive binary relations on \\(I\\). Moreover, let \\(\\mathcal{P}\\), \\(\\mathcal{W}\\) and \\(\\mathcal{T}\\) be the set of all possible partial orders, weak orders and total orders on \\(I\\), respectively. Hereafter, we call \\(\\succ_{s}\\in\\mathcal{B}\\) a **priority order**.4 Let \\(\\succ=\\left(\\succ_{s}\\right)_{s\\in S}\\in\\mathcal{B}^{\\left|S\\right|}\\) and \\(\\succ^{\\prime}=\\left(\\succ_{s}^{\\prime}\\right)_{s\\in S}\\in\\mathcal{B}^{\\left| S\\right|}\\) be typical profiles of priority orders on \\(I\\).\n\nFootnote 4: To put it precisely, \\(\\succ_{s}\\in\\mathcal{B}\\) may be improper to call a priority \u201corder\u201d, because it may not be transitive. However, with few exceptions, we assume that \\(\\succ_{s}\\) is transitive and thus we call it a priority order.\n\nFor \\(\\succ_{s}\\in\\mathcal{B}\\), let \\(\\succ_{s}^{*}\\) be an (total order) **extension** of \\(\\succ_{s}\\) if \\(\\succ_{s}^{*}\\in\\mathcal{T}\\) and \\(\\succ_{s}\\subseteq\\succ_{s}^{*}\\). The following result is provided by Kitahara and Okumura (2023, Proposition 1).\n\n**Remark 1**: _There is an extension of \\(\\succ_{s}\\in\\mathcal{B}\\) if and only if \\(\\succ_{s}\\) is P-acyclic._\n\nTherefore, for any \\(\\succ_{s}\\in\\mathcal{P}\\), there is an extension of \\(\\succ_{s}\\). Note that there may exist multiple extensions for \\(\\succ_{s}\\). Let \\(\\succ^{*}\\) be an **extension profile of \\(\\succ\\)** if \\(\\succ_{s}^{*}\\) is an extension of \\(\\succ_{s}\\) for all \\(s\\in S\\).\n\nWe let\n\n\\[G_{S}=\\left(I,S,P,\\succ,q\\right)\\]\n\nbe a **school choice problem with "]}, {"edit": ["Consider a \\(d\\)-dimensional process, \\(x_{t}\\in\\mathcal{X}\\subseteq\\)\\(\\mathbb{R}^{d}\\) that solves the following stochastic differential equation (SDE):\n\n\\[dx_{t}=\\mu\\left(x_{t}\\right)dt+\\sigma\\left(x_{t}\\right)dW_{t}+J_{t}dN_{t}, \\tag{2.1}\\]\n\nwhere \\(\\mu\\left(x\\right)\\) and \\(\\sigma\\left(x\\right)\\) are the so-called drift and diffusion functions, respectively, \\(W_{t}\\) is a \\(d\\)-dimensional standard Brownian motion, \\(N_{t}\\) is a Poisson process with jump intensity \\(\\lambda\\left(x_{t}\\right)\\), and \\(J_{t}\\) captures the jump-sizes and has conditional density \\(\\nu\\left(\\cdot|x_{t}\\right)\\). The precise form of \\(\\mu\\left(x\\right)\\), \\(\\sigma\\left(x\\right)\\), \\(\\lambda\\left(x\\right)\\) and \\(\\nu\\left(\\cdot|x\\right)\\) are chosen by the researcher according to the dynamic problem that is being considered and so are known to us. To keep notation simple, we restrict ourselves to the time-homogenous case meaning that none of the functions entering the model depend on \\(t\\); the extension to the time-inhomogenous case can be found in Appendix B.\n\nWe are interested in computing conditional moments on the form\n\n\\[u_{t}\\left(x\\right)=E_{t}f\\left(x\\right) \\tag{2.2}\\]\n\nwhere\n\n\\[\\left(t,f\\right)\\mapsto E_{t}f\\left(x\\right)\\equiv\\mathbb{E}\\left[\\exp\\left(- \\int_{0}^{t}r\\left(x_{s}\\right)ds\\right)f\\left(x_{t}\\right)\\right|x_{0}=x\\right] \\tag{2.3}\\]\n\nis a conditional moment operator. This family of operators, indexed by the time variable \\(t\\geq 0\\), constitutes a so-called semi-group of linear operators; for an overview of the general theory of semi-groups with applications to Markov processes we refer to Ethier and Kurtz (1986); for applications of semi-group theory in econometrics and finance, see Ait-Sahalia et al. (2010).1\n\nFootnote 1: Note that we here opt for the so\u2013called Musiela parameterization where \\(t\\) measures the time distance between the current and some future calendar time point. One could alternatively have defined the function of interest as, for some given \\(T<\\infty\\),\n\n\\[\\tilde{u}_{\\tau}\\left(x\\right)=\\mathbb{E}\\left[\\exp\\left(-\\int_{\\tau}^{T}r \\left(x_{s}\\right)ds\\right|f\\left(x_{T}\\right)\\right|x_{\\tau}=x\\right],\\]\n\n where now \\(\\tau\\leq T\\) is a calendar time point. In the current time-homogenous case, it is easily seen that \\(\\tilde{u}_{\\tau}\\left(x\\right)=u_{T-\\tau}\\left(x\\right)\\), where \\(u_{t}\\) was defined in (2.2).\n\n "], "nougat": ["Consider a \\(d\\)-dimensional process, \\(x_{t}\\in\\mathcal{X}\\subseteq\\)\\(\\mathbb{R}^{d}\\) that solves the following stochastic differential equation (SDE):\n\n\\[dx_{t}=\\mu\\left(x_{t}\\right)dt+\\sigma\\left(x_{t}\\right)dW_{t}+J_{t}dN_{t}, \\tag{2.1}\\]\n\nwhere \\(\\mu\\left(x\\right)\\) and \\(\\sigma\\left(x\\right)\\) are the so-called drift and diffusion functions, respectively, \\(W_{t}\\) is a \\(d\\)-dimensional standard Brownian motion, \\(N_{t}\\) is a Poisson process with jump intensity \\(\\lambda\\left(x_{t}\\right)\\), and \\(J_{t}\\) captures the jump-sizes and has conditional density \\(\\nu\\left(\\cdot|x_{t}\\right)\\). The precise form of \\(\\mu\\left(x\\right)\\), \\(\\sigma\\left(x\\right)\\), \\(\\lambda\\left(x\\right)\\) and \\(\\nu\\left(\\cdot|x\\right)\\) are chosen by the researcher according to the dynamic problem that is being considered and so are known to us. To keep notation simple, we restrict ourselves to the time\u2013homogenous case meaning that none of the functions entering the model depend on \\(t\\); the extension to the time\u2013inhomogenous case can be found in Appendix B .\n\nWe are interested in computing conditional moments on the form\n\n\\[u_{t}\\left(x\\right)=E_{t}f\\left(x\\right) \\tag{2.2}\\]\n\nwhere\n\n\\[\\left(t,f\\right)\\mapsto E_{t}f\\left(x\\right)\\equiv\\mathbb{E}\\left[\\exp\\left(- \\int_{0}^{t}r\\left(x_{s}\\right)ds\\right)f\\left(x_{t}\\right)\\right|x_{0}=x\\right] \\tag{2.3}\\]\n\nis a conditional moment operator. This family of operators, indexed by the time variable \\(t\\geq 0\\), constitutes a so\u2013called semi\u2013group of linear operators; for an overview of the general theory of semi\u2013 groups with applications to Markov processes we refer to Ethier and Kurtz ( 1986 ); for applications of semi\u2013group theory in econometrics and finance, see A \u0308\u0131t-Sahalia et al. ( 2010 ). 1 1 Note that we here opt for the so\u2013called Musiela parameterization where \\(t\\) measures the time distance between the current and some future calendar time point. One could alternatively have defined the function of interest as, for some given \\(T<\\infty\\),\n\n\\[\\tilde{u}_{\\tau}\\left(x\\right)=\\mathbb{E}\\left[\\exp\\left(-\\int_{\\tau}^{T}r \\left(x_{s}\\right)ds\\right|f\\left(x_{T}\\right)\\right|x_{\\tau}=x\\right],\\]\n\n where now \\(\\tau\\leq T\\) is a calendar time point. In the current time\u2013homogenous case, it is easily seen that \\(\\tilde{u}_{\\tau}\\left(x\\right)=u_{T-\\tau}\\left(x\\right)\\), where \\(u_{t}\\) was defined in (2.2).\n\n "]}, {"edit": ["consequence, the two generators \\(A_{D}\\) and \\(A_{J}\\) commute, \\(A_{D}A_{J}=A_{J}A_{D}\\), in which case\n\n\\[u_{t}\\left(x\\right)=e^{\\left(A_{D}+A_{J}\\right)t}f\\left(x\\right)=e^{A_{D}t}B_{J, t}\\left(x\\right)=e^{A_{J}t}B_{D,t}\\left(x\\right), \\tag{3.6}\\]\n\nwhere\n\n\\[B_{J,t}\\left(x\\right)=\\mathbb{E}\\left[\\left.f\\left(x_{J,t}\\right)\\right|x_{J,0 }=x\\right],\\quad B_{D,t}\\left(x\\right)=\\mathbb{E}\\left[\\left.f\\left(x_{D,t} \\right)\\right|x_{D,0}=x\\right].\\]\n\nNow, consider first the case where (ii) is satisfied. In this scenario, \\(x_{J,t}|x_{J,0}=x\\) has density\n\n\\[p_{J,t}\\left(y|x\\right)=\\sum_{k=0}^{\\infty}e^{-\\lambda t}\\frac{\\left(\\lambda t \\right)^{k}}{k!}\\nu_{k}\\left(y-x\\right), \\tag{3.7}\\]\n\nwhere \\(\\nu_{k}\\left(y\\right)\\) is the density of the sum of \\(k\\) independent jumps, \\(\\sum_{i=1}^{k}J_{i}\\), \\(J_{i}\\nu\\left(\\cdot\\right)\\). Since \\(p_{J,t}\\left(y|x\\right)\\) is a smooth function then \\(x\\mapsto B_{J,t}\\left(\\Delta x_{D,T}+x\\right)\\) is also a smooth function even if \\(f\\left(x\\right)\\) is irregular. Thus, if \\(B_{J,t}\\left(x\\right)\\) is available on closed form then the smoothing device is not needed and we can approximate \\(u_{t}\\) by\n\n\\[\\hat{u}_{t}\\left(x\\right)=\\sum_{m=0}^{M}\\frac{t^{m}}{m!}A_{D}^{m}B_{J,t}\\left( x\\right). \\tag{3.8}\\]\n\nSimilar, if (i) is satisfied then \\(x_{D,t}|x_{D,0}\\) is a Brownian motion with drift and has Gaussian density as given in (3.4). Because of its simple dynamics, \\(B_{D,t}\\left(x\\right)\\) is available on closed form in many cases and will again be a smooth function; if so, we propose to approximate \\(u_{t}\\) by\n\n\\[\\hat{u}_{t}\\left(x\\right)=\\sum_{m=0}^{M}\\frac{t^{m}}{m!}A_{J}^{m}B_{D,t}\\left( x\\right).\\]\n\nIf closed form expressions of neither \\(B_{D,t}\\) nor \\(B_{J,t}\\) are available, it is still possible to simplify the computation using, for example,\n\n\\[\\hat{u}_{t}\\left(x\\right)=\\left[\\sum_{m=0}^{M}\\frac{t^{m}}{m!}e^{A_{J}t}\\left( A_{D}-\\partial_{t}\\right)^{ "], "nougat": ["consequence, the two generators \\(A_{D}\\) and \\(A_{J}\\) commute, \\(A_{D}A_{J}=A_{J}A_{D}\\), in which case\n\n\\[u_{t}\\left(x\\right)=e^{\\left(A_{D}+A_{J}\\right)t}f\\left(x\\right)=e^{A_{D}t}B_{J, t}\\left(x\\right)=e^{A_{J}t}B_{D,t}\\left(x\\right), \\tag{3.6}\\]\n\nwhere\n\n\\[B_{J,t}\\left(x\\right)=\\mathbb{E}\\left[\\left.f\\left(x_{J,t}\\right)\\right|x_{J,0 }=x\\right],\\quad B_{D,t}\\left(x\\right)=\\mathbb{E}\\left[\\left.f\\left(x_{D,t} \\right)\\right|x_{D,0}=x\\right].\\]\n\nNow, consider first the case where (ii) is satisfied. In this scenario, \\(x_{J,t}|x_{J,0}=x\\) has density\n\n\\[p_{J,t}\\left(y|x\\right)=\\sum_{k=0}^{\\infty}e^{-\\lambda t}\\frac{\\left(\\lambda t \\right)^{k}}{k!}\\nu_{k}\\left(y-x\\right), \\tag{3.7}\\]\n\nwhere \\(\\nu_{k}\\left(y\\right)\\) is the density of the sum of \\(k\\) independent jumps, \\(\\sum_{i=1}^{k}J_{i}\\), \\(J_{i}\\nu\\left(\\cdot\\right)\\). Since \\(p_{J,t}\\left(y|x\\right)\\) is a smooth function even if \\(f\\left(x\\right)\\) is irregular. Thus, if \\(B_{J,t}\\left(x\\right)\\) is available on closed form then the smoothing device is not needed and we can approximate \\(u_{t}\\) by\n\n\\[\\hat{u}_{t}\\left(x\\right)=\\sum_{m=0}^{M}\\frac{t^{m}}{m!}A_{D}^{m}B_{J,t}\\left( x\\right). \\tag{3.8}\\]\n\nSimilar, if (i) is satisfied then \\(x_{D,t}|x_{D,0}\\) is a Brownian motion with drift and has Gaussian density as given in ( 3.4 ). Because of its simple dynamics, \\(B_{D,t}\\left(x\\right)\\) is available on closed form in many cases and will again be a smooth function; if so, we propose to approximate \\(u_{t}\\) by\n\n\\[\\hat{u}_{t}\\left(x\\right)=\\sum_{m=0}^{M}\\frac{t^{m}}{m!}A_{J}^{m}B_{D,t}\\left( x\\right).\\]\n\nIf closed form expressions of neither \\(B_{D,t}\\) nor \\(B_{J,t}\\) are available, it is still possible to simplify the computation using, for example,\n\n\\[\\hat{u}_{t}\\left(x\\right)=\\left[\\sum_{m=0}^{M}\\frac{t^{m}}{m!}e^{A_{J}t}\\left( A_{D}-\\partial_{t}\\right)^{m}u_{0,t}\\left(x\\right)\\right], \\tag{3.9}\\]\n\nassuming that closed form "]}, {"edit": ["We will illustrate an application of Theorem 2 to Example 1 by showing that the optimal experiment choice function is strongly \\(c\\)-monotone and then discussing how to construct an outcome-equivalent results-based contract.\n\n**Example 1**.: (Continued.)\n\nWith two types, strong \\(c\\)-monotonicity reduces to a simple condition \\(S_{2}\\subseteq U_{1}\\). As shown in the Figure 3, the vertices of the thick-lined triangle represent posteriors in \\(S_{1}=\\{p_{1},p_{1}^{\\prime},p_{1}^{\\prime\\prime}\\}\\) for the less efficient type \\(\\theta_{1}\\); the vertices of the dashed-lined triangle are posteriors in \\(S_{2}=\\{p_{2},p_{2}^{\\prime},p_{2}^{\\prime\\prime}\\}\\) for the more efficient type \\(\\theta_{2}\\). The light grey area represents set \\(D_{1}\\); the dark grey area represents set \\(U_{1}\\). We find \\(S_{2}\\subseteq U_{1}\\).\n\nFigure 3: The optimal experiment choice function is strongly \\(c\\)-monotone\n\n"], "nougat": ["We will illustrate an application of Theorem 2 to Example 1 by showing that the optimal experiment choice function is strongly \\(c\\)-monotone and then discussing how to construct an outcome-equivalent results-based contract.\n\n**Example 1**.: (Continued.)\n\nWith two types, strong \\(c\\)-monotonicity reduces to a simple condition \\(S_{2}\\subseteq U_{1}\\). As shown in the Figure 3, the vertices of the thick-lined triangle represent posteriors in \\(S_{1}=\\{p_{1},p_{1}^{\\prime},p_{1}^{\\prime\\prime}\\}\\) for the less efficient type \\(\\theta_{1}\\); the vertices of the dashed-lined triangle are posteriors in \\(S_{2}=\\{p_{2},p_{2}^{\\prime},p_{2}^{\\prime\\prime}\\}\\) for the more efficient type \\(\\theta_{2}\\). The light grey area represents set \\(D_{1}\\); the dark grey area represents set \\(U_{1}\\). We find \\(S_{2}\\subseteq U_{1}\\).\n\nFigure 3: The optimal experiment choice function is strongly \\(c\\)-monotone\n\n"]}, {"edit": ["Assuming that all the miners are _interchangeable_ (which assumes a homogenous miner population), we drop the subscript '\\(k\\)' in (5.5), and the objective of a _typical_ miner is:\n\n\\[U(x):=\\sup_{\\{(\\nu(t),b(t))\\}}J(\\nu,b):=\\mathbb{E}\\left\\{\\int_{0}^{\\mathcal{T}}e ^{-\\beta t}\\left[dc(t)+\\ \\ell(X(t))dt\\right]+e^{-\\beta\\mathcal{T}}\\left[b( \\mathcal{T})+h(X(\\mathcal{T})\\right]\\right\\} \\tag{5.6}\\]\n\n\\[\\text{subject to }X^{\\prime}(t)=\\nu(t)+\\frac{N^{\\prime}(t)}{N(t)-Z(t)}X(t),\\,X(0) =x,\\] (C0)\n\n\\[dc(t)=rb(t)dt-db(t)-P(t)\\nu(t)dt-N^{\\prime}(t)\\,L\\left(\\frac{\\nu(t)}{N^{\\prime} (t)}\\right)dt,\\] (C1)\n\n\\[b(0)=0,\\,b(t)\\geqslant 0\\text{ and }0\\leqslant X(t)\\leqslant N(t),\\] (C2)\n\n\\[P(t)=P(0)+\\sigma B(t)-\\eta(Z(t)-Z(0)),\\] (C3)\n\nwhere (C0) is a repeat of the state dynamics in (5.2), and (C3) is the price dynamics in (5.4). Compared to (4.4), the volume constraint \\(|\\nu(t)|\\leqslant\\overline{\\nu}\\) for \\(\\overline{\\nu}>0\\) is removed; instead the transaction cost \\(-N^{\\prime}(t)\\,L\\left(\\frac{\\nu(t)}{N^{\\prime}(t)}\\right)dt\\) is introduced in the budget constraint (C1). This way, the miner's strategy will no longer be a bang-bang control but depend on the specific market impact mechanism.\n\nLet\n\n\\[\\widetilde{P}_{\\beta}(t):=e^{-\\beta t}\\mathbb{E}P(t)=e^{-\\beta t}\\left[P(0)- \\eta(Z(t)-Z(0))\\right]\\quad\\text{and}\\quad\\widetilde{P}(t):=\\widetilde{P}_{0} (t). \\tag{5.7}\\]\n\nThe same argument as in (4.6)-(4.7) shows that the consumption-investment problem (5.6) is separable:\n\n\\[U(x):=\\sup_{\\{(\\nu,b)\\}}J(\\nu,b)=\\sup_{b}J_{1}(b)+\\sup_{\\nu}J_{2}(\\nu),\\]\n\nwhere \\(J_{1}(b):=(r-\\beta)\\int_{0}^{\\mathcal{T}}e^{-\\beta t}b(t)dt\\) and\n\n\\[J_{2}(\\nu):=\\int_{0}^{\\mathcal{T}}\\left[-\\widetilde{P}_{\\beta}(t)\\nu(t)-e^{- \\beta t}N^{\\prime}(t)L\\left(\\frac{\\nu(t)}{N^{\\prime}(t)}\\right)+e^{-\\beta t} \\ell(X(t)\\right]dt+e^{-\\beta\\mathcal{T}}\n\n "], "nougat": ["Assuming that all the miners are _interchangeable_ (which assumes a homogenous miner population), we drop the subscript \u2018\\(k\\)' in (5.5), and the objective of a _typical_ miner is:\n\n\\[U(x):=\\sup_{\\{(\\nu(t),b(t))\\}}J(\\nu,b):=\\mathbb{E}\\left\\{\\int_{0}^{\\mathcal{T}}e ^{-\\beta t}\\left[dc(t)+\\ \\ell(X(t))dt\\right]+e^{-\\beta\\mathcal{T}}\\left[b( \\mathcal{T})+h(X(\\mathcal{T})\\right]\\right\\}\\] (5.2), and (C3) is the price dynamics in (5.4). Compared to (4.4), the volume constraint \\(|\\nu(t)|\\leqslant\\overline{\\nu}\\) for \\(\\overline{\\nu}>0\\) is removed; instead the transaction cost \\(-N^{\\prime}(t)\\,L\\left(\\frac{\\nu(t)}{N^{\\prime}(t)}\\right)dt\\) is introduced in the budget constraint (C1). This way, the miner\u2019s strategy will no longer be a bang-bang control but depend on the specific market impact mechanism.\n\nLet\n\n\\[\\widetilde{P}_{\\beta}(t):=e^{-\\beta t}\\mathbb{E}P(t)=e^{-\\beta t}\\left[P(0)- \\eta(Z(t)-Z(0))\\right]\\quad\\text{and}\\quad\\widetilde{P}(t):=\\widetilde{P}_{0} (t).\\] (5.6) is separable: \\[U(x):=\\sup_{\\{(\\nu,b)\\}}J(\\nu,b)=\\sup_{b}J_{1}(b)+\\sup_{\\nu}J_{2}(\\nu), \\tag{5.7}\\] \\[\\text{where }J_{1}(b):=(r-\\beta)\\int_{0}^{\\mathcal{T}}e^{-\\beta t}b( t)dt\\text{ and }\\] (5.8) \\[J_{2}(\\nu):=\\int_{0}^{\\mathcal{T}}\\left[-\\widetilde{P}_{\\beta}(t) \\nu(t)-e^{-\\beta t}N^{\\prime}(t)L\\left(\\frac{\\nu(t)}{N^{\\prime}(t)}\\right)+e^{ -\\beta t}\\ell(X(t)\\right]dt+e^{-\\beta\\mathcal{T}}h(X(\\mathcal{T})). \\tag{5.9}\\]\n\nAgain suppose \\(\\beta\\geqslant r\\). Then \\(\\sup_{b}J_{1}(b)=0\\) with the optimality binding at \\(b_{*}(t)=0\\) for all \\(t\\). So the problem (4.4) is reduced to\n\n\\[U(x)=\\sup_{\\nu}J_{2}(\\nu)\\quad\\text{subject to (C2\u2019) is (C2) without the constraints on }b(\\cdot). \\tag{5.10}\\]\n\nNext we argue by dynamic programming, and let\n\n\\[v(t,x):=\\sup_{\\{\\nu(s),s\\geqslant t\\}}\\int_{t}^{\\mathcal{T}}\\left[- \\widetilde{P}_{\\beta}(s)\\nu(s)-e^{-\\beta s}N^{\\prime}(s)L\\left(\\frac{\\nu(s)}{ N^{\\prime}( "]}, {"edit": ["The above example illustrates the agent's power the choice of the information structure provides. By choosing the aforementioned information structure, not only the agent is able to implement \\(e_{3}\\), i.e., the efficient level of effort, but also she is able to capture the entire surplus. In what follows, we show that under some conditions on the performance technology, this is always possible.\n\n## 5 Efficient Surplus Extraction\n\nIn this section, we provide sufficient conditions on the performance technology \\(f\\left(\\cdot|\\cdot\\right)\\) and cost function \\(c\\left(\\cdot\\right)\\) so that the agent is able to implement the first-best effort and extract the entire surplus.\n\nLet \\(e^{*}\\) be the first-best level of effort that satisfies\n\n\\[e^{*}\\in\\arg\\max_{e\\in E}\\quad\\mathbb{E}\\left[g(x)|e\\right]-c\\left(e\\right).\\]\n\nSuppose that the agent wishes to implement \\(e^{*}\\) and capture the entire surplus\n\nFigure 3: Agent-optimal information structure in Example 1\n\n"], "nougat": ["The above example illustrates the agent\u2019s power the choice of the information structure provides. By choosing the aforementioned information structure, not only the agent is able to implement \\(e_{3}\\), i.e., the efficient level of effort, but also she is able to capture the entire surplus. In what follows, we show that under some conditions on the performance technology, this is always possible.\n\n## 5 Efficient Surplus Extraction\n\nIn this section, we provide sufficient conditions on the performance technology \\(f\\left(\\cdot|\\cdot\\right)\\) and cost function \\(c\\left(\\cdot\\right)\\) so that the agent is able to implement the first-best effort and extract the entire surplus.\n\nLet \\(e^{*}\\) be the first-best level of effort that satisfies\n\n\\[e^{*}\\in\\arg\\max_{e\\in E}\\quad\\mathbb{E}\\left[g(x)|e\\right]-c\\left(e\\right).\\]\n\nSuppose that the agent wishes to implement \\(e^{*}\\) and capture the entire surplus\n\nFigure 3: Agent-optimal information structure in Example 1\n\n"]}, {"edit": ["Motivating Examples Revisited\n\nIn this section, we apply the results in Sections 3-5 to the examples introduced in Section 2.\n\n### Partial Identification of Treatment Effects\n\nIn addition to characterizing \\(\\Theta(\\delta)\\) introduced in Section 2, we also study the identified set for \\(\\theta_{Do}=\\mathbb{E}_{o}[f(Y_{1},Y_{2})]\\) without using the covariate information:\n\n\\[\\Theta_{\\mathrm{D}}(\\delta):=\\left\\{\\int_{\\mathcal{Y}_{1}\\times\\mathcal{Y}_{2} }f(y_{1},y_{2})\\,d\\gamma(y_{1},y_{2}):\\gamma\\in\\Sigma_{\\mathrm{D}}(\\delta) \\right\\},\\]\n\nwhere\n\n\\[\\Sigma_{\\mathrm{D}}(\\delta)=\\{\\gamma\\in\\mathcal{P}(\\mathcal{Y}_{1}\\times \\mathcal{Y}_{2}):\\boldsymbol{K}_{Y_{1}}(\\mu_{Y_{1}},\\gamma_{1})\\leq\\delta_{1} ,\\boldsymbol{K}_{Y_{1}}(\\mu_{Y_{2}},\\gamma_{2})\\leq\\delta_{2}\\}\\]\n\nin which \\(\\boldsymbol{K}_{Y_{1}}\\) and \\(\\boldsymbol{K}_{Y_{2}}\\) are the optimal transport costs associated with cost functions \\(c_{Y_{1}}\\) and \\(c_{Y_{2}}\\), respectively.\n\n#### 6.1.1 Characterization of the Identified Sets\n\nWhen \\(f\\) is continuous and conditions in Proposition 4.1 are satisfied, the identified sets \\(\\Theta_{\\mathrm{D}}(\\delta)\\) and \\(\\Theta(\\delta)\\) are both closed intervals with upper limits given by W-DMR for non-overlapping and overlapping marginals respectively. This allows us to apply our duality results in Section 3 to evaluate and compare \\(\\Theta_{\\mathrm{D}}(\\delta)\\) and \\(\\Theta(\\delta)\\).\n\nLet \\(\\mathcal{I}_{\\mathrm{D}}(\\delta)\\) and \\(\\mathcal{I}(\\delta)\\) denote the upper bounds of \\(\\Theta_{\\mathrm{D}}(\\delta)\\) and \\(\\Theta(\\delta)\\), respectively, where\n\n\\[\\mathcal{I}_{\\mathrm{D}}(\\delta)=\\sup_{\\gamma\\in\\Sigma_{\\mathrm{D}}(\\delta)} \\int_{\\mathcal{Y}_{1}\\times\\mathcal{Y}_{2}}f(y_{1},y_{2})\\,d\\gamma(y_{1},y_{2} )\\text{ and }\\mathcal{I}(\\delta)=\\sup_{\\gamma\\in\\Sigma(\\delta)}\\int_{\\mathcal{S }}f(y_{1},y_{2})\\,d\\gamma(y_{1},y_{2},x).\\]\n\nProposition 4.1 establishes robust versions of existing results on the identified sets of treatment effects under Assumption 2.4, see Fan et al. (2017). Sensitivity to deviations from Assumption 2.4 can be examined via \\(\\Theta_{\\mathrm{D}}(\\delta)\\) and \\(\\Theta(\\delta)\\) by varying \\(\\delta\\). For example, when \\(f\\) satisfies assumptions in Theorems 5.1 and 5.2, \\(\\mathcal{I}(\\delta)\\) and \\(\\mathcal{I}_{\\mathrm{D}}(\\delta)\\) are continuous on \\(\\mathbb{R}^{2}_{+}\\ "], "nougat": ["Motivating Examples Revisited\n\nIn this section, we apply the results in Sections 3-5 to the examples introduced in Section 2.\n\n### Partial Identification of Treatment Effects\n\nIn addition to characterizing \\(\\Theta(\\delta)\\) introduced in Section 2, we also study the identified set for \\(\\theta_{Do}=\\mathbb{E}_{o}[f(Y_{1},Y_{2})]\\) without using the covariate information:\n\n\\[\\Theta_{\\mathrm{D}}(\\delta):=\\left\\{\\int_{\\mathcal{Y}_{1}\\times\\mathcal{Y}_{2} }f(y_{1},y_{2})\\,d\\gamma(y_{1},y_{2}):\\gamma\\in\\Sigma_{\\mathrm{D}}(\\delta) \\right\\},\\]\n\nwhere\n\n\\[\\Sigma_{\\mathrm{D}}(\\delta)=\\{\\gamma\\in\\mathcal{P}(\\mathcal{Y}_{1}\\times \\mathcal{Y}_{2}):\\boldsymbol{K}_{Y_{1}}(\\mu_{Y_{1}},\\gamma_{1})\\leq\\delta_{1} ,\\boldsymbol{K}_{Y_{1}}(\\mu_{Y_{2}},\\gamma_{2})\\leq\\delta_{2}\\}\\]\n\nin which \\(\\boldsymbol{K}_{Y_{1}}\\) and \\(\\boldsymbol{K}_{Y_{2}}\\) are the optimal transport costs associated with cost functions \\(c_{Y_{1}}\\) and \\(c_{Y_{2}}\\), respectively.\n\n#### 6.1.1 Characterization of the Identified Sets\n\nWhen \\(f\\) is continuous and conditions in Proposition 4.1 are satisfied, the identified sets \\(\\Theta_{\\mathrm{D}}(\\delta)\\) and \\(\\Theta(\\delta)\\) are both closed intervals with upper limits given by W-DMR for non-overlapping and overlapping marginals respectively. This allows us to apply our duality results in Section 3 to evaluate and compare \\(\\Theta_{\\mathrm{D}}(\\delta)\\) and \\(\\Theta(\\delta)\\).\n\nLet \\(\\mathcal{I}_{\\mathrm{D}}(\\delta)\\) and \\(\\mathcal{I}(\\delta)\\) denote the upper bounds of \\(\\Theta_{\\mathrm{D}}(\\delta)\\) and \\(\\Theta(\\delta)\\), respectively, where\n\n\\[\\mathcal{I}_{\\mathrm{D}}(\\delta)=\\sup_{\\gamma\\in\\Sigma_{\\mathrm{D}}(\\delta)} \\int_{\\mathcal{Y}_{1}\\times\\mathcal{Y}_{2}}f(y_{1},y_{2})\\,d\\gamma(y_{1},y_{2} )\\text{ and }\\mathcal{I}(\\delta)=\\sup_{\\gamma\\in\\Sigma(\\delta)}\\int_{\\mathcal{S }}f(y_{1},y_{2})\\,d\\gamma(y_{1},y_{2},x).\\]\n\nProposition 4.1 establishes robust versions of existing results on the identified sets of treatment effects under Assumption 2.4 , see Fan et al. ( 2017 ). Sensitivity to deviations from Assumption 2.4 can be examined via \\(\\Theta_{\\mathrm{D}}(\\delta)\\) and \\(\\Theta(\\delta)\\) by varying \\(\\delta\\). For example, when \\(f\\) satisfies assumptions in Theorems 5.1 and 5.2 , \\(\\mathcal{I}(\\delta)\\) and \\(\\mathcal{I}_{\\mathrm{D}}(\\delta)\\) are continuous on \\(\\mathbb{R}^{2}_ "]}, {"edit": ["\n\n# PM-Gati Shakti: Advancing India's Energy Future through Demand Forecasting - A Case Study\n\n SujayKumar Reddy M\n\nCSE Department\n\nVellore Institute of Technology ,Vellore\n\nsujaykumarreddy.m2020@vitstudent.ac.in&Gopakumar G\n\nCSE Department\n\nNational Institute of Technology, Calicut\n\ngopakumarg@nitc.ac.in\n\n###### Abstract\n\nPM-Gati-Shakti Initiative, integration of ministries, including railways, ports, waterways, logistic infrastructure, mass transport, airports, and roads. Aimed at enhancing connectivity and bolstering the competitiveness of Indian businesses, the initiative focuses on six pivotal pillars known as \"Connectivity for Productivity\": comprehensiveness, prioritization, optimization, synchronization, analytical, and dynamic. In this study, we explore the application of these pillars to address the problem of \"Maximum Demand Forecasting in Delhi.\" Through a detailed case study, we seek to comprehend and formalize the use cases associated with this crucial forecasting task, illuminating the potential and impact of the PM-Gati Shakti scheme in shaping India's energy landscape and driving sustainable growth. Electricity forecasting plays a very significant role in the power grid as it is required to maintain a balance between supply and load demand at all times, to provide a quality electricity supply, for Financial planning, generation reserve, and many more. Forecasting helps not only in Production Planning but also in Scheduling like Import / Export which is very often in India and mostly required by the rural areas and North Eastern Regions of India. As Electrical Forecasting includes many factors which cannot be detected by the models out there, We use Classical Forecasting Techniques to extract the seasonal patterns from the daily data of Maximum Demand for the Union Territory Delhi. This research contributes to the power supply industry by helping to reduce the occurrence of disasters such as blackouts, power cuts, and increased tariffs imposed by regulatory commissions. The forecasting techniques can also help in reducing OD and UD of Power for different regions. We use the Data provided by a department from the Ministry of Power and use different forecast models including Seasonal forecasts for daily data.\n\nMachine Learning Time-Series Forecasting Demand Forecasting PM Gati-Shakti Ministry of Power Delhi\n\n## 1 Introduction\n\nThe Indian Energy GRID is maintained by POWERGRID [0] which has the objectives of running the GRID efficiently and installing transmission lines etc... and the second one is the National Load Dispatch Center (NLDC) [0] which concentrates on Supervision over the Regional Load Dispatch Centres, Scheduling and dispatch of electricity over inter-regional links in accordance with grid standards specified by the Authority and grid code specified by Central Commission in coordination with Regional Load Dispatch Centres, Monitoring of operations and grid security of the National Grid, etc... This research mainly focusses on this NLDC which is a Division of Ministry of Power.\n\nFig 1[0] describes the Yearly Installed Power Capacity in Delhi. The highest installed Capacity was 8,346.72 MW in Fiscal year 2016. which is responsible for sending the energy from Stations to sub-stations and to discuss and then to homes, industries, commercials, etc... As of 21-06-2023 The installed Capacity Sector-wise data [0] gives an overview of what type of Thermal Plants which are present in Delhi and also what types of Energy resources are present in Delhi.\n\n"], "nougat": ["PM-Gati-Shakti Initiative, integration of ministries, including railways, ports, waterways, logistic infrastructure, mass transport, airports, and roads. Aimed at enhancing connectivity and bolstering the competitiveness of Indian businesses, the initiative focuses on six pivotal pillars known as \"Connectivity for Productivity\": comprehensiveness, prioritization, optimization, synchronization, analytical, and dynamic. In this study, we explore the application of these pillars to address the problem of \"Maximum Demand Forecasting in Delhi.\" Through a detailed case study, we seek to comprehend and formalize the use cases associated with this crucial forecasting task, illuminating the potential and impact of the PM-Gati Shakti scheme in shaping India\u2019s energy landscape and driving sustainable growth. Electricity forecasting plays a very significant role in the power grid as it is required to maintain a balance between supply and load demand at all times, to provide a quality electricity supply, for Financial planning, generation reserve, and many more. Forecasting helps not only in Production Planning but also in Scheduling like Import / Export which is very often in India and mostly required by the rural areas and North Eastern Regions of India. As Electrical Forecasting includes many factors which cannot be detected by the models out there, We use Classical Forecasting Techniques to extract the seasonal patterns from the daily data of Maximum Demand for the Union Territory Delhi. This research contributes to the power supply industry by helping to reduce the occurrence of disasters such as blackouts, power cuts, and increased tariffs imposed by regulatory commissions. The forecasting techniques can also help in reducing OD and UD of Power for different regions. We use the Data provided by a department from the Ministry of Power and use different forecast models including Seasonal forecasts for daily data.\n\n###### Abstract\n\nPM-Gati-Shakti Initiative, integration of ministries, including railways, ports, waterways, logistic infrastructure, mass transport, airports, and roads. Aimed at enhancing connectivity and bolstering the competitiveness of Indian businesses, the initiative focuses on six pivotal pillars known as \"Connectivity for Productivity\": comprehensiveness, prioritization, optimization, synchronization, analytical, and dynamic. In this study, we explore the application of these pillars to address the problem of \"Maximum Demand Forecasting in Delhi. The highest installed Capacity was 8,346.72 MW in Fiscal year 2016. which is responsible for sending the energy from Stations to sub-stations and to discuss and then to homes, industries, commercials, etc... As of 21-06-2023 The installed Capacity Sector-wise data [ 0 ] gives an overview of what type of Thermal Plants which are present in Delhi and also what types of Energy resources are present in Delhi.\n\n "]}, {"edit": ["However, the main reason to make this assumption is technical: if there is no budget constraint, then all individuals would weakly prefer higher levels of rewards.12\n\nFootnote 12: This issue is discussed in Patty (2008) in the related context of how legislators might create incentives to maintain party unity.\n\nWe amend the payoff function for individuals given in Equation 2 to accommodate our balanced budget constraint by requiring that each individual pay a tax equal to the average reward that is awarded to individuals, \\(r\\cdot\\overline{d}\\). This tax is equivalent to assuming that people receive reward \\(r\\cdot(1-\\overline{d})\\) if \\(d_{i}=1\\), and pay penalty \\(-r\\cdot\\overline{d}\\) if \\(d_{i}=0\\).\n\nAdditionally, we also add a term to the payoff function that allows individuals to have a taste aggregate behavior. Letting \\(\\pi\\) denote the prevalence of compliance in the population, we capture this term by \\(t\\cdot\\pi\\), for \\(t\\geq 0\\). Neither this term nor the tax \\(r\\cdot\\overline{d}\\) affect an individual's decision to comply, and so they do not change any of our results thus far. However, these terms do affect preferences over optimal rewards and penalties.\n\nIncorporating a taste for aggregate compliance into individuals' payoffs allows us to consider that individuals, as members of a common community, may share preferences over aggregate behavior. The marginal value of the (positive) externality generated by others' choices to comply (\\(\\beta_{j}=1\\)) is represented by \\(t\\geq 0\\). As \\(t\\) increases, all individuals value aggregate compliance more, which can be conceived as an increased negative externality of non-compliance. Residents of a dense urban community may, for example, value safe driving in the aggregate more than residents of a rural community. We will see that as \\(t\\) increases, all individuals become more supportive of subsidizing compliance. That said, individual tastes for this subsidy also depend on their private costs to compliance. Consequently, even when \\(t=0\\) every individual will prefer a system of positive rewards and fines. Incorporating these terms into individuals' payoffs, individual preferences are now described by\n\n\\[r\\cdot(d_{i}-\\overline{d})-\\beta_{i}\\cdot\\gamma_{i}+t\\cdot\\pi. \\tag{8}\\]\n\nWe assume that the structure of the problem is common knowledge to all individuals (including the designer).13 We analyze equilibrium behavior, and the starting point of this analysis is to consider how each individual \\(i\\) should calculate his or her most-preferred reward level. Each individual will realize that he or she will ultimately choose either to comply or not. Conditional on each of these possible choices, the distribution of types, \\(F\\), the algorithm, \\(\\delta\\), and individual \\(i\\)'s type, \\(\\gamma_{i}\\), \\(i\\) calculates his or her most-preferred reward in each of the two cases. This yields the "], "nougat": ["However, the main reason to make this assumption is technical: if there is no budget constraint, then all individuals would weakly prefer higher levels of rewards. 12 This issue is discussed in Patty (2008) in the related context of how legislators might create incentives to maintain party unity.\n\nFootnote 12: This doesn\u2019t preclude the possibility that individuals have privately observed types, but our analysis also clarifies that, because we require the algorithm designer to use the algorithm to render individual decisions, it is not important whether the designer is aware of any given individuals\u2019 types, because the algorithm is not allowed to condition upon this information.\n\nWe amend the payoff function for individuals given in Equation 2 to accommodate our balanced budget constraint by requiring that each individual pay a tax equal to the average reward that is awarded to individuals, \\(r\\cdot\\overline{d}\\). This tax is equivalent to assuming that people receive reward \\(r\\cdot(1-\\overline{d})\\) if \\(d_{i}=1\\), and pay penalty \\(-r\\cdot\\overline{d}\\) if \\(d_{i}=0\\).\n\nAdditionally, we also add a term to the payoff function that allows individuals to have a taste aggregate behavior. Letting \\(\\pi\\) denote the prevalence of compliance in the population, we capture this term by \\(t\\cdot\\pi\\), for \\(t\\geq 0\\). Neither this term nor the tax \\(r\\cdot\\overline{d}\\) affect an individual\u2019s decision to comply, and so they do not change any of our results thus far. However, these terms do affect preferences over optimal rewards and penalties.\n\nIncorporating a taste for aggregate compliance into individuals\u2019 payoffs allows us to consider that individuals, as members of a common community, may share preferences over aggregate behavior. The marginal value of the (positive) externality generated by others\u2019 choices to comply (\\(\\beta_{j}=1\\)) is represented by \\(t\\geq 0\\). As \\(t\\) increases, all individuals value aggregate compliance more, which can be conceived as an increased negative externality of non-compliance. Residents of a dense urban community may, for example, value safe driving in the aggregate more than residents of a rural community. We will see that as \\(t\\) increases, all individuals become more supportive of subsidizing compliance. That said, individual tastes for this subsidy also depend on their private costs to compliance. Consequently, even when \\(t=0\\) every individual will prefer a system of positive rewards and fines. Incorporating these terms into individuals\u2019 payoffs, individual preferences are now described by\n\n\\[r\\cdot(d_{i}-\\overline{d})-\\beta_{i}\\cdot\\gamma_{i}+t\\cdot\\pi. \\tag{8}\\]\n\nWe assume that the structure of the problem is common knowledge to all individuals (including the designer). 13 We analyze equilibrium behavior, and the starting point of this analysis is to consider how each individual \\(i\\) should calculate his or her most-preferred reward level. Each individual will realize that he or she will ultimately choose either to comply or not. Conditional on each of these possible choices, the distribution of types, \\(F\\), the algorithm, \\(\\delta\\), and individual \\(i\\)'s type, \\(\\gamma_{i}\\), \\(i\\) calculates his or her most-preferred reward in each of the two cases. This yields the "]}, {"edit": ["\\[\\leq\\max_{i\\in\\mathcal{N}}\\left\\|\\lambda_{i}^{0}\\right\\|_{2}^{2}\\max_{t \\in\\mathcal{T}_{\\ell}}\\left\\|f_{t}^{0}\\right\\|_{2}^{2}\\frac{1}{nT_{\\ell}}\\sum_{ i\\in\\mathcal{N}}\\sum_{i^{\\prime}\\in\\mathcal{N}}\\sum_{t\\in\\mathcal{T}_{\\ell}} \\sum_{t^{\\prime}\\in\\mathcal{T}_{\\ell}}|\\mathbb{E}\\left(e_{it}e_{i^{*}t^{*}}| \\mathscr{D}\\right)|\\] \\[\\lesssim\\frac{1}{nT_{\\ell}}\\sum_{i\\in\\mathcal{N}}\\sum_{t\\in \\mathcal{T}_{\\ell}}\\left|\\mathbb{E}\\left(e_{it}^{2}|\\mathscr{D}\\right)\\right|=O (1)\\ a.s.,\\]\n\n_where the last line holds by Lemma B.7(i) and Assumption 1\\({}^{*}\\) . Similarly as above, we can also show that \\(\\mathbb{E}\\left[\\left\\|\\frac{1}{\\sqrt{nT_{\\ell}}}\\sum_{i\\in\\mathcal{N}}\\lambda _{i}^{0}e_{i}^{(\\ell)\\prime}\\right\\|^{2}\\left|\\mathscr{D}\\right.\\right]=O_{p}( 1)\\). Then_\n\n\\[\\left\\|\\frac{1}{\\sqrt{nT_{\\ell}}}\\sum_{i\\in\\mathcal{N}}\\lambda_{i}^{0}e_{i}^{( \\ell)\\prime}F^{0,(\\ell)}\\right\\|=O_{p}(1)\\ \\text{and}\\ \\left\\|\\frac{1}{\\sqrt{nT_{ \\ell}}}\\sum_{i\\in\\mathcal{N}}\\lambda_{i}^{0}e_{i}^{(\\ell)\\prime}\\right\\|=O_{ p}(1).\\]\n\n_Furthermore, we have_\n\n\\[\\left\\|\\frac{1}{nT_{\\ell}}\\sum_{i\\in\\mathcal{N}}\\lambda_{i}^{0}e_{ i}^{(\\ell)\\prime}\\hat{F}^{(\\ell)}\\right\\| \\leqslant\\frac{1}{\\sqrt{n}}\\frac{1}{\\sqrt{T_{\\ell}}}\\left\\|\\hat{F} ^{(\\ell)}-F^{0,(\\ell)}H^{(\\ell)}\\right\\|\\left\\|\\frac{1}{\\sqrt{nT_{\\ell}}}\\sum_ {i\\in\\mathcal{N}}\\lambda_{i}^{0}e_{i}^{(\\ell)\\prime}\\right\\|+\\frac{1}{\\sqrt{nT _{\\ell}}}\\left\\|\\frac{1}{\\sqrt{nT_{\\ell}}}\\sum_{i\\in\\mathcal{N}}\\lambda_{i}^{0 }e_{i}^{(\\ell)\\prime}F^{0,(\\ell)}\\right\\|\\left\\|H^{(\\ell)}\\right\\|\\] \\[=O_{p}\\left(\\frac{B_{N}}{\\sqrt{N}}+\\frac{1}{N+\\sqrt{NT}}\\right)+O _{p}\\left(\\frac{1}{\\sqrt{NT}}\n\n "], "nougat": ["\\[\\leq\\max_{i\\in\\mathcal{N}}\\left\\|\\lambda_{i}^{0}\\right\\|_{2}^{2}\\max_{t \\in\\mathcal{T}_{\\ell}}\\left\\|f_{t}^{0}\\right\\|_{2}^{2}\\frac{1}{nT_{\\ell}}\\sum_{ i\\in\\mathcal{N}}\\sum_{i^{\\prime}\\in\\mathcal{N}}\\sum_{t\\in\\mathcal{T}_{\\ell}} \\sum_{t^{\\prime}\\in\\mathcal{T}_{\\ell}}|\\mathbb{E}\\left(e_{it}e_{i^{*}t^{*}}| \\mathscr{D}\\right)|\\] \\[\\lesssim\\frac{1}{nT_{\\ell}}\\sum_{i\\in\\mathcal{N}}\\sum_{t\\in \\mathcal{T}_{\\ell}}\\left|\\mathbb{E}\\left(e_{it}^{2}|\\mathscr{D}\\right)\\right|=O (1)\\ a.s.,\\]\n\n_where the last line holds by Lemma B.7 (i) and Assumption 1\\({}^{*}\\) . Similarly as above, we can also show that \\(\\mathbb{E}\\left[\\left\\|\\frac{1}{\\sqrt{nT_{\\ell}}}\\sum_{i\\in\\mathcal{N}}\\lambda _{i}^{0}e_{i}^{(\\ell)\\prime}\\right\\|^{2}\\left|\\mathscr{D}\\right.\\right]=O_{p}( 1)\\). Then_\n\n\\[\\left\\|\\frac{1}{\\sqrt{nT_{\\ell}}}\\sum_{i\\in\\mathcal{N}}\\lambda_{i}^{0}e_{i}^{( \\ell)\\prime}F^{0,(\\ell)}\\right\\|=O_{p}(1)\\text{ and }\\left\\|\\frac{1}{\\sqrt{nT_{ \\ell}}}\\sum_{i\\in\\mathcal{N}}\\lambda_{i}^{0}e_{i}^{(\\ell)\\prime}\\right\\|=O_{p }(1).\\]\n\n_Furthermore, we have_\n\n\\[\\left\\|\\frac{1}{nT_{\\ell}}\\sum_{i\\in\\mathcal{N}}\\lambda_{i}^{0}e_{ i}^{(\\ell)\\prime}\\hat{F}^{(\\ell)}\\right\\| \\leqslant\\frac{1}{\\sqrt{n}}\\frac{1}{\\sqrt{T_{\\ell}}}\\left\\|\\hat{F} ^{(\\ell)}-F^{0,(\\ell)}H^{(\\ell)}\\right\\|\\left\\|\\frac{1}{\\sqrt{nT_{\\ell}}}\\sum _{i\\in\\mathcal{N}}\\lambda_{i}^{0}e_{i}^{(\\ell)\\prime}\\right\\|+\\frac{1}{\\sqrt{nT _{\\ell}}}\\left\\|\\frac{1}{\\sqrt{nT_{\\ell}}}\\sum_{i\\in\\mathcal{N}}\\lambda_{i}^{0} e_{i}^{(\\ell)\\prime}F^{0,(\\ell)}\\right\\|\\left\\|H^{(\\ell)}\\right\\|\\] \\[=O_{p}\\left(\\frac{B_{N}}{\\sqrt{N}}+\\frac{1}{N+\\sqrt{NT}}\\right)+O _{p}\\left(\\frac{1}{\\sqrt{NT}}\\right "]}, {"edit": ["into the potential outcomes model. Our distinct goal is to provide statistical methods to credibly infer heterogeneous treatment effects (HTEs) using experimental network data sets. The knowledge of HTEs is useful in the design of welfare-maximizing policies as it allows for the targeting of specific subgroups that would benefit the most from a particular intervention. For instance, a study by Viviano (2019) demonstrates how to use HTE estimates to improve a weather insurance policy take-up among rice farmers in rural China. Similarly, Han et al. (2022) use HTE estimates to design multinomial success rules in populations where interactions occur within non-overlapping groups. To achieve our goal, we develop randomization testing methods that are valid asymptotically for three useful notions of HTEs in the presence of network interference: (i) the null hypothesis of constant treatment effects across the population; (ii) heterogeneous treatment effects across network exposure values only; and (iii) heterogeneous treatment effects across network exposure values and covariate-defined discrete groups only.\n\nOur reliance on a randomization-based testing method for HTEs in the current paper is motivated by two main reasons. First, since units are linked through social networks, we cannot assume that the variables of units are independent. Therefore, the traditional asymptotic-based inferential methods are not directly applicable. The second advantage of the randomization-based testing method is that it is fully parametric and provides exact p-values without imposing restrictive conditions on the data generating process (DGP) for _sharp_ null hypotheses.1 Moreover, recent studies show that even for non-sharp null hypotheses, there are conditional randomization-based methods that can generate exact conditional p-values without any assumptions on the DGP.\n\nFootnote 1: Under a sharp null hypothesis, all potential outcomes for each unit can be imputed.\n\nThe null hypotheses we consider in this paper are _not_ sharp due to two reasons. First, our null hypotheses contain nuisance parameters which are unknown values in the science tables one will construct under the null. Hence, one can only partially impute the potential outcomes that depend on these nuisance parameters under the null hypotheses. The problem of nuisance parameters in null hypotheses is not exclusive to network interference, but there may be more of these parameters present in this setting. (See Ding et al. (2016) for a hypothesis of constant treatment effects under no interference). Second, under network interference, the number of potential outcomes depends on the exposure mapping one imposes. Therefore, without additional restrictions on the underlying DGP, we cannot impute all missing potential outcomes under the null hypotheses. As a result "], "nougat": ["into the potential outcomes model. Our distinct goal is to provide statistical methods to credibly infer heterogeneous treatment effects (HTEs) using experimental network data sets. The knowledge of HTEs is useful in the design of welfare-maximizing policies as it allows for the targeting of specific subgroups that would benefit the most from a particular intervention. For instance, a study by Viviano (2019) demonstrates how to use HTE estimates to improve a weather insurance policy take-up among rice farmers in rural China. Similarly, Han et al. (2022) use HTE estimates to design multinomial success rules in populations where interactions occur within non-overlapping groups. To achieve our goal, we develop randomization testing methods that are valid asymptotically for three useful notions of HTEs in the presence of network interference: (i) the null hypothesis of constant treatment effects across the population; (ii) heterogeneous treatment effects across network exposure values only; and (iii) heterogeneous treatment effects across network exposure values and covariate-defined discrete groups only.\n\nOur reliance on a randomization-based testing method for HTEs in the current paper is motivated by two main reasons. First, since units are linked through social networks, we cannot assume that the variables of units are independent. Therefore, the traditional asymptotic-based inferential methods are not directly applicable. The second advantage of the randomization-based testing method is that it is fully parametric and provides exact p-values without imposing restrictive conditions on the data generating process (DGP) for _sharp_ null hypotheses.1 Moreover, recent studies show that even for non-sharp null hypotheses, there are conditional randomization-based methods that can generate exact conditional p-values without any assumptions on the DGP.\n\nFootnote 1: Under a sharp null hypothesis, all potential outcomes for each unit can be imputed.\n\nThe null hypotheses we consider in this paper are _not_ sharp due to two reasons. First, our null hypotheses contain nuisance parameters which are unknown values in the science tables one will construct under the null. Hence, one can only partially impute the potential outcomes that depend on these nuisance parameters under the null hypotheses. The problem of nuisance parameters in null hypotheses is not exclusive to network interference, but there may be more of these parameters present in this setting. (See Ding et al. (2016) for a hypothesis of constant treatment effects under no interference). Second, under network interference, the number of potential outcomes depends on the exposure mapping one imposes. Therefore, without additional restrictions on the underlying DGP, we cannot impute all missing potential outcomes under the null hypotheses. As a result "]}, {"edit": ["difference in belief between the cutoff belief for the first-best and the cutoff in any MPE). Changing \\(\\lambda\\) scales the noncooperative game and the first-best solution identically and hence also has no impact on efficiency.\n\nMore notably, the efficiency condition is independent of \\(R_{w}\\) and \\(\\pi_{w}\\); that is, the condition for efficiency does not depend on what the winner receives. To understand this, consider an incremental increase in \\(R_{w}\\) or \\(\\pi_{w}\\), fixing \\(R_{l}\\) and \\(\\pi_{l}\\). Any incremental increase induces agents in the noncooperative game to experiment more but also induces more experimentation in the social planner problem. To understand why this does not affect the efficiency implications, consider the best-response problem of agent \\(i\\). For agent \\(i\\), the cumulative effort of other agents \\(K_{-i}\\) brings about a \"loss\" event at a rate of \\(\\lambda K_{-i}\\), but agent \\(i\\) has no agency over the effort of the other agents. Instead, the effort choice for agent \\(i\\) weighs the relative benefit of winning versus the status quo project (and in fact, \\(b_{I}\\) and \\(c_{I}\\) have no \\(R_{l},\\pi_{l}\\) dependence). Hence, in the noncooperative game, agents are intuitively already trading off the relative benefits of winning versus the status quo in an efficient manner; however, they do not properly account for the externalities induced by their research effort on other players. Thus, the source of any potential inefficiency is the extent to which the externalities harm or help the other agents.\n\nThe next subsection outlines the technical steps to prove Theorem 2. A reader less concerned with these details may skip the next subsection and proceed to Section 5.\n\n### Best Response Problem\n\nSince I am interested in Markov perfect equilibria, I start by considering the best response problem of a single agent reacting to a given profile of Markov strategies of the other agents. That is, suppose that the strategies of all other agents are exogenously fixed at \\(\\{\\sigma_{j}\\}_{j\\neq i}\\). The cumulative effort of the other agents at some belief \\(p\\) is \\(K_{-i}(p)=\\sum_{j\\neq i}\\sigma_{j}(p)\\), which is also left-continuous and finite piecewise Lipschitz by assumption. Using standard arguments, I derive the HJB equation characterizing best-response \\(k_{i}\\) given the function \\(K_{-i}(p)\\). Let \\(u(\\cdot)\\) denote the value function of agent \\(i\\)'s best-response Markov decision process. Then the HJB equation characterizing \\(u\\) is given by\n\n\\[u(p)=\\pi_{s}+K_{-i}(p)\\left(p\\lambda R_{l}+b_{I}(p,u,u^{\\prime})-p\\frac{\\lambda }{r}(\\pi_{w}-\\pi_{l})\\right)+\\max_{k_{i}}\\left[k_{i}\\left(b_{I}(p,u,u^{\\prime })-c_{I}(p)\\right)\\right], \\tag{5}\\]\n\nwhere\n\n\\[b_{I}(p,u,u^{\\prime})=p\\frac{\\lambda}{r}(\\pi_{w}-u(p)-(1-p)u^{\\prime}(p)),\\] (6) "], "nougat": ["difference in belief between the cutoff belief for the first-best and the cutoff in any MPE). Changing \\(\\lambda\\) scales the noncooperative game and the first-best solution identically and hence also has no impact on efficiency.\n\nMore notably, the efficiency condition is independent of \\(R_{w}\\) and \\(\\pi_{w}\\); that is, the condition for efficiency does not depend on what the winner receives. To understand this, consider an incremental increase in \\(R_{w}\\) or \\(\\pi_{w}\\), fixing \\(R_{l}\\) and \\(\\pi_{l}\\). Any incremental increase induces agents in the noncooperative game to experiment more but also induces more experimentation in the social planner problem. To understand why this does not affect the efficiency implications, consider the best-response problem of agent \\(i\\). For agent \\(i\\), the cumulative effort of other agents \\(K_{-i}\\) brings about a \u201closs\u201d event at a rate of \\(\\lambda K_{-i}\\), but agent \\(i\\) has no agency over the effort of the other agents. Instead, the effort choice for agent \\(i\\) weighs the relative benefit of winning versus the status quo project (and in fact, \\(b_{I}\\) and \\(c_{I}\\) have no \\(R_{l},\\pi_{l}\\) dependence). Hence, in the noncooperative game, agents are intuitively already trading off the relative benefits of winning versus the status quo in an efficient manner; however, they do not properly account for the externalities induced by their research effort on other players. Thus, the source of any potential inefficiency is the extent to which the externalities harm or help the other agents.\n\nThe next subsection outlines the technical steps to prove Theorem 2. A reader less concerned with these details may skip the next subsection and proceed to Section 5.\n\n### Best Response Problem\n\nSince I am interested in Markov perfect equilibria, I start by considering the best response problem of a single agent reacting to a given profile of Markov strategies of the other agents. That is, suppose that the strategies of all other agents are exogenously fixed at \\(\\{\\sigma_{j}\\}_{j\\neq i}\\). The cumulative effort of the other agents at some belief \\(p\\) is \\(K_{-i}(p)=\\sum_{j\\neq i}\\sigma_{j}(p)\\), which is also left-continuous and finite piecewise Lipschitz by assumption. Using standard arguments, I derive the HJB equation characterizing best-response \\(k_{i}\\) given the function \\(K_{-i}(p)\\). Let \\(u(\\cdot)\\) denote the value function of agent \\(i\\)'s best-response Markov decision process. Then the HJB equation characterizing \\(u\\) is given by\n\n\\[u(p)=\\pi_{s}+K_{-i}(p)\\left(p\\lambda R_{l}+b_{I}(p,u,u^{\\prime})-p\\frac{ \\lambda}{r}(\\pi_{w}-\\pi_{l})\\right)+\\max_{k_{i}}\\left[k_{i}\\left(b_{I}(p,u,u^ {\\prime})-c_{I}(p)\\right)\\right], \\tag{5}\\]\n\nwhere\n\n\\[b_{I}(p,u,u^{\\prime})=p\\frac{\\lambda}{r}(\\pi_{w}-u(p)-(1-p)u^{\\prime}(p)),\\] (6) "]}, {"edit": ["In Table 3.1 we have four groups consisting of four distributions. The groups reflect the fact that in Figures 3.1-3.3, the distributions are grouped into four rows each containing four panels. The figures depict the three income-equality curves \\(\\psi_{k}\\) for the distributions specified in Table 3.1.\n\nSince the curves are ratios of percentiles, the scale parameter of each distribution has no effect on the inequality indices. The same is true for the log-location parameter (\\(e^{\\mu}\\)) of the lognormal and log-Cauchy distributions. However, the shape (\\(\\alpha\\), \\(\\gamma\\)) and the log-scale (\\(e^{\\sigma}\\)) parameters are the primary drivers of the underlying inequality. To explore this effect, we choose a couple of values of each of these parameters for plotting. In the plots of Figures 3.1-3.3, the uniform distribution serves as a benchmark for comparing the curves. In each plot, the dash-dotted line (invisible in the top left panels of the figures) marks the curve \\(\\psi_{k}\\) in the case of the uniform distribution. Numerical evaluations labeled 'area' represent the areas of the corresponding shaded regions above the curves \\(\\psi_{k}\\), which are the values of the inequality indices.\n\nFrom Table 3.1 and Figures 3.1-3.3 we observe several facts, which can also be verified mathematically:\n\n* \\(\\psi_{1}\\) for _Pareto-III_\\((\\sigma,\\gamma=2)\\) and \\(\\psi_{3}\\) for _Pareto-II_\\((\\sigma,\\alpha=1)\\) coincide, thus giving identical\n\n\\begin{table}\n\\begin{tabular}{l|c c c|c c c} \\hline \\hline Distributions & \\multicolumn{3}{c|}{Inequality indices} & \\multicolumn{3}{c}{Ranks based on} \\\\  & \\(\\Psi_{1}\\) & \\(\\Psi_{2}\\) & \\(\\Psi_{3}\\) & \\(\\Psi_{1}\\) & \\(\\Psi_{2}\\) & \\(\\Psi_{3}\\) \\\\ \\hline _Uniform_\\((0,\\theta)\\) & 0.5010 & 0.6936 & 0.6147 & 6 & 2 & 3-4 \\\\ _Exponential_\\((0,\\theta)\\) & 0.5583 & 0.8327 & 0.7026 & 7 & 7 & 7 \\\\ _Gamma_\\((\\theta,\\alpha=0.5)\\) & 0.6874 & 0.9378 & 0.8020 & 12 & 10 & 11 \\\\ _Gamma_\\((\\theta,\\alpha=2)\\) & 0.4360 & 0.6974 & 0.5956 & 3 & 3 & 2 \\\\ \\hline _Weibull_\\((\\theta,\\tau=0.5)\\) & 0.7237 & 0.9681 & 0.8358 & 13 & 13 & 13 \\\\ _Weibull_\\((\\theta,\\tau=2)\\) & 0.3810 & 0.6022 & 0.5239 & 1 & 1 & 1 \\\\ _Lognormal_\\((\\mu,\\sigma=1)\\) & 0.4779 & 0.7886 & 0.6648 & 4 & 5 & 5 \\\\ _Lognormal_\\((\\mu,\\sigma=2)\\) & 0.6648 & "], "nougat": ["In Table 3.1 we have four groups consisting of four distributions. The groups reflect the fact that in Figures 3.1 \u2013 3.3 , the distributions are grouped into four rows each containing four panels. The figures depict the three income-equality curves \\(\\psi_{k}\\) for the distributions specified in Table 3.1 .\n\nSince the curves are ratios of percentiles, the scale parameter of each distribution has no effect on the inequality indices. The same is true for the log-location parameter (\\(e^{\\mu}\\)) of the lognormal and log-Cauchy distributions. However, the shape (\\(\\alpha\\), \\(\\gamma\\)) and the log-scale (\\(e^{\\sigma}\\)) parameters are the primary drivers of the underlying inequality. To explore this effect, we choose a couple of values of each of these parameters for plotting. In the plots of Figures 3.1 \u2013 3.3 , the uniform distribution serves as a benchmark for comparing the curves. In each plot, the dash-dotted line (invisible in the top left panels of the figures) marks the curve \\(\\psi_{k}\\) in the case of the uniform distribution. Numerical evaluations labeled \u2018area\u2019 represent the areas of the corresponding shaded regions above the curves \\(\\psi_{k}\\), which are the values of the inequality indices.\n\nFrom Table 3.1 and Figures 3.1 \u2013 3.3 we observe several facts, which can also be verified mathematically:\n\n* \\(\\psi_{1}\\) for _Pareto-III_\\((\\sigma,\\gamma=2)\\) and \\(\\psi_{3}\\) for _Pareto-II_\\((\\sigma,\\alpha=1)\\) coincide, thus giving identical\n\n\\begin{table}\n\\begin{tabular}{l|c c c|c c c} \\hline \\hline Distributions & \\multicolumn{3}{c|}{Inequality indices} & \\multicolumn{3}{c}{Ranks based on} \\\\  & \\(\\Psi_{1}\\) & \\(\\Psi_{2}\\) & \\(\\Psi_{3}\\) & \\(\\Psi_{1}\\) & \\(\\Psi_{2}\\) & \\(\\Psi_{3}\\) \\\\ \\hline _Uniform\\((0,\\theta)\\)_ & 0.5010 & 0.6936 & 0.6147 & 6 & 2 & 3-4 \\\\ _Exponential\\((0,\\theta)\\)_ & 0.5583 & 0.8327 & 0.7026 & 7 & 7 & 7 \\\\ _Gamma\\((\\theta,\\alpha=0.5)\\)_ & 0.6874 & 0.9378 & 0.8020 & 12 & 10 & 11 \\\\ _Gamma\\((\\theta,\\alpha=2)\\)_ & 0.4360 & 0.6974 & 0.5956 & 3 & 3 & 2 \\\\ \\hline _Weibull\\((\\theta,\\tau=0.5)\\)_ & 0.7237 & 0.9681 & 0.8358 & 13 & 13 & 13 \\\\ _Weibull\\((\\theta,\\tau=2)\\)_ & 0.3810 & 0.6022 & 0.5239 & 1 & 1 & 1 \\\\ _Lognormal\\((\\mu,\\sigma=1)\\)_ & 0.4779 & 0.7886 & 0.6648 & 4 & 5 & 5 \\\\ _Lognormal\\((\\mu,\\sigma=2)\\)_ & 0.6 "]}, {"edit": ["_To conclude this section, notice that the main cost of our approach is that we are not able to identify as many group-time average treatment effects as would be possible using other identification strategies. For example, \\(-\\)_**color [RGB]-0; 0; 0degcallaway-karami-2023deg** _suppose that the researcher has access to a time-invariant covariate that does not affect the path of untreated potential outcomes and show that this sort of covariate can be used to generate additional moment conditions to identify the parameters in the model for untreated potential outcomes. Our approach does not require this sort of covariate (which is a key advantage of our approach), but it comes at the cost of only being able to identify \\(ATT(3,3)\\) without recovering \\(ATT(3,4)\\) or \\(ATT(4,4)\\) which would be feasible using the approach in \\(-\\)_**color [RGB]-0; 0degcallaway-karami-2023deg**_._\n\n#### 2.2.2 General Case\n\n_In this section, we extend the results above to a setting with more periods, more groups, and allow for more interactive fixed effects. The arguments in this section target recovering \\(ATT(g,t)\\) for a particular group \\(g\\in\\bar{\\mathcal{G}}\\). Given the model in Assumption 4, after taking first differences, we have that_\n\n\\[\\Delta Y_{it}(0)=\\Delta\\theta_{t}+\\lambda_{i}^{\\prime}\\Delta F_{t}+\\Delta e_{it}\\]\n\n_which eliminates the unit fixed effect \\(\\eta_{i}\\). Next, define_\n\n\\[\\Delta Y_{i}(0):=\\begin{bmatrix}\\Delta Y_{i2}\\\\ \\vdots\\\\ \\Delta Y_{i\\mathcal{T}}\\end{bmatrix};\\quad\\Delta\\theta:=\\begin{bmatrix}\\Delta \\theta_{2}\\\\ \\vdots\\\\ \\Delta\\theta_{\\mathcal{T}}\\end{bmatrix};\\quad\\mathbf{\\Delta F}:=\\begin{bmatrix} \\Delta F_{2}^{\\prime}\\\\ \\vdots\\\\ \\Delta F_{\\mathcal{T}}^{\\prime}\\end{bmatrix};\\text{ and }\\Delta e_{i}:=\\begin{bmatrix} \\Delta e_{i2}\\\\ \\vdots\\\\ \\Delta e_{i\\mathcal{T}}\\end{bmatrix}\\]\n\n_where \\(\\Delta Y_{i}(0)\\), \\(\\Delta\\theta\\), and \\(\\Delta e_{i}\\) are all \\((\\mathcal{T}-1)\\) dimensional vectors, and \\(\\mathbf{\\Delta F}\\) is \\((\\mathcal{T}-1)\\times R\\) matrix. Similarly, define_\n\n\\[\\Delta Y_{i}^{pre(g)}(0):=\\begin{bmatrix}\\Delta Y_{i2}\\\\ \\vdots\\\\ \\Delta Y_{ig-1}\\end{bmatrix};\\quad\\Delta\\theta^{pre(g)}:=\\begin{bmatrix} \\Delta\\theta_{2}\\\\ \\vdots\\\\ \\Delta\\theta_{g-1}\\end{bmatrix};\\quad\\mathbf{\\Delta F}^{pre(g)}:=\\begin{bmatrix} \\Delta F_{2}^{\\prime}\\\\ \\vdots\\\\ \\Delta F_{g-1}^{\\prime}\\end{bmatrix};\\text{ and }\\Delta e_{i}^{pre(g)}:=\\begin{bmatrix} \\Delta e_{i2}\\\\ \\vdots\\\\ \\Delta e_{ig-1}\\end{bmatrix}\\]\n\n_where \\(\\Delta Y_{i}^{pre(g)}(0)\\), \\(\\Delta\\theta^{pre(g)}\\), and \\(\\Delta e_{i}^{pre(g)}\\) are all \\((g- "], "nougat": ["_To conclude this section, notice that the main cost of our approach is that we are not able to identify as many group-time average treatment effects as would be possible using other identification strategies. For example, \\(-\\)_**\"color [RGB]\u20130; 0; 0\"callaway-karami-2023 \u030b** _suppose that the researcher has access to a time-invariant covariate that does not affect the path of untreated potential outcomes and show that this sort of covariate can be used to generate additional moment conditions to identify the parameters in the model for untreated potential outcomes. Our approach does not require this sort of covariate (which is a key advantage of our approach), but it comes at the cost of only being able to identify ATT without recovering ATT\\((3,4)\\) or ATT\\((4,4)\\) which would be feasible using the approach in \\(-\\)_**\"color [RGB]\u20130; 0\"callaway-karami-2023''**_._\n\n#### 2.2.2 General Case\n\n_In this section, we extend the results above to a setting with more periods, more groups, and allow for more interactive fixed effects. The arguments in this section target recovering ATT\\((g,t)\\) for a particular group \\(g\\in\\bar{\\mathcal{G}}\\). Given the model in Assumption 4, after taking first differesences, we have that_\n\n\\[\\Delta Y_{it}(0)=\\Delta\\theta_{t}+\\lambda_{i}^{\\prime}\\Delta F_{t}+\\Delta e_{it}\\]\n\n_which eliminates the unit fixed effect \\(\\eta_{i}\\). Next, define_\n\n\\[\\Delta Y_{i}(0):=\\begin{bmatrix}\\Delta Y_{i2}\\\\ \\vdots\\\\ \\Delta Y_{i\\mathcal{T}}\\end{bmatrix};\\quad\\Delta\\theta:=\\begin{bmatrix}\\Delta \\theta_{2}\\\\ \\vdots\\\\ \\Delta\\theta_{\\mathcal{T}}\\end{bmatrix};\\quad\\mathbf{\\Delta F}:=\\begin{bmatrix} \\Delta F_{2}^{\\prime}\\\\ \\vdots\\\\ \\Delta F_{\\mathcal{T}}^{\\prime}\\end{bmatrix};\\text{ and }\\Delta e_{i}:=\\begin{bmatrix} \\Delta e_{i2}\\\\ \\vdots\\\\ \\Delta e_{i\\mathcal{T}}\\end{bmatrix}\\]\n\n_where \\(\\Delta Y_{i}(0)\\), \\(\\Delta\\theta\\), and \\(\\Delta e_{i}\\) are all \\((\\mathcal{T}-1)\\) dimensional vectors, and \\(\\mathbf{\\Delta F}\\) is \\((\\mathcal{T}-1)\\times R\\) matrix. Similarly, define_\n\n\\[\\Delta Y_{i}^{pre(g)}(0):=\\begin{bmatrix}\\Delta Y_{i2}\\\\ \\vdots\\\\ \\Delta Y_{ig-1}\\end{bmatrix};\\quad\\Delta\\theta^{pre(g)}:=\\begin{bmatrix}\\Delta \\theta_{2}\\\\ \\vdots\\\\ \\Delta\\theta_{g-1}\\end{bmatrix};\\quad\\mathbf{\\Delta F}^{pre(g)}:=\\begin{bmatrix} \\Delta F_{2}^{\\prime}\\\\ \\vdots\\\\ \\Delta F_{g-1}^{\\prime}\\end{bmatrix};\\text{ and }\\Delta e_{i}^{pre(g)}:=\\begin{bmatrix} \\Delta e_{i2}\\\\ \\vdots\\\\ \\Delta e_{ig-1}\\end{bmatrix}\\]\n\n_where \\(\\Delta Y_{i}^{pre(g)}(0)\\), \\(\\Delta\\theta^{pre(g)}\\), and \\(\\Delta e_{i}^{pre(g)}\\) are all \\((g-2)\\times 1\\) vectors "]}, {"edit": ["subject to the dynamic participation constraints: for all \\(i\\) and all possible histories \\(h_{t}\\) with \\(t\\leq\\tau\\),\n\n\\[\\mathbb{E}\\left[e^{-r(\\tau-t)}g^{i}d_{\\tau}^{i}-\\int_{0}^{\\tau}e^{-r(s-t)}c^{i} \\left(X_{T^{i}(s)}^{i}\\right)dT^{i}(s)\\mid h_{t}\\right]\\geq 0.\\]\n\nAs a first step in the proof, I consider the relaxed problem in which the principal can randomize over possible stopping point. To introduce it formally, I need to define a number of new objects. For a filtration \\(\\mathcal{H}=\\left\\{\\mathcal{H}_{t}\\right\\}_{t\\geq 0}\\), define the set of \\(\\mathcal{H}\\)**-randomized** stopping times as\n\n\\[\\mathcal{S}\\left(\\mathcal{H}\\right)\\coloneqq\\left\\{S\\in\\mathcal{N}_{0}^{\\infty }(\\mathcal{H})\\,:\\,dS\\in\\mathcal{M}_{+}^{\\infty}(\\mathcal{H}),\\,S_{0^{-}}=0, \\,S_{\\infty}\\leq 1\\right\\}.\\]\n\n\\(\\mathcal{N}_{0}^{\\infty}\\left(\\mathcal{H}\\right)\\) is the set of \\(\\mathcal{H}\\)-adapted process with values in \\([0,\\infty)\\) such that \\(n\\in\\mathcal{N}_{0}^{\\infty}\\left(\\mathcal{H}\\right)\\) if \\(n\\) has nondecreasing paths \\(\\mathbb{P}\\)-a.s.. \\(\\mathcal{M}_{+}^{\\infty}\\left(\\mathcal{H}\\right)\\) is the set of \\(\\mathcal{H}\\)-optional random measure. Observe that any randomized stopping time is equivalent to a \\(\\mathcal{F}_{t}\\otimes\\mathcal{B}([0,1])\\)-stopping time defined on the enlarged filtered probability space \\((\\Omega\\times[0,1],\\mathcal{H}\\times\\mathcal{B}([0,1]),\\left\\{\\mathcal{H}_{t} \\times\\mathcal{B}([0,1])\\right\\}_{t\\geq 0},\\mathbb{P}\\otimes\\lambda)\\), where \\(\\lambda\\) is the Lebesgue measure on \\([0,1]\\).25 Finally, let \\(\\mathcal{C}\\) be the set of \\(\\bar{\\mathcal{F}}\\)-measurable promotion rule:\n\nFootnote 25: See, for example, Camboni and Durandard (2022).\n\n\\[\\mathcal{C}\\coloneqq\\left\\{d\\,:\\,\\text{ for all }t\\geq 0,\\,d_{t}\\text{ is }\\bar{\\mathcal{F}}\\text{-measurable and }\\sum_{i=0}^{N}d_{t}^{i}=1\\ \\mathbb{P}\\text{-a.s.}\\right\\},\\]\n\nand \\(\\mathcal{C}^{*}\\) be the set of nondecreasing promotion rule:\n\n\\[\\mathcal{C}^{*}\\coloneqq\\left\\{d\\in\\mathcal{C}\\,:\\,d^{i}\\text{'s paths are cadlag and nondecreasing }\\mathbb{P}\\text{-a.s. for }i=1,\\ldots,N\\right\\}.\\]\n\nThe set of **randomized promotion contest** consists of all the promotion contests such that the promotion time \\(\\tau\\) is a randomized stopping time: "], "nougat": ["subject to the dynamic participation constraints: for all \\(i\\) and all possible histories \\(h_{t}\\) with \\(t\\leq\\tau\\),\n\n\\[\\mathbb{E}\\left[e^{-r(\\tau-t)}g^{i}d_{\\tau}^{i}-\\int_{0}^{\\tau}e^{-r(s-t)}c^{i} \\left(X_{T^{i}(s)}^{i}\\right)dT^{i}(s)\\mid h_{t}\\right]\\geq 0.\\]\n\nAs a first step in the proof, I consider the relaxed problem in which the principal can randomize over possible stopping point. To introduce it formally, I need to define a number of new objects. For a filtration \\(\\mathcal{H}=\\left\\{\\mathcal{H}_{t}\\right\\}_{t\\geq 0}\\), define the set of \\(\\mathcal{H}\\)**-randomized** stopping times as\n\n\\[\\mathcal{S}\\left(\\mathcal{H}\\right)\\coloneqq\\left\\{S\\in\\mathcal{N}_{0}^{ \\infty}(\\mathcal{H})\\,:\\,dS\\in\\mathcal{M}_{+}^{\\infty}(\\mathcal{H}),\\,S_{0^{- }}=0,\\,S_{\\infty}\\leq 1\\right\\}.\\]\n\n\\(\\mathcal{N}_{0}^{\\infty}\\left(\\mathcal{H}\\right)\\) is the set of \\(\\mathcal{H}\\)-adapted process with values in \\([0,\\infty)\\) such that \\(n\\in\\mathcal{N}_{0}^{\\infty}\\left(\\mathcal{H}\\right)\\) if \\(n\\) has nondecreasing paths \\(\\mathbb{P}\\)-a.s.. \\(\\mathcal{M}_{+}^{\\infty}\\left(\\mathcal{H}\\right)\\) is the set of \\(\\mathcal{H}\\)-optional random measure. Observe that any randomized stopping time is equivalent to a \\(\\mathcal{F}_{t}\\otimes\\mathcal{B}([0,1])\\)-stopping time defined on the enlarged filtered probability space \\((\\Omega\\times[0,1],\\mathcal{H}\\times\\mathcal{B}([0,1]),\\left\\{\\mathcal{H}_{t} \\times\\mathcal{B}([0,1])\\right\\}_{t\\geq 0},\\mathbb{P}\\otimes\\lambda)\\), where \\(\\lambda\\) is the Lebesgue measure on \\([0,1]\\).25 Finally, let \\(\\mathcal{C}\\) be the set of \\(\\bar{\\mathcal{F}}\\)-measurable promotion rule:\n\nFootnote 25: See, for example, Camboni and Durandard (2022).\n\n\\[\\mathcal{C}\\coloneqq\\left\\{d\\,:\\,\\text{ for all }t\\geq 0,\\,d_{t}\\text{ is }\\bar{\\mathcal{F}}\\text{-measurable and }\\sum_{i=0}^{N}d_{t}^{i}=1\\,\\,\\mathbb{P}\\text{-a.s.}\\right\\},\\]\n\nand \\(\\mathcal{C}^{*}\\) be the set of nondecreasing promotion rule:\n\n\\[\\mathcal{C}^{*}\\coloneqq\\left\\{d\\in\\mathcal{C}\\,:\\,d^{i}\\text{'s paths are cadlag and nondecreasing }\\mathbb{P}\\text{-a.s. for }i=1,\\ldots,N\\right\\}.\\]\n\nThe set of **randomized promotion contest** consists of all the promotion contests such that the promotion time \\(\\tau\\) is a randomized stopping time "]}, {"edit": ["Condition MOM is imposed to allow for \\(X_{i1}\\varepsilon_{1}\\) and \\(X_{i1}X_{j1}\\), \\(i,j\\in[p]\\), to not be sub-Gaussian, sub-exponential random variables. In financial settings, the state variables and idiosyncratic errors may be heavy-tailed, such that the sub-Gaussianity or sub-exponentiality property is restrictive. Condition MOM may be rewritten to allow for different order of moment bounds on the random variables \\(X_{i1}\\varepsilon_{1}\\) and \\(X_{i1}X_{j1}\\), \\(i,j\\in[p]\\), at the cost of more involved notation. This condition is also weaker than imposing the sub-Weibull property of Wong et al. (2020).\n\n**Condition R.** Let \\(\\bar{\\beta}(k)\\leq Bk^{-\\mu}\\), for some constants \\(B\\geq 0\\) and \\(\\mu>2\\). Moreover, let \\(a_{T}=[T^{1/(1+\\mu^{\\prime})}]\\) and \\(d_{T}=[T/(2a_{T})]\\) for some \\(\\mu^{\\prime}\\in(0,\\mu)\\). Finally,\n\n1. \\(pd_{T}a_{T}^{-\\mu}=o(1)\\) if \\(B>0\\);\n2. \\(s_{0}\\frac{(pa_{T})^{2/q}}{d_{T}^{(q-1)/q}}=o(1)\\);\n3. \\(s_{0}\\frac{\\sqrt{\\log(pa_{T})}}{\\sqrt{d_{T}}}=o(1)\\);\n4. \\(\\|\\hat{Y}-\\tilde{Y}\\|_{2}^{2}/T=o_{P}(1)\\).\n\nFinally, Condition R imposes rates on the parameters \\(s_{0}\\), \\(p\\), \\(T\\), and the prediction error \\(\\|\\hat{Y}-\\tilde{Y}\\|_{2}^{2}/T\\). Conditions R.1 is a consequence of the time-series features of the data and need not be imposed for iid data. Condition R.2 reflects the polynomial tail part in the Fuk-Nagaev inequality for the maxima of high-dimensional sums and is required if \\(X_{i1}\\varepsilon_{1}\\) and \\(X_{i1}X_{j1}\\) are not sub-Gaussian or sub-exponential random variables, for any \\(i,j\\in[p]\\). From Condition MOM we observe that, if the value of \\(q\\) increases, such that the random variables satisfy stricter moment conditions, Condition R.2 becomes less restrictive on the rates of \\(s_{0}\\) and \\(p\\). Condition R.3 is the standard exponential rate imposed in the LASSO problem if \\(X_{i1}\\varepsilon_{1}\\) and \\(X_{i1}X_{j1}\\) are iid sub-Gaussian or sub-exponential random variables, for each \\(i,j\\in[p]\\). Finally, Condition R.4 requires that the estimation error in the auxiliary conditional variable converges to zero in probability.\n\nCondition R introduces the terms \\(a_{T}\\) and \\(d_{T}\\), which depend on \\(T\\) and a parameter \\(\\mu^{\\prime}\\). Moreover, the parameter \\(\\mu\\) describes the dependence in the data and is large if dependence is low. The value of \\(\\mu^{\\prime}\\) may be chosen close to \\(\\mu\\), such that \\(d_{T}\\) may be set approximately equal to \\(T\\) if dependence is low. Given the choice of penalty parameter "], "nougat": ["Condition MOM is imposed to allow for different order of moment bounds on the random variables \\(X_{i1}\\varepsilon_{1}\\) and \\(X_{i1}X_{j1}\\), \\(i,j\\in[p]\\), at the cost of more involved notation. This condition is also weaker than imposing the sub-Weibull property of Wong et al. (2020).\n\n**Condition R.** Let \\(\\bar{\\beta}(k)\\leq Bk^{-\\mu}\\), for some constants \\(B\\geq 0\\) and \\(\\mu>2\\). Moreover, let \\(a_{T}=[T^{1/(1+\\mu^{\\prime})}]\\) and \\(d_{T}=\\lfloor T/(2a_{T})\\rfloor\\) for some \\(\\mu^{\\prime}\\in(0,\\mu)\\). Finally,\n\n1. \\(pd_{T}a_{T}^{-\\mu}=o(1)\\) if \\(B>0\\);\n2. \\(s_{0}\\frac{(pa_{T})^{2/q}}{d_{T}^{(q-1)/q}}=o(1)\\);\n3. \\(s_{0}\\frac{\\sqrt{\\log(pa_{T})}}{\\sqrt{d_{T}}}=o(1)\\);\n4. \\(\\|\\hat{Y}-\\tilde{Y}\\|_{2}^{2}/T=o_{P}(1)\\).\n\nFinally, Condition R imposes rates on the parameters \\(s_{0}\\), \\(p\\), \\(T\\), and the prediction error \\(\\|\\hat{Y}-\\tilde{Y}\\|_{2}^{2}/T\\). Conditions R.1 is a consequence of the time-series features of the data and need not be imposed for iid data. Condition R.2 reflects the polynomial tail part in the Fuk-Nagaev inequality for the maxima of high-dimensional sums and is required if \\(X_{i1}\\varepsilon_{1}\\) and \\(X_{i1}X_{j1}\\) are not sub-Gaussian or sub-exponential random variables, for any \\(i,j\\in[p]\\). From Condition MOM we observe that, if the value of \\(q\\) increases, such that the random variables satisfy stricter moment conditions, Condition R.2 becomes less restrictive on the rates of \\(s_{0}\\) and \\(p\\). Condition R.3 is the standard exponential rate imposed in the LASSO problem if \\(X_{i1}\\varepsilon_{1}\\) and \\(X_{i1}X_{j1}\\) are iid sub-Gaussian or sub-exponential random variables, for each \\(i,j\\in[p]\\). Finally, Condition R.4 requires that the estimation error in the auxiliary conditional variable converges to zero in probability.\n\nCondition R introduces the terms \\(a_{T}\\) and \\(d_{T}\\), which depend on \\(T\\) and a parameter \\(\\mu^{\\prime}\\). Moreover, the parameter \\(\\mu\\) describes the dependence in the data and is large if dependence is low. The value of \\(\\mu^{\\prime}\\) may be chosen close to \\(\\mu\\), such that \\(d_{T}\\) may be set approximately equal to \\(T\\) if dependence is low. Given the choice of penalty parameter \\(\\lambda\\) below, there is a trade-off between the convergence rates of the estimator and predictions (see Lemma 1) and the rates of the parameters. The convergence rate improves for large \\(d_{T}\\) and small \\(a_{T}\\), such that we like to choose \\(\\mu^{\\prime}\\) large. On the other hand, Condition R.1 restricts \\(p\\) more for "]}, {"edit": ["By considering these key takeaways, organizations can learn from the experiences of Pinterest and Prime Video, and apply similar strategies to optimize their systems for cost efficiency while ensuring high performance and quality. A proactive and iterative cost optimization approach, combined with a thorough understanding of system requirements, can result in significant cost savings and improved overall efficiency.\n\n## 5 Future Research Considerations\n\nThe cost optimization techniques as well as case studies of Pinterest and Amazon Prime Video discussed in this paper offer valuable insights into cost optimization strategies and architectural improvements. These discoveries lay the groundwork for future research in the fields of cost optimization and system scalability. Here are some possible areas for further investigation:\n\n#### 5.0.1 Automated System Monitoring and Optimization\n\nOne area of focus for the future research could be the creation of automated monitoring and optimization frameworks. Future research could look into developing frameworks that continuously analyze system performance, identify inefficiencies, and recommend changes to resource allocation and job configurations. Using artificial intelligence and machine learning techniques, the optimization process can be automated, allowing systems to adapt and optimize in real-time.\n\n#### 5.0.2 Advanced Resource Allocation Techniques\n\nFuture research could focus on advanced resource allocation techniques that go beyond traditional methods. Exploring machine learning algorithms or optimization models to dynamically allocate resources based on workload characteristics, job priorities, and cost constraints could be part of this. To optimize resource utilization and reduce costs, techniques such as predictive resource allocation and proactive resource provisioning can be investigated.\n\n#### 5.0.3 Cost-Performance Trade-off Analysis\n\nFurther research can be carried out to conduct in-depth studies on the trade-off between cost optimization and system performance. It is critical to assess the impact of various cost-cutting measures on overall system performance, user experience, and quality. Researchers can develop methodologies and models that strike a balance between cost reduction and maintaining or improving service quality by understanding the complexities of this trade-off.\n\n#### 5.0.4 Adaptive Scaling and Bursting\n\nAdaptive scaling techniques, which allow systems to dynamically adjust resource allocation based on real-time workload demands, could be the focus of future research. Exploring strategies for efficient bursting during peak times and scaling down during off-peak times can help to "], "nougat": ["By considering these key takeaways, organizations can learn from the experiences of Pinterest and Prime Video, and apply similar strategies to optimize their systems for cost efficiency while ensuring high performance and quality. A proactive and iterative cost optimization approach, combined with a thorough understanding of system requirements, can result in significant cost savings and improved overall efficiency.\n\n## 5 Future Research Considerations\n\nThe cost optimization techniques as well as case studies of Pinterest and Amazon Prime Video discussed in this paper offer valuable insights into cost optimization strategies and architectural improvements. These discoveries lay the groundwork for future research in the fields of cost optimization and system scalability. Here are some possible areas for further investigation:\n\n#### 5.0.1 Automated System Monitoring and Optimization\n\nOne area of focus for the future research could be the creation of automated monitoring and optimization frameworks. Future research could look into developing frameworks that continuously analyze system performance, identify inefficiencies, and recommend changes to resource allocation and job configurations. Using artificial intelligence and machine learning techniques, the optimization process can be automated, allowing systems to adapt and optimize in real-time.\n\n#### 5.0.2 Advanced Resource Allocation Techniques\n\nFuture research could focus on advanced resource allocation techniques that go beyond traditional methods. Exploring machine learning algorithms or optimization models to dynamically allocate resources based on workload characteristics, job priorities, and cost constraints could be part of this. To optimize resource utilization and reduce costs, techniques such as predictive resource allocation and proactive resource provisioning can be investigated.\n\n#### 5.0.3 Cost-Performance Trade-off Analysis\n\nFurther research can be carried out to conduct in-depth studies on the trade-off between cost optimization and system performance. It is critical to assess the impact of various cost-cutting measures on overall system performance, user experience, and quality. Researchers can develop methodologies and models that strike a balance between cost reduction and maintaining or improving service quality by understanding the complexities of this trade-off.\n\n#### 5.0.4 Adaptive Scaling and Bursting\n\nAdaptive scaling techniques, which allow systems to dynamically adjust resource allocation based on real-time workload demands, could be the focus of future research. Exploring strategies for efficient bursting during peak times and scaling down during off-peak times can help to "]}, {"edit": ["taxi market.\n\nThe t-test in Table 11 and Table 12 show a significant higher unanticipated transitory wage for both yellow and green taxis than their base year. The relationship between TNC trips and unanticipated transitory wage variation can be seen in Figure 7. The results indicate that drivers are facing more uncertainty in earning opportunities and clearly illustrate the change of drivers' labor supply behavior over time due to the increasing number of TNC trips. Therefore, we reject the third hypothesis and conduct that RDP behavior presents among taxi drivers with the increasing number of TNC trips. Based on the results, we observe that yellow taxi drivers face much more serious competition than green taxi drivers before 2017. Moreover, such competition leads to an unsustainable state in the ride-sharing market and results in 20% yellow taxi drivers having RDP behavior at the end of 2017, which means a high proportion of taxi drivers lose confidence in the taxi industry. Finally, yellow taxi drivers quit the taxi market. Besides, the green taxi market benefits from the increase in demand at the beginning of TNCs' growth. Meanwhile, green taxi drivers present NS behavior before July 2017. However, the unanticipated transitory wage variation of green taxi drivers is found to increase after June 2017 and account for over 20% of the total wage variation in both experiments at the end of 2018, which is over three times as compared to when the taxi market is still monopolistic. Consequently, the RDP behavior should not be ignored, and NS behavior is no longer suitable to interpret the total taxi drivers' work behavior in a competitive market. Instead, at least 20% of green taxi drivers perform in a loss-aversion manner in the market rather than the revenue-maximizing behavior, which is widely used when the taxi market is still monopolistic. This finding is aligned with the second explanation for the question that we raised to the OLS model. That is, the driver has a specific reference target. Moreover, the fact that individual labor supply is found to be barely unaffected (see Figure 8) while their monthly income is found to be significantly decreased can be explained by the co-existence of NS and the RDP behavior in the market. Furthermore, combining the examinations of the second and third hypotheses, we conclude that drivers decrease their income target and some of them even quit the market, so that the remaining drivers are observed to still serve the same amount of work hours. It points out the "], "nougat": ["taxi market.\n\nThe t-test in Table 11 and Table 12 show a significant higher unanticipated transitory wage for both yellow and green taxis than their base year. The relationship between TNC trips and unanticipated transitory wage variation can be seen in Figure 7. The results indicate that drivers are facing more uncertainty in earning opportunities and clearly illustrate the change of drivers\u2019 labor supply behavior over time due to the increasing number of TNC trips. Therefore, we reject the third hypothesis and conduct that RDP behavior presents among taxi drivers with the increasing number of TNC trips. Based on the results, we observe that yellow taxi drivers face much more serious competition than green taxi drivers before 2017. Moreover, such competition leads to an unsustainable state in the ride-sharing market and results in 20% yellow taxi drivers having RDP behavior at the end of 2017, which means a high proportion of taxi drivers lose confidence in the taxi industry. Finally, yellow taxi drivers quit the taxi market. Besides, the green taxi market benefits from the increase in demand at the beginning of TNCs\u2019 growth. Meanwhile, green taxi drivers present NS behavior before July 2017. However, the unanticipated transitory wage variation of green taxi drivers is found to increase after June 2017 and account for over 20% of the total wage variation in both experiments at the end of 2018, which is over three times as compared to when the taxi market is still monopolistic. Consequently, the RDP behavior should not be ignored, and NS behavior is no longer suitable to interpret the total taxi drivers\u2019 work behavior in a competitive market. Instead, at least 20% of green taxi drivers perform in a loss-aversion manner in the market rather than the revenue-maximizing behavior, which is widely used when the taxi market is still monopolistic. This finding is aligned with the second explanation for the question that we raised to the OLS model. That is, the driver has a specific reference target. Moreover, the fact that individual labor supply is found to be barely unaffected (see Figure 8) while their monthly income is found to be significantly decreased can be explained by the co-existence of NS and the RDP behavior in the market. Furthermore, combining the examinations of the second and third hypotheses, we conclude that drivers decrease their income target and some of them even quit the market, so that the remaining drivers are observed to still serve the same amount of work hours. It points out the "]}, {"edit": ["information disclosure and whether hidden equal-sharing or public winner-take-all contests result in a higher probability of breakthrough for the principal, not on whether the outcome is necessarily socially efficient. Instead, this paper focuses on payoff characterizations that result in social efficiency, rather than maximizing total effort, which was the principal's objective in Halac et al. (2017).\n\nThis paper is also related to the literature on efficient dynamic mechanism design. This literature primarily focuses on the social choice setting. Bergemann & Valimaki (2002) study the incentives for agents to acquire information about their own types in a static social choice setting, and Bergemann & Valimaki (2010) formulates a dynamic pivot mechanism. More relatedly, Athey & Segal (2013) propose a VCG-like mechanism for social choice in a dynamic environment that is also budget-balanced. This paper differs from these other papers in that these other papers require an assumption of private values, which fails when there are explicit payoff externalities resulting from experimentation. Although the Athey & Segal (2013) model can capture informational externalities via the evolution of the belief stochastic process, the payoff externalities in this paper fail the assumption of private values because they introduce interdependence in the instantaneous incentives.\n\nIndeed, in the presence of payoff interdependence, Jehiel & Moldovanu (2001) show that even in a static setting, efficiency may not attain. In such environments, Mezzetti (2004) shows that requiring transfers after uncertainty resolution can restore efficiency. However, those insights do not apply to the dynamic experimentation setting; because experimentation stops with positive probability, there are outcomes where the state of the world never fully realizes to the agents.\n\n## 2 Game Structure\n\nI first formally lay out the baseline structure of the research game.\n\nExpositionThere are \\(N\\) agents \\(i\\in\\{1,2,\\ldots N\\}\\) investigating a potential research breakthrough. The research idea is good or bad, which is drawn by Nature prior to the start of the game and unobserved by the agents. Formally, the quality of the research idea is the state of the world, \\(\\omega\\in\\Omega:=\\{\\text{good},\\text{bad}\\}\\). Nature draws the state of the world to be good with probability \\(p(0)\\), which is the initial prior belief shared by the agents on the state of the world. Time is continuous, \\(t\\in[0,\\infty)\\), and at every instant of time, each agent is endowed with a unit measure of a resource (effort) that it allocates over two projects, the status quo "], "nougat": ["information disclosure and whether hidden equal-sharing or public winner-take-all contests result in a higher probability of breakthrough for the principal, not on whether the outcome is necessarily socially efficient. Instead, this paper focuses on payoff characterizations that result in social efficiency, rather than maximizing total effort, which was the principal\u2019s objective in Halac et al. (2017).\n\nThis paper is also related to the literature on efficient dynamic mechanism design. This literature primarily focuses on the social choice setting. Bergemann & V \u0308alim \u0308aki (2002) study the incentives for agents to acquire information about their own types in a static social choice setting, and Bergemann & V \u0308alim \u0308aki (2010) formulates a dynamic pivot mechanism. More relatedly, Athey & Segal (2013) propose a VCG-like mechanism for social choice in a dynamic environment that is also budget-balanced. This paper differs from these other papers in that these other papers require an assumption of private values, which fails when there are explicit payoff externalities resulting from experimentation. Although the Athey & Segal (2013) model can capture informational externalities via the evolution of the belief stochastic process, the payoff externalities in this paper fail the assumption of private values because they introduce interdependence in the instantaneous incentives.\n\nIndeed, in the presence of payoff interdependence, Jehiel & Moldovanu (2001) show that even in a static setting, efficiency may not attain. In such environments, Mezzetti (2004) shows that requiring transfers after uncertainty resolution can restore efficiency. However, those insights do not apply to the dynamic experimentation setting; because experimentation stops with positive probability, there are outcomes where the state of the world never fully realizes to the agents.\n\n## 2 Game Structure\n\nI first formally lay out the baseline structure of the research game.\n\nExpositionThere are \\(N\\) agents \\(i\\in\\{1,2,\\ldots N\\}\\) investigating a potential research breakthrough. The research idea is good or bad, which is drawn by Nature prior to the start of the game and unobserved by the agents. Formally, the quality of the research idea is the state of the world, \\(\\omega\\in\\Omega:=\\{\\text{good},\\text{bad}\\}\\). Nature draws the state of the world to be good with probability \\(p(0)\\), which is the initial prior belief shared by the agents on the state of the world. Time is continuous, \\(t\\in[0,\\infty)\\), and at every instant of time, each agent is endowed with a unit measure of a resource (effort) that it allocates over two projects, the status quo "]}, {"edit": ["Introduction\n\nMeasuring income inequality has been a challenging task, as each of the indices used for the purpose attempt to condense the complexities of populations into just one number. Among the many indices, we have the Atkinson, Bonferroni, Gini, Palma, Pietra, Theil, and Zenga indices, to name just a few associated with the names of their inventors. Many treatises have been written on the topic, such as the handbook by Atkinson and Bourguignon (2000, 2015), which also contains many references to earlier studies, and they are voluminous.\n\nThe indices are often the areas under certain income-equality curves, which are considerably more difficult to present and explain to the general audience, let alone to easily compare. For example, the Gini index of inequality is 1 minus twice the area under the Lorenz curve. (We shall give mathematical definitions later in this paper.) The curves and thus the indices are based on comparing the mean income of the poor with other means, such as the mean income of the entire population, the mean income of the nonpoor, and the mean income of the rich, whatever the definitions of \"poor\" and \"rich\" might be. Hence, to be well defined, the curves and the indices inevitably assume that the mean of the underlying population is finite. With the rising income inequality, and thus with the distribution of incomes becoming more skewed and heavily tailed, researchers have therefore sought other ways for measuring inequality.\n\nGastwirth (2014) proposed to use the median instead of the mean when \"normalizing\" the absolute Gini mean difference, widely known as the GMD. The author noted, however, that the proposed index might fall outside the class of normalized indices because it compares the _mean_ income of the poor with the _median_ income of the entire population. There is a natural remedy to this normalization issue: compare the _median_ income of the poor with the _median_ of the population. Even more, we can compare the median income of the poor with the median of the \"not poor\" or, for example, with the median of the rich, whatever the latter might mean. This is the path that we take in this paper to arrive at the indices to be formally introduced in the next section.\n\nIn this regard we wish to mention the study of Bennett and Zitikis (2015) where it is shown that a number of classical indices of income inequality arise naturally from a Harsanyi-inspired model of choice under risk, with persons acting as _reference-dependent_ expected-utility maximizers in the face of an income quantile lottery, thus giving rise to a reinterpretation of the classical indices as measures of the desirability of redistribution in society. This relativistic approach to constructing indices of income inequality was further explored by Greselin and Zitikis (2018), although more from the modeller's perspective than "], "nougat": ["Introduction\n\nMeasuring income inequality has been a challenging task, as each of the indices used for the purpose attempt to condense the complexities of populations into just one number. Among the many indices, we have the Atkinson, Bonferroni, Gini, Palma, Pietra, Theil, and Zenga indices, to name just a few associated with the names of their inventors. Many treatises have been written on the topic, such as the handbook by Atkinson and Bourguignon ( 2000 , 2015 ), which also contains many references to earlier studies, and they are voluminous.\n\nThe indices are often the areas under certain income-equality curves, which are considerably more difficult to present and explain to the general audience, let alone to easily compare. For example, the Gini index of inequality is 1 minus twice the area under the Lorenz curve. (We shall give mathematical definitions later in this paper.) The curves and thus the indices are based on comparing the mean income of the poor with other means, such as the mean income of the entire population, the mean income of the nonpoor, and the mean income of the rich, whatever the definitions of \u201cpoor\u201d and \u201crich\u201d might be. Hence, to be well defined, the curves and the indices inevitably assume that the mean of the underlying population is finite. With the rising income inequality, and thus with the distribution of incomes becoming more skewed and heavily tailed, researchers have therefore sought other ways for measuring inequality. Gastwirth ( 2014 ) proposed to use the median instead of the mean when \u201cnormalizing\u201d the absolute Gini mean difference, widely known as the GMD. The author noted, however, that the proposed index might fall outside the class of normalized indices because it compares the _mean_ income of the poor with the _median_ income of the entire population. There is a natural remedy to this normalization issue: compare the _median_ income of the poor with the _median_ of the population. Even more, we can compare the median income of the poor with the median of the \u201cnot poor\u201d or, for example, with the median of the rich, whatever the latter might mean. This is the path that we take in this paper to arrive at the indices to be formally introduced in the next section.\n\nIn this regard we wish to mention the study of Bennett and Zitikis ( 2015 ) where it is shown that a number of classical indices of income inequality arise naturally from a Harsanyi-inspired model of choice under risk, with persons acting as _reference-dependent_ expected-utility maximizers in the face of an income quantile lottery, thus giving rise to a reinterpretation of the classical indices as measures of the desirability of redistribution in society. This relativistic approach to constructing indices of income inequality was further explored by Greselin and Zitikis ( 2018 ), although more from the modeller\u2019s perspective than "]}, {"edit": ["\n\n# Large Language Models at Work in China's Labor Market\n\n Qin Chen\\({}^{1}\\)1, Jinfeng Ge\\({}^{2*}\\), Huaqing Xie\\({}^{2*}\\),\n\n**Xingcheng Xu\\({}^{2*}\\), Yanqing Yang\\({}^{2,3*}\\)2,**\n\nMetroData\\({}^{1}\\)\n\nShanghai Artificial Intelligence Laboratory\\({}^{2}\\)\n\nFudan Univeristy\\({}^{3}\\)\n\nqinchen1986@hotmail.com, {gejinfeng,xiehuaqing}@pjlab.org.cn,\n\nxingcheng.xu18@gmail.com, yanqingyang@fudan.edu.cn\n\nThese authors contributed equally.Corresponding author.\n\nFootnote 1: footnotemark:\n\n###### Abstract\n\nThis paper explores the potential impacts of large language models (LLMs) on the Chinese labor market. We analyze occupational exposure to LLM capabilities by incorporating human expertise and LLM classifications, following Eloundou et al. (2023)'s methodology. We then aggregate occupation exposure to the industry level to obtain industry exposure scores. The results indicate a positive correlation between occupation exposure and wage levels/experience premiums, suggesting higher-paying and experience-intensive jobs may face greater displacement risks from LLM-powered software. The industry exposure scores align with expert assessments and economic intuitions. We also develop an economic growth model incorporating industry exposure to quantify the productivity-employment trade-off from AI adoption. Overall, this study provides an analytical basis for understanding the labor market impacts of increasingly capable AI systems in China. Key innovations include the occupation-level exposure analysis, industry aggregation approach, and economic modeling incorporating AI adoption and labor market effects. The findings will inform policymakers and businesses on strategies for maximizing the benefits of AI while mitigating adverse disruption risks.\n\n## Introduction\n\nThe recent remarkable progress in the field of generative AI and large language models (LLMs) [1, 13] has provoke many pressing questions about the effects of these powerful technologies on the economy. One of the most significant questions surrounding advances in Generative AI and LLMs is the impact these technologies will have on the dynamics of labor market due to the influence of LLMs on labor inputs. A branch of research emphasizing on the disruptive labor market impacts of LLMs is emerging quickly, however, it predominantly focuses attention on the labor market in developed economy, in particularly U.S. [1, 14, 15, 16, 17, 18]. Nevertheless, countries differ in their labor market structures such as occupation and industry composition. Even for the same occupation, the detailed task composition or work content may show great discrepancies across countries. Therefore, this paper analyzes the potential impacts of LLMs on China's labor market. To construct our primary exposure index, we use a recently developed methodology to systematically assess which occupations are most exposed to advances in LLMs in China. Specifically, we employ three large language models - GPT4 (OpenAI 2023), InternLM (InternLM-Team 2023), and GLM [14, 15] - as classifiers to determine the occupational exposure based on the detailed description for each occupation contained in the general code of occupational classification of the People's Republic of China. We also employ expert annotators to explore the impacts of LLMs, to make comparisons, and to shed more light on this issue.\n\nWe then characterize the profile of occupational exposure to LLMs based on characteristics of the occupation in China. Our analysis indicates that the impacts of LLMs on China's labor market are likely to be pervasive and diverse. The results show a great heterogeneous exposure across occupations and suggest that more educated, relatively high-paid, white-collar occupations may be most exposed to LLMs. In addition to the positive correlation between wage, education and occupational exposure, we also find a positive correlation between experience premium and exposure to LLMs, implying a potential diminishing return of learning by doing in the future. We analyze exposure by industry and discover that the education and healthcare industries exhibit high exposure, while manufacturing, agriculture, mining, and construction demonstrate lower exposure. Unlike other developed countries, that young and old cohort distribute highly unevenly across industries in China demonstrate a great variance of demographic exposure of LLMs. Relatively LLMs inevitably exert a higher impact on young age employers.\n\nOur analysis also indicates that LLMs will have a greater impact on labor demand. We utilize an online job postings dataset from January"], "nougat": ["\n\n# Large Language Models at Work in China\u2019s Labor Market\n\n Qin Chen\\({}^{1}\\)1, Jinfeng Ge\\({}^{2*}\\), Huaqing Xie\\({}^{2*}\\),\n\n**Xingcheng Xu\\({}^{2*}\\), Yanqing Yang\\({}^{2,3*}\\)2,**\n\nMetroData\\({}^{1}\\)\n\nShanghai Artificial Intelligence Laboratory\\({}^{2}\\)\n\nFudan Univeristy\\({}^{3}\\)\n\nqinchen1986@hotmail.com, {gejinfeng,xiehuaqing}@pjlab.org.cn,\n\nxingcheng.xu18@gmail.com, yanqingyang@fudan.edu.cn\n\nThese authors contributed equally.Corresponding author.\n\nFootnote 1: footnotemark:\n\n###### Abstract\n\nThis paper explores the potential impacts of large language models (LLMs) on the Chinese labor market. We analyze occupational exposure to LLM capabilities by incorporating human expertise and LLM classifications, following Eloundou et al. (2023)'s methodology. We then aggregate occupation exposure to the industry level to obtain industry exposure scores. The results indicate a positive correlation between occupation exposure and wage levels/experience premiums, suggesting higher-paying and experience-intensive jobs may face greater displacement risks from LLM-powered software. The industry exposure scores align with expert assessments and economic intuitions. We also develop an economic growth model incorporating industry exposure to quantify the productivity-employment trade-off from AI adoption. Overall, this study provides an analytical basis for understanding the labor market impacts of increasingly capable AI systems in China. Key innovations include the occupation-level exposure analysis, industry aggregation approach, and economic modeling incorporating AI adoption and labor market effects. The findings will inform policymakers and businesses on strategies for maximizing the benefits of AI while mitigating adverse disruption risks.\n\n## Introduction\n\nThe recent remarkable progress in the field of generative AI and large language models (LLMs) [1, 13] has provoke many pressing questions about the effects of these powerful technologies on the economy. One of the most significant questions surrounding advances in Generative AI and LLMs is the impact these technologies will have on the dynamics of labor market due to the influence of LLMs on labor inputs. A branch of research emphasizing on the disruptive labor market impacts of LLMs is emerging quickly, however, it predominantly focuses attention on the labor market in developed economy, in particularly U.S. [1, 13, 14, 15, 16, 17]. Nevertheless, countries differ in their labor market structures such as occupation and industry composition. Even for the same occupation, the detailed task composition or work content may show great discrepancies across countries. Therefore, this paper analyzes the potential impacts of LLMs on China's labor market. To construct our primary exposure index, we use a recently developed methodology to systematically assess which occupations are most exposed to advances in LLMs in China. Specifically, we employ three large language models - GPT4 (OpenAI 2023), InternLM (InternLM-Team 2023), and GLM [13, 14] - as classifiers to determine the occupational exposure based on the detailed description for each occupation contained in the general code of occupational classification of the People's Republic of China. We also employ expert annotators to explore the impacts of LLMs, to make comparisons, and to shed more light on this issue.\n\nWe then characterize the profile of occupational exposure to LLMs based on characteristics of the occupation in China. Our analysis indicates that the impacts of LLMs on China's labor market are likely to be pervasive and diverse. The results show a great heterogeneous exposure across occupations and suggest that more educated, relatively high-paid, white-collar occupations may be most exposed to LLMs. In addition to the positive correlation between wage, education and occupational exposure, we also find a positive correlation between experience premium and exposure to LLMs, implying a potential diminishing return of learning by doing in the future. We analyze exposure by industry and discover that the education and healthcare industries exhibit high exposure, while manufacturing, agriculture, mining, and construction demonstrate lower exposure. Unlike other developed countries, that young and old cohort distribute highly unevenly across industries in China demonstrate a great variance of demographic exposure of LLMs. Relatively LLMs inevitably exert a higher impact on young age employers.\n\nOur analysis also indicates that LLMs will have a greater impact on labor demand. We utilize an online job postings dataset from January"]}, {"edit": ["Our rate explicitly accomodates for function classes of increasing complexity. If the sequence of functions has finite (albeit possibly increasing) VC dimension, then Theorem 2.6.7 of van der Vaart and Wellner (1996) shows that the \\(v_{n}\\) and \\(C_{n}\\) may be taken such that \\(C_{n}^{1/v_{n}}\\) is bounded. In this case, the complexity of the function class only affects the rate through the \\(\\sqrt{v_{n}}\\) term, where \\(v_{n}\\) scales linearly with the VC dimension.\n\nFinally, we consider the problem of kernel density estimation. In this case, the function class is the same for every \\(n\\in\\mathbb{N}\\). If the bandwidths are taken such that \\(a_{n}\\asymp b_{n}\\), then our rate simplifies to:\n\n\\[\\sqrt{-\\log(a_{n})}\\sqrt{\\log(n)\\vee-\\log(a_{n})}\\,.\\]\n\nIn contrast, Remark 2 in Einmahl and Mason (2005) shows that, in the iid setting, one can achieve the rate:\n\n\\[\\sqrt{\\log\\log(n)\\vee-\\log(a_{n})}\\,,\\]\n\nwhich coincides with our rate except for logarithmic factors. Note that, if we consider, as it is usually done in practice, polynomial bandwidths, i.e \\(a_{n}=Cn^{-\\alpha}\\) for \\(\\alpha>0\\), then our rate simplifies to \\(\\log(n)\\), whereas ?'s collapses to \\(\\sqrt{\\log(n)}\\).\n\n## 4. Concluding remarks\n\nIn this paper, we introduced a maximal inequality for the uniform estimation error of a local empirical process under strongly mixing data, where uniformity holds simultaneously over the function class, bandwidth and evaluation point. Our nonasymptotic bounds accomodate function classes with increasing complexity, which is a useful feature for \"high-dimensional\" statistical analyses. As an application, we computed our bounds to function classes that exhibit polynomial decay in their uniform entropy. When specialized to the kernel density estimation problem, these results show that our bound leads to the same optimal rates derived by Einmahl and Mason (2005) in the iid setting.\n\nMore generally, we view our results as a first step in the development of rigorous uniform inference tools in local estimation problems under weak dependence and data-driven bandwidths. Specifically, one may combine our results with couplings in the weakly dependent setting (e.g. 1, 2022) to devise test statistics that control size uniformly over the evaluation point \\(x\\). An example is the construction of uniform-in-\\(x\\) confidence bands for local polynomial quantile regression estimators with time series data. We intend to study such procedures in future research.\n\n "], "nougat": ["Our rate explicitly accomodates for function classes of increasing complexity. If the sequence of functions has finite (albeit possibly increasing) VC dimension, then Theorem 2.6.7 of van der Vaart and Wellner ( 1996 ) shows that the \\(v_{n}\\) and \\(C_{n}\\) may be taken such that \\(C_{n}^{1/v_{n}}\\) is bounded. In this case, the complexity of the function class only affects the rate through the \\(\\sqrt{v_{n}}\\) term, where \\(v_{n}\\) scales linearly with the VC dimension.\n\nFinally, we consider the problem of kernel density estimation. In this case, the function class is the same for every \\(n\\in\\mathbb{N}\\). If the bandwidths are taken such that \\(a_{n}\\asymp b_{n}\\), then our rate simplifies to:\n\n\\[\\sqrt{-\\log(a_{n})}\\sqrt{\\log(n)\\vee-\\log(a_{n})}\\,.\\]\n\nIn contrast, Remark 2 in Einmahl and Mason ( 2005 ) shows that, in the iid setting, one can achieve the rate:\n\n\\[\\sqrt{\\log\\log(n)\\vee-\\log(a_{n})}\\,,\\]\n\nwhich coincides with our rate except for logarithmic factors. Note that, if we consider, as it is usually done in practice, polynomial bandwidths, i.e \\(a_{n}=Cn^{-\\alpha}\\) for \\(\\alpha>0\\), then our rate simplifies to \\(\\log(n)\\), whereas ?'s collapses to \\(\\sqrt{\\log(n)}\\).\n\n## 4. Concluding remarks\n\nIn this paper, we introduced a maximal inequality for the uniform estimation error of a local empirical process under strongly mixing data, where uniformity holds simultaneously over the function class, bandwidth and evaluation point. Our nonasymptotic bounds accomodate function classes with increasing complexity, which is a useful feature for \u201chigh-dimensional\u201d statistical analyses. As an application, we computed our bounds to function classes that exhibit polynomial decay in their uniform entropy. When specialized to the kernel density estimation problem, these results show that our bound leads to the same optimal rates derived by Einmahl and Mason ( 2005 ) in the iid setting.\n\nMore generally, we view our results as a first step in the development of rigorous uniform inference tools in local estimation problems under weak dependence and data-driven bandwidths. Specifically, one may combine our results with couplings in the weakly dependent setting (e.g. 1, 2022) to devise test statistics that control size uniformly over the evaluation point \\(x\\). An example is the construction of uniform-in-\\(x\\) confidence bands for local polynomial quantile regression estimators with time series data. We intend to study such procedures in future research.\n\n "]}, {"edit": ["implications for the observed data. It is therefore useful to characterize a falsification test based on these implications to aid researchers to screen out assumptions rejected by the data. The test relies on the fact that the right-hand sides of the inequalities in Proposition 4 must provide a lower bound for a probability mass function if the model assumptions are correct. For any partition \\(\\mathcal{A}\\) of \\(\\mathcal{J}^{0}\\times\\mathcal{J}^{0}\\), we must have \\(\\sum_{A\\in\\mathcal{A}}\\mathbb{P}\\left[Q_{j}\\in A|S_{j}=c_{j}\\right]=1\\) for any given distribution in the sharp set of Proposition 4. Thus the same sum applied to the right-hand sides of the inequalities above must be less than or equal to one.\n\n**Corollary 2** (Model's Falsification Test).: _Assume the setup of Proposition 4, which presupposes that the model assumptions utilized to construct \\(\\mathbf{Q}_{j}\\) are true. Then, for any partition \\(\\mathcal{A}\\) of \\(\\mathcal{J}^{0}\\times\\mathcal{J}^{0}\\) we have that_\n\n\\[\\sum_{A\\in\\mathcal{A}}\\left\\{\\begin{array}{ll}&\\mathbb{I}\\left\\{A \\in\\mathbf{\\Lambda}_{j}^{\\cup+}\\cap\\mathbf{\\Lambda}_{j}^{\\cup-}\\right\\}\\max \\left\\{\\ \\mathbb{P}\\left[\\mathbf{Q}_{j}\\subseteq A|S_{j}=c_{j}^{+}\\right]\\ ;\\ \\mathbb{P}\\left[\\mathbf{Q}_{j}\\subseteq A|S_{j}=c_{j}^{-}\\right]\\ \\right\\}\\\\ &+\\,\\mathbb{I}\\left\\{A\\in\\mathbf{\\Lambda}_{j}^{\\cup+}\\setminus\\mathbf{\\Lambda}_{j}^{\\cup-}\\right\\}\\mathbb{P}\\left[\\mathbf{Q}_{j}\\subseteq A|S_{j}=c_{j}^{+}\\right]\\\\ &+\\,\\mathbb{I}\\left\\{A\\in\\mathbf{\\Lambda}_{j}^{\\cup-}\\setminus\\mathbf{\\Lambda}_{j}^{\\cup+}\\right\\}\\mathbb{P}\\left[\\mathbf{Q}_{j}\\subseteq A|S_{j}=c_{j}^{-}\\right]\\ \\ \\ \\ \\right\\}\\leqslant 1.\\end{array}\\right.\\]\n\nPartial identification of the distribution of local preferences allows us to bound the fraction of individuals near cutoff \\(c_{j}\\) that have \\(Q_{j}=(j,k)\\). The average outcome near the cutoff is a weighted average of average outcomes from two different groups: first, individuals with \\(Q_{j}=(j,k)\\), who interest us for the identification of treatment effects; and second, individuals with \\(Q_{j}\\neq(j,k)\\). The overall average is identified but not the average in each of the groups. A strictly positive lower bound on the fraction of individuals in the \\(Q_{j}=(j,k)\\) group allows us to construct lower and upper bounds on the average outcome for that group.\n\nStart with all individuals above and near cutoff \\(c_{j}\\) whose \\(\\mathbf{Q}_{j}\\) contain the comparable pair of interest, \\((j,k)\\in\\mathcal{P}\\). The fraction of those individuals that have \\(Q_{j}=(j,k)\\) equals to\n\n\\[\\delta_{j,k}^{+}=\\frac{\\mathbb{P}\\left[Q_{j}=(j,k)|S_{j}=c_{j}\\right]}{ "], "nougat": ["implications for the observed data. It is therefore useful to characterize a falsification test based on these implications to aid researchers to screen out assumptions rejected by the data. The test relies on the fact that the right-hand sides of the inequalities in Proposition 4 must provide a lower bound for a probability mass function if the model assumptions are correct. For any partition \\(\\mathcal{A}\\) of \\(\\mathcal{J}^{0}\\times\\mathcal{J}^{0}\\), we must have \\(\\sum_{A\\in\\mathcal{A}}\\mathbb{P}\\left[Q_{j}\\in A|S_{j}=c_{j}\\right]=1\\) for any given distribution in the sharp set of Proposition 4. Thus the same sum applied to the right-hand sides of the inequalities above must be less than or equal to one.\n\n**Corollary 2** (Model\u2019s Falsification Test).: _Assume the setup of Proposition 4, which presupposes that the model assumptions utilized to construct \\(\\mathbf{Q}_{j}\\) are true. Then, for any partition \\(\\mathcal{A}\\) of \\(\\mathcal{J}^{0}\\times\\mathcal{J}^{0}\\) we have that_\n\n\\[\\sum_{A\\in\\mathcal{A}}\\left\\{\\begin{array}{ll}&\\mathbb{I}\\left\\{A \\in\\mathbf{\\Lambda}_{j}^{\\cup+}\\cap\\mathbf{\\Lambda}_{j}^{\\cup-}\\right\\}\\max \\left\\{\\ \\mathbb{P}\\left[\\mathbf{Q}_{j}\\subseteq A|S_{j}=c_{j}^{+}\\right]\\ ;\\ \\mathbb{P}\\left[\\mathbf{Q}_{j}\\subseteq A|S_{j}=c_{j}^{-}\\right]\\ \\right\\}\\\\ &+\\,\\mathbb{I}\\left\\{A\\in\\mathbf{\\Lambda}_{j}^{\\cup+}\\setminus\\mathbf{\\Lambda}_{j}^{\\cup-}\\right\\}\\mathbb{P}\\left[\\mathbf{Q}_{j}\\subseteq A|S_{j}=c_{j}^{+}\\right]\\\\ &+\\,\\mathbb{I}\\left\\{A\\in\\mathbf{\\Lambda}_{j}^{\\cup-}\\setminus\\mathbf{\\Lambda}_{j}^{\\cup+}\\right\\}\\mathbb{P}\\left[\\mathbf{Q}_{j}\\subseteq A|S_{j}=c_{j}^{-}\\right]\\ \\ \\ \\ \\right\\}\\leqslant 1.\\end{array}\\right.\\]\n\nPartial identification of the distribution of local preferences allows us to bound the fraction of individuals near cutoff \\(c_{j}\\) that have \\(Q_{j}=(j,k)\\). The average outcome near the cutoff is a weighted average of average outcomes from two different groups: first, individuals with \\(Q_{j}=(j,k)\\), who interest us for the identification of treatment effects; and second, individuals with \\(Q_{j}\\neq(j,k)\\). The overall average is identified but not the average in each of the groups. A strictly positive lower bound on the fraction of individuals in the \\(Q_{j}=(j,k)\\) group allows us to construct lower and upper bounds on the average outcome for that group.\n\nStart with all individuals above and near cutoff \\(c_{j}\\) whose \\(\\mathbf{Q}_{j}\\) contain the comparable pair of interest, \\((j,k)\\in\\mathcal{P}\\). The fraction of those individuals that have \\(Q_{j}=(j,k)\\) equals to\n\n\\[\\delta_{j,k}^{+}=\\frac{\\mathbb{P}\\left[Q_{j}=(j,k)|S_{j}=c_{j}\\right]}{ "]}, {"edit": ["generic element of \\(\\mathcal{X}\\) by \\(x=(\\omega,a)\\). Thus, \\(\\bar{x}_{t}=(\\bar{\\omega}_{t},\\bar{a}_{t})\\in\\mathcal{X}\\) denotes the state-action pair at time \\(t\\), and \\(p(\\omega^{\\prime}|x)\\) with \\(x=(\\omega,a)\\) stands for \\(p(\\omega^{\\prime}|\\omega,a)\\). Next, for any \\(k\\geq 1\\) and at any time \\(t\\), a _slice_ of history \\(\\bar{h}_{t}^{k}\\) of length \\(k\\) describes the sequence of states-action pairs in the past \\(k\\) time periods: \\(\\bar{h}_{t}^{k}=(\\bar{x}_{t-k},\\ldots,\\bar{x}_{t-1})\\in\\mathcal{X}^{k}\\). We denote a generic element of \\(\\mathcal{X}^{k}\\) by \\(h^{k}=(x_{-k},\\ldots,x_{-1})\\). Finally, we let \\(\\mathcal{X}^{0}\\) denote the singleton set consisting of the unique (empty) slice of history of length zero.\n\nA signaling mechanism is a mapping \\(\\sigma\\colon\\mathcal{X}^{k}\\times\\Omega\\to\\Delta(A)\\) (for some \\(k\\geq 0\\)) that specifies for each \\(h^{k}\\in\\mathcal{X}^{k}\\) and \\(\\omega\\in\\Omega\\), the probability \\(\\sigma(a|h^{k},\\omega)\\) with which the sender shares the signal \\(\\bar{s}_{t}=a\\in A\\) if the (slice of) history is \\(\\bar{h}_{t}^{k}=h^{k}\\) and the current state is \\(\\bar{\\omega}_{t}=\\omega\\). We let \\(\\Sigma_{k}\\) denote the set of all signaling mechanisms that depend only on history slices of length \\(k\\), and let \\(\\Sigma=\\cup_{k\\geq 0}\\Sigma_{k}\\). The set \\(\\Sigma_{0}\\) contains the signaling mechanisms that do not depend on the history.\n\n### Beliefs and Persuasiveness\n\nNext, we describe the notion of persuasiveness as applied to signaling mechanisms. To do this, we need to model the receivers' beliefs about the history of the process, which in general depends endogenously on how much information they have about the past. We capture this endogenous level of historical information through the concept of an _information model_ (see Section 3.4). However, to develop our concepts, we will initially consider the receiver's prior beliefs as exogenously specified.\n\nSuppose the sender commits to a signaling mechanism \\(\\sigma\\in\\Sigma_{k}\\) for some \\(k\\geq 0\\). Fix a time \\(t\\), and let the corresponding receiver's belief over the history \\(\\bar{h}_{t}\\) and the current state \\(\\bar{\\omega}_{t}\\) (prior to receiving any signal) be denoted by \\(\\phi_{t}\\). Then, upon receiving an action recommendation \\(\\bar{s}_{t}=a\\), the receiver's posterior belief that \\(\\bar{\\omega}_{t}=\\omega\\) can be found using Bayes' rule as\n\n\\[F(\\omega|a;\\phi_{t},\\sigma)=\\frac{\\sum_{h^{k}}\\phi_{t}(h^{k}, \\omega)\\sigma(a|h^{k},\\omega)}{\\sum_{\\omega^{\\prime}}\\sum_{h^{ "], "nougat": ["generic element of \\(\\mathcal{X}\\) by \\(x=(\\omega,a)\\). Thus, \\(\\bar{x}_{t}=(\\bar{\\omega}_{t},\\bar{a}_{t})\\in\\mathcal{X}\\) denotes the state-action pair at time \\(t\\), and \\(p(\\omega^{\\prime}|x)\\) with \\(x=(\\omega,a)\\) stands for \\(p(\\omega^{\\prime}|\\omega,a)\\). Next, for any \\(k\\geq 1\\) and at any time \\(t\\), a _slice_ of history \\(\\bar{h}_{t}^{k}\\) of length \\(k\\) describes the sequence of states-action pairs in the past \\(k\\) time periods: \\(\\bar{h}_{t}^{k}=(\\bar{x}_{t-k},\\ldots,\\bar{x}_{t-1})\\in\\mathcal{X}^{k}\\). We denote a generic element of \\(\\mathcal{X}^{k}\\) by \\(h^{k}=(x_{-k},\\ldots,x_{-1})\\). Finally, we let \\(\\mathcal{X}^{0}\\) denote the singleton set consisting of the unique (empty) slice of history of length zero.\n\nA signaling mechanism is a mapping \\(\\sigma\\colon\\mathcal{X}^{k}\\times\\Omega\\to\\Delta(A)\\) (for some \\(k\\geq 0\\)) that specifies for each \\(h^{k}\\in\\mathcal{X}^{k}\\) and \\(\\omega\\in\\Omega\\), the probability \\(\\sigma(a|h^{k},\\omega)\\) with which the sender shares the signal \\(\\bar{s}_{t}=a\\in A\\) if the (slice of) history is \\(\\bar{h}_{t}^{k}=h^{k}\\) and the current state is \\(\\bar{\\omega}_{t}=\\omega\\). We let \\(\\Sigma_{k}\\) denote the set of all signaling mechanisms that depend only on history slices of length \\(k\\), and let \\(\\Sigma=\\cup_{k\\geq 0}\\Sigma_{k}\\). The set \\(\\Sigma_{0}\\) contains the signaling mechanisms that do not depend on the history.\n\n### Beliefs and Persuasiveness\n\nNext, we describe the notion of persuasiveness as applied to signaling mechanisms. To do this, we need to model the receivers' beliefs about the history of the process, which in general depends endogenously on how much information they have about the past. We capture this endogenous level of historical information through the concept of an _information model_ (see Section 3.4). However, to develop our concepts, we will initially consider the receiver's prior beliefs as exogenously specified.\n\nSuppose the sender commits to a signaling mechanism \\(\\sigma\\in\\Sigma_{k}\\) for some \\(k\\geq 0\\). Fix a time \\(t\\), and let the corresponding receiver's belief over the history \\(\\bar{h}_{t}\\) and the current state \\(\\bar{\\omega}_{t}\\) (prior to receiving any signal) be denoted by \\(\\phi_{t}\\). Then, upon receiving an action recommendation \\(\\bar{s}_{t}=a\\), the receiver's posterior belief that \\(\\bar{\\omega}_{t}=\\omega\\) can be found using Bayes' rule as\n\n\\[F(\\omega|a;\\phi_{t},\\sigma)=\\frac{\\sum_{h^{k}}\\phi_{t}(h^{k}, \\omega)\\sigma(a|h^{k},\\omega)}{\\sum_{\\omega^{\\prime}}\\sum_{h^{ "]}, {"edit": ["Dur, U., Gitmez, A., Yilmaz, O. 2019. School choice under partial fairness, Theoretical Economics 14(4), 1309-1346.\n* Dur et al. (2018) Dur, U., Kominers, S.D., Pathak, P.A., Sonmez, T. 2018. Reserve Design: Unintended Consequences and the Demise of Boston's Walk Zones. Journal of Political Economy 126(6) 2457-2479\n* Dur and Xie (2023) Dur, U., Xie, Y. 2023. Responsiveness to priority-based affirmative action policy in school choice. Journal of Public Economic Theory 25(2), 229-244\n* Ehlers and Morrill (2020) Ehlers, L., Morrill, T. 2020. (II)legal assignments in school choice. Review of Economic Studies 87, 1837-1875.\n* Erdil and Ergin (2008) Erdil, A., Ergin, H. 2008. What's the matter with tie-breaking? Improving efficiency in school choice, American Economic Review 98(3), 669-689.\n* Fang and Yasuda (2021) Fang, Y., Yasuda, Y. 2021. Misalignment between Test Scores and Colleges' Preferences: Chinese College Admission Reconsidered. Available at SSRN: [https://ssrn.com/abstract=3914742](https://ssrn.com/abstract=3914742)\n* Gale and Shapley (1962) Gale D., Shapley L.S. 1962. College admissions and the stability of marriage. American Mathematical Monthly 69(1):9-15\n* Hafalir et al. (2013) Hafalir I.E., Yenmez M.B., Yildirim M.A. 2013. Effective affirmative action in school choice. Theoretical Economics 8(2):325-363\n* Hirata et al. (2021) Hirata, D., Kasuya, Y., Okumura, Y. Stability, Strategy-Proofness, and Respect for Improvements, Mimeo Available at SSRN: [https://ssrn.com/abstract=3876865](https://ssrn.com/abstract=3876865)\n* Jiao and Shen (2021) Jiao, Z., Shen, Z., 2021. School choice with priority-based affirmative action: A responsive solution, Journal of Mathematical Economics 92, 1-9.\n* Jiao et al. (2022) Jiao, Z., Shen, Z., Tian, G., 2022. When is the deferred acceptance mechanism responsive to priority-based affirmative action? Social Choice and Welfare 58, 257-282.\n* Jiao and Tian (2018) Jiao, Z., Tian, G., 2018. Two further impossibility results on responsive affirmative action in school choice. Economics Letters 166, 60-62.\n* Kesten (2010) Kesten, O. 2010. School choice with consent. Quarterly Journal of Economics 125(3), 1297-1348.\n* Kesten et al. (2013) "], "nougat": ["Dur, U., Gitmez, A., Y\u0131lmaz,  \u0308 O. 2019. School choice under partial fairness, Theoretical Economics 14(4), 1309-1346. Dur, U., Kominers, S.D., Pathak, P.A., S \u0308onmez, T. 2018. Reserve Design: Unintended Consequences and the Demise of Boston\u2019s Walk Zones. Journal of Political Economy 126(6) 2457\u20132479 Dur, U., Xie, Y. 2023. Responsiveness to priority-based affirmative action policy in school choice. Journal of Public Economic Theory 25(2), 229-244 Ehlers, L., Morrill, T. 2020. (Il)legal assignments in school choice. Review of Economic Studies 87, 1837\u20131875. Erdil, A., Ergin, H. 2008. What\u2019s the matter with tie-breaking? Improving efficiency in school choice, American Economic Review 98(3), 669\u2013689. Fang, Y., Yasuda, Y. 2021. Misalignment between Test Scores and Colleges\u2019 Preferences: Chinese College Admission Reconsidered. Available at SSRN: [https://ssrn.com/abstract=3914742](https://ssrn.com/abstract=3914742) Gale D., Shapley L.S. 1962. College admissions and the stability of marriage. American Mathematical Monthly 69(1):9\u201315 Hafalir I.E., Yenmez M.B., Yildirim M.A. 2013. Effective affirmative action in school choice. Theoretical Economics 8(2):325\u2013363 Hirata, D., Kasuya, Y., Okumura, Y. Stability, Strategy-Proofness, and Respect for Improvements, Mimeo Available at SSRN: [https://ssrn.com/abstract=3876865](https://ssrn.com/abstract=3876865) Jiao, Z., Shen, Z., 2021. School choice with priority-based affirmative action: A responsive solution, Journal of Mathematical Economics 92, 1\u20139. Jiao, Z., Shen, Z., Tian, G., 2022. When is the deferred acceptance mechanism responsive to priority-based affirmative action? Social Choice and Welfare 58, 257\u2013282. Jiao, Z., Tian, G., 2018. Two further impossibility results on responsive affirmative action in school choice. Economics Letters 166, 60\u201362. Kesten, O. 2010. School choice with consent. Quarterly Journal of Economics 125(3), 1297\u20131348.\n\n "]}, {"edit": ["This figure below depicts the Linear Interpolation Imputation dataset Plot.\n\nThe ADF test for Linear Interpolation imputation dataset test-statistic = -5.390 p-value = 3.53e-06 for first difference to try to change the time-series to stationary test-statistic = -10.072 p-value = 1.24e-17 for Second Difference, which is not suggested as the p-value is zero (over-differencing) test-statistic = -21.44 p-value = 0.0\n\n## 5 Analysis using ACF and PACF plots\n\n### No Imputation\n\nThe Auto-Correlation Function and Partial Auto-Correlation Function Graph for the original dataset "], "nougat": ["This figure below depicts the Linear Interpolation Imputation dataset Plot.\n\nThe ADF test for Linear Interpolation imputation dataset test-statistic = -5.390 p-value = 3.53e-06 for first difference to try to change the time-series to stationary test-statistic = -10.072 p-value = 1.24e-17 for Second Difference, which is not suggested as the p-value is zero (over-differencing) test-statistic = -21.44 p-value = 0.0\n\n## 5 Analysis using ACF and PACF plots\n\n### No Imputation\n\nThe Auto-Correlation Function and Partial Auto-Correlation Function Graph for the original dataset "]}, {"edit": ["These results show the difference between the global and local effects of an attribute, which our model can capture by flexibly defining the global and local utility functions. Moreover, it was also shown that we can analyze to what extent and which attributes globally and locally affect the path choice behavior by estimating and comparing different specifications with respect to attributes of interest, including one with the attribute introduced to both the global and local utilities.\n\n### Real pedestrian path choice application\n\nWe then show an application of the proposed path choice model to pedestrian path choice analysis using real path observations. Because walking is a slow mode of transportation, pedestrians may visually perceive the street environment while walking and locally adjust their path choice behavior, which we analyze by using the proposed model. The data is the same as used in Oyama (2023), based on GPS trajectories collected through a complementary survey of the Sixth Tokyo Metropolitan Region Person Trip Survey (Ministry of Land, Infrastructure, Transport and Tourism of Japan, 2018), in the Kannai area, Yokohama city, Japan. The pedestrian network for the case study contains 724 nodes and 2398 links with 8434 link pairs, as shown in Figure 4.\n\nIn this case study, we consider the following three specifications of the reward function:\n\n\\[\\left\\{\\begin{array}{ll}v_{G}(a|k)&=(\\beta_{\\text{len}}^{G}+\\beta_{\\text{ walk}}^{G}x_{a}^{\\text{walk}}+\\beta_{\\text{green}}^{G}x_{a}^{\\text{green}})x_{a}^{ \\text{len}}+\\beta_{\\text{cross}}^{G}x_{a}^{\\text{cross}}-20x_{a|k}^{\\text{utturn}} \\\\ v_{L}(a|k)&=0\\\\ \\left\\{\\begin{array}{ll}v_{G}(a|k)&=(\\beta_{\\text{len}}^{G}+\\beta_{\\text{walk }}^{G}x_{a}^{\\text{walk}})x_{a}^{\\text{len}}+\\beta_{\\text{cross}}^{G}x_{a}^{ \\text{cross}}-20x_{a|k}^{\\text{utturn}}\\\\ v_{L}(a|k)&=\\beta_{\\text{green}}^{L}x_{a}^{\\text{green}}x_{a}^{\\text{len}}\\\\ \\end{array}\\right.\\\\ \\left\\{\\begin{array}{ll}v_{G}(a|k)&=(\\beta_{\\text{len}}^{G}+\\beta_{\\text{ walk}}^{G}x_{a}^{\\text{walk}}+\\beta_{\\text{green}}^{G}x_{a}^{\\text{green}})x_{a}^{ \\text{len}}+\\beta_{\\text{cross}}^{G}x_{a}^{\\text{cross}}-20x_{a|k}^{\\text{utturn }}\\\\ v_{L}(a|k)&=\\beta_{\\text{green}}^{L}x_{a}^{\\text{green}}x_{a}^{\\text{len}} \\end{array}\\right. \\tag{5.2c}\\]\n\nwhere \\(x_{a}^{\\text{len}}\\) and \\(x_{a}^{\\text{walk}}\\) are respectively the length and sidewalk width (m/10) of link \\(a\\); \\(x_{a}^{\\text{cross}}\\) is the dummy variable of \\(a\\) being a crosswalk; \\(x_{a}^{\\text{green}}\\) is the green view index (GVI) of the street, extracted as the vegetation pixel ratio (\\( "], "nougat": ["These results show the difference between the global and local effects of an attribute, which our model can capture by flexibly defining the global and local utility functions. Moreover, it was also shown that we can analyze to what extent and which attributes globally and locally affect the path choice behavior by estimating and comparing different specifications with respect to attributes of interest, including one with the attribute introduced to both the global and local utilities.\n\n### Real pedestrian path choice application\n\nWe then show an application of the proposed path choice model to pedestrian path choice analysis using real path observations. Because walking is a slow mode of transportation, pedestrians may visually perceive the street environment while walking and locally adjust their path choice behavior, which we analyze by using the proposed model. The data is the same as used in Oyama ( 2023 ), based on GPS trajectories collected through a complementary survey of the Sixth Tokyo Metropolitan Region Person Trip Survey ( Ministry of Land, Infrastructure, Transport and Tourism of Japan , 2018 ), in the Kannai area, Yokohama city, Japan. The pedestrian network for the case study contains 724 nodes and 2398 links with 8434 link pairs, as shown in Figure 4 .\n\nIn this case study, we consider the following three specifications of the reward function:\n\n\\[\\left\\{\\begin{array}{ll}v_{G}(a|k)&=(\\beta_{\\text{len}}^{G}+\\beta_{\\text{ walk}}^{G}x_{a}^{\\text{walk}}+\\beta_{\\text{green}}^{G}x_{a}^{\\text{green}})x_{a}^{ \\text{len}}+\\beta_{\\text{cross}}^{G}x_{a}^{\\text{cross}}-20x_{a|k}^{\\text{utturn}} \\\\ v_{L}(a|k)&=0\\\\ \\left\\{\\begin{array}{ll}v_{G}(a|k)&=(\\beta_{\\text{len}}^{G}+\\beta_{\\text{ walk}}^{G}x_{a}^{\\text{walk}})x_{a}^{\\text{len}}+\\beta_{\\text{cross}}^{G}x_{a}^{ \\text{cross}}-20x_{a|k}^{\\text{utturn}}\\\\ v_{L}(a|k)&=\\beta_{\\text{green}}^{L}x_{a}^{\\text{green}}x_{a}^{\\text{len}}\\\\ \\end{array}\\right.\\\\ \\left\\{\\begin{array}{ll}v_{G}(a|k)&=(\\beta_{\\text{len}}^{G}+\\beta_{\\text{ walk}}^{G}x_{a}^{\\text{walk}}+\\beta_{\\text{green}}^{G}x_{a}^{\\text{green}})x_{a}^{ \\text{len}}+\\beta_{\\text{cross}}^{G}x_{a}^{\\text{cross}}-20x_{a|k}^{\\text{utturn }}\\\\ v_{L}(a|k)&=\\beta_{\\text{green}}^{L}x_{a}^{\\text{green}}x_{a}^{\\text{len}} \\end{array}\\right. \\tag{5.2c}\\]\n\nwhere \\(x_{a}^{\\text{len}}\\) and \\(x_{a}^{\\text{walk}}\\) are respectively the length and sidewalk width (m/10) of link \\(a\\); \\(x_{a}^{\\text{cross}}\\) is the dummy variable of \\(a\\) being a crosswalk; \\(x_{a}^{\\text{green}}\\) is the green "]}, {"edit": ["I consider two-period data generating processes of the following form:\n\n\\[\\boldsymbol{y}_{r,t}=f\\left(mw_{t},\\boldsymbol{\\theta}_{r,t}\\right) \\tag{1}\\]\n\nwhere \\(r\\in\\{1,\\ldots,R\\}\\) denotes region, \\(t\\in\\{0,1\\}\\) denotes time, \\(\\boldsymbol{y}_{r,t}\\) is a vector of equilibrium outcomes (employment to population ratio and quantiles of the log wage distribution), \\(mw_{t}\\) denotes the logarithm of the national minimum wage, \\(\\boldsymbol{\\theta}_{r,t}\\) is a vector of region-time-specific parameters, and \\(f\\) is a function that depends on the particular economic model (to be described later in the paper). Minimum wage levels are different between periods. Without loss of generality, I assume henceforth that \\(mw_{1}>mw_{0}\\).\n\nGiven this data-generating process, there are two natural ways to define the ceteris paribus average treatment effects of the national minimum wage:\n\n\\[\\boldsymbol{ATE}_{0} =\\frac{1}{R}\\sum_{r}f\\left(mw_{1},\\theta_{r,0}\\right)-f\\left(mw_{ 0},\\theta_{r,0}\\right)\\] \\[=\\frac{1}{R}\\sum_{r}f\\left(mw_{1},\\theta_{r,0}\\right)-\\boldsymbol {y}_{r,0}\\] \\[\\boldsymbol{ATE}_{1} =\\frac{1}{R}\\sum_{r}f\\left(mw_{1},\\theta_{r,1}\\right)-f\\left(mw_ {0},\\theta_{r,1}\\right)\\] \\[=\\frac{1}{R}\\sum_{r}\\boldsymbol{y}_{r,1}-f\\left(mw_{0},\\theta_{r ,1}\\right)\\]\n\nThe first considers a counterfactual where the minimum wage is increased to the level valid in period 1, keeping other characteristics constant at their \\(t=0\\) levels. The other compares the outcomes as of \\(t=1\\) to a counterfactual scenario where the minimum wage remained at the \\(t=0\\) level. The two definitions are identical if the minimum wage is the only time-varying component of the model, and are equal in expectation if the parameters \\(\\boldsymbol{\\theta}_{r,t}\\) have the same distribution across regions in both periods. This is true for some of the models I study, but not all. To keep the analysis as simple as possible, I use the average of these two definitions as the \"true\" causal effects to be recovered by the econometric designs:\n\n\\[\\boldsymbol{ATE}=\\frac{\\boldsymbol{ATE}_{0}+\\boldsymbol{ATE}_{1}}{2}\\]\n\nwhich gives equal weight to all observations in the data, and is thus closely related to the usual definition of average treatment effects in potential outcomes models.\n\nEquation (1) is fairly general, but it imposes important constraints that limit the scope of my "], "nougat": ["I consider two-period data generating processes of the following form:\n\n\\[\\boldsymbol{y}_{r,t}=f\\left(mw_{t},\\boldsymbol{\\theta}_{r,t}\\right) \\tag{1}\\]\n\nwhere \\(r\\in\\{1,\\ldots,R\\}\\) denotes region, \\(t\\in\\{0,1\\}\\) denotes time, \\(\\boldsymbol{y}_{r,t}\\) is a vector of equilibrium outcomes (employment to population ratio and quantiles of the log wage distribution), \\(mw_{t}\\) denotes the logarithm of the national minimum wage, \\(\\boldsymbol{\\theta}_{r,t}\\) is a vector of region-time-specific parameters, and \\(f\\) is a function that depends on the particular economic model (to be described later in the paper). Minimum wage levels are different between periods. Without loss of generality, I assume henceforth that \\(mw_{1}>mw_{0}\\).\n\nGiven this data-generating process, there are two natural ways to define the ceteris paribus average treatment effects of the national minimum wage:\n\n\\[\\boldsymbol{ATE}_{0} =\\frac{1}{R}\\sum_{r}f\\left(mw_{1},\\theta_{r,0}\\right)-f\\left(mw_{ 0},\\theta_{r,0}\\right)\\] \\[=\\frac{1}{R}\\sum_{r}f\\left(mw_{1},\\theta_{r,0}\\right)-\\boldsymbol {y}_{r,0}\\] \\[\\boldsymbol{ATE}_{1} =\\frac{1}{R}\\sum_{r}f\\left(mw_{1},\\theta_{r,1}\\right)-f\\left(mw_{ 0},\\theta_{r,1}\\right)\\] \\[=\\frac{1}{R}\\sum_{r}\\boldsymbol{y}_{r,1}-f\\left(mw_{0},\\theta_{r, 1}\\right)\\]\n\nThe first considers a counterfactual where the minimum wage is increased to the level valid in period 1, keeping other characteristics constant at their \\(t=0\\) levels. The other compares the outcomes as of \\(t=1\\) to a counterfactual scenario where the minimum wage remained at the \\(t=0\\) level. The two definitions are identical if the minimum wage is the only timevarying component of the model, and are equal in expectation if the parameters \\(\\boldsymbol{\\theta}_{r,t}\\) have the same distribution across regions in both periods. This is true for some of the models I study, but not all. To keep the analysis as simple as possible, I use the average of these two definitions as the \u201ctrue\u201d causal effects to be recovered by the econometric designs:\n\n\\[\\boldsymbol{ATE}=\\frac{\\boldsymbol{ATE}_{0}+\\boldsymbol{ATE}_{1}}{2}\\]\n\nwhich gives equal weight to all observations in the data, and is thus closely related to the usual definition of average treatment effects in potential outcomes models.\n\nEquation (1) is fairly general, but it imposes important constraints that limit the scope of my "]}, {"edit": ["following conditional expected payoff function for any given individual \\(i\\in N\\):14\n\nFootnote 14: Equation (9) is derived in Appendix B (Equations (20) and (21)).\n\n\\[EU_{V}(r|\\gamma_{i},\\delta,t,F,\\phi)=\\begin{cases}-\\gamma_{i}+r\\cdot\\rho(\\delta, \\phi)(1-F(r\\cdot\\rho(\\delta,\\phi)))+t\\cdot F(r\\cdot\\rho(\\delta,\\phi))&\\text{if }\\gamma_{i}\\leq r\\cdot\\rho(\\delta,\\phi),\\\\ -r\\cdot\\rho(\\delta,\\phi)F(r\\cdot\\rho(\\delta,\\phi))+t\\cdot F(r\\cdot\\rho(\\delta, \\phi))&\\text{otherwise.}\\end{cases} \\tag{9}\\]\n\nThe following proposition establishes that each individual's maximization problem is well-defined.\n\n**Proposition 5**.: _If \\(\\delta\\) is not null, then conditional on behavior \\(\\beta_{i}\\), voter payoffs are strictly quasi-concave in rewards, \\(r\\), and maximized at an interior \\(r\\). If \\(\\delta\\) is null, each voter \\(i\\) is indifferent between all reward levels._\n\nThe following corollary presents the two potentially optimal rewards for any individual \\(i\\) (one conditional on \\(i\\) subsequently choosing to comply, and the other conditional on \\(i\\) subsequently choosing to not comply).\n\n**Corollary 1**.: _The optimal \\(r_{1}^{*}\\) and \\(r_{0}^{*}\\) (rewards for individuals choosing to comply and not comply respectively) are of the form:_\n\n\\[r_{\\beta_{i}}^{*}(\\delta\\mid t,F,\\phi)=\\frac{k_{j}^{*}(t,F)}{\\rho(\\delta,\\phi )}\\ \\ \\forall\\beta_{i}\\in\\{0,1\\}, \\tag{10}\\]\n\n_with the values \\(k_{0}^{*}(t,F)\\) and \\(k_{1}^{*}(t,F)\\) defined implicitly as follows:_\n\n\\[k_{0}^{*}(t,F) = t-\\frac{F(k_{0}^{*}(t,F))}{f(k_{0}^{*}(t,F))},\\] \\[k_{1}^{*}(t,F) = t+\\frac{1-F(k_{1}^{*}(t,F))}{f(k_{1}^{*}(t,F))}.\\]\n\nCorollary 1 shows that, for any given \\(\\delta\\), \\(t\\), \\(F\\), and \\(\\phi\\), there are only two possible ideal rewards -- \\(r_{0}^{*}\\) or \\(r_{1}^{*}\\) -- for any given individual \\(i\\). Furthermore, _these two possible ideal reward levels are identical across all individuals_. This is the combined result of the assumption of budget balance and the assumption that all individuals have a common marginal preference for compliance by others (\\(t\\) is common to all). That said, what is especially surprising about this is that individuals are not homogeneous -- they each know their own types.\n\n "], "nougat": ["following conditional expected payoff function for any given individual \\(i\\in N\\):14\n\nFootnote 14: Equation (9) is derived in Appendix B (Equations (20) and (21)).\n\n\\[EU_{V}(r|\\gamma_{i},\\delta,t,F,\\phi)=\\begin{cases}-\\gamma_{i}+r\\cdot\\rho(\\delta, \\phi)(1-F(r\\cdot\\rho(\\delta,\\phi)))+t\\cdot F(r\\cdot\\rho(\\delta,\\phi))&\\text{if }\\gamma_{i}\\leq r\\cdot\\rho(\\delta,\\phi),\\\\ -r\\cdot\\rho(\\delta,\\phi)F(r\\cdot\\rho(\\delta,\\phi))+t\\cdot F(r\\cdot\\rho(\\delta, \\phi))&\\text{otherwise.}\\end{cases} \\tag{9}\\]\n\nThe following proposition establishes that each individual\u2019s maximization problem is well-defined.\n\n**Proposition 5**.: _If \\(\\delta\\) is not null, then conditional on behavior \\(\\beta_{i}\\), voter payoffs are strictly quasi-concave in rewards, \\(r\\), and maximized at an interior \\(r\\). If \\(\\delta\\) is null, each voter \\(i\\) is indifferent between all reward levels._\n\nThe following corollary presents the two potentially optimal rewards for any individual \\(i\\) (one conditional on \\(i\\) subsequently choosing to comply, and the other conditional on \\(i\\) subsequently choosing to not comply).\n\n**Corollary 1**.: _The optimal \\(r_{1}^{*}\\) and (21))._\n\n_with the values \\(k_{0}^{*}(t,F)\\) and \\(k_{1}^{*}(t,F)\\) defined implicitly as follows:_\n\n\\[k_{0}^{*}(t,F) = t-\\frac{F(k_{0}^{*}(t,F))}{f(k_{0}^{*}(t,F))},\\] \\[k_{1}^{*}(t,F) = t+\\frac{1-F(k_{1}^{*}(t,F))}{f(k_{1}^{*}(t,F))}.\\]\n\nCorollary 1 shows that, for any given \\(\\delta\\), \\(t\\), \\(F\\), and \\(\\phi\\), there are only two possible ideal rewards -- \\(r_{0}^{*}\\) or \\(r_{1}^{*}\\) -- for any given individual \\(i\\). Furthermore, _these two possible ideal reward levels are identical across all individuals_. This is the combined result of the assumption of budget balance and the assumption that all individuals have a common marginal preference for compliance by others (\\(t\\) is common to all). That said, what is especially surprising about this is that individuals are not homogeneous -- they each know their own types.\n\n "]}, {"edit": ["but they have decreased, easing the cost pressure to sell inventories, and prices are lower than their long-run values with expectations of converging back. So stock accumulation gradually increases with lower interest rates. Since the stock accumulation is slow in our example, prices converge to their long-run values from below without overshooting, but overshooting is possible. After 4 years, prices have converged to their long-run values, so the only driver of stock increase is the convergence of interest rate to its long-run average.\n\nStock levels present a different sensitivity to storage costs and demand elasticity. Stocks decrease more with an increase in interest rate when storage costs are lower, because for lower physical storage costs, stock levels are higher on average and more sensitive to variation in the opportunity costs. Stocks decrease less for a more inelastic demand. This is explained by the fact that with a more inelastic demand, a small increase of sales from inventory depresses prices much more, limiting the incentives to sell too much from the stocks.\n\nThe right panels of Figure 5 displays the IRFs for price volatility, namely, the conditionally expected standard deviation of price.22 It shows that price volatility mostly follows stock dynamics with a peak attained after a year and an incomplete convergence after 4 years. A larger response in storage generates an oppositely larger response in price volatility. These results match the empirical results of Gruber and Vigfusson (2018) who show that higher interest rates imply higher price volatility.\n\nFigure 6: Contemporaneous IRFs (in percentage points) to a 25 bp interest rate shock for various parameters and realized states\n\n"], "nougat": ["but they have decreased, easing the cost pressure to sell inventories, and prices are lower than their long-run values with expectations of converging back. So stock accumulation gradually increases with lower interest rates. Since the stock accumulation is slow in our example, prices converge to their long-run values from below without overshooting, but overshooting is possible. After 4 years, prices have converged to their long-run values, so the only driver of stock increase is the convergence of interest rate to its long-run average. Stock levels present a different sensitivity to storage costs and demand elasticity. Stocks decrease more with an increase in interest rate when storage costs are lower, because for lower physical storage costs, stock levels are higher on average and more sensitive to variation in the opportunity costs. Stocks decrease less for a more inelastic demand. This is explained by the fact that with a more inelastic demand, a small increase of sales from inventory depresses prices much more, limiting the incentives to sell too much from the stocks. The right panels of Figure 5 displays the IRFs for price volatility, namely, the conditionally expected standard deviation of price.22 It shows that price volatility mostly follows stock dynamics with a peak attained after a year and an incomplete convergence after 4 years. A larger response in storage generates an oppositely larger response in price volatility. These results match the empirical results of Gruber and Vigfusson (2018) who show that higher interest rates imply higher price volatility.\n\nFootnote 22: Mathematically, the price volatility is defined here as the square root of the conditional variance: \\(\\sqrt{\\mathbb{E}_{t-1}[f^{*}(X_{t},Z_{t})]^{2}-[\\mathbb{E}_{t-1}\\,f^{*}(X_{t} ,Z_{t})]^{2}}\\).\n\nFigure 6: Contemporaneous IRFs (in percentage points) to a 25 bp interest rate shock for various parameters and realized states\n\n"]}, {"edit": ["Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.\n* [11] Hana Choi, Carl F Mela, Santiago R Balseiro, and Adam Leary. Online display advertising markets: A literature review and future directions. _Information Systems Research_, 31(2):556-575, 2020.\n* [12] Vincent Conitzer, Christian Kroer, Eric Sodomka, and Nicolas E. Stier-Moses. Multiplicative pacing equilibria in auction markets. _Operations Research_, 70(2):963-989, 2022.\n* [13] Yuan Deng, Vahab Mirrokni, and Hanrui Zhang. Posted pricing and dynamic prior-independent mechanisms with value maximizers. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.\n* [14] Eustache Diemert, Julien Meynet, Pierre Galland, and Damien Lefortier. Attribution modeling increases efficiency of bidding in display advertising. In _Proceedings of the ADKDD'17_, ADKDD'17, New York, NY, USA, 2017. Association for Computing Machinery.\n* [15] John R Dormand and Peter J Prince. A family of embedded runge-kutta formulae. _Journal of computational and applied mathematics_, 6(1):19-26, 1980.\n* [16] Alexey Drutsa. Horizon-independent optimal pricing in repeated auctions with truthful and strategic buyers. In _Proceedings of the 26th International Conference on World Wide Web_, WWW '17, page 33-42, Republic and Canton of Geneva, CHE, 2017. International World Wide Web Conferences Steering Committee.\n* [17] Yuan Gao, Kaiyu Yang, Yuanlong Chen, Min Liu, and Noureddine El Karoui. Bidding agent design in the linkedin ad marketplace. _arXiv preprint arXiv:2202.12472_, 2022.\n* [18] Negin Golrezaei, Adel Javanmard, and Vahab Mirrokni. Dynamic incentive-aware learning: Robust pricing in contextual auctions. _Advances in Neural Information Processing Systems_, 32, 2019.\n* [19] Benjamin Heymann. Cost per action constrained auctions. In _Proceedings of the 14th Workshop on the Economics of Networks, Systems and Computation_, pages 1-8, 2019.\n* [20] Yash Kanoria and Hamid Nazerzadeh. Dynamic reserve prices for repeated auctions: Learning from bids. In Tie-Yan Liu, Qi Qi, and Yinyu Ye, editors, _Web and Internet Economics_, pages 232-232, Cham, 2014. Springer International Publishing.\n* [21] Raphael Krikorian. M1 systemes dynamiques, 2021.\n* [22] F Shampine Lawrence. Some practical runge-kutta formulas. _Mathematics of Computation_, 46:135-150, 1986.\n* [23] Vasilis Livanos and Ruta Mehta. Prophet inequalities for cost minimization. _arXiv preprint arXiv:2209.07988_, 2022.\n* [24] Hao Ma, Xueqing Liu, and Zhihong Shen. User fatigue in online news recommendation. In _Proceedings of the 25th International Conference on World Wide Web_, pages 1363-1372, 2016.\n* [25] Thomas Nedelec, Clement Calauzenes, Noureddine El Karoui, Vianney Perchet, et al. Learning in repeated auctions. _Foundations and Trends(r) in Machine Learning_, 15(3):176-334, 2022.\n* [26] Navdeep S Sahni. Effect of temporal spacing between advertising exposures: Evidence from online field "], "nougat": ["Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.\n* [11] Hana Choi, Carl F Mela, Santiago R Balseiro, and Adam Leary. Online display advertising markets: A literature review and future directions. _Information Systems Research_, 31(2):556-575, 2020.\n* [12] Vincent Conitzer, Christian Kroer, Eric Sodomka, and Nicolas E. Stier-Moses. Multiplicative pacing equilibria in auction markets. _Operations Research_, 70(2):963-989, 2022.\n* [13] Yuan Deng, Vahab Mirrokni, and Hanrui Zhang. Posted pricing and dynamic prior-independent mechanisms with value maximizers. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.\n* [14] Eustache Diemert, Julien Meynet, Pierre Galland, and Damien Lefortier. Attribution modeling increases efficiency of bidding in display advertising. In _Proceedings of the ADKDD'17_, ADKDD'17, New York, NY, USA, 2017. Association for Computing Machinery.\n* [15] John R Dormand and Peter J Prince. A family of embedded runge-kutta formulae. _Journal of computational and applied mathematics_, 6(1):19-26, 1980.\n* [16] Alexey Drutsa. Horizon-independent optimal pricing in repeated auctions with truthful and strategic buyers. In _Proceedings of the 26th International Conference on World Wide Web_, WWW '17, page 33-42, Republic and Canton of Geneva, CHE, 2017. International World Wide Web Conferences Steering Committee.\n* [17] Yuan Gao, Kaiyu Yang, Yuanlong Chen, Min Liu, and Noureddine El Karoui. Bidding agent design in the linkedin ad marketplace. _arXiv preprint arXiv:2202.12472_, 2022.\n* [18] Negin Golrezaei, Adel Javanmard, and Vahab Mirrokni. Dynamic incentive-aware learning: Robust pricing in contextual auctions. _Advances in Neural Information Processing Systems_, 32, 2019.\n* [19] Benjamin Heymann. Cost per action constrained auctions. In _Proceedings of the 14th Workshop on the Economics of Networks, Systems and Computation_, pages 1-8, 2019.\n* [20] Yash Kanoria and Hamid Nazerzadeh. Dynamic reserve prices for repeated auctions: Learning from bids. In Tie-Yan Liu, Qi Qi, and Yinyu Ye, editors, _Web and Internet Economics_, pages 232-232, Cham, 2014. Springer International Publishing.\n* [21] Raphael Krikorian. M1 systemes dynamiques, 2021.\n* [22] F Shampine Lawrence. Some practical runge-kutta formulas. _Mathematics of Computation_, 46:135-150, 1986.\n* [23] Vasilis Livanos and Ruta Mehta. Prophet inequalities for cost minimization. _arXiv preprint arXiv:2209.07988_, 2022.\n* [24] Hao Ma, Xueqing Liu, and Zhihong Shen. User fatigue in online news recommendation. In _Proceedings of the 25th International Conference on World Wide Web_, pages 1363-1372, 2016.\n* [25] Thomas Nedelec, Clement Calauzenes, Noureddine El Karoui, Vianney Perchet, et al. Learning in repeated auctions. _Foundations and Trends(r) in Machine Learning_, 15(3):176-334, 2022.\n* [26] Navdeep S Sahni. Effect of temporal spacing between advertising exposures: Evidence from online field "]}, {"edit": ["tive effect for women (columns (1) to (4)), suggesting a reduced probability of encountering gender discrimination.\n\nThe empirical findings presented in Tables III to V provide support for the following narrative: exposure to high school courses on the 1991 Colombian Constitution is associated with a more favorable attitude towards gender equality, particularly among men. Furthermore, it leads to a diminished perception of sex discrimination, particularly among women. These findings suggest that the hypothesized change in social norms primarily involves altering men's attitudes towards gender equality. This shift in stance may have contributed to women perceiving reduced instances of sex discrimination and experiencing increased participation in the labor market (as indicated in Tables I and II). This hypothesis, while broadly formulated, aligns with specific aspects of Colombian culture, notably the prevailing male-dominated labor market where women are often relegated to the domestic sphere (Iregui-Bohorquez _et al._, 2020). Furthermore, this hypothesis is in line with the idea put forth by **?**'s that promoting women's rights requires men to have incentives to share power with women.\n\n### Work and Home Spheres\n\nIn the preceding subsection, we briefly discussed the role of men in facilitating the transformation of social norms that affected women's participation in the labor market. However, we need to provide more clarity on this matter. In this subsection, we will delve deeper into this topic by addressing the following questions: Which women are the beneficiaries of men's most favorable views on gender equality? Are these views limited to women residing within the households of these men (e.g., their wives or daughters)? Or do they extend to women beyond their immediate households? Although the analysis in this subsection is exploratory in nature and the results are presented at a broader level, they provide valuable insights into the specific mechanisms underlying our main findings.\n\nIn Table VI, we examine whether women's increased labor market participation, resulting from exposure to high school courses on the 1991 Colombian Constitution, can be attributed to their male partners' more favorable attitude towards gender equality.12 The estimates reveal no statistically significant difference between women who have partners supportive of gender equality (columns (1) to (4)) and those without such partners (columns (5) to (8)). This suggests that women's increased participation in the labor market is unlikely due to greater support or fewer barriers imposed by their male partners.13 "], "nougat": ["tive effect for women (columns (1) to (4)), suggesting a reduced probability of encountering gender discrimination.\n\nThe empirical findings presented in Tables III to V provide support for the following narrative: exposure to high school courses on the 1991 Colombian Constitution is associated with a more favorable attitude towards gender equality, particularly among men. Furthermore, it leads to a diminished perception of sex discrimination, particularly among women. These findings suggest that the hypothesized change in social norms primarily involves altering men\u2019s attitudes towards gender equality. This shift in stance may have contributed to women perceiving reduced instances of sex discrimination and experiencing increased participation in the labor market (as indicated in Tables I and II ). This hypothesis, while broadly formulated, aligns with specific aspects of Colombian culture, notably the prevailing male-dominated labor market where women are often relegated to the domestic sphere ( Iregui-Boh \u0301orquez _et al._, 2020 ). Furthermore, this hypothesis is in line with the idea put forth by **?**'s that promoting women\u2019s rights requires men to have incentives to share power with women.\n\n### Work and Home Spheres\n\nIn the preceding subsection, we briefly discussed the role of men in facilitating the transformation of social norms that affected women\u2019s participation in the labor market. However, we need to provide more clarity on this matter. In this subsection, we will delve deeper into this topic by addressing the following questions: Which women are the beneficiaries of men\u2019s most favorable views on gender equality? Are these views limited to women residing within the households of these men (e.g., their wives or daughters)? Or do they extend to women beyond their immediate households? Although the analysis in this subsection is exploratory in nature and the results are presented at a broader level, they provide valuable insights into the specific mechanisms underlying our main findings.\n\nIn Table VI, we examine whether women\u2019s increased labor market participation, resulting from exposure to high school courses on the 1991 Colombian Constitution, can be attributed to their male partners\u2019 more favorable attitude towards gender equality. 12 12 It is important to note that the attitude of an individual\u2019s partner towards gender equality can be influenced by the treatment, making this variable a \u201cbad control.\u201d Therefore, the interpretation of these results should be exercised with caution.\n\nFootnote 12: The estimates presented in Table A1 in Appendix A.1 further support these findings, indicating that exposure to high school courses on the 1991 Colombian Constitution increase men\u2019s awareness of gender equality regardless of their role as household heads.\n\nThe estimates reveal no statistically significant difference between women who have partners supportive of gender equality (columns (1) to (4)) and those without such partners (columns (5) to (8)). This suggests that women's increased participation in the labor market is unlikely due to greater support or fewer barriers imposed by their male partners.13 "]}, {"edit": ["\n\n# Flexible heat pumps: must-have or nice to have in a power sector with renewables?\n\nAlexander Roth, Dana Kirchem, Carlos Gaete-Morales, Wolf-Peter Schill\n\n###### Abstract\n\nHeat pumps are a key technology for reducing fossil fuel use in the heating sector. A transition to heat pumps implies an increase in electricity demand, especially in cold winter months. Using an open-source power sector model, we examine the power sector impacts of a massive expansion of decentralized heat pumps in Germany in 2030, combined with buffer heat storage of different sizes. Assuming that the additional electricity used by heat pumps has to be fully covered by renewable energies in a yearly balance, we quantify the required additional investments in renewable energy sources. If wind power expansion potentials are limited, the roll-out of heat pumps can also be accompanied by solar PV with little additional costs, making use of the European interconnection. The need for additional firm capacity and electricity storage generally remains limited even in the case of temporally inflexible heat pumps. We further find that relatively small heat storage capacities of 2 to 6 hours can substantially reduce the need for short- and long-duration electricity storage and other generation capacities, as well as power sector costs. We further show that 5.8 million additional heat pumps save around 120 TWh of natural gas and 24 million tonnes of CO\\({}_{2}\\) emissions per year.\n\n## 1 Introduction\n\nIn light of the climate crisis, heat pumps are regarded as a central technology to reduce greenhouse gas emissions in the heating sector [10]. When powered with electricity from renewable energy sources (RES), heat pumps can displace traditional heating technologies such as oil- and gas-fired heating and thus mitigate greenhouse gas emissions. In addition, the Russian invasion of Ukraine has led to a further political push in Europe, but especially Germany, to reduce the dependence on Russian natural gas imports. In Germany, natural gas is, at the moment, still the principal source of residential heating. The electrification of heating can therefore be seen as a critical measure to reduce the use of natural gas.\n\nIn Germany, policymakers aim for an accelerated roll-out of decentralized heat pumps, with a declared target of six million installed heat pumps by the year 2030 [15]. Given the current stock of around 1.5 million heat pumps, such a transition implies an increase in the electricity demand. So far, it is not yet understood how an increased heat pump stock affects the power sector in detail, considering that the electricity needs for mobility, hydrogen production, and other energy services will also increase. One common concern is that heat pumps constitute an additional burden on the power sector if they are operated in an inflexible manner. Given that electricity load profiles often coincide with heat demand profiles, inflexible heat pumps could add to existing load peaks and thus increase the need for firm generation capacity or electricity storage. Therefore, we"], "nougat": ["\n\n# Flexible heat pumps: must-have or nice to have in a power sector with renewables?\n\nAlexander Roth, Dana Kirchem, Carlos Gaete-Morales, Wolf-Peter Schill\n\n###### Abstract\n\nHeat pumps are a key technology for reducing fossil fuel use in the heating sector. A transition to heat pumps implies an increase in electricity demand, especially in cold winter months. Using an open-source power sector model, we examine the power sector impacts of a massive expansion of decentralized heat pumps in Germany in 2030, combined with buffer heat storage of different sizes. Assuming that the additional electricity used by heat pumps has to be fully covered by renewable energies in a yearly balance, we quantify the required additional investments in renewable energy sources. If wind power expansion potentials are limited, the roll-out of heat pumps can also be accompanied by solar PV with little additional costs, making use of the European interconnection. The need for additional firm capacity and electricity storage generally remains limited even in the case of temporally inflexible heat pumps. We further find that relatively small heat storage capacities of 2 to 6 hours can substantially reduce the need for short- and long-duration electricity storage and other generation capacities, as well as power sector costs. We further show that 5.8 million additional heat pumps save around 120 TWh of natural gas and 24 million tonnes of CO\\({}_{2}\\) emissions per year.\n\n## 1 Introduction\n\nIn light of the climate crisis, heat pumps are regarded as a central technology to reduce greenhouse gas emissions in the heating sector [10]. When powered with electricity from renewable energy sources (RES), heat pumps can displace traditional heating technologies such as oil- and gas-fired heating and thus mitigate greenhouse gas emissions. In addition, the Russian invasion of Ukraine has led to a further political push in Europe, but especially Germany, to reduce the dependence on Russian natural gas imports. In Germany, natural gas is, at the moment, still the principal source of residential heating. The electrification of heating can therefore be seen as a critical measure to reduce the use of natural gas.\n\nIn Germany, policymakers aim for an accelerated roll-out of decentralized heat pumps, with a declared target of six million installed heat pumps by the year 2030 [15]. Given the current stock of around 1.5 million heat pumps, such a transition implies an increase in the electricity demand. So far, it is not yet understood how an increased heat pump stock affects the power sector in detail, considering that the electricity needs for mobility, hydrogen production, and other energy services will also increase. One common concern is that heat pumps constitute an additional burden on the power sector if they are operated in an inflexible manner. Given that electricity load profiles often coincide with heat demand profiles, inflexible heat pumps could add to existing load peaks and thus increase the need for firm generation capacity or electricity storage. Therefore, we"]}, {"edit": ["Conclusion\n\nWe proposed an innovative framework for examining the impact of money growth on inflation and its disaggregate measures in the US and the Euro area. Our framework focuses on quantile dependence, providing a novel approach to analyze the distributional effects. This aspect of the relationship between money growth and inflation has not been previously explored in the existing literature.\n\nThrough empirical analysis of both economies, we found evidence that variations in the upper quantile of the money growth distribution significantly influence the distribution of inflation and its disaggregate measures, affecting both its level and skewness. On the other hand, the lower and median quantiles of the money growth distribution exhibit a relatively negligible effect on the distribution of inflation.\n\nOur proposed framework is versatile and can be applied to investigate various other macroeconomic or financial phenomena. For instance, it can be extended to explore the relationship between oil prices and stock returns or a growth-at-risk scenario.\n\n## References\n\n* Benati (2009) Benati, L. (2009). Long run evidence on money growth and inflation. Technical report, ECB working paper.\n* Borio et al. (2023) Borio, C., B. Hofmann, and E. Zakrajsek (2023). Does money growth help explain the recent inflation surge? Technical report, Bank for International Settlements.\n* Chavleishvili and Manganelli (2021) Chavleishvili, S. and S. Manganelli (2021). Forecasting and Stress Testing with Quantile Vector Autoregression. Technical Report 2330, ECB Working Paper.\n* Chernozhukov and Hansen (2008) Chernozhukov, V. and C. Hansen (2008). Instrumental variable quantile regression: A robust inference approach. _Journal of Econometrics__142_(1), 379-398.\n* Chernozhukov and Hansen (2012) "], "nougat": ["Conclusion\n\nWe proposed an innovative framework for examining the impact of money growth on inflation and its disaggregate measures in the US and the Euro area. Our framework focuses on quantile dependence, providing a novel approach to analyze the distributional effects. This aspect of the relationship between money growth and inflation has not been previously explored in the existing literature.\n\nThrough empirical analysis of both economies, we found evidence that variations in the upper quantile of the money growth distribution significantly influence the distribution of inflation and its disaggregate measures, affecting both its level and skewness. On the other hand, the lower and median quantiles of the money growth distribution exhibit a relatively negligible effect on the distribution of inflation.\n\nOur proposed framework is versatile and can be applied to investigate various other macroeconomic or financial phenomena. For instance, it can be extended to explore the relationship between oil prices and stock returns or a growth-at-risk scenario.\n\n## References\n\n* Benati (2009). Long run evidence on money growth and inflation. Technical report, ECB working paper. Borio, C., B. Hofmann, and E. Zakraj\u0161ek (2023). Does money growth help explain the recent inflation surge? Technical report, Bank for International Settlements. Chavleishvili, S. and S. Manganelli (2021). Forecasting and Stress Testing with Quantile Vector Autoregression. Technical Report 2330, ECB Working Paper. Chernozhukov, V. and C. Hansen (2008). Instrumental variable quantile regression: A robust inference approach. _Journal of Econometrics 142_(1), 379\u2013398.\n\n "]}, {"edit": ["there exists a \\(\\Delta_{j}\\in\\bar{\\Delta}\\), such that \\(\\Delta_{i}\\cap\\Delta_{j}\\neq\\varnothing\\). We call \\(\\Delta_{i}\\) as the **sub-descriptor set** of \\(\\Delta\\), moreover, the sub-descriptor set and the states of them called a **subsystem** corresponding to the sub-descriptor set. Denote \\(T=\\{D_{t}|D_{t}\\in\\Delta_{i}\\cap\\Delta_{j},i,j=1,2,\\cdots,X\\}\\) as **transitional descriptor set**. Use \\(|\\cdot|\\) to denote the number of elements of a set. We suppose that for all \\(\\Delta_{i}\\cap\\Delta_{j}=\\varnothing\\), for each \\(D_{i}\\in\\Delta_{i},D_{j}\\in\\Delta_{j}\\), we have \\(D_{ij}=O\\), it means the descriptors in non-overlapped sub-descriptor set do not have direct impact with each other.\n\nAs we do not change the state of the descriptors when split the system into subsystems, we call the CIM corresponding to the subsystem corresponding to the sub-descriptor set as the CIM corresponding to the sub-descriptor set for short.\n\nFor all the sub-descriptor set split from the descriptor set, we construct all the CIM corresponding to the sub-descriptor set, then, by applying the consistency algorithm, we acquire each **consistent scenario set**\\(S_{i}\\). Denote \\(\\Xi=\\{\\{z_{1},z_{2},\\cdots,z_{X}\\}|z_{i}\\in S_{i},i=1,2,\\cdots X\\}\\) as the **combinatorial consistent scenario set** between the sub-descriptor sets, which \\(\\xi=\\{z_{1},z_{2},\\cdots,z_{X}\\}\\) called a **combinatorial consistent scenario** between the sub-descriptor sets, we call it as a **combinatorial** for short.\n\nWhen constructing the CIM of \\(\\Delta_{i}\\), if it exists \\(D_{i},D_{j}\\in T\\) and \\(\\Delta_{m},\\Delta_{n}\\in\\Delta\\) such that it holds \\(D_{i},D_{j}\\in\\Delta_{m}\\) and \\(D_{i},D_{j}\\in\\Delta_{n}\\) at the same time, then we must ensure that the judgement sections of \\(D_{ij}\\) in the CIM of \\(\\Delta_{m},\\Delta_{n}\\) must be the same.\n\nFor \\(T\\) and \\(\\Xi\\), if \\(\\xi=\\{z_{1},z_{2},\\cdots,z_{X}\\}\\in\\Xi\\) meets\n\n\\[\\bigcup_{1\\leqslant i,j\\leqslant X,i\\neq j}(z_{i}\\cap z_{j})=|T|,\\;\\text{which }z_{i}\\in\\xi\\]\n\nthen we say that combinatorial \\(\\xi\\) can be **aggregated** into a consistent scenario in the scenario space of \\(\\Delta\\), and for all such combinatorial \\(\\xi=\\{z_{1},z_{2},\\cdots,z_{X}\\}\\in\\Xi\\), the consistent scenarios of \\(\\Delta\\) is \\(z=\\bigcup_{i=1}^{X}z_{i}\\).\n\n## References\n\n* (1) Wolfgang Weimer-Jehle. Cross-impact balances: A system-theoretical approach to cross-impact analysis. _Technological Forecasting and Social "], "nougat": ["there exists a \\(\\Delta_{j}\\in\\bar{\\Delta}\\), such that \\(\\Delta_{i}\\cap\\Delta_{j}\\neq\\varnothing\\). We call \\(\\Delta_{i}\\) as the **sub-descriptor set and the states of them called a subsystem** corresponding to the sub-descriptor set. Denote \\(T=\\{D_{t}|D_{t}\\in\\Delta_{i}\\cap\\Delta_{j},i,j=1,2,\\cdots,X\\}\\) as transitional descriptor set**. Use \\(|\\cdot|\\) to denote the number of elements of a set. We suppose that for all \\(\\Delta_{i}\\cap\\Delta_{j}=\\varnothing\\), for each \\(D_{i}\\in\\Delta_{i},D_{j}\\in\\Delta_{j}\\), we have \\(D_{ij}=O\\), it means the descriptors in non-overlapped sub-descriptor set do not have direct impact with each other.\n\nAs we do not change the state of the descriptors when split the system into subsystems, we call the CIM corresponding to the subsystem corresponding to the sub-descriptor set as the CIM corresponding to the sub-descriptor set for short.\n\nFor all the sub-descriptor set split from the descriptor set, we construct all the CIM corresponding to the sub-descriptor set, then, by applying the consistency algorithm, we acquire each consistent scenario set \\(S_{i}\\). Denote \\(\\Xi=\\{\\{z_{1},z_{2},\\cdots,z_{X}\\}|z_{i}\\in S_{i},i=1,2,\\cdots X\\}\\) as the combinatorial consistent scenario set between the sub-descriptor sets, which \\(\\xi=\\{z_{1},z_{2},\\cdots,z_{X}\\}\\) called a combinatorial consistent scenario between the sub-descriptor sets, we call it as a combinatorial for short.\n\nWhen constructing the CIM of \\(\\Delta_{i}\\), if it exists \\(D_{i},D_{j}\\in T\\) and \\(\\Delta_{m},\\Delta_{n}\\in\\Delta\\) such that it holds \\(D_{i},D_{j}\\in\\Delta_{m}\\) and \\(D_{i},D_{j}\\in\\Delta_{n}\\) at the same time, then we must ensure that the judgement sections of \\(D_{ij}\\) in the CIM of \\(\\Delta_{m},\\Delta_{n}\\) must be the same.\n\nFor \\(T\\) and \\(\\Xi\\), if \\(\\xi=\\{z_{1},z_{2},\\cdots,z_{X}\\}\\in\\Xi\\) meets\n\n\\[\\bigcup_{1\\leqslant i,j\\leqslant X,i\\neq j}(z_{i}\\cap z_{j})=|T|,\\;\\text{which }z_{i}\\in\\xi\\]\n\nthen we say that combinatorial \\(\\xi\\) can be aggregated into a consistent scenario in the scenario space of \\(\\Delta\\), and for all such combinatorial \\(\\xi=\\{z_{1},z_{2},\\cdots,z_{X}\\}\\in\\Xi\\), the consistent scenarios of \\(\\Delta\\) is \\(z=\\bigcup_{i=1}^{X}z_{i}\\).\n\n## References\n\n* (1) Wolfgang Weimer-Jehle. Cross-impact balances: A system-theoretical approach to cross-impact analysis. _Technological Forecasting and Social "]}, {"edit": ["\n\n#### 4.2.2 Optimizing Network Traffic Patterns\n\nIn order to find chances for cost reductions, it is essential to examine the network traffic patterns inside a cloud environment. Enterprises may optimize their network infrastructure and cut expenses by looking at the amount and kind of data flows using network traffic analysis. Enterprises can use monitoring tools and analytics platforms offered by cloud providers or third-party solutions to do network traffic analysis. These tools give users insight into the patterns of network traffic, including data transfer rates, peak usage periods, and the types of data being transferred [30].\n\nEnterprises can find potential areas for optimization with the use of this information. For instance, they can discover data-intensive programs or services that cause a lot of network traffic and look for ways to improve how data is transferred between them. This may entail putting data compression techniques into practice, leveraging content delivery networks (CDNs) or edge caching to shorten the distances between data transfers, or using data deduplication techniques to stop repeated transfers.\n\nIn order to maximize network efficiency, network traffic analysis can also help find opportunities for traffic rerouting or load balancing. Enterprises can lower bandwidth consumption and possibly lower data transfer costs by intelligently directing traffic through efficient channels or dispersing it across numerous network resources.\n\n#### 4.2.3 Content Delivery Networks (CDNs) and Edge Caching\n\nContent Delivery Networks (CDNs) play a key role in improving network performance and lowering costs. When using CDNs, content can be delivered from the edge location that is closest to the end users by utilizing edge servers that are dispersed across different locations. The usage of CDNs, edge caching, and traffic control strategies to save costs and boost overall network effectiveness are examined in this section. By caching and distributing content closer to end users, CDNs aim to lower latency and boost speed. CDNs reduce the distance that data must travel across the network by strategically distributing content in geographically dispersed edge servers. By serving content from edge locations rather than the origin server, this not only improves the user experience but also lowers network egress costs.\n\nFrequently accessed content is stored at edge server locations as part of the edge caching approach. The content is delivered directly from the nearest edge cache when a user wants it, avoiding the need for data to travel across the entire network. Enterprises may drastically lower network egress costs and enhance end-user response times by utilizing edge caching. Additionally, by offloading the traffic from the origin server to the edge locations, enterprises can lessen the stress on the origin server, increase network bandwidth, and lower egress costs by dumping data closer to the end users. As a result, less expensive origin server resources and data transfer are not required for serving static content.\n\nFinally, optimizing network utilization and cutting costs need effective traffic engineering. To achieve the best distribution of network traffic, a network engineer may use smart load balancing and intelligent routing algorithms. Enterprises can reduce network egress and ingress expenses by routing traffic through the"], "nougat": ["\n\n#### 4.2.2 Optimizing Network Traffic Patterns\n\nIn order to find chances for cost reductions, it is essential to examine the network traffic patterns inside a cloud environment. Enterprises may optimize their network infrastructure and cut expenses by looking at the amount and kind of data flows using network traffic analysis. Enterprises can use monitoring tools and analytics platforms offered by cloud providers or third-party solutions to do network traffic analysis. These tools give users insight into the patterns of network traffic, including data transfer rates, peak usage periods, and the types of data being transferred [30].\n\nEnterprises can find potential areas for optimization with the use of this information. For instance, they can discover data-intensive programs or services that cause a lot of network traffic and look for ways to improve how data is transferred between them. This may entail putting data compression techniques into practice, leveraging content delivery networks (CDNs) or edge caching to shorten the distances between data transfers, or using data deduplication techniques to stop repeated transfers.\n\nIn order to maximize network efficiency, network traffic analysis can also help find opportunities for traffic rerouting or load balancing. Enterprises can lower bandwidth consumption and possibly lower data transfer costs by intelligently directing traffic through efficient channels or dispersing it across numerous network resources.\n\n#### 4.2.3 Content Delivery Networks (CDNs) and Edge Caching\n\nContent Delivery Networks (CDNs) play a key role in improving network performance and lowering costs. When using CDNs, content can be delivered from the edge location that is closest to the end users by utilizing edge servers that are dispersed across different locations. The usage of CDNs, edge caching, and traffic control strategies to save costs and boost overall network effectiveness are examined in this section. By caching and distributing content closer to end users, CDNs aim to lower latency and boost speed. CDNs reduce the distance that data must travel across the network by strategically distributing content in geographically dispersed edge servers. By serving content from edge locations rather than the origin server, this not only improves the user experience but also lowers network egress costs.\n\nFrequently accessed content is stored at edge server locations as part of the edge caching approach. The content is delivered directly from the nearest edge cache when a user wants it, avoiding the need for data to travel across the entire network. Enterprises may drastically lower network egress costs and enhance end-user response times by utilizing edge caching. Additionally, by offloading the traffic from the origin server to the edge locations, enterprises can lessen the stress on the origin server, increase network bandwidth, and lower egress costs by dumping data closer to the end users. As a result, less expensive origin server resources and data transfer are not required for serving static content.\n\nFinally, optimizing network utilization and cutting costs need effective traffic engineering. To achieve the best distribution of network traffic, a network engineer may use smart load balancing and intelligent routing algorithms. Enterprises can reduce network egress and ingress expenses by routing traffic through the"]}, {"edit": ["_Assumption 1(i). It follows that_\n\n\\[\\hat{\\Gamma}^{(\\ell)}_{K^{(\\ell)}}=\\max_{k\\in[K^{(\\ell)}]}\\left(\\hat{\\Gamma}^{( \\ell)}_{k,K^{(\\ell)}}\\right)^{2}=\\max_{k\\in[m]}\\left(\\hat{\\Gamma}^{0,(\\ell)}_{k ,K^{(\\ell)}}\\right)^{2}+o_{p}(1)\\rightsquigarrow\\mathcal{Z},\\]\n\n_where \\(\\mathcal{Z}\\) is the maximum of \\(m\\) independent \\(\\chi^{2}\\left(1\\right)\\) random variables. Then Theorem 4.3(i.b) follows._\n\n_(ii) When \\(m<K^{(\\ell)},\\) Theorem 4.3(i.l) does not hold and we can not change the estimated group membership \\(\\hat{\\mathcal{G}}^{(\\ell)}_{K^{(\\ell)}}\\) to the true group membership \\(\\mathcal{G}^{(\\ell)}\\). To get around of this issue, we define the \"pseudo groups\". For \\(m<K^{(\\ell)}\\), let \\(\\mathbb{G}^{(\\ell)}_{m}:=\\{G^{(\\ell)}_{1,m},\\cdots,G^{(\\ell)}_{m,m}\\}\\) such that \\([N]=G^{(\\ell)}_{1,m}\\cup\\cdots\\cup G^{(\\ell)}_{m,m}\\), which indicates one possible partition of the set \\([N]\\). We further define \\(\\mathcal{G}^{(\\ell)}_{m}\\) to be the collection of all possible \\(\\mathbb{G}^{(\\ell)}_{m}\\)._\n\n_By Theorem 4.3(i.c), we can conclude that \\(\\mathbb{P}\\left(\\hat{K}^{(\\ell)}\\neq K^{(\\ell)}\\right)\\leqslant\\varsigma+o(1)\\) provided we can show that \\(\\hat{\\Gamma}^{(\\ell)}_{m}\\rightarrow\\infty\\) when \\(m<K^{(\\ell)}\\). By Lemma B.10, we notice that \\(\\hat{\\mathcal{G}}^{(\\ell)}_{m}\\in\\mathbb{G}^{(\\ell)}_{m}\\) w.p.a.1. Conditioning on the event \\(\\{\\hat{\\mathcal{G}}^{(\\ell)}_{m}\\in\\mathbb{G}^{(\\ell)}_{m}\\}\\cap\\{\\hat{T}_{1}= T_{1}\\}\\), we have_\n\n\\[\\hat{\\Gamma}^{(\\ell)}_{m}>\\min_{\\mathcal{G}^{(\\ell)}_{m}\\in\\mathbb{G}^{(\\ell)}_ {m}}\\hat{\\Gamma}^{0,(\\ell)}_{m}(\\mathcal{G}^{(\\ell)}_{m}):=\\min_{\\mathcal{G}^ {(\\ell)}_{m}\\in\\mathbb{G}^{(\\ell)}_{m}}\\left\\{\\max_{k\\in[m]}\\left[\\hat{\\Gamma} ^{0,(\\ell)}_{k,m}(G^{(\\ell)}_{k,m})\\right]^{2}\\right\\},\\]\n\n_where_\n\n\\[\\hat{\\Gamma}^{0,( "], "nougat": ["_Assumption 1(i). It follows that_\n\n\\[\\hat{\\Gamma}^{(\\ell)}_{K^{(\\ell)}}=\\max_{k\\in[K^{(\\ell)}]}\\left(\\hat{\\Gamma}^{( \\ell)}_{k,K^{(\\ell)}}\\right)^{2}=\\max_{k\\in[m]}\\left(\\hat{\\Gamma}^{0,(\\ell)}_{k ,K^{(\\ell)}}\\right)^{2}+o_{p}(1)\\rightsquigarrow\\mathcal{Z},\\]\n\n_where \\(\\mathcal{Z}\\) is the maximum of \\(m\\) independent \\(\\chi^{2}\\left(1\\right)\\) random variables. Then Theorem 4.3 (i.b) follows. (ii) When \\(m<K^{(\\ell)},\\) Theorem 4.3 (i.1) does not hold and we can not change the estimated group membership \\(\\hat{\\mathcal{G}}^{(\\ell)}_{K^{(\\ell)}}\\) to the true group membership \\(\\mathcal{G}^{(\\ell)}\\). To get around of this issue, we define the \u201cpseudo groups\u201d. For \\(m<K^{(\\ell)}\\), let \\(\\mathbb{G}^{(\\ell)}_{m}:=\\{G^{(\\ell)}_{1,m},\\cdots,G^{(\\ell)}_{m,m}\\}\\) such that \\([N]=G^{(\\ell)}_{1,m}\\cup\\cdots\\cup G^{(\\ell)}_{m,m}\\), which indicates one possible partition of the set \\([N]\\). We further define \\(\\mathcal{G}^{(\\ell)}_{m}\\) to be the collection of all possible \\(\\mathbb{G}^{(\\ell)}_{m}\\)._\n\n_By Theorem 4.3 (i.c), we can conclude that \\(\\mathbb{P}\\left(\\hat{K}^{(\\ell)}\\neq K^{(\\ell)}\\right)\\leqslant\\varsigma+o(1)\\) provided we can show that \\(\\hat{\\Gamma}^{(\\ell)}_{m}\\rightarrow\\infty\\) when \\(m<K^{(\\ell)}\\). By Lemma B.10, we notice that \\(\\hat{\\mathcal{G}}^{(\\ell)}_{m}\\in\\mathbb{G}^{(\\ell)}_{m}\\) w.p.a.1. Conditioning on the event \\(\\{\\hat{\\mathcal{G}}^{(\\ell)}_{m}\\in\\mathbb{G}^{(\\ell)}_{m}\\}\\cap\\{\\hat{T}_{1}= T_{1}\\}\\), we have_\n\n\\[\\hat{\\Gamma}^{(\\ell)}_{m}>\\min_{\\mathcal{G}^{(\\ell)}_{m}\\in\\mathbb{G}^{(\\ell)}_ {m}}\\hat{\\Gamma}^{0,(\\ell)}_{m}(\\mathcal{G}^{(\\ell)}_{m}):=\\min_{\\mathcal{G}^ {(\\ell)}_{m}\\in\\mathbb{G}^{(\\ell)}_{m}}\\left\\{\\max_{k\\in[m]}\\left[\\hat{\\Gamma }^{0,(\\ell)}_{k,m}(G^{(\\ell)}_{k,m})\\right]^{2}\\right\\},\\]\n\n_where_\n\n\\[\\hat{\\Gamma}^{0,(\\ "]}, {"edit": ["_Define_\n\n\\[\\xi^{0,(\\ell)}_{i}:=\\frac{X^{(\\ell)\\prime}_{i}M_{F^{0,(\\ell)}}e^{( \\ell)}_{i}}{T_{\\ell}},\\quad S^{0,(\\ell)}_{ii^{*}}:=\\frac{X^{(\\ell)\\prime}_{i}M_{ F^{0,(\\ell)}}X^{(\\ell)}_{i^{*}}}{T_{\\ell}},\\quad a^{0}_{ii^{*}}:=\\lambda^{0\\prime} _{i}\\left(\\frac{\\Lambda^{0\\prime}_{n}\\Lambda^{0}_{n}}{n}\\right)^{-1}\\lambda^{0 }_{i^{*}},\\] \\[G^{0,(\\ell)}_{ii^{*}}:=S^{0,(\\ell)}_{ii^{*}}a^{0}_{ii^{*}},\\quad \\text{and}\\ \\Omega^{0,(\\ell)}_{i}:=Var(\\xi^{0,(\\ell)}_{i}).\\]\n\n_Lemma 0.5_: _Under Assumptions 1\\({}^{*}\\) , 2 and 8, we have_\n\n* \\(\\mathbb{E}(S^{0,(\\ell)}_{ii}|\\mathscr{D})(\\hat{\\theta}^{(\\ell)}_{i}-\\theta^{0, (\\ell)}_{i})(1-\\frac{a^{0}_{ii}}{n})=\\xi^{0,(\\ell)}_{i}+\\mathcal{R}^{(\\ell)}_{i }\\) _such that_ \\(\\max_{i\\in\\mathcal{N}}||\\mathcal{R}^{(\\ell)}_{i}||=O_{p}(\\log N/(N\\wedge T))\\)_,_\n* \\(\\sqrt{T_{\\ell}}(\\Omega^{0,(\\ell)}_{i})^{-1/2}\\mathbb{E}(S^{0,(\\ell)}_{ii}| \\mathscr{D})(\\hat{\\theta}^{(\\ell)}_{i}-\\theta^{0,(\\ell)}_{i})(1-\\frac{a^{0}_{ ii}}{n})\\rightsquigarrow\\mathbb{N}(0,1)\\)_,_\n* \\(\\max_{i\\in\\mathcal{N}}\\|\\hat{\\theta}^{(\\ell)}_{i}-\\theta^{0,(\\ell)}_{i}||=O_{p} (\\sqrt{(\\log N)/T})\\)_._\n\n_Proof._ _(i) Noting from (_C.3_) that_ \\(\\hat{\\theta}^{(\\ell)}_{i}=(\\hat{S}^{(\\ell)}_{i})^{-1}T_{\\ell}^{-1}X^{(\\ell) \\prime}_{i}M_{\\hat{F}^{(\\ell)}}Y^{(\\ell)}_{i}\\) _with_ \\(\\hat{S}^{(\\ell)}_{ii}\\)\\(=\\)\\(T_{\\ell}^{-1}X^{(\\ell)\\prime}_{i}M_{\\hat{F}^{(\\ell)}}X^{(\\ell)}_{i}\\)_, we have_\n\n\\[\\hat{\\theta}^{( "], "nougat": ["_Define_\n\n\\[\\xi^{0,(\\ell)}_{i}:=\\frac{X^{(\\ell)\\prime}_{i}M_{F^{0,(\\ell)}}e^{( \\ell)}_{i}}{T_{\\ell}},\\quad S^{0,(\\ell)}_{ii^{*}}:=\\frac{X^{(\\ell)\\prime}_{i}M_{ F^{0,(\\ell)}}X^{(\\ell)}_{i^{*}}}{T_{\\ell}},\\quad a^{0}_{ii^{*}}:=\\lambda^{0\\prime} _{i}\\left(\\frac{\\Lambda^{0\\prime}_{n}\\Lambda^{0}_{n}}{n}\\right)^{-1}\\lambda^{0 }_{i^{*}},\\] \\[G^{0,(\\ell)}_{ii^{*}}:=S^{0,(\\ell)}_{ii^{*}}a^{0}_{ii^{*}},\\quad \\text{and}\\ \\Omega^{0,(\\ell)}_{i}:=Var(\\xi^{0,(\\ell)}_{i}).\\]\n\n_Lemma 0.5_: _Under Assumptions 1\\({}^{*}\\) , 2 and 8, we have_\n\n* \\(\\mathbb{E}(S^{0,(\\ell)}_{ii}|\\mathscr{D})(\\hat{\\theta}^{(\\ell)}_{i}-\\theta^{0, (\\ell)}_{i})(1-\\frac{a^{0}_{ii}}{n})=\\xi^{0,(\\ell)}_{i}+\\mathcal{R}^{(\\ell)}_{i }\\) _such that_ \\(\\max_{i\\in\\mathcal{N}}||\\mathcal{R}^{(\\ell)}_{i}||=O_{p}(\\log N/(N\\wedge T))\\)_,_\n* \\(\\sqrt{T_{\\ell}}(\\Omega^{0,(\\ell)}_{i})^{-1/2}\\mathbb{E}(S^{0,(\\ell)}_{ii}| \\mathscr{D})(\\hat{\\theta}^{(\\ell)}_{i}-\\theta^{0,(\\ell)}_{i})(1-\\frac{a^{0}_{ ii}}{n})\\rightsquigarrow\\mathbb{N}(0,1)\\)_,_\n* \\(\\max_{i\\in\\mathcal{N}}\\|\\hat{\\theta}^{(\\ell)}_{i}-\\theta^{0,(\\ell)}_{i}||=O_{p} (\\sqrt{(\\log N)/T})\\)_._\n\n_Proof._ _(i) Noting from (_C.3_) that_ \\(\\hat{\\theta}^{(\\ell)}_{i}=(\\hat{S}^{(\\ell)}_{i})^{-1}T_{\\ell}^{-1}X^{(\\ell) \\prime}_{i}M_{\\hat{F}^{(\\ell)}}Y^{(\\ell)}_{i}\\) _with_ \\(\\hat{S}^{(\\ell)}_{ii}\\)\\(=\\)\\(T_{\\ell}^{-1}X^{(\\ell)\\prime}_{i}M_{\\hat{F}^{(\\ell)}}X^{(\\ell)}_{i}\\)_, we have_\n\n\\[\\hat{\\theta}^{( "]}, {"edit": ["worker. On the other hand, when motivating effort is more important, other promotion systems, such as the classic winner-take-all promotion contest of Lazear and Rosen (1981) or the cyclical egalitarian contest proposed by Ely et al. (2021) may be better, and, hence, more common. Intuitively, these promotion systems are very good at incentivizing effort but less so at ensuring that the promoted worker's potential is high. The _index contest_ guarantees that the non-routine task runs smoothly after the promotion decision is made. It balances incentives provision and selection.\n\n### Features of the index contest\n\nNo commitment:Often, one may want to assume little commitment within an organization: most of the day-to-day activities are not governed by formal contracts, it is unlikely that the performance of a worker is verifiable...In my setting, the principal does not need any commitment power, as Proposition 2.3 shows. The _index contest_ is implementable even if the principal cannot commit to the delegation rule, delegation time, or promotion decision. Maybe even more interestingly, it does not require sophisticated coordinated punishments. It is implementable in a (weak) Perfect Bayesian equilibrium by \"grim trigger\" strategies. Moreover, each worker's strategy only depends on his current type, the running minimum of his type process, and whether the principal has promoted a worker yet.\n\nFast track:In the **index contest**, the promotion thresholds are decreasing over time (as increasing functions of the running minimums of the workers' types). So a worker's potential upon promotion decreases with time:\n\n**Proposition 2.5** (Speed and accuracy).: _If \\(\\pi^{i}(\\cdot)=\\pi(\\cdot)\\) for all \\(i\\in\\{1,\\ldots,N\\}\\) and the processes \\(X^{i}\\)'s have the same law, then the promoted worker's type and the principal's continuation value upon promotion are nonincreasing over time \\(\\mathbb{P}\\)-a.s.._\n\nProposition 2.5 follows from the fact that the promotion threshold is \\(\\mathbb{P}\\)-a.s. nonincreasing over time. The proof is omitted.\n\nPushing the interpretation beyond the model, the above proposition suggests that fast tracks21 should not be surprising. When a worker is promoted quickly, his type upon promotion is high. So, when entering a potential new contest for further promotion at the "], "nougat": ["worker. On the other hand, when motivating effort is more important, other promotion systems, such as the classic winner-take-all promotion contest of Lazear and Rosen (1981) or the cyclical egalitarian contest proposed by Ely et al. (2021) may be better, and, hence, more common. Intuitively, these promotion systems are very good at incentivizing effort but less so at ensuring that the promoted worker\u2019s potential is high. The _index contest_ guarantees that the non-routine task runs smoothly after the promotion decision is made. It balances incentives provision and selection.\n\n### Features of the index contest\n\nNo commitment:Often, one may want to assume little commitment within an organization: most of the day-to-day activities are not governed by formal contracts, it is unlikely that the performance of a worker is verifiable...In my setting, the principal does not need any commitment power, as Proposition 2.3 shows. The _index contest_ is implementable even if the principal cannot commit to the delegation rule, delegation time, or promotion decision. Maybe even more interestingly, it does not require sophisticated coordinated punishments. It is implementable in a (weak) Perfect Bayesian equilibrium by \"grim trigger\" strategies. Moreover, each worker's strategy only depends on his current type, the running minimum of his type process, and whether the principal has promoted a worker yet.\n\nFast track:In the **index contest**, the promotion thresholds are decreasing over time (as increasing functions of the running minimums of the workers' types). So a worker's potential upon promotion decreases with time:\n\n**Proposition 2.5** (Speed and accuracy).: _If \\(\\pi^{i}(\\cdot)=\\pi(\\cdot)\\) for all \\(i\\in\\{1,\\ldots,N\\}\\) and the processes \\(X^{i}\\)'s have the same law, then the promoted worker\u2019s type and the principal\u2019s continuation value upon promotion are nonincreasing over time \\(\\mathbb{P}\\)-a.s.._\n\nProposition 2.5 follows from the fact that the promotion threshold is \\(\\mathbb{P}\\)-a.s. nonintracks 21 21 I.e., that a quickly promoted worker often gets another promotion soon after. See Baker et al. (1994) and Ariga et al. (1999).\n\nPushing the interpretation beyond the model, the above proposition suggests that fast tracks22 should not be surprising. When a worker is promoted quickly, his type upon promotion is high. So, when entering a potential new contest for further promotion at the "]}, {"edit": ["The full set of instrumental variables is given by\n\n\\[Z=\\left[W_{1}:Z_{2}\\right],\\]\n\nso that the number of instrumental variables, \\(k_{1}+k_{3}\\), can be larger than the number of endogenous variables, \\(k_{1}+k_{2}\\), i.e. we are possibly dealing with an overidentified model. Hence, I will consider the 2SLS estimator, also known as the generalized instrumental variables estimator (Davidson and MacKinnon, 2004, p. 321), for the full regression model (2).4 The unique 2SLS estimator for coefficients in the full regression model (2) is given by (Greene, 2012, p. 231),\n\nFootnote 4: The 2SLS estimator uses, from among all possible linear combination of the set of instruments, the one that minimizes the asymptotic covariance matrix of the instrumental variables estimator (Brundy and Jorgenson, 1971).\n\n\\[b=\\begin{pmatrix}b_{1}\\\\ \\left(k_{1}\\times 1\\right)\\\\ b_{2}\\\\ \\left(k_{2}\\times 1\\right)\\end{pmatrix}=\\left[\\left(P_{Z}W\\right)^{\\prime}W \\right]^{-1}\\left(P_{Z}W\\right)^{\\prime}Y, \\tag{6}\\]\n\nwhere \\(P_{Z}=Z\\left(Z^{\\prime}Z\\right)^{-1}Z^{\\prime}\\) projects on to the column space of \\(Z\\). Hence,\n\n\\[b_{2}=\\ (2,1)\\text{ block of }\\left[\\left(P_{Z}W\\right)^{\\prime}W\\right]^{-1} \\left(P_{Z}W\\right)^{\\prime}Y. \\tag{7}\\]\n\nTurning to the partial model (3), we estimate it with the instrumental variables formed by the residualized set of instrumental variables, \\(\\widetilde{Z}_{2}=M_{W_{1}}Z_{2}\\), which is a \\(N\\times k_{3}\\) matrix. Thus, the 2SLS estimator for the partial regression model (3) is given by\n\n\\[\\tilde{b}_{2}=\\left[\\left(P_{\\widetilde{Z}_{2}}\\widetilde{W}_{2}\\right)^{ \\prime}\\widetilde{W}_{2}\\right]^{-1}\\left(P_{\\widetilde{Z}_{2}}\\widetilde{W}_ {2}\\right)^{\\prime}\\widetilde{Y}, \\tag{8}\\]\n\nwhere \\(P_{\\widetilde{Z}_{2}}=\\widetilde{Z}_{2}\\left(\\widetilde{Z}_{2}^{\\prime} \\widetilde{Z}_{2}\\right)^{-1}\\widetilde{Z}_{2}^{\\prime}\\) projects on to the column space of \\(\\widetilde{Z}_{2}\\).\n\n**Theorem 1**.: _For instrumental variables estimation of the coefficients in the full regression model (2) and the partial regression model (3), we have_\n\n1. _the estimated coefficient vectors_ \\(b_{2}\\) _in (_7_) and_ \\(\\tilde{b}_{2}\\) _in (_8_) are identical, and_\n2. _the residual vectors from the full model (_2_) and from the partial model (_3_) are identical._\n\nThe results in theorem 1 applies to the following special cases: (a) the instrumental variables (IV) estimator for an exactly identified model, when the number of instruments is exactly equal to the number "], "nougat": ["The full set of instrumental variables is given by\n\n\\[Z=\\left[W_{1}:Z_{2}\\right],\\]\n\nso that the number of instrumental variables, \\(k_{1}+k_{3}\\), can be larger than the number of endogenous variables, \\(k_{1}+k_{2}\\), i.e. we are possibly dealing with an overidentified model. Hence, I will consider the 2SLS estimator, also known as the generalized instrumental variables estimator ( Davidson and MacKinnon , 2004 , p. 321), for the full regression model ( 2 ). 4 The unique 2SLS estimator uses, from among all possible linear combination of the set of instruments, the one that minimizes the asymptotic covariance matrix of the instrumental variables estimator (Brundy and Jorgenson, 1971).\n\nFootnote 4: The 2SLS estimator for coefficients in the full regression model ( 2 ) is given by (Greene, 2012, p. 231),\n\n\\[b=\\begin{pmatrix}b_{1}\\\\ \\left(k_{1}\\times 1\\right)\\\\ b_{2}\\\\ \\left(k_{2}\\times 1\\right)\\end{pmatrix}=\\left[\\left(P_{Z}W\\right)^{\\prime}W \\right]^{-1}\\left(P_{Z}W\\right)^{\\prime}Y, \\tag{6}\\]\n\nwhere \\(P_{Z}=Z\\left(Z^{\\prime}Z\\right)^{-1}Z^{\\prime}\\) projects on to the column space of \\(Z\\). Hence,\n\n\\[b_{2}=\\ (2,1)\\text{ block of }\\left[\\left(P_{Z}W\\right)^{\\prime}W\\right]^{-1} \\left(P_{Z}W\\right)^{\\prime}Y. \\tag{7}\\]\n\nTurning to the partial model ( 3 ), we estimate it with the instrumental variables formed by the residualized set of instrumental variables, \\(\\widetilde{Z}_{2}=M_{W_{1}}Z_{2}\\), which is a \\(N\\times k_{3}\\) matrix. Thus, the 2SLS estimator for the partial regression model ( 3 ) is given by\n\n\\[\\tilde{b}_{2}=\\left[\\left(P_{\\widetilde{Z}_{2}}\\widetilde{W}_{2}\\right)^{ \\prime}\\widetilde{W}_{2}\\right]^{-1}\\left(P_{\\widetilde{Z}_{2}}\\widetilde{W}_{ 2}\\right)^{\\prime}\\widetilde{Y}, \\tag{8}\\]\n\nwhere \\(P_{\\widetilde{Z}_{2}}=\\widetilde{Z}_{2}\\left(\\widetilde{Z}_{2}^{\\prime} \\widetilde{Z}_{2}\\right)^{-1}\\widetilde{Z}_{2}^{\\prime}\\) projects on to the column space of \\(\\widetilde{Z}_{2}\\).\n\n**Theorem 1**.: _For instrumental variables estimation of the coefficients in the full regression model ( 2 ) and the partial regression model ( 3 ), we have_\n\n1. _the estimated coefficient vectors_ \\(b_{2}\\) _in (_7_) and_ \\(\\tilde{b}_{2}\\) _in (_8_) are identical, and_\n2. _the residual vectors from the full model ( 2 ) and from the partial model ( 3 ) are identical._\n\nThe results in theorem 1 applies to the following special cases: (a) the instrumental variables (IV) estimator for an exactly identified model, when the number of instruments is exactly equal to the number "]}, {"edit": ["(1980/81); see also Zitikis and Gastwirth (2002) for further references and statistical inference.\n\n**Note 4.1**.: The asymptotically negligible term \\(1/n\\) on the right-hand side of the first equation of \\(G_{n}\\) ensures that \\(G_{n}\\) makes sense for all sample sizes. Without this term we may get counterintuitive values. For example, when the 'incomes' are \\(X_{1}=1\\), \\(X_{2}=2\\) and \\(X_{3}=3\\), we have \\(G_{n}=2/9\\), whereas \\(G_{n}\\) without the added \\(1/n=1/3\\) would give the negative value \\(-1/9\\), which is incompatible with the meaning of the index.\n\nFinally, using the same arguments as above but now with the right-most expression for \\(G_{2}\\) given in Section 2.2 as our starting point, we arrive at\n\n\\[G_{2,n}=\\frac{\\bar{X}}{X_{\\lceil n/2\\rceil:n}}-\\frac{2}{n^{2}}\\sum_{i=1}^{n} \\frac{\\sum_{k=1}^{i}X_{k:n}}{X_{\\lceil n/2\\rceil:n}}\\]\n\nas an empirical estimator of \\(G_{2}\\). As before, \\(\\bar{X}\\) stands for the mean of \\(X_{1},\\ldots,X_{n}\\).\n\n### An analysis of capital incomes from the ECHP 2001 survey\n\nUsing the formulas for calculating the aforementioned indices from data, we now analyze capital incomes reported in the European Community Household Panel survey (ECHP, 2001) that was conducted by Eurostat in 2001, which is the last of the eight waves of the survey.\n\nSpecifically, the data come from 59,750 households with 121,122 persons from the fifteen European countries specified in Table 4.1 using the ISO 3166-1 alpha-2 (two-letter) codes. By looking at the means and medians in Table 4.1, we see how skewed to the right the distributions of the countries are. Figure 4.1 (with \\(G_{2,n}\\) excluded due to its large values) visualizes the index values calculated using formulas (4.1)-(4.3) and reported in Table 4.1. For a more detailed description of the data and relevant references, we refer to Greselin et al. (2014, Section 1). Next are several observations based on Table 4.1 and Figure 4.1.\n\nPortugal has the lowest value of \\(\\Psi_{1,n}\\), with the median income of the poorest \\(p\\times 100\\%\\) persons equal, after averaging over all \\(p\\in(0,1)\\), to \\(84.7\\%\\) of the median income of the entire population.\n\nThe opposite happens in France, which provides the highest contrast among the countries when comparing the median income of the poorest \\(p\\times 100\\%\\) persons with the overall median income: after averaging such ratios over all \\(p\\in(0,1)\\), we obtain \\(21.7\\%\\).\n\nFor France, we also observe the largest value of \\(\\Psi_{3,n}\\). The median income of the poorest \\(p\\times 100\\%\\) people is equal, after averaging over all \\(p\\in(0,1)\\), to only \\(15.5\\%\\) of the median "], "nougat": ["(1980/81 ); see also Zitikis and Gastwirth ( 2002 ) for further references and statistical inference.\n\n**Note 4.1.** The asymptotically negligible term \\(1/n\\) on the right-hand side of the first equation of \\(G_{n}\\) ensures that \\(G_{n}\\) makes sense for all sample sizes. Without this term we may get counterintuitive values. For example, when the \u2018incomes\u2019 are \\(X_{1}=1\\), \\(X_{2}=2\\) and \\(X_{3}=3\\), we have \\(G_{n}=2/9\\), whereas \\(G_{n}\\) without the added \\(1/n=1/3\\) would give the negative value \\(-1/9\\), which is incompatible with the meaning of the index.\n\nFinally, using the same arguments as above but now with the right-most expression for \\(G_{2}\\) given in Section 2.2 as our starting point, we arrive at\n\n\\[G_{2,n}=\\frac{\\bar{X}}{X_{\\lceil n/2\\rceil:n}}-\\frac{2}{n^{2}}\\sum_{i=1}^{n} \\frac{\\sum_{k=1}^{i}X_{k:n}}{X_{\\lceil n/2\\rceil:n}}\\]\n\nas an empirical estimator of \\(G_{2}\\). As before, \\(\\bar{X}\\) stands for the mean of \\(X_{1},\\ldots,X_{n}\\).\n\n### An analysis of capital incomes from the ECHP 2001 survey\n\nUsing the formulas for calculating the aforementioned indices from data, we now analyze capital incomes reported in the European Community Household Panel survey ( ECHP , 2001 ) that was conducted by Eurostat in 2001, which is the last of the eight waves of the survey.\n\nSpecifically, the data come from 59,750 households with 121,122 persons from the fifteen European countries specified in Table 4.1 using the ISO 3166-1 alpha-2 (two-letter) codes. By looking at the means and medians in Table 4.1 , we see how skewed to the right the distributions of the countries are. Figure 4.1 (with \\(G_{2,n}\\) excluded due to its large values) visualizes the index values calculated using formulas ( 4.1 )\u2013( 4.3 ) and reported in Table 4.1 . For a more detailed description of the data and relevant references, we refer to Greselin et al. ( 2014 , Section 1). Next are several observations based on Table 4.1 and Figure 4.1 .\n\nPortugal has the lowest value of \\(\\Psi_{1,n}\\), with the median income of the poorest \\(p\\times 100\\%\\) persons equal, after averaging over all \\(p\\in(0,1)\\), to \\(84.7\\%\\) of the median income of the entire population.\n\nThe opposite happens in France, which provides the highest contrast among the countries when comparing the median income of the poorest \\(p\\times 100\\%\\) persons with the overall median income: after averaging such ratios over all \\(p\\in(0,1)\\), we obtain \\(21.7\\%\\).\n\nFor France, we also observe the largest value of \\(\\Psi_{3,n}\\). The median income of the poorest \\(p\\times 100\\%\\) people is equal, after averaging over all \\(p\\in(0,1)\\), to only \\(15.5\\%\\) of the median "]}, {"edit": ["which is equivalent to\n\n\\[\\max_{\\underline{\\Lambda}}\\frac{1}{nT}\\mathrm{tr}\\left\\{\\mathbf{X}^{\\prime}\\mathbf{X} \\underline{\\Lambda}(\\mathbf{\\Lambda}^{\\prime}\\mathbf{\\Lambda})^{-1}\\underline{\\mathbf{ \\Lambda}}^{\\prime}\\right\\}=\\max_{\\underline{\\Lambda}}\\frac{1}{n}\\mathrm{tr} \\left\\{(\\underline{\\Lambda}^{\\prime}\\mathbf{\\Lambda})^{-1/2}\\underline{\\mathbf{ \\Lambda}}^{\\prime}\\frac{\\mathbf{X}^{\\prime}\\mathbf{X}}{T}\\underline{\\Lambda}(\\underline {\\Lambda}^{\\prime}\\mathbf{\\Lambda})^{-1/2}\\right\\} \\tag{7}\\]\n\nNow since by construction each column of \\(\\underline{\\mathbf{\\Lambda}}(\\underline{\\Lambda}^{\\prime}\\mathbf{\\Lambda})^{-1/2}\\) is normalized (since we assumed \\(\\frac{\\underline{\\Lambda}^{\\prime}\\mathbf{\\Lambda}}{n}\\) to be diagonal), then the above maximizaton, once solved, should return the \\(r\\) largest eigenvalues of \\(\\widehat{\\mathbf{\\Gamma}}^{x}\\) divided by \\(n\\), i.e., it must give \\(\\frac{\\widehat{\\mathbf{M}}^{x}}{n}\\). In other words, our estimator \\(\\widehat{\\mathbf{\\Lambda}}\\) must be such that \\(\\widehat{\\mathbf{\\Lambda}}(\\widehat{\\mathbf{\\Lambda}}^{\\prime}\\widehat{\\mathbf{\\Lambda}})^ {-1/2}\\) is the matrix of normalized eigenvectors corresponding the \\(r\\) largest eigenvalues of \\(\\frac{\\mathbf{X}^{\\prime}\\mathbf{X}}{nT}\\), i.e., such that:\n\n\\[(\\widehat{\\mathbf{\\Lambda}}^{\\prime}\\widehat{\\mathbf{\\Lambda}})^{-1/2}\\widehat{\\mathbf{ \\Lambda}}^{\\prime}\\frac{\\mathbf{X}^{\\prime}\\mathbf{X}}{nT}\\widehat{\\mathbf{\\Lambda}}( \\widehat{\\mathbf{\\Lambda}}^{\\prime}\\widehat{\\mathbf{\\Lambda}})^{-1/2}=\\frac{\\widehat{ \\mathbf{M}}^{x}}{n}, \\tag{8}\\]\n\nbut also, by definition of eigenvectors,\n\n\\[\\widehat{\\mathbf{V}}^{x\\prime}\\frac{\\mathbf{X}^{\\prime}\\mathbf{X}}{nT}\\widehat{\\mathbf{V}}^{x }=\\frac{\\widehat{\\mathbf{M}}^{x}}{n}. \\tag{9}\\]\n\nTherefore, from (8) and (9),\n\n\\[\\widehat{\\mathbf{\\Lambda}}=\\widehat{\\mathbf{V}}^{x}(\\widehat{\\mathbf{M}}^{x})^{1/2}, \\tag{10}\\]\n\nwhich is such that \\(\\frac{\\widehat{\\mathbf{\\Lambda}}^{\\prime}\\widehat{\\mathbf{\\Lambda}}}{n}=\\frac{\\widehat {\\mathbf{M}}^{x}}{n}\\) is diagonal. The factors are then estimated as the linear projections: \\(\\widehat{\\mathbf{F}}=\\mathbf{X}\\widehat{\\mathbf{\\Lambda}}(\\widehat{\\mathbf{\\Lambda}}^{\\prime} \\widehat{\\mathbf{\\Lambda}})^{-1}=\\mathbf{X}\\widehat{\\mathbf{V}}^{x}(\\widehat{\\mathbf{M "], "nougat": ["which is equivalent to\n\n\\[\\max_{\\underline{\\Lambda}}\\frac{1}{nT}\\mathrm{tr}\\left\\{\\mathbf{X}^{\\prime}\\mathbf{X} \\underline{\\Lambda}(\\mathbf{\\Lambda}^{\\prime}\\mathbf{\\Lambda})^{-1}\\underline{\\mathbf{ \\Lambda}}^{\\prime}\\right\\}=\\max_{\\underline{\\Lambda}}\\frac{1}{n}\\mathrm{tr} \\left\\{(\\underline{\\Lambda}^{\\prime}\\mathbf{\\Lambda})^{-1/2}\\underline{\\mathbf{ \\Lambda}}^{\\prime}\\frac{\\mathbf{X}^{\\prime}\\mathbf{X}}{T}\\underline{\\Lambda}(\\underline {\\Lambda}^{\\prime}\\mathbf{\\Lambda})^{-1/2}\\right\\} \\tag{7}\\]\n\nNow since by construction each column of \\(\\underline{\\mathbf{\\Lambda}}(\\underline{\\Lambda}^{\\prime}\\mathbf{\\Lambda})^{-1/2}\\) is normalized (since we assumed \\(\\frac{\\underline{\\Lambda}^{\\prime}\\mathbf{\\Lambda}}{n}\\) to be diagonal), then the above maximizaton, once solved, should return the \\(r\\) largest eigenvalues of \\(\\widehat{\\mathbf{\\Gamma}}^{x}\\) divided by \\(n\\), i.e., it must give \\(\\frac{\\widehat{\\mathbf{M}}^{x}}{n}\\). In other words, our estimator \\(\\widehat{\\mathbf{\\Lambda}}\\) must be such that \\(\\widehat{\\mathbf{\\Lambda}}(\\widehat{\\mathbf{\\Lambda}}^{\\prime}\\widehat{\\mathbf{\\Lambda}})^ {-1/2}\\) is the matrix of normalized eigenvectors corresponding the \\(r\\) largest eigenvalues of \\(\\frac{\\mathbf{X}^{\\prime}\\mathbf{X}}{nT}\\), i.e., such that:\n\n\\[(\\widehat{\\mathbf{\\Lambda}}^{\\prime}\\widehat{\\mathbf{\\Lambda}})^{-1/2}\\widehat{\\mathbf{ \\Lambda}}^{\\prime}\\frac{\\mathbf{X}^{\\prime}\\mathbf{X}}{nT}\\widehat{\\mathbf{\\Lambda}}( \\widehat{\\mathbf{\\Lambda}}^{\\prime}\\widehat{\\mathbf{\\Lambda}})^{-1/2}=\\frac{\\widehat{ \\mathbf{M}}^{x}}{n}, \\tag{8}\\]\n\nbut also, by definition of eigenvectors,\n\n\\[\\widehat{\\mathbf{V}}^{x\\prime}\\frac{\\mathbf{X}^{\\prime}\\mathbf{X}}{nT}\\widehat{\\mathbf{V}}^{x }=\\frac{\\widehat{\\mathbf{M}}^{x}}{n}. \\tag{9}\\]\n\nTherefore, from ( 8 ) and ( 9 ),\n\n\\[\\widehat{\\mathbf{\\Lambda}}=\\widehat{\\mathbf{V}}^{x}(\\widehat{\\mathbf{M}}^{x})^{1/2}, \\tag{10}\\]\n\nwhich is such that \\(\\frac{\\widehat{\\mathbf{\\Lambda}}^{\\prime}\\widehat{\\mathbf{\\Lambda}}}{n}=\\frac{\\widehat {\\mathbf{M}}^{x}}{n}\\) is diagonal. The factors are then estimated as the linear projections: \\(\\widehat{\\mathbf{F}}=\\mathbf{X}\\widehat{\\mathbf{\\Lambda}}(\\widehat{\\mathbf{\\Lambda}}^{\\prime} \\widehat{\\mathbf{\\Lambda}})^{-1}=\\mathbf{X}\\widehat{\\mathbf{V}}^{x}(\\widehat{ "]}, {"edit": ["of this transformative change. While previous studies have primarily focused on examining historical events from a more distant past, there has been relatively limited investigation into recent changes in history and their effects on gender roles. By providing insights into the influence of constitutional inclusion and the promotion of gender equality through education, our research enhances our understanding of the dynamics surrounding gender norms. It emphasizes the crucial role of constitutional frameworks in shaping outcomes related to gender equality and sheds light on the impact of societal attitudes and women's labor force participation.\n\nThe rest of the paper is organized as follows. Section 2 provides an overview of the process of creating and drafting the 1991 constitution, focusing on the inclusion of women's rights. Section 3 outlines the data utilized in our study and presents our empirical strategy. Section 4 presents our main findings. Section 5 offers additional evidence supporting our primary identification assumption and provides a range of robustness checks. Section 6 concludes.\n\n## 2 Background\n\nColombia, throughout the majority of the 20th century, adhered to traditional and conservative gender norms deeply embedded in the long-standing political constitution dating back to 1886. Despite a notable constitutional reform in 1936 and various legislative measures aimed at protecting women's economic and social rights, the formal recognition of women's rights as a constitutional requirement was only accomplished with the drafting of a new political constitution in 1991 (Iregui-Bohorquez _et al._, 2020).\n\nThe National Constituent Assembly (NCA) of 1991 played a pivotal role in this process. Convened specifically to draft the current political constitution, known as the Constitution of 1991, it marked a significant turning point in Colombia's 20th-century history. The NCA aimed to establish an institutional framework to address the persistent cycles of violence and armed conflicts. The approval of the new Constitution formalized a social pac and provided the foundation for the development of a social state of law, promoting the peaceful resolution of social and political conflicts. This transformative process brought together various initiatives and fostered the expansion of representative democracy.\n\nUnder the new constitution, significant progress was made in recognizing and protecting the rights of marginalized groups, including the recognition of women's rights that were absent from the original 1886 constitution. Subsequent reforms addressed this gap by granting women the right to vote, access higher education, exercise independent control over their assets in marriage, and abolishing marital authority, among other changes. These advancements primarily took place within the legislative domain rather than the constitutional realm. However, it is important to note that despite these legal improvements, "], "nougat": ["of this transformative change. While previous studies have primarily focused on examining historical events from a more distant past, there has been relatively limited investigation into recent changes in history and their effects on gender roles. By providing insights into the influence of constitutional inclusion and the promotion of gender equality through education, our research enhances our understanding of the dynamics surrounding gender norms. It emphasizes the crucial role of constitutional frameworks in shaping outcomes related to gender equality and sheds light on the impact of societal attitudes and women\u2019s labor force participation.\n\nThe rest of the paper is organized as follows. Section 2 provides an overview of the process of creating and drafting the 1991 constitution, focusing on the inclusion of women\u2019s rights. Section 3 outlines the data utilized in our study and presents our empirical strategy. Section 4 presents our main findings. Section 5 offers additional evidence supporting our primary identification assumption and provides a range of robustness checks. Section 6 concludes.\n\n## 2 Background\n\nColombia, throughout the majority of the 20th century, adhered to traditional and conservative gender norms deeply embedded in the long-standing political constitution dating back to 1886. Despite a notable constitutional reform in 1936 and various legislative measures aimed at protecting women\u2019s economic and social rights, the formal recognition of women\u2019s rights as a constitutional requirement was only accomplished with the drafting of a new political constitution in 1991 ( Iregui-Boh \u0301orquez _et al._, 20th-century history. The NCA aimed to establish an institutional framework to address the persistent cycles of violence and armed conflicts. The approval of the new Constitution formalized a social pact and provided the foundation for the development of a social state of law, promoting the peaceful resolution of social and political conflicts. This transformative process brought together various initiatives and fostered the expansion of representative democracy.\n\nUnder the new constitution, significant progress was made in recognizing and protecting the rights of marginalized groups, including the recognition of women\u2019s rights that were absent from the original 1886 constitution. Subsequent reforms addressed this gap by granting women the right to vote, access higher education, exercise independent control over their assets in marriage, and abolishing marital authority, among other changes. These advancements primarily took place within the legislative domain rather than the constitutional realm. However, it is important to note that despite these legal improvements, "]}, {"edit": ["and the proof is finished. Therefore, we assume that some student is rejected by some school in the SPDA of Round \\(k-1\\).\n\nSuppose not; that is,\n\n\\[I^{\\prime}=\\left\\{i\\in I\\ \\left|\\ \\hat{\\mu}^{k-1}\\left(i\\right)P_{i}\\hat{\\mu}^{k} \\left(i\\right)\\right.\\right\\}\\neq\\emptyset.\\]\n\nSince \\(\\hat{\\mu}^{k}\\left(i\\right)=\\hat{\\mu}^{k-1}\\left(i\\right)\\) for all \\(i\\in\\mathcal{E}^{k-1}\\), \\(I^{\\prime}\\subseteq I\\setminus\\mathcal{E}^{k-1}\\). Let \\(i\\in I^{\\prime}\\) be the student who is rejected by \\(\\hat{\\mu}^{k-1}\\left(i\\right)=DA\\left(G_{S}^{k-1}\\right)\\left(i\\right)\\) in the earliest step of the SPDA in Round \\(k\\) among \\(I^{\\prime}\\). Let \\(s=DA\\left(G_{S}^{k-1}\\right)\\left(i\\right)\\) and \\(t\\) be that step. Then, in Step \\(t\\) (of the SPDA in Round \\(k\\)), there are \\(q_{s}\\) students who is temporarily accepted by \\(s\\). We let \\(j\\) be a student who is temporarily accepted by \\(s\\) in Step \\(t\\). Then, \\(sR_{j}DA\\left(G_{S}^{k}\\right)\\left(j\\right)\\) and \\(\\left(j,i\\right)\\in\\succ_{s}^{\\ast}\\). On the other hand, since \\(i\\in\\hat{\\mu}^{k-1}\\left(s\\right)\\) and \\(\\hat{\\mu}^{k-1}\\) is stable, there is \\(j\\notin\\hat{\\mu}^{k-1}\\left(s\\right)\\) among the temporarily accepted students. This implies \\(DA\\left(G_{S}^{k-1}\\right)\\left(j\\right)P_{j}s\\) and therefore \\(j\\in I^{\\prime}\\). Since \\(j\\) is temporarily accepted by \\(s\\) in Step \\(t\\), \\(j\\) is rejected by \\(DA\\left(G_{S}^{k-1}\\right)\\left(j\\right)\\) before Step \\(t\\). Since \\(i\\) is the student who is firstly rejected by \\(DA\\left(G_{S}^{k-1}\\right)\\left(i\\right)\\), this is a contradiction. **Q.E.D.**\n\n**Claim 2**: _If \\(\\hat{\\mu}^{k-1}\\) is stable for \\(\\succ\\), then \\(\\left|\\hat{\\mu}^{k-1}\\left(s\\right)\\right|=\\left|\\hat{\\mu}^{k}\\left(s\\right) \\right|\\) for all \\(s\\in S\\)._\n\n**Proof.** Suppose that \\(\\hat{\\mu}^{k-1}\\) is stable for \\(\\succ\\). By Claim 1 and the individually rationality of \\(\\hat{\\mu}^{k-1}\\), for any \\(i\\) such that \\(\\hat{\\mu}^{k-1}\\left(i\\right)\\in S\\), we have \\(\\hat{\\mu}^{k}\\left(i\\right)R_{i "], "nougat": ["and the proof is finished. Therefore, we assume that some student is rejected by some school in the SPDA of Round \\(k-1\\).\n\nSuppose not; that is,\n\n\\[I^{\\prime}=\\left\\{i\\in I\\ \\left|\\ \\hat{\\mu}^{k-1}\\left(i\\right)P_{i}\\hat{\\mu}^{k} \\left(i\\right)\\right.\\right\\}\\neq\\emptyset.\\]\n\nSince \\(\\hat{\\mu}^{k}\\left(i\\right)=\\hat{\\mu}^{k-1}\\left(i\\right)\\) for all \\(i\\in\\mathcal{E}^{k-1}\\), \\(I^{\\prime}\\subseteq I\\setminus\\mathcal{E}^{k-1}\\). Let \\(i\\in I^{\\prime}\\) be the student who is rejected by \\(\\hat{\\mu}^{k-1}\\left(i\\right)=DA\\left(G_{S}^{k-1}\\right)\\left(i\\right)\\) in the earliest step of the SPDA in Round \\(k\\) among \\(I^{\\prime}\\). Let \\(s=DA\\left(G_{S}^{k-1}\\right)\\left(i\\right)\\) and \\(t\\) be that step. Then, in Step \\(t\\) (of the SPDA in Round \\(k\\)), there are \\(q_{s}\\) students who is temporarily accepted by \\(s\\). We let \\(j\\) be a student who is temporarily accepted by \\(s\\) in Step \\(t\\). Then, \\(sR_{j}DA\\left(G_{S}^{k}\\right)\\left(j\\right)\\) and \\(\\left(j,i\\right)\\in\\succ_{s}^{\\ast}\\). On the other hand, since \\(i\\in\\hat{\\mu}^{k-1}\\left(s\\right)\\) and \\(\\hat{\\mu}^{k-1}\\) is stable, there is \\(j\\notin\\hat{\\mu}^{k-1}\\left(s\\right)\\) among the temporarily accepted students. This implies \\(DA\\left(G_{S}^{k-1}\\right)\\left(j\\right)P_{j}s\\) and therefore \\(j\\in I^{\\prime}\\). Since \\(j\\) is temporarily accepted by \\(s\\) in Step \\(t\\), \\(j\\) is rejected by \\(DA\\left(G_{S}^{k-1}\\right)\\left(j\\right)\\) before Step \\(t\\). Since \\(i\\) is the student who is firstly rejected by \\(DA\\left(G_{S}^{k-1}\\right)\\left(i\\right)\\), this is a contradiction. **Q.E.D.**\n\n**Claim 2**: _If \\(\\hat{\\mu}^{k-1}\\) is stable for \\(\\succ\\), then \\(\\left|\\hat{\\mu}^{k-1}\\left(s\\right)\\right|=\\left|\\hat{\\mu}^{k}\\left(s\\right) \\right|\\) for all \\(s\\in S\\)._\n\n**Proof.** Suppose that \\(\\hat{\\mu}^{k-1}\\) is stable for \\(\\succ\\). By Claim 1 and the individually rationality of \\(\\hat{\\mu}^{k-1}\\), for any \\(i\\) such that \\(\\hat{\\mu}^{k-1}\\left(i\\right)\\in S\\), we have \\(\\hat{\\mu}^{k}\\left(i\\right)R_{i "]}, {"edit": ["_([color For a reordering of the blocks of transfer functions in Lemma 1\\(k^{(j)}\\) is strictly miniphase and bounded away from one, i.e. \\(\\lambda_{q}(k^{(j)}(\\theta))>s>0\\) for all \\(\\theta\\)._\n\n**Theorem 10**: _Let \\((y_{it})\\) be a purely nondeterministic \\(q\\)-DFS. Then the innovations \\((\\varepsilon_{t})\\) of the common component are causally subordinated to \\((y_{it})\\), i.e. \\(\\varepsilon_{t}\\in\\mathbb{H}_{t}(y)\\)._\n\nBy Theorem 9, it immediately follows, that also the common component of a purely non-deterministic \\(q\\)-DFS, is causally subordinated to the output, i.e. \\(\\chi_{it}\\in\\mathbb{H}_{t}(y)\\). Note that a violation of the strict miniphase condition is not an impediment for causal subordination and can be relaxed. Consider a 1-DFS, given by\n\n\\[y_{it}=\\chi_{it}+\\xi_{it}=(1-L)u_{t}+\\xi_{it}\\ ,\\]\n\nwhere \\((u_{t})\\) is orthonormal white noise and \\((\\xi_{it})\\) is dynamically idiosyncratic. Taking the cross sectional average over \\((y_{it})\\) will reveal \\((1-L)u_{t}\\) from which we can compute the innovations \\((u_{t})\\) directly and causally subordinated to \\((y_{it})\\).\n\n**Proof.** Proof of Theorem 10 Suppose that \\((k_{i}:i\\in\\mathbb{N})\\) is in an order such that all \\(q\\times q\\) blocks \\(k^{(j)}\\) for \\(j=1,2,...\\) are of full rank a.e. on \\(\\Theta\\) (Lemma 1). Let \\(y_{it}=\\chi_{it}+\\xi_{it}\\) be the corresponding decomposition from Theorem 2. Again, suppose \\(q\\) divides \\(n\\) without loss of generality. We look at\n\n\\[\\chi_{t}^{n}=\\begin{pmatrix}\\chi_{t}^{(1)}\\\\ \\chi_{t}^{(2)}\\\\ \\vdots\\\\ \\chi_{t}^{(n/q)}\\end{pmatrix}=\\begin{pmatrix}\\underline{k}^{(1)}(L)\\\\ \\vdots\\\\ \\underline{k}^{(J)}(L)\\end{pmatrix}\\varepsilon_{t}=\\begin{pmatrix}\\underline{k} ^{(1)}(L)\\\\ &\\ddots\\\\ &&\\underline{k}^{(n/q)}(L)\\end{pmatrix}\\begin{pmatrix}I_{q}\\\\ \\vdots\\\\ I_{q}\\end{pmatrix}\\varepsilon_{t}\\ .\\]\n\nBy the Wold Representation, and Theorem 9, we know that all \\(k^{(j)},j=1,...,n/q\\) are analytic in the open unit disc and \\(\\det\\underline{k}^{(j)}(z)\\neq 0\\) for all \\(|z|<1\\) and \\(j=1,2,...\\).\n\n\\[\\varphi_{t}^{n} \\coloneqq\\begin{pmatrix}\\left(\\underline{k}^{(1)}\\right)^{-1}(L) \\\\ &\\ddots\\\\ &&\\left(\\underline{k}^{(n/q)}\\right)^{-1}(L)\\end{pmatrix}\\begin{pmatrix}y_{t}^{ (1) "], "nougat": ["_([color For a reordering of the blocks of transfer functions in Lemma 1\\(k^{(j)}\\) is strictly miniphase and bounded away from one, i.e. \\(\\lambda_{q}(k^{(j)}(\\theta))>s>0\\) for all \\(\\theta\\)._\n\n**Theorem 10**: _Let \\((y_{it})\\) be a purely nondeterministic \\(q\\)-DFS. Then the innovations \\((\\varepsilon_{t})\\) of the common component are causally subordinated to \\((y_{it})\\), i.e. \\(\\varepsilon_{t}\\in\\mathbb{H}_{t}(y)\\)._\n\nBy Theorem 9 , it immediately follows, that also the common component of a purely non-deterministic \\(q\\)-DFS, is causally subordinated to the output, i.e. \\(\\chi_{it}\\in\\mathbb{H}_{t}(y)\\). Note that a violation of the strict miniphase condition is not an impediment for causal subordination and can be relaxed. Consider a 1-DFS, given by\n\n\\[y_{it}=\\chi_{it}+\\xi_{it}=(1-L)u_{t}+\\xi_{it}\\ ,\\]\n\nwhere \\((u_{t})\\) is orthonormal white noise and \\((\\xi_{it})\\) is dynamically idiosyncratic. Taking the cross sectional average over \\((y_{it})\\) will reveal \\((1-L)u_{t}\\) from which we can compute the innovations \\((u_{t})\\) directly and causally subordinated to \\((y_{it})\\).\n\n**Proof.** Proof of Theorem 10 Suppose that \\((k_{i}:i\\in\\mathbb{N})\\) is in an order such that all \\(q\\times q\\) blocks \\(k^{(j)}\\) for \\(j=1,2,...\\) are of full rank a.e. on \\(\\Theta\\) (Lemma 1). Let \\(y_{it}=\\chi_{it}+\\xi_{it}\\) be the corresponding decomposition from Theorem 2 . Again, suppose \\(q\\) divides \\(n\\) without loss of generality. We look at\n\n\\[\\chi_{t}^{n}=\\begin{pmatrix}\\chi_{t}^{(1)}\\\\ \\chi_{t}^{(2)}\\\\ \\vdots\\\\ \\chi_{t}^{(n/q)}\\end{pmatrix}=\\begin{pmatrix}\\underline{k}^{(1)}(L)\\\\ \\vdots\\\\ \\underline{k}^{(J)}(L)\\end{pmatrix}\\varepsilon_{t}=\\begin{pmatrix}\\underline{k} ^{(1)}(L)\\\\ &\\ddots\\\\ &&\\underline{k}^{(n/q)}(L)\\end{pmatrix}\\begin{pmatrix}I_{q}\\\\ \\vdots\\\\ I_{q}\\end{pmatrix}\\varepsilon_{t}\\ .\\]\n\nBy the Wold Representation, and Theorem 9 , we know that all \\(k^{(j)},j=1,...,n/q\\) are analytic in the open unit disc and \\(\\det\\underline{k}^{(j)}(z)\\neq 0\\) for all \\(|z|<1\\) and \\(j=1,2,...\\).\n\n\\[\\varphi_{t}^{n} \\coloneqq\\begin{pmatrix}\\left(\\underline{k}^{(1)}\\right)^{-1}(L) \\\\ &\\ddots\\\\ &&\\left(\\underline{k}^{(n/q)}\\right)^{-1}(L)\\end{pmatrix}\\begin{pmatrix}y_{t}^ {( "]}, {"edit": ["Of course, the best way to answer the question about the quality of the approximation of the characteristics of the posterior forecast distribution is to compare it with the characteristics of the posterior forecast distribution itself. Unfortunately, for most simulation models, and in particular for ABM, which is the focus of this paper, constructing an exact posterior distribution of forecasts is impossible, and approximations of it based on MCMC algorithms (see Andrieu et al. (2010)) or sequential Monte Carlo algorithms (SMC; see Chopin et al. (2012)) require computationally complex algorithms with a particle filter (see Gordon et al. (1993)) or variations of it.\n\n**Forecast error standardization.** In research on probabilistic time series forecasting,7 the forecasts are often tested by interval calibration or probability integral transforms. These methods cannot be applied directly to a case in which only the mean and the standard deviation of the forecast are estimated, since the probability distribution of the forecasts is not fully specified. However, it can be noted that, similarly to the probability integral transforms, which must match the normal distribution and have zero autocorrelation, the standardized forecast errors (with the mean removed and divided by the standard deviation) should have the same properties (zero mean and unit standard deviation), other than the form of distribution,8 for a well-trained model. Thus, we look at the mean and the standard deviation of the standardized forecast errors and the mean and the standard deviation of the product of standardized errors separated by \\(k\\) periods to test the quality. The latter, in fact, is equivalent to testing for autocorrelation, but it does not require the adjustment of the asymptotic distribution due to the finite length of the time series. Thus, there is no worry that the distribution of the correlation estimates will have a mean and standard deviation different from zero and one.\n\nFootnote 7: See Clark (2011)\n\nFootnote 8: The distribution need not be normal.\n\nThere are two points to note about this quality metric. First, the results of the test described above should be considered one stage of the verification of the quality of the model. Not passing it should serve as a signal of problems with the quality of the neural network. The reverse situation, in which the test is passed, is not a guarantee that the model works well. For example, if the conditional forecasting model is poorly trained and does not take the presence of a scenario into account in any way, but produces only an unconditional forecast, it will pass the test. Second, we cannot apply the standard formal hypothesis testing about the mean and standard deviation, since the drift of the neural network coefficients at non-zero learning rates (see Mandt et al. (2017)) makes a comparable contribution to the distribution of the mean and standard deviation estimates.9 Therefore, below, we look at these quantities without formal hypothesis testing.\n\nFootnote 9: It is probable that the estimates could be improved by applying ensembles of models based on several runs of the training procedure, or by using averaging at different iterations within the same training procedure, but this issue is beyond the scope of this paper.\n\n**Comparison with the benchmark model.** As mentioned above, passing the test on standardized forecast errors is only an indirect confirmation of the quality of the neural network, since, among other things, it can be passed by models that do not take all relevant information into account. To see how well the neural network takes historical information into account, we estimate the lower bound of forecast quality on a test dataset.\n\nSuch an estimate can be made with a benchmark model. Note that after optimization, the neural network should have the smallest mean square forecast error (hereinafter, MSFE) for each variable and for each horizon by construction of the loss function. Moreover, the mean log predictive scores (hereinafter, LPS) should be the largest in the class of normal distributions. These two facts allow us to conclude that, no matter which forecasting model we build on the test data, it should not outperform the neural network in terms of MSFE and LPS.\n\nThe benchmark model should be chosen based on a balance of flexibility and training time. On the one hand, the more flexible the model, the tighter the lower bound will be. On the other hand, estimating the lower bound should take adequate time. A representative test dataset often contains thousands or tens of thousands of time series sets, so it is necessary to train the model hundreds of thousands of times (the number of test sets multiplied by the number of periods in which forecasts are made) to test one forecast horizon for one variable.\n\nIn this paper, vector autoregression (VAR) with ridge regularization is chosen as a benchmark model. It is computationally easy enough to estimate and allows the calculation of both conditional and unconditional "], "nougat": ["Of course, the best way to answer the question about the quality of the approximation of the characteristics of the posterior forecast distribution is to compare it with the characteristics of the posterior forecast distribution itself. Unfortunately, for most simulation models, and in particular for ABM, which is the focus of this paper, constructing an exact posterior distribution of forecasts is impossible, and approximations of it based on MCMC algorithms (see Andrieu et al. [2010]) or sequential Monte Carlo algorithms (SMC; see Chopin et al. [2012]) require computationally complex algorithms with a particle filter (see Gordon et al. [1993]) or variations of it.\n\n**Forecast error standardization.** In research on probabilistic time series forecasting, 7 the forecasts See Clark [2011] are often tested by interval calibration or probability integral transforms. These methods cannot be applied directly to a case in which only the mean and the standard deviation of the forecast are estimated, since the probability distribution of the forecasts is not fully specified. However, it can be noted that, similarly to the probability integral transforms, which must match the normal distribution and have zero autocorrelation, the standardized forecast errors (with the mean removed and divided by the standard deviation) should have the same properties (zero mean and unit standard deviation), other than the form of distribution, 8 for a well-trained model. Thus, we look at the mean and the standard deviation of the standardized forecast errors and the mean and the standard deviation of the product of standardized errors separated by \\(k\\) periods to test the quality. The latter, in fact, is equivalent to testing for autocorrelation, but it does not require the adjustment of the asymptotic distribution due to the finite length of the time series. Thus, there is no worry that the distribution of the correlation estimates will have a mean and standard deviation different from zero and one.\n\nFootnote 7: See Clark [2017]) makes a comparable contribution to the distribution of the mean and standard deviation estimates. 9 Therefore, below, we look at these quantities without formal hypothesis testing.\n\nFootnote 9: It is probable that the estimates could be improved by applying ensembles of models based on several runs of the training procedure, or by using averaging at different iterations within the same training procedure, but this issue is beyond the scope of this paper.\n\nThere are two points to note about this quality metric. First, the results of the test described above should be considered one stage of the verification of the quality of the model. Not passing it should serve as a signal of problems with the quality of the neural network. The reverse situation, in which the test is passed, is not a guarantee that the model works well. For example, if the conditional forecasting model is poorly trained and does not take the presence of a scenario into account in any way, but produces only an unconditional forecast, it will pass the test. Second, we cannot apply the standard formal hypothesis testing. the mean and standard deviation, since the drift of the neural network coefficients at non-zero learning rates (see Mandt et al. [2017]) makes a comparable contribution to the distribution of the mean and standard deviation estimates. 9 Therefore, below, we look at these quantities without formal hypothesis testing.\n\nFootnote 9: The distribution need not be normal.\n\n**Comparison with the benchmark model.** As mentioned above, passing the test on standardized forecast errors is only an indirect confirmation of the quality of the neural network, since, among other things, it can be passed by models that do not take all relevant information into account. To see how well the neural network takes historical information into account, we estimate the lower bound of forecast quality on a test dataset.\n\nSuch an estimate can be made with a benchmark model. Note that after optimization, the neural network should have the smallest mean square forecast error (hereinafter, MSFE) for each variable and for each horizon by construction of the loss function. Moreover, the mean log predictive scores (hereinafter, LPS) should be the largest in the class of normal distributions. These two facts allow us to conclude that, no matter which forecasting model we build on the test data, it should not outperform the neural network in terms of MSFE and LPS.\n\nThe benchmark model should be chosen based on a balance of flexibility and training time. On the one hand, the more flexible the model, the tighter the lower bound will be. On the other hand, estimating the lower bound should take adequate time. A representative test dataset often contains thousands or tens of thousands of time series sets, so it is necessary to train the model hundreds of thousands of times (the number of test sets multiplied by the number of periods in which forecasts are made) to test one forecast horizon for one variable.\n\nIn this paper, vector autoregression "]}, {"edit": ["Proof.: Consider a failing set \\(J\\subseteq C\\). Since \\(T\\) is a fault-tolerant committee, there exists a valid replacement set \\(R\\) such that \\((|(T\\setminus J)\\cup R|\\leqslant|T|\\) and \\(\\sigma_{0}((T\\setminus J)\\cup R)\\leqslant\\sigma_{f}(T)\\). Let \\(L^{\\prime}=\\{l_{a}\\in L:l_{a}\\cap((T\\setminus J)\\cup R)\\neq\\emptyset\\}\\). Moreover, let \\(J^{*}=J\\cap T^{*}\\). Then we will show that there exists a replacement set \\(R^{*}\\) for \\(J^{*}\\) such that \\(|R^{*}|\\leqslant|J^{*}|\\) and \\(\\sigma_{0}((T^{*}\\setminus J^{*})\\cup R^{*})-\\sigma_{0}((T\\setminus J)\\cup R) \\leqslant 2/h\\).\n\nWe construct the set \\(R^{*}\\) as follows: Consider a cell \\(l_{a}\\in L^{\\prime}\\). Since \\(l_{a}\\cap((T\\setminus J)\\cup R)\\neq\\emptyset\\), \\(l_{a}\\) is nonempty. This implies \\(T^{*}\\cap l_{a}\\neq\\emptyset\\) from the way we construct \\(T^{*}\\). Let \\(c_{a}\\) be the only candidate in \\(T^{*}\\cap l_{a}\\). If \\(c_{a}\\in J\\), then we replace \\(c_{a}\\) with an arbitrary \\(c^{\\prime}_{a}\\in l_{a}\\setminus J\\) (i.e., we add \\(c^{\\prime}_{a}\\) to \\(R^{*}\\)). We know that such a candidate \\(c^{\\prime}_{a}\\) exists because \\(l_{a}\\cap((T\\setminus J)\\cup R)\\neq\\emptyset\\). Since we only add a candidate to \\(R^{*}\\) from \\(l_{a}\\) such that \\(J^{*}\\cap l_{a}\\neq\\emptyset\\), we have \\(|R^{*}|\\leqslant|J^{*}|\\).\n\nWe will now show that \\(\\sigma_{0}((T^{*}\\setminus J^{*})\\cup R^{*})-\\sigma_{0}((T\\setminus J)\\cup R) \\leqslant 2/h\\). Observe that for a pair of candidates \\(c_{1},c_{2}\\in l_{a}\\), \\(d(c_{1},c_{2})\\leqslant 2/h\\) (see Figure 4). Let \\(C_{a}=l_{a}\\cap((T\\setminus J)\\cup R)\\). Moreover, let \\(V_{a}\\subseteq V\\) be the set of voters which have a candidate in \\(C_{a}\\) as their closest candidate in the committee \\((T\\setminus J)\\cup R\\). Using the triangle inequality, we know that \\(d(V_{a},c^{\\prime}_{a})\\leqslant d(V_{a},C_{a})+2/h\\). The above statement holds for all cells \\(l_{a}\\in L^{\\prime}\\). Therefore, \\(\\sigma_{0}((T^{*}\\setminus J^{*})\\cup R^{*})-\\sigma_{0}((T\\setminus J)\\cup R) \\leqslant 2/h "], "nougat": ["Proof.: Consider a failing set \\(J\\subseteq C\\). Since \\(T\\) is a fault-tolerant committee, there exists a valid replacement set \\(R\\) such that \\((|(T\\setminus J)\\cup R|\\leqslant|T|\\) and \\(\\sigma_{0}((T\\setminus J)\\cup R)\\leqslant\\sigma_{f}(T)\\). Let \\(L^{\\prime}=\\{l_{a}\\in L:l_{a}\\cap((T\\setminus J)\\cup R)\\neq\\emptyset\\}\\). Moreover, let \\(J^{*}=J\\cap T^{*}\\). Then we will show that there exists a replacement set \\(R^{*}\\) for \\(J^{*}\\) such that \\(|R^{*}|\\leqslant|J^{*}|\\) and \\(\\sigma_{0}((T^{*}\\setminus J^{*})\\cup R^{*})-\\sigma_{0}((T\\setminus J)\\cup R) \\leqslant 2/h\\).\n\nWe construct the set \\(R^{*}\\) as follows: Consider a cell \\(l_{a}\\in L^{\\prime}\\). Since \\(l_{a}\\cap((T\\setminus J)\\cup R)\\neq\\emptyset\\), \\(l_{a}\\) is nonempty. This implies \\(T^{*}\\cap l_{a}\\neq\\emptyset\\) from the way we construct \\(T^{*}\\). Let \\(c_{a}\\) be the only candidate in \\(T^{*}\\cap l_{a}\\). If \\(c_{a}\\in J\\), then we replace \\(c_{a}\\) with an arbitrary \\(c^{\\prime}_{a}\\in l_{a}\\setminus J\\) (i.e., we add \\(c^{\\prime}_{a}\\) to \\(R^{*}\\)). We know that such a candidate \\(c^{\\prime}_{a}\\) exists because \\(l_{a}\\cap((T\\setminus J)\\cup R)\\neq\\emptyset\\). Since we only add a candidate to \\(R^{*}\\) from \\(l_{a}\\) such that \\(J^{*}\\cap l_{a}\\neq\\emptyset\\), we have \\(|R^{*}|\\leqslant|J^{*}|\\).\n\nWe will now show that \\(\\sigma_{0}((T^{*}\\setminus J^{*})\\cup R^{*})-\\sigma_{0}((T\\setminus J)\\cup R) \\leqslant 2/h\\). Observe that for a pair of candidates \\(c_{1},c_{2}\\in l_{a}\\), \\(d(c_{1},c_{2})\\leqslant 2/h\\) (see Figure 4). Let \\(C_{a}=l_{a}\\cap((T\\setminus J)\\cup R)\\). Moreover, let \\(V_{a}\\subseteq V\\) be the set of voters which have a candidate in \\(C_{a}\\) as their closest candidate in the committee \\((T\\setminus J)\\cup R\\). Using the triangle inequality, we know that \\(d(V_{a},c^{\\prime}_{a})\\leqslant d(V_{a},C_{a})+2/h\\). The above statement holds for all cells \\(l_{a}\\in L^{\\prime}\\). Therefore, \\(\\sigma_{0}((T^{*}\\setminus J^{*})\\cup R^{*})-\\sigma_{0}((T\\setminus J)\\cup R) \\leqslant 2/h "]}, {"edit": ["decomposition, we can define the projection matrix onto the subspace spanned by the top \\(k\\) right singular vectors as \\(\\mathbf{P}_{k}\\in\\mathbb{R}^{d\\times d}\\) given by \\(\\mathbf{P}_{k}:=\\mathbf{V}_{k}\\mathbf{V}_{k}^{T}\\).\n\nFor \\(n\\geq 1\\), \\(a\\in[A]\\), and \\(\\mathbf{Z}_{n}(a)\\) as defined above, we write the \\(k\\)-truncated singular value decomposition of \\(\\mathbf{Z}_{n}(a)\\) as \\(\\mathbf{Z}_{n,k}(a)=\\widetilde{\\mathbf{U}}_{n,k}(a)\\mathrm{diag}(\\sigma_{1}( \\mathbf{Z}_{n}(a)),\\ldots,\\sigma_{k\\wedge n\\wedge d}(\\mathbf{Z}_{n}(a))) \\widetilde{\\mathbf{V}}_{n,k}^{T}(a)\\), and the corresponding projection onto the top \\(k\\) right singular vectors of \\(\\mathbf{Z}_{n}(a)\\) as \\(\\widehat{\\mathbf{P}}_{n,k}(a)\\). When \\(k=r\\), we leverage the simplified notation \\(\\widehat{\\mathbf{P}}_{n}(a):=\\widehat{\\mathbf{P}}_{n,r}(a)\\). (Recall \\(r=\\dim(W^{*})\\).) By \\(\\mathbf{P}\\), we denote the projection matrix onto the true, underlying subspace \\(W^{*}\\). While \\(\\mathbf{P}\\) is never known, our results leverage the fact that \\(\\widehat{\\mathbf{P}}_{n}(a)\\) converges to \\(\\mathbf{P}\\) nicely over time. We define the projected noisy covariate matrix matrix to be \\(\\widehat{\\mathbf{Z}}_{n}(a):=\\mathbf{Z}_{n}(a)\\widehat{\\mathbf{P}}_{n}(a)\\), and define \\(\\widehat{\\mathbf{X}}_{n}(a),\\widehat{\\mathcal{E}}_{n}(a)\\) similarly. Any quantity with a \"\\(\\,\\widehat{}\\,\\)\" is defined equivalently to quantities with \"\\(\\,\\widehat{}\\,\\)\", except with \\(\\mathbf{P}\\) in place of \\(\\widehat{\\mathbf{P}}_{n}(a)\\). We are now ready to introduce our procedure for estimating \\(\\theta(a)\\) for \\(a\\in[A]\\), called _adaptive_ (or _online_) principal component regression.-1\n\n**Definition 3.4** (**Adaptive Principal Component Regression**).: _Given regularization parameter \\(\\rho\\geq 0\\) and truncation level \\(k\\in\\mathbb{N}\\), for \\(a\\in[A]\\) and \\(n\\geq 1\\) let \\(\\widehat{\\mathbf{Z}}_{n}(a):=\\mathbf{Z}_{n}(a)\\widehat{\\mathbf{P}}_{n,k}(a)\\) and \\(\\widehat{\\mathcal{V}}_{n}(a):=\\widehat{\\mathbf{Z}}_{n}(a)^{T}\\widehat{\\mathbf{Z }}_{n}(a)+\\rho\\widehat{\\mathbf{P}}_{n,k}(a)\\). Regularized principal component regression estimates \\(\\theta(a)\\) as_\n\n\\[\\widehat{\\theta}_{n}(a):=\\widehat{\\mathcal{V}}_{n}( "], "nougat": ["decomposition, we can define the projection matrix onto the subspace spanned by the top \\(k\\) right singular vectors as \\(\\mathbf{P}_{k}\\in\\mathbb{R}^{d\\times d}\\) given by \\(\\mathbf{P}_{k}:=\\mathbf{V}_{k}\\mathbf{V}_{k}^{T}\\).\n\nFor \\(n\\geq 1\\), \\(a\\in[A]\\), and \\(\\mathbf{Z}_{n}(a)\\) as defined above, we write the \\(k\\)-truncated singular value decomposition of \\(\\mathbf{Z}_{n}(a)\\) as \\(\\mathbf{Z}_{n,k}(a)=\\widetilde{\\mathbf{U}}_{n,k}(a)\\mathrm{diag}(\\sigma_{1}( \\mathbf{Z}_{n}(a)),\\ldots,\\sigma_{k\\wedge n\\wedge d}(\\mathbf{Z}_{n}(a))) \\widetilde{\\mathbf{V}}_{n,k}^{T}(a)\\), and the corresponding projection onto the top \\(k\\) right singular vectors of \\(\\mathbf{Z}_{n}(a)\\) as \\(\\widehat{\\mathbf{P}}_{n,k}(a)\\). When \\(k=r\\), we leverage the simplified notation \\(\\widehat{\\mathbf{P}}_{n}(a):=\\widehat{\\mathbf{P}}_{n,r}(a)\\). (Recall \\(r=\\dim(W^{*})\\).) By \\(\\mathbf{P}\\), we denote the projection matrix onto the true, underlying subspace \\(W^{*}\\). While \\(\\mathbf{P}\\) is never known, our results leverage the fact that \\(\\widehat{\\mathbf{P}}_{n}(a)\\) converges to \\(\\mathbf{P}\\) nicely over time. We define the projected noisy covariate matrix matrix to be \\(\\widehat{\\mathbf{Z}}_{n}(a):=\\mathbf{Z}_{n}(a)\\widehat{\\mathbf{P}}_{n}(a)\\), and define \\(\\widehat{\\mathbf{X}}_{n}(a),\\widehat{\\mathcal{E}}_{n}(a)\\) similarly. Any quantity with a \u201c\\(\\,\\)' is defined equivalently to quantities with \u201c\\(\\,\\)\\(\\widehat{\\cdot}\\,\\)\", except with \\(\\mathbf{P}\\) in place of \\(\\widehat{\\mathbf{P}}_{n}(a)\\). We are now ready to introduce our procedure for estimating \\(\\theta(a)\\) for \\(a\\in[A]\\), called _adaptive_ (or _online_) principal component regression estimates.-1\n\n**Definition 3.4** (**Adaptive Principal Component Regression**).: _Given regularization parameter \\(\\rho\\geq 0\\) and truncation level \\(k\\in\\mathbb{N}\\), for \\(a\\in[A]\\) and \\(n\\geq 1\\) let \\(\\widehat{\\mathbf{Z}}_{n}(a):=\\mathbf{Z}_{n}(a)\\widehat{\\mathbf{P}}_{n,k}(a)\\) and \\(\\widehat{\\mathcal{V}}_{n}(a):=\\widehat{\\mathbf{Z}}_{n}(a)^{T}\\widehat{\\mathbf{Z }}_{n}(a)+\\rho\\widehat{\\mathbf{P}}_{n,k}(a)\\). Regularized principal component regression estimates \\(\\theta(a)\\) as_\n\n\\[\\widehat{\\theta}_{n}(a):=\\widehat{\\mathcal{V}}_{n}(a "]}, {"edit": ["where the MIDAS weight function is \\(\\omega(s;\\beta_{k})=\\sum_{l=0}^{L-1}\\beta_{l,k}w_{l}(s)\\), \\((w_{l})_{l\\geq 0}\\) is a collection of \\(L\\) approximating functions, called the _dictionary_, and \\(\\beta_{k}\\in\\mathbf{R}^{L}\\) is the unknown parameter. An example of a dictionary used in the MIDAS ML literature is the set of orthogonal Legendre polynomials. To streamline notation it will be convenient to assume, without loss of generality, a common lag length, i.e. \\(\\bar{k}_{max}=k_{max}\\ \\forall\\ k\\in[K]\\). The linear in parameters dictionaries map the MIDAS regression to a standard linear regression framework. In particular, define \\(\\mathbf{x}_{i}=(X_{i,1}W,\\ldots,X_{i,K}W)\\), where for each \\(k\\in[K]\\), \\(X_{i,k}=(x_{i,\\tau-j/n_{k}^{H},k},j=0,\\ldots,\\bar{k}_{max}-1)_{\\tau\\in[T]}\\) is a \\(T\\times\\bar{k}_{max}\\) matrix of covariates and \\(\\bar{k}_{max}W=(w_{l}(j/n_{k}^{H};\\beta_{k})_{0\\leq l\\leq L-1,0\\leq j\\leq\\bar{ k}_{max}}\\) is a \\(\\bar{k}_{max}\\times L\\) matrix corresponding to the dictionary. In addition, let \\(\\mathbf{y}_{i}=(y_{i,t|\\tau},t,\\tau\\in[T])^{\\top}\\) and \\(\\mathbf{u}_{i}=(u_{i,t|\\tau},t,\\tau\\in[T])^{\\top}\\). The regression equation after stacking time series observations for each firm \\(i\\in[N]\\) is as follows\n\n\\[\\mathbf{y}_{i}=\\iota\\alpha_{i}+\\mathbf{x}_{i}\\beta+\\mathbf{u}_{i},\\]\n\nwhere \\(\\iota\\in\\mathbf{R}^{T}\\) is the all-ones vector and \\(\\beta\\in\\mathbf{R}^{LK}\\) is a vector of slope coefficients. Lastly, put \\(\\mathbf{y}=(\\mathbf{y}_{1}^{\\top},\\ldots,\\mathbf{y}_{N}^{\\top})^{\\top}\\), \\(\\mathbf{X}=(\\mathbf{x}_{1}^{\\top},\\ldots,\\mathbf{x}_{N}^{\\top})^{\\top}\\), and \\(\\mathbf{u}=(\\mathbf{u}_{1}^{\\top},\\ldots,\\mathbf{u}_{N}^{\\top})^{\\top}\\). Then the regression equation after stacking all cross-sectional observations is\n\n\\[\\mathbf{y}=B\\alpha+\\mathbf{X}\\beta+\\mathbf{u},\\]\n\nwhere \\(B=I_{N}\\otimes\\iota\\), \\(I_{N}\\) is \\(N\\times N\\) identity matrix, and \\(\\otimes\\) is the Kronecker product. Given that the number of potential predictors \\(K\\) can be large, additional regularization can improve the predictive performance in small samples. To that end, we take advantage of the sg-LASSO regularization, suggested by Babii, Ghysels, and Striaukas (2022).\n\nThe fixed effects sg-LASSO estimator \\(\\hat{\\rho}=(\\hat{\\alpha}^{ "], "nougat": ["where the MIDAS weight function is \\(\\omega(s;\\beta_{k})=\\sum_{l=0}^{L-1}\\beta_{l,k}w_{l}(s)\\), \\((w_{l})_{l\\geq 0}\\) is a collection of \\(L\\) approximating functions, called the _dictionary_, and \\(\\beta_{k}\\in\\mathbf{R}^{L}\\) is the unknown parameter. An example of a dictionary used in the MIDAS ML literature is the set of orthogonal Legendre polynomials. To streamline notation it will be convenient to assume, without loss of generality, a common lag length, i.e. \\(\\bar{k}_{max}=k_{max}\\ \\forall\\ k\\in[K]\\). The linear in parameters dictionaries map the MIDAS regression to a standard linear regression framework. In particular, define \\(\\mathbf{x}_{i}=(X_{i,1}W,\\ldots,X_{i,K}W)\\), where for each \\(k\\in[K]\\), \\(X_{i,k}=(x_{i,\\tau-j/n_{k}^{H},k},j=0,\\ldots,\\bar{k}_{max}-1)_{\\tau\\in[T]}\\) is a \\(T\\times\\bar{k}_{max}\\) matrix of covariates and \\(\\bar{k}_{max}W=(w_{l}(j/n_{k}^{H};\\beta_{k})_{0\\leq l\\leq L-1,0\\leq j\\leq\\bar{ k}_{max}}\\) is a \\(\\bar{k}_{max}\\times L\\) matrix corresponding to the dictionary. In addition, let \\(\\mathbf{y}_{i}=(y_{i,t|\\tau},t,\\tau\\in[T])^{\\top}\\) and \\(\\mathbf{u}_{i}=(u_{i,t|\\tau},t,\\tau\\in[T])^{\\top}\\). The regression equation after stacking time series observations for each firm \\(i\\in[N]\\) is as follows\n\n\\[\\mathbf{y}_{i}=\\iota\\alpha_{i}+\\mathbf{x}_{i}\\beta+\\mathbf{u}_{i},\\]\n\nwhere \\(\\iota\\in\\mathbf{R}^{T}\\) is the all-ones vector and \\(\\beta\\in\\mathbf{R}^{LK}\\) is a vector of slope coefficients. Lastly, put \\(\\mathbf{y}=(\\mathbf{y}_{1}^{\\top},\\ldots,\\mathbf{y}_{N}^{\\top})^{\\top}\\), \\(\\mathbf{X}=(\\mathbf{x}_{1}^{\\top},\\ldots,\\mathbf{x}_{N}^{\\top})^{\\top}\\), and \\(\\mathbf{u}=(\\mathbf{u}_{1}^{\\top},\\ldots,\\mathbf{u}_{N}^{\\top})^{\\top}\\). Then the regression equation after stacking all cross-sectional observations is\n\n\\[\\mathbf{y}=B\\alpha+\\mathbf{X}\\beta+\\mathbf{u},\\]\n\nwhere \\(B=I_{N}\\otimes\\iota\\), \\(I_{N}\\) is \\(N\\times N\\) identity matrix, and \\(\\otimes\\) is the Kronecker product. Given that the number of potential predictors \\(K\\) can be large, additional regularization can improve the predictive performance in small samples. To that end, we take advantage of the sg-LASSO regularization, suggested by Babii, Ghysels, and Striaukas ( 2022 ).\n\nThe fixed effects sg-LASSO estimator \\(\\hat{\\rho}=(\\hat{\\alpha}\n\n "]}, {"edit": ["of interaction will typically lead to homogeneous well-mixed systems, or to some condensation into a dense phase where agents are well-mixed, but since frustration is not implemented in the microscopic rules, we reasonably expect stationary states.\n* Second, each agent type may decide to settle among peers and/or avoid agents of the other type in their surroundings. One should then expect a complete phase separation into two domains, one displaying a majority of \\(A\\)s and, the other, a majority of \\(B\\)s. Whether the \\(A-B\\) phase separation additionally displays some condensation depends on the self-affinity of each agent type.\n* Third, frustrated situations in which \\(A\\) settles with \\(A\\) but wants to avoid \\(B\\) agents, while \\(B\\) agents would like to gather and settle close to \\(A\\). In this situation, we may expect non stationary patterns, stemming from the fact that all agents cannot be satisfied at the same time.\n\nWith this last situation in mind, we have considered the following utility functions (\\(u_{A}\\) for \\(A\\) agents and \\(u_{B}\\) for \\(B\\) agents):\n\n\\[u_{A}(x,[\\varphi_{A,B}]) =-|\\varphi_{A}(x)-\\rho^{\\star}|^{2}+c_{1}\\varphi_{B}(x) \\tag{39}\\] \\[u_{B}(x,[\\varphi_{A,B}]) =-|\\varphi_{A}(x)-\\rho^{\\star}|^{2}+c_{2}\\varphi_{B}(x), \\tag{40}\\]\n\nwhere \\(c_{1}<0\\) translates the fact that \\(A\\)s are fleeing from \\(B\\), and \\(c_{2}>0\\) translates the fact that \\(B\\)s have a tendency to gather with \\(B\\)s. The term \\(-|\\varphi_{A}-\\rho^{\\star}|^{2}\\) enjoins both populations to settle among \\(A\\) populated areas. Of course, the specific shape of utilities taken here may be restrictive and can be easily generalized.\n\nThe extension of the mean-field dynamics to this two population problem is rather straightforward. Writing \\(\\rho_{A}(x,t)\\) (resp. \\(\\rho_{B}(x,t)\\)) the density of agents \\(A\\) (resp. \\(B\\)) at location \\(x\\) and time \\(t\\), and denoting the total density by \\(\\rho(x,t)\\equiv\\rho_{A}(x,t)+\\rho_{B}(x,t)\\), we now have an evolution equation of the form\n\n\\[\\partial_{t}\\rho_{A}(x,t) =[1-\\rho(x,t)]\\int\\rho_{A}(y,t)w_{{}_{A}}([\\varphi_{A,B}],y,x,t) \\,\\mathrm{d}y \\tag{41}\\] \\[-\\rho_{A}(x,t)\\int[1-\\rho(y,t)]w_{{}_{A}}([\\varphi_{A,B}],x,y,t) \\,\\mathrm{d}y,\\]\n\nand, by symmetry, a similar equation for \\(B\\). The transition rates depend on the utility function of each agent type and are _a priori_ agent specific. Denoting \\(u_{Z}(x)\\equiv u_{Z}(x,[\\varphi_{A,B}])\\) (with \\(Z=A\\) or \\(B\\)), we set\n\n\\[w_{{}_{Z}}([\\varphi_{A,B}],y,x,t)=\\omega_{Z}f_{{}_{Z}}[u_{Z}(x)- "], "nougat": ["of interaction will typically lead to homogeneous wellmixed systems, or to some condensation into a dense phase where agents are well-mixed, but since frustration is not implemented in the microscopic rules, we reasonably expect stationary states.\n* Second, each agent type may decide to settle among peers and/or avoid agents of the other type in their surroundings. One should then expect a complete phase separation into two domains, one displaying a majority of \\(A\\)s and, the other, a majority of \\(B\\)s. Whether the \\(A-B\\) phase separation additionally displays some condensation depends on the self-affinity of each agent type.\n* Third, frustrated situations in which \\(A\\) settles with \\(A\\) but wants to avoid \\(B\\) agents, while \\(B\\) agents would like to gather and settle close to \\(A\\). In this situation, we may expect non stationary patterns, stemming from the fact that all agents cannot be satisfied at the same time.\n\nWith this last situation in mind, we have considered the following utility functions (\\(u_{A}\\) for \\(A\\) agents and \\(u_{B}\\) for \\(B\\) agents):\n\n\\[u_{A}(x,[\\varphi_{A,B}]) =-|\\varphi_{A}(x)-\\rho^{\\star}|^{2}+c_{1}\\varphi_{B}(x) \\tag{39}\\] \\[u_{B}(x,[\\varphi_{A,B}]) =-|\\varphi_{A}(x)-\\rho^{\\star}|^{2}+c_{2}\\varphi_{B}(x), \\tag{40}\\]\n\nwhere \\(c_{1}<0\\) translates the fact that \\(A\\)s are fleeing from \\(B\\), and \\(c_{2}>0\\) translates the fact that \\(B\\)s have a tendency to gather with \\(B\\)s. The term \\(-|\\varphi_{A}-\\rho^{\\star}|^{2}\\) enjoins both populations to settle among \\(A\\) populated areas. Of course, the specific shape of utilities taken here may be restrictive and can be easily generalized.\n\nThe extension of the mean-field dynamics to this two population problem is rather straightforward. Writing \\(\\rho_{A}(x,t)\\) (resp. \\(\\rho_{B}(x,t)\\)) the density of agents \\(A\\) (resp. \\(B\\)) at location \\(x\\) and time \\(t\\), and denoting the total density by \\(\\rho(x,t)\\equiv\\rho_{A}(x,t)+\\rho_{B}(x,t)\\), we now have an evolution equation of the form\n\n\\[\\partial_{t}\\rho_{A}(x,t) =[1-\\rho(x,t)]\\int\\rho_{A}(y,t)w_{{}_{A}}([\\varphi_{A,B}],y,x,t) \\,\\mathrm{d}y \\tag{41}\\] \\[-\\rho_{A}(x,t)\\int[1-\\rho(y,t)]w_{{}_{A}}([\\varphi_{A,B}],x,y,t) \\,\\mathrm{d}y,\\]\n\nand, by symmetry, a similar equation for \\(B\\). The transition rates depend on the utility function of each agent type and are _a priori_ agent specific. Denoting \\(u_{Z}(x)\\equiv u_{Z}(x,[\\varphi_{A,B}])\\) (with \\(Z=A\\) or \\(B\\)), we set\n\n\\[w_{{}_{Z}}([\\varphi_{A,B}],y,x,t)=\\omega_{Z}f_{{}_{Z}}[u_{Z}(x)-u "]}, {"edit": ["Model & Data on Choices in Automobile Insurance\n\n### Empirical model\n\nAs introduced in Section 2.1, we model agents' choices in two contexts of insurance coverage, where each coverage provides full insurance against covered losses in excess of a deductible chosen by the agent. In our data, the decision maker is a household; hence, we refer to agents as households. As a reminder, \\(\\mu_{i}^{j}\\) denotes the probability of household \\(i\\) experiencing a claim in context \\(j\\); for each coverage \\(j\\in\\{\\mathtt{I},\\mathtt{II}\\}\\), household \\(i\\) faces a menu of premium-deductible pairs, \\(\\mathcal{M}_{i}^{j}=\\{(\\mathsf{d}^{\\ell j},\\mathbf{x}_{i}^{\\ell j}):\\ell\\in \\mathcal{D}^{j}\\}\\), where \\(\\mathbf{x}_{i}^{\\ell j}\\) is the household-specific premium associated with deductible \\(\\mathsf{d}^{\\ell j}\\) and \\(\\mathcal{D}^{j}\\) is the set of deductible options offered in context \\(j\\). As discussed in Section 2.1, for each context \\(j\\in\\{\\mathtt{I},\\mathtt{II}\\}\\) the ratio of the price of deductible \\(\\mathsf{d}^{\\ell j}\\) to the price of deductible \\(\\mathsf{d}^{kj}\\) is constant across households for all \\(\\mathsf{d}^{\\ell j},\\mathsf{d}^{kj}\\in\\mathcal{D}^{j}\\).\n\nWe make assumptions, that are widespread in the literature on property insurance, related to filing claims and their probabilities:\n\n**Assumption 4.1**: _[Restrictions Related to Claim Probabilities]___\n\n1. _Households disregard the possibility of experiencing more than one claim during the policy period._\n2. _Any claim exceeds the highest available deductible; payment of the deductible is the only cost associated with a claim; the household's deductible choice does not influence its claim probability._\n\nWe assume that the two types of preferences described in Section 2.2 result from either Expected Utility Theory (EU) or Yaari's (1987) Dual Theory (DT). Within EU, a single-context lottery is evaluated through\n\n\\[U_{i}(\\mathcal{L}(\\mathsf{d}^{\\ell j},\\mathbf{x}_{i}^{\\ell j},\\mu_{i}^{j})) \\equiv(1-\\mu_{i}^{j})u_{i}(w_{i}-\\mathbf{x}_{i}^{\\ell j})+\\mu_{i}^{j}u_{i}(w_{ i}-\\mathbf{x}_{i}^{\\ell j}-\\mathsf{d}^{\\ell j}), \\tag{4.1}\\]\n\nwhere \\(w_{i}\\) is the household's wealth and \\(u_{i}(\\cdot)\\) is its Bernoulli utility function, which under Assumption 2.2 is the same for each context. In the EU model, utility is linear in the probabilities and aversion to risk is driven by the shape of the utility function \\(u_{i}(\\cdot)\\).\n\n "], "nougat": ["Model & Data on Choices in Automobile Insurance\n\n### Empirical model\n\nAs introduced in Section 2.1, we model agents' choices in two contexts of insurance coverage, where each coverage provides full insurance against covered losses in excess of a deductible chosen by the agent. In our data, the decision maker is a household; hence, we refer to agents as households. As a reminder, \\(\\mu_{i}^{j}\\) denotes the probability of household \\(i\\) experiencing a claim in context \\(j\\); for each coverage \\(j\\in\\{\\mathtt{I},\\mathtt{II}\\}\\), household \\(i\\) faces a menu of premium-deductible pairs, \\(\\mathcal{M}_{i}^{j}=\\{(\\mathsf{d}^{\\ell j},\\mathbf{x}_{i}^{\\ell j}):\\ell\\in \\mathcal{D}^{j}\\}\\), where \\(\\mathbf{x}_{i}^{\\ell j}\\) is the household-specific premium associated with deductible \\(\\mathsf{d}^{\\ell j}\\) and \\(\\mathcal{D}^{j}\\) is the set of deductible options offered in context \\(j\\). As discussed in Section 2.1, for each context \\(j\\in\\{\\mathtt{I},\\mathtt{II}\\}\\) the ratio of the price of deductible \\(\\mathsf{d}^{\\ell j}\\) to the price of deductible \\(\\mathsf{d}^{kj}\\) is constant across households for all \\(\\mathsf{d}^{\\ell j},\\mathsf{d}^{kj}\\in\\mathcal{D}^{j}\\).\n\nWe make assumptions, that are widespread in the literature on property insurance, related to filing claims and their probabilities:\n\n**Assumption 4.1**: _[Restrictions Related to Claim Probabilities]___\n\n1. _Households disregard the possibility of experiencing more than one claim during the policy period._\n2. _Any claim exceeds the highest available deductible; payment of the deductible is the only cost associated with a claim; the household's deductible choice does not influence its claim probability._\n\nWe assume that the two types of preferences described in Section 2.2 result from either Expected Utility Theory (EU) or Yaari's (1987) Dual Theory (DT). Within EU, a single-context lottery is evaluated through\n\n\\[U_{i}(\\mathcal{L}(\\mathsf{d}^{\\ell j},\\mathbf{x}_{i}^{\\ell j},\\mu_{i}^{j})) \\equiv(1-\\mu_{i}^{j})u_{i}(w_{i}-\\mathbf{x}_{i}^{\\ell j})+\\mu_{i}^{j}u_{i}(w_{ i}-\\mathbf{x}_{i}^{\\ell j}-\\mathsf{d}^{\\ell j}), \\tag{4.1}\\]\n\nwhere \\(w_{i}\\) is the household's wealth and \\(u_{i}(\\cdot)\\) is its Bernoulli utility function, which under Assumption 2.2 is the same for each context. In the EU model, utility is linear in the probabilities and aversion to risk is driven by the shape of the utility function \\(u_{i}(\\cdot)\\).\n\n "]}, {"edit": ["tackled using inverse probability weighting methods. Since there are various types of reserves in general insurance, in this section we provide the overall idea of the methodology for the total reserve of outstanding claims only. Section 3 will delve into the specific details of the methodology for the most prevalent and significant reserves in general insurance, namely RBNS and IBNR reserves.\n\n### The claim reserving problem\n\nSuppose an insurance company is analyzing its total liabilities associated with claims whose accident times occur between \\(t=0\\) and \\(t=\\tau\\), where \\(\\tau\\) is the valuation time of analysis as defined by the actuary. In general insurance, accidents are often not immediately reported to the insurance company for various reasons, resulting in a significant delay between the occurrence of a claimable accident and the time the insurance company is notified. Therefore, at a given valuation time \\(\\tau\\), the insurance company only has information on the claims reported by \\(\\tau\\) and is unaware of the unreported claims. Furthermore, the complexity of the problem increases due to another delay in the payment process. When a claim is reported, it is common for it to be paid in several sub-payments over time rather than as a lump sum. This is because the impact of an accident can evolve, requiring additional payments until it is fully settled. Therefore, at a given valuation time \\(\\tau\\), the insurance company is only aware of the claims that were reported on time, and for each one, it may have paid only a partial amount of the associated claim size, rather than the entire amount.\n\nAs a result, the insurance company is interested in estimating the total claim amount of these unreported claims, as well as the remaining payments of the reported claims, to construct the overall reserve of outstanding claims. This reserve is also known in the insurance jargon as the Incurred But Not Settled (IBNS), and it's usually decomposed into further subcomponents depending on whether the payment is associated with a reported or not reported claim. For simplicity, here we consider the estimation of the overall reserve of outstanding claims without referring to the components.\n\nThat said, let's describe the payment process as follows:\n\n* Let \\(N(\\tau)\\) represent the total number of different payments associated with all the claims whose accident time is before the valuation time \\(\\tau\\).\n* Let \\(Y_{i}\\), \\(i=1,\\ldots,N(\\tau)\\) denote the sequence payments. Note that some payments may belong to the same claim/accident, but we will not make any distinction.\n* Let \\(T_{i}\\), \\(i=1,\\ldots,N(\\tau)\\) denote the sequence of accident times associated with the claim underlying each payment; let \\(R_{i}\\), \\(i=1,\\ldots,N(\\tau)\\) denote the sequence of the associated reporting times; let \\(S_{i}\\), \\(i=1,\\ldots,N(\\tau)\\) denote the sequence of the associated times in which the payments take place. Clearly, \\(T_{i}<R_{i}<S_{i}\\) and note that the values \\(T_{i},R_{i}\\) would be the same for payments associated with the same claim, but the \\(S_{i}\\) would differ.\n* Let \\(U_{i}=R_{i}-T_{i}\\), \\(i=1,\\ldots,N(\\tau)\\) be the sequence of the reporting delay times associated with the claim underlying each payment, and \\(V_{i}=S_{i}-R_{i}\\), \\(i=1,\\ldots,N(\\tau)\\) be the sequence of the associated payment delay time of each payment. Note that \\(U_{i}\\) is the same for all the payments associated with the same claim.\n* Let \\(X_{i}\\), \\(i=1,\\ldots,N(\\tau)\\) be the sequence of information/attributes of relevance, that is associated with the accident, the type of claim, the policyholder attributes, or the characteristics of the payment itself.\n* Let \\(N^{P}(\\tau)\\) the number of payments made by valuation time \\(\\tau\\) out of the total \\(N(\\tau)\\) i "], "nougat": ["tackled using inverse probability weighting methods. Since there are various types of reserves in general insurance, in this section we provide the overall idea of the methodology for the total reserve of outstanding claims only. Section 3 will delve into the specific details of the methodology for the most prevalent and significant reserves in general insurance, namely RBNS and IBNR reserves.\n\n### The claim reserving problem\n\nSuppose an insurance company is analyzing its total liabilities associated with claims whose accident times occur between \\(t=0\\) and \\(t=\\tau\\), where \\(\\tau\\) is the valuation time of analysis as defined by the actuary. In general insurance, accidents are often not immediately reported to the insurance company for various reasons, resulting in a significant delay between the occurrence of a claimable accident and the time the insurance company is notified. Therefore, at a given valuation time \\(\\tau\\), the insurance company only has information on the claims reported by \\(\\tau\\) and is unaware of the unreported claims. Furthermore, the complexity of the problem increases due to another delay in the payment process. When a claim is reported, it is common for it to be paid in several subpayments over time rather than as a lump sum. This is because the impact of an accident can evolve, requiring additional payments until it is fully settled. Therefore, at a given valuation time \\(\\tau\\), the insurance company is only aware of the claims that were reported on time, and for each one, it may have paid only a partial amount of the associated claim size, rather than the entire amount.\n\nAs a result, the insurance company is interested in estimating the total claim amount of these unreported claims, as well as the remaining payments of the reported claims, to construct the overall reserve of outstanding claims. This reserve is also known in the insurance jargon as the Incurred But Not Settled (IBNS), and it\u2019s usually decomposed into further subcomponents depending on whether the payment is associated with a reported or not reported claim. For simplicity, here we consider the estimation of the overall reserve of outstanding claims without referring to the components.\n\nThat said, let\u2019s describe the payment process as follows:\n\n* Let \\(N(\\tau)\\) represent the total number of different payments associated with all the claims whose accident time is before the valuation time \\(\\tau\\).\n* Let \\(Y_{i}\\), \\(i=1,\\ldots,N(\\tau)\\) denote the sequence payments. Note that some payments may belong to the same claim/accident, but we will not make any distinction.\n* Let \\(T_{i}\\), \\(i=1,\\ldots,N(\\tau)\\) denote the sequence of accident times associated with the claim underlying each payment; let \\(R_{i}\\), \\(i=1,\\ldots,N(\\tau)\\) denote the sequence of the associated reporting times; let \\(S_{i}\\), \\(i=1,\\ldots,N(\\tau)\\) denote the sequence of the associated times in which the payments take place. Clearly, \\(T_{i}<R_{i}<S_{i}\\) and note that the values \\(T_{i},R_{i}\\) would be the same for payments associated with the same claim, but the \\(S_{i}\\) would differ.\n* Let \\(U_{i}=R_{i}-T_{i}\\), \\(i=1,\\ldots,N(\\tau)\\) be the sequence of the reporting delay times associated with the claim underlying each payment, and \\(V_{i}=S_{i}-R_{i}\\), \\(i=1,\\ldots,N(\\tau)\\) be the sequence of the associated payment delay time of each payment. Note that \\(U_{i}\\) is the same for all the payments associated with the same claim.\n* Let \\(X_{i}\\), \\(i=1,\\ldots,N(\\tau)\\) be the sequence of information/attributes of relevance, that is associated with the accident, the type of claim, the policyholder attributes, or the characteristics of the payment itself.\n* Let \\(N^{P}(\\tau)\\) the number of payments made by valuation time \\(\\tau\\) out of the total \\(N(\\tau)\\) i.\n\n "]}, {"edit": ["aged collateral, exogenous/decentrally managed collateral, and endogenous/decentrally managed collateral, we observe consistent results across the five sensitivity factors. The primary difference we observed was in the initial deviations of the price, which were smaller or larger depending on the sensitivity factor. However, crucially, the price and demand of these types were able to recover over time. For stablecoin types with endogenous/centrally managed collateral, we observe similar results for positive demand shocks. However, for negative demand shocks, we observed that this type only experiences a crash if the magnitude of the demand shock is sufficiently large; i.e., for sensitivity factors of 0.5 and 0.75, this type was able to recover without crashing[36]. Conversely, for sensitivity factors of 1.25 and 1.5, we observed a crash, which intuitively occurred more rapidly with a higher magnitude of the demand shock.\n* [2]_Volatility of demand_: Across all stablecoin types, we consistently observe the same outcomes across the five sensitivity factors. While there is increased volatility in both demand and price, the overall findings and conclusions remain unchanged.\n* [3]_Fees_: For stablecoin types with exogenous/centrally managed collateral, variations in fees did not have a significant effect on either the demand or the price. However, for stablecoin types with exogenous/decentrally managed collateral or endogenous/decentrally managed collateral, adjusting the fees led to changes in both the demand and subsequent price. Additionally, for stablecoin types with endogenous/centrally managed collateral, changes in fees can even result in a crash, independent of any external shocks.\n* [4]_Price of collateral_: For stablecoin types with exogenous/decentrally managed collateral, endogenous/decentrally managed collateral, and endogenous/centrally managed collateral, we observe a strong dependence of the stablecoin price on the price of the collateral. In our simulation, stablecoin types with exogenous/centrally managed collateral are directly backed by fiat money, which restricts our ability to analyze shocks in the collateral's price. This limitation arises because fiat money serves as both the collateral and the reference currency in the model. However, if we were to adjust the model similarly to the other stablecoin types, we would expect to obtain comparable results.\n\nThese sensitivity analyses confirm the robustness of our results, emphasizing the instability of endogenous/centrally managed stablecoins and the significance of collateral and fees in stablecoin dynamics.\n\n### Bank runs\n\nA bank run is where a large number of depositors withdraw their deposits from a bank because they fear that the bank may not have enough money to meet their withdrawal requests. Bank runs are often triggered by rumors or fears of a bank's insolvency or bankruptcy. As depositors withdraw their money, other depositors start to panic and also withdraw their funds, leading to a decline in the bank's reserves. In extreme cases, bank runs can result in the bank being unable to meet its obligations. During a bank run, the bank may be forced to sell its assets at a loss, which can lead to a chain reaction of other banks being affected and a disruption of the monetary system, causing a reduction in production[37].\n\nIn a centralized managed collateral system, a single entity--such as a central bank or a smart contract--controls the monetary system. As a result, depositors must rely on this entity's ability to maintain the system's stability and solvency. If the entity fails to do so, depositors may lose confidence in the system, triggering a bank run. In contrast, systems with decentral managed "], "nougat": ["aged collateral, exogenous/decentrally managed collateral, and endogenous/decentrally managed collateral, we observe consistent results across the five sensitivity factors. The primary difference we observed was in the initial deviations of the price, which were smaller or larger depending on the sensitivity factor. However, crucially, the price and demand of these types were able to recover over time. For stablecoin types with endogenous/centrally managed collateral, we observe similar results for positive demand shocks. However, for negative demand shocks, we observed that this type only experiences a crash if the magnitude of the demand shock is sufficiently large; i.e., for sensitivity factors of 0.5 and 0.75, this type was able to recover without crashing 36 . Conversely, for sensitivity factors of 1.25 and 1.5, we observed a crash, which intuitively occurred more rapidly with a higher magnitude of the demand shock.\n* [2]_Volatility of demand_: Across all stablecoin types, we consistently observe the same outcomes across the five sensitivity factors. While there is increased volatility in both demand and price, the overall findings and conclusions remain unchanged.\n* [3]_Fees_: For stablecoin types with exogenous/centrally managed collateral, variations in fees did not have a significant effect on either the demand or the price. However, for stablecoin types with exogenous/decentrally managed collateral or endogenous/decentrally managed collateral, adjusting the fees led to changes in both the demand and subsequent price. Additionally, for stablecoin types with endogenous/centrally managed collateral, changes in fees can even result in a crash, independent of any external shocks.\n* [4]_Price of collateral_: For stablecoin types with exogenous/decentrally managed collateral, endogenous/decentrally managed collateral, and endogenous/centrally managed collateral, we observe a strong dependence of the stablecoin price on the price of the collateral. In our simulation, stablecoin types with exogenous/centrally managed collateral are directly backed by fiat money, which restricts our ability to analyze shocks in the collateral\u2019s price. This limitation arises because fiat money serves as both the collateral and the reference currency in the model. However, if we were to adjust the model similarly to the other stablecoin types, we would expect to obtain comparable results.\n\nThese sensitivity analyses confirm the robustness of our results, emphasizing the instability of endogenous/centrally managed stablecoins and the significance of collateral and fees in stablecoin dynamics.\n\n### Bank runs\n\nA bank run is where a large number of depositors withdraw their deposits from a bank because they fear that the bank may not have enough money to meet their withdrawal requests. Bank runs are often triggered by rumors or fears of a bank\u2019s insolvency or bankruptcy. As depositors withdraw their money, other depositors start to panic and also withdraw their funds, leading to a decline in the bank\u2019s reserves. In extreme cases, bank runs can result in the bank being unable to meet its obligations. During a bank run, the bank may be forced to sell its assets at a loss, which can lead to a chain reaction of other banks being affected and a disruption of the monetary system, causing a reduction in production 37 .\n\nIn a centralized managed collateral system, a single entity\u2014such as a central bank or a smart contract\u2014controls the monetary system. As a result, depositors must rely on this entity\u2019s ability to maintain the system\u2019s stability and solvency. If the entity fails to do so, depositors may lose confidence in the system, triggering a bank run. In contrast, systems with decentral managed "]}, {"edit": ["This is true by referring to (A17)-(A18). In particular, by (A16) and (A19),\n\n\\[\\hat{\\hat{M}}=\\tilde{m}(\\hat{\\hat{Y}},\\hat{\\varepsilon})=m(\\hat{Y},\\hat{ \\varepsilon})=\\hat{M},\\quad\\tilde{p}(\\tilde{x})=p(x)\\quad\\text{and}\\quad\\tilde{ p}(\\tilde{b})=p(b).\\]\n\nFurthermore, \\(\\tilde{\\Psi}(\\tilde{y},\\hat{\\hat{Y}})=\\Psi(y,\\hat{Y})\\) by the definition in (A15) and\n\n\\[\\tilde{f}\\left[\\mathrm{e}^{-\\delta}\\tilde{t}(\\tilde{x},\\tilde{y}) +\\hat{\\hat{Y}},\\hat{\\hat{Y}}\\right] =\\tilde{f}\\left[\\mathrm{e}^{-\\delta}\\sigma i(x,y)+\\mu+\\sigma\\hat {Y},\\hat{\\hat{Y}}\\right]\\] \\[=\\tilde{f}\\left[\\mu+\\sigma\\left(\\mathrm{e}^{-\\delta}i(x,y)+\\hat{ Y}\\right),\\hat{\\hat{Y}}\\right]=f\\left[\\mathrm{e}^{-\\delta}i(x,y)+\\hat{Y},\\hat{ \\hat{Y}}\\right].\\]\n\nThe above analysis implies that (A20) holds. To see that (A21) holds, note that\n\n\\[x_{\\tilde{f}}(\\tilde{y}) =\\inf\\left\\{\\mu+\\sigma x\\geq\\mu-\\frac{a\\sigma}{d}:f(x,y)=0\\right\\}\\] \\[=\\mu+\\sigma\\inf\\left\\{x\\geq p^{-1}(0):f(x,y)=0\\right\\}=\\mu+ \\sigma x_{f}(y),\\]\n\nwhere we have used the definition of \\(p\\) and \\(\\tilde{p}\\). This yields\n\n\\[\\tilde{x}<x_{\\tilde{f}}(\\tilde{y})\\iff x<x_{f}(y).\\]\n\nIn combination with (A18), we obtain\n\n\\[\\tilde{t}(\\tilde{x},\\tilde{y})=\\sigma i(x,y)=\\begin{cases}\\sigma \\left(x-p^{-1}[f(x,y)]\\right),&\\tilde{x}<x_{\\tilde{f}}(\\tilde{y}),\\\\ \\sigma\\left(x_{f}(y)-p^{-1}(0)\\right),&\\tilde{x}\\geq x_{\\tilde{f}}(\\tilde{y}). \\end{cases}\\]\n\nWhen \\(\\tilde{x}<x_{\\tilde{f}}(\\tilde{y})\\), using (A19) and the definition of \\(p\\) and \\(\\tilde{p}\\), we obtain\n\n\\[\\sigma\\left(x-p^{-1}[f(x,y)]\\right) =\\sigma x-\\sigma p^{-1}[\\tilde{f}(\\tilde{x},\\tilde{y})]\\] \\[=\\tilde{x}-\\mu-\\sigma\\left(\\frac{\\tilde{f}(\\tilde{x},\\tilde{y})-a }{d}\\right)=\\tilde{x}-\\tilde{p}^{-1}[\\tilde{f}(\\tilde{x},\\tilde{y})].\\ "], "nougat": ["This is true by referring to ( A17 )\u2013( A18 ). In particular, by ( A16 ) and ( A19 ),\n\n\\[\\hat{\\hat{M}}=\\tilde{m}(\\hat{\\hat{Y}},\\hat{\\varepsilon})=m(\\hat{Y},\\hat{ \\varepsilon})=\\hat{M},\\quad\\tilde{p}(\\tilde{x})=p(x)\\quad\\text{and}\\quad\\tilde{ p}(\\tilde{b})=p(b).\\]\n\nFurthermore, \\(\\tilde{\\Psi}(\\tilde{y},\\hat{\\hat{Y}})=\\Psi(y,\\hat{Y})\\) by the definition in ( A20 ) holds. To see that ( A21 ) holds, note that\n\n\\[x_{\\tilde{f}}(\\tilde{y}) =\\inf\\Big{\\{}\\mu+\\sigma x\\geq\\mu-\\frac{a\\sigma}{d}:f(x,y)=0\\Big{\\}}\\] \\[=\\mu+\\sigma\\inf\\Big{\\{}x\\geq p^{-1}(0):f(x,y)=0\\Big{\\}}=\\mu+ \\sigma x_{f}(y),\\]\n\nwhere we have used the definition of \\(p\\) and \\(\\tilde{p}\\). This yields\n\n\\[\\tilde{x}<x_{\\tilde{f}}(\\tilde{y})\\iff x<x_{f}(y).\\]\n\nIn combination with ( A18 ), we obtain\n\n\\[\\tilde{\\imath}(\\tilde{x},\\tilde{y})=\\sigma i(x,y)=\\begin{cases} \\sigma\\left(x-p^{-1}[f(x,y)]\\right),&\\tilde{x}<x_{\\tilde{f}}(\\tilde{y}),\\\\ \\sigma\\left(x_{f}(y)-p^{-1}(0)\\right),&\\tilde{x}\\geq x_{\\tilde{f}}(\\tilde{y}). \\end{cases}\\]\n\nWhen \\(\\tilde{x}<x_{\\tilde{f}}(\\tilde{y})\\), using ( A19 ) and the definition of \\(p\\) and \\(\\tilde{p}\\), we obtain\n\n\\[\\sigma\\left(x-p^{-1}[f(x,y)]\\right) =\\sigma x-\\sigma p^{-1}[\\tilde{f}(\\tilde{x},\\tilde{y})]\\] \\[=\\tilde{x}-\\mu-\\sigma\\left(\\frac{\\tilde{f}(\\tilde{x},\\tilde{y})-a }{d}\\right)=\\tilde{x}-\\tilde{p}^{-1}[\\tilde{f}(\\tilde{x},\\tilde{y})].\\]\n\nWhen \\(\\tilde{x}\\geq x_{\\tilde{f}}(\\tilde{y})\\), using the definition of \\(p\\) and \\(\\tilde{p}\\) again yields\n\n\\[\\sigma\\left(x_{f}(y)-p^{-1}(0)\\right)=x_{\\tilde{f}}(\\tilde{y})-\\mu+\\frac{a \\sigma}{d}=x_{\\tilde{f}}(\\tilde{y})-\\tilde{p}^{-1}(0).\\]\n\nThe above analysis implies that ( A21 ) holds. Therefore, economies \\(E\\) and \\(\\tilde{E}\\) generate the same commodity price process. \n\n## Appendix E Algorithms\n\nThe storage model is "]}, {"edit": ["\n\n**Model Diversity:** Our platform currently supports only OpenAI's models, chosen for their leading-edge capabilities and robust API access. This model-specific dependency could limit the platform's flexibility, as different models may offer unique strengths and capabilities that could be beneficial in diverse research scenarios. We are therefore actively testing other open source and commercial AI models to potentially integrate into our platform (Bai et al., 2022; Touvron et al., 2023), thus expanding its versatility and research applicability. It should be noted that we avoid models that essentially add new prompt-engineering layers on top of base ALMs to improve decision performance in specific tasks (Shinn et al., 2023; Yao et al., 2023).\n\n**Realistic Condition Profiles:** A minor drawback of our platform is the occasional generation of unrealistic agent profiles due to the randomness of profile construction. This approach also means that sometimes we end up with \"interesting\" agent profile combinations that may seldom present in the real world (e.g., a male lesbian). While rare, these cases can disrupt the research process and lead to unrealistic model responses. One solution lies in conditional profile construction, where agent attributes are selected based on real-world prevalence and correlations. However, it is important to note that this randomness can sometimes yield unique case studies that might not have been otherwise considered, offering unexpected insights and interesting research avenues.\n\n**Multi-Agent Games and Interaction:** Currently, SurveyLM allows for the simulation of an agent's participation in games with other players only when the other players and interaction rules are hard-coded into the uploaded questions and answer instruction. However, we are yet to develop the capacity for interactions between different agents within the same simulated population. To achieve this, a series of sophisticated enhancements would be necessary. Key among these improvements is the ability to form and manage groups of agents. These groups function as independent social entities, engaging in intricate social interactions within their own set boundaries. In this envisioned application, users could define group sizes, roles within these groups (e.g., the proposer and responder roles in the ultimatum game), and any variations thereof. Furthermore, we would offer the ability to set randomisation parameters for both roles and variations, adding yet another layer of complexity and realism to the simulations. Another captivating idea under this future avenue is the introduction of a chat function between different agents. This feature would allow the observation of direct communication patterns and language use within and across agent groups, offering researchers another dimension to their social agent studies.\n\nIn summary, while our platform faces certain limitations, we view these as opportunities for growth and enhancement. Our commitment to continuous development and user satisfaction drives us to persistently explore innovative solutions to these challenges. This means that we are open for feedback and suggestions. As we progress on this journey, we look forward to unlocking further potential in facilitating complex research through advanced ALM models.\n\nFor beta access and further details about the SurveyLM platform, please contact the corresponding author of this paper.\n\n"], "nougat": ["\n\n**Model Diversity:** Our platform currently supports only OpenAI\u2019s models, chosen for their leading-edge capabilities and robust API access. This model-specific dependency could limit the platform\u2019s flexibility, as different models may offer unique strengths and capabilities that could be beneficial in diverse research scenarios. We are therefore actively testing other open source and commercial AI models to potentially integrate into our platform (Bai et al., 2022; Touvron et al., 2023), thus expanding its versatility and research applicability. It should be noted that we avoid models that essentially add new prompt-engineering layers on top of base ALMs to improve decision performance in specific tasks (Shinn et al., 2023; Yao et al., 2023).\n\n**Realistic Condition Profiles:** A minor drawback of our platform is the occasional generation of unrealistic agent profiles due to the randomness of profile construction. This approach also means that sometimes we end up with \u201cinteresting\u201d agent profile combinations that may seldom present in the real world (e.g., a male lesbian). While rare, these cases can disrupt the research process and lead to unrealistic model responses. One solution lies in conditional profile construction, where agent attributes are selected based on real-world prevalence and correlations. However, it is important to note that this randomness can sometimes yield unique case studies that might not have been otherwise considered, offering unexpected insights and interesting research avenues.\n\n**Multi-Agent Games and Interaction:** Currently, SurveyLM allows for the simulation of an agent\u2019s participation in games with other players only when the other players and interaction rules are hard-coded into the uploaded questions and answer instruction. However, we are yet to develop the capacity for interactions between different agents within the same simulated population. To achieve this, a series of sophisticated enhancements would be necessary. Key among these improvements is the ability to form and manage groups of agents. These groups function as independent social entities, engaging in intricate social interactions within their own set boundaries. In this envisioned application, users could define group sizes, roles within these groups (e.g., the proposer and responder roles in the ultimatum game), and any variations thereof. Furthermore, we would offer the ability to set randomisation parameters for both roles and variations, adding yet another layer of complexity and realism to the simulations. Another captivating idea under this future avenue is the introduction of a chat function between different agents. This feature would allow the observation of direct communication patterns and language use within and across agent groups, offering researchers another dimension to their social agent studies.\n\nIn summary, while our platform faces certain limitations, we view these as opportunities for growth and enhancement. Our commitment to continuous development and user satisfaction drives us to persistently explore innovative solutions to these challenges. This means that we are open for feedback and suggestions. As we progress on this journey, we look forward to unlocking further potential in facilitating complex research through advanced ALM models. For beta access and further details about the SurveyLM platform, please contact the corresponding author of this paper.\n\n"]}, {"edit": ["second-best option. We compare our bounds to naive estimates that simply control for reported preferences as if they were true preferences. Naive estimates generally fall within our bounds but may lead to erroneous conclusions in some cases where bounds are wide and do not pin down the sign of the true effect.\n\n\\begin{table}\n\\begin{tabular}{l l c} \\hline \\hline  & Estimate bounds & Naive estimate \\\\ \\hline _Graduation from Bus. Adm., PUC Santiago_ & & \\\\ Economics, UChile Santiago & [.319 ; .418] & .369 \\\\ Science \\& Engineering, UChile Santiago & [.000 ; 188] & .159 \\\\ Social Sciences \\& Humanities, PUC Santiago & [.230 ; .239] & .272 \\\\ Law, PUC Santiago & [.087 ; .407] & .321 \\\\ _Reapplication_ & & \\\\ Economics, UChile Santiago & [-.178 ; -.079] & -.114 \\\\ Science \\& Engineering, UChile Santiago & [-.369 ; -.107] & -.199 \\\\ Social Sciences \\& Humanities, PUC Santiago & [-.164 ; -.155] & -.156 \\\\ Law, PUC Santiago & [.065 ; .174] & .116 \\\\ _Graduation from any program_ & & \\\\ Economics, UChile Santiago & [-.218 ; -.029] & -.084 \\\\ Science \\& Engineering, UChile Santiago & [-.126 ; .164] & -.003 \\\\ Social Sciences \\& Humanities, PUC Santiago & [-.055 ; -.064] & -.063 \\\\ Law, PUC Santiago & [.032 ; .139] & .058 \\\\ _Graduation from STEM program_ & & \\\\ Economics, UChile Santiago & [-.016 ; .021] & .004 \\\\ Science \\& Engineering, UChile Santiago & [.000 ; .076] & .064 \\\\ Social Sciences \\& Humanities, PUC Santiago & [.000 ; .015] & .000 \\\\ Law, PUC Santiago & [.000 ; .000] & .000 \\\\ _Graduation from a top university (PUC Santiago or UChile)_ & & \\\\ Economics, UChile Santiago & [-.086 ; .013] & -.040 \\\\ Science \\& Engineering, UChile Santiago & [-.058 ; .232] & .060 \\\\ Social Sciences \\& Humanities, PUC Santiago & [.209 ; .218] & .210 \\\\ Law, PUC Santiago & [.123 ; .411] & .193 \\\\ \\hline \\hline \\end{tabular} Outcomes and second-best options \\(k\\) in \\(Q_{j}=(j,k)\\) are shown in the leftmost column of the table. For out parameter of interest, the first-best option in \\(Q_{j}\\) is always \\(j=\\)\u201cBA in Business administration at PUC Santiago\u201d. The second column shows bounds for our parameter of interest (Equation 1). Bounds are estimated using local linear polynomials, and without imposing UMAS (Assumption 5). Bandwidths are set to 30 points, except if another program \\(\\ell\\) has its cutoff within 30 points of \\ "], "nougat": ["second-best option. We compare our bounds to naive estimates that simply control for reported preferences as if they were true preferences. Naive estimates generally fall within our bounds but may lead to erroneous conclusions in some cases where bounds are wide and do not pin down the sign of the true effect.\n\n\\begin{table}\n\\begin{tabular}{l l c} \\hline \\hline  & Estimate bounds & Naive estimate \\\\ \\hline _Graduation from Bus. Adm., PUC Santiago_ & & \\\\ Economics, UChile Santiago & [.319 ; .418] & .369 \\\\ Science \\& Engineering, UChile Santiago & [.000 ; 188] & .159 \\\\ Social Sciences \\& Humanities, PUC Santiago & [.230 ; .239] & .272 \\\\ Law, PUC Santiago & [.087 ; .407] & .321 \\\\ _Reapplication_ & & \\\\ Economics, UChile Santiago & [-.178 ; -.079] & -.114 \\\\ Science \\& Engineering, UChile Santiago & [-.369 ; -.107] & -.199 \\\\ Social Sciences \\& Humanities, PUC Santiago & [-.164 ; -.155] & -.156 \\\\ Law, PUC Santiago & [.065 ; .174] & .116 \\\\ _Graduation from any program_ & & \\\\ Economics, UChile Santiago & [-.218 ; -.029] & -.084 \\\\ Science \\& Engineering, UChile Santiago & [-.126 ; .164] & -.003 \\\\ Social Sciences \\& Humanities, PUC Santiago & [-.055 ; -.064] & -.063 \\\\ Law, PUC Santiago & [.032 ; .139] & .058 \\\\ _Graduation from STEM program_ & & \\\\ Economics, UChile Santiago & [-.016 ; .021] & .004 \\\\ Science \\& Engineering, UChile Santiago & [.000 ; .076] & .064 \\\\ Social Sciences \\& Humanities, PUC Santiago & [.000 ; .015] & .000 \\\\ Law, PUC Santiago & [.000 ; .000] & .000 \\\\ _Graduation from a top university (PUC Santiago or UChile)_ & & \\\\ Economics, UChile Santiago & [-.086 ; .013] & -.040 \\\\ Science \\& Engineering, UChile Santiago & [-.058 ; .232] & .060 \\\\ Social Sciences \\& Humanities, PUC Santiago & [.209 ; .218] & .210 \\\\ Law, PUC Santiago & [.123 ; .411] & .193 \\\\ \\hline \\hline \\end{tabular} Outcomes and second-best options \\(k\\) in \\(Q_{j}=(j,k)\\) are shown in the leftmost column of the table. For out parameter of interest, the first-best option in \\(Q_{j}\\) is always \\(j=\\)\u201cBA in Business administration at PUC Santiago\u201d. The second column shows bounds for our parameter of interest (Equation 1). Bounds are estimated using local linear polynomials, and without imposing UMAS (Assumption 5). Bandwidths are set to 30 points, except if another program \\(\\ell\\) has its cutoff within 30 points of \\ "]}, {"edit": ["obvious like ill-health. If climate policy is successful, there is not much of a problem to solve anymore, making it harder to continue to justify spending large sums of money. In order to keep costs down, carbon capture will be done where land is cheap--that is, where few people live--and heavily mechanized. Paying 7% of your income in taxes to keep grandma alive and your nurse friend in work is one thing. Paying 7% to a multinational company to suck carbon dioxide out of the air in a faraway country is something else.\n\n### Regional results\n\nThe above results are for the world as a whole. The models in the IPCC database also report regional results. I restrict the attention to imaclim and one particular scenario which reduces emissions by 94% in 2050. The carbon tax is $300/tCO\\({}_{2}\\) in 2030, rising to $1,298/tCO\\({}_{2}\\) in 2040 and $2,253/tCO\\({}_{2}\\) in 2050. Figure 2 shows carbon tax revenue and sequestration subsidy, as a percentage of GDP, for 2030, 2040, and 2050.\n\nGlobal carbon tax revenue is 4% of GDP in 2050, a reasonable number, but 11% in 2030 and 19% in 2040--underlining yet again the fiscal challenge posed by stringent climate policy.\n\nThe results in Figure 2 are ordered by per capita income in 2010. Carbon tax revenue is below the global average in the three richest regions, but above the global average in the seven poorest regions--with the exception of almost completely decarbonized India in 2050. The carbon tax revenue is very high in the carbon-intensive economies of the Middle East and the former Soviet Union.\n\nThe bottom panel of Figure 2 shows the sequestration subsidies. The world total is 0.04% of GDP in 2030, rising to 3.8% in 2040 and 15% in 2050. As with tax revenues, the numbers are lower for the three rich regions and higher for the seven poor regions. Note, however, that it may well be that there will substantial transfers between regions. This is less likely with direct subsidies, more likely with tradable permits and offsets.\n\nThat said, Figure 2 highlights the scale of the activity. The sequestration sector would occupy almost 15% of the world economy, over 35% of the economy in the former Soviet Union.\n\n### Results for more lenient climate policy\n\nThe above results are for very stringent climate policy. Cutting carbon dioxide emissions by 95% or more by 2050 is highly ambitious. The major fiscal implications highlighted above rapidly disappear for less stringent climate policy. This is because the fiscal implications are the product of carbon price and emissions. Take the subsidies for carbon dioxide removal first. A more lenient target would mean a lower volume at a lower price. The carbon tax revenue would fall too: Emissions would be higher but the carbon price lower; the former is linear, the latter exponential.\n\nFigure 3 illustrates this for the imaclim model for 2050. The top left panel plots the carbon price against emission reduction from baseline. The carbon price inches up until emissions are halved and then starts rising very quickly. However, emissions, shown in the bottom left\n\n"], "nougat": ["obvious like ill-health. If climate policy is successful, there is not much of a problem to solve anymore, making it harder to continue to justify spending large sums of money. In order to keep costs down, carbon capture will be done where land is cheap\u2014that is, where few people live\u2014and heavily mechanized. Paying 7% of your income in taxes to keep grandma alive and your nurse friend in work is one thing. Paying 7% to a multinational company to suck carbon dioxide out of the air in a faraway country is something else.\n\n### Regional results\n\nThe above results are for the world as a whole. The models in the IPCC database also report regional results. I restrict the attention to imaclim and one particular scenario which reduces emissions by 94% in 2050. The carbon tax is $300/tCO\\({}_{2}\\) in 2030, rising to $1,298/tCO\\({}_{2}\\) in 2040 and $2,253/tCO\\({}_{2}\\) in 2050. Figure 2 shows carbon tax revenue and sequestration subsidy, as a percentage of GDP, for 2030, 2040, and 2050.\n\nGlobal carbon tax revenue is 4% of GDP in 2050, a reasonable number, but 11% in 2030 and 19% in 2040--underlining yet again the fiscal challenge posed by stringent climate policy.\n\nThe results in Figure 2 are ordered by per capita income in 2010. Carbon tax revenue is below the global average in the three richest regions, but above the global average in the seven poorest regions--with the exception of almost completely decarbonized India in 2050. The carbon tax revenue is very high in the carbon-intensive economies of the Middle East and the former Soviet Union.\n\nThe bottom panel of Figure 2 shows the sequestration subsidies. The world total is 0.04% of GDP in 2030, rising to 3.3. Regional results in 2040 and 15% in 2050. As with tax revenues, the numbers are lower for the three rich regions and higher for the seven poor regions. Note, however, that it may well be that there will substantial transfers between regions. This is less likely with direct subsidies, more likely with tradable permits and offsets.\n\nThat said, Figure 2 highlights the scale of the activity. The sequestration sector would occupy almost 15% of the world economy, over 35% of the economy in the former Soviet Union.\n\n### Results for more lenient climate policy\n\nThe above results are for the world as a whole. The models in the IPCC database also report regional results. I restrict the attention to imaclim and one particular scenario which reduces emissions by 94% in 2050. The carbon tax is $300/tCO\\({}_{2}\\) in 2030, rising to $ 1,298/tCO\\({}_{2}\\) in 2040 and $2,253/tCO\\({}_{2}\\) in 2050. Figure 2 shows carbon tax revenue and sequestration subsidy, as a percentage of GDP, for 2030, 2040, and 2050.\n\nGlobal carbon tax revenue is 4% of GDP in 2050, a reasonable number, but 11% in 2030 and 19% in 2040\u2014underlining yet again the fiscal challenge posed by stringent climate policy. The results in Figure 2 are ordered by per capita income in 2010. Carbon tax revenue is below the global average in the three richest regions, but above the global average in the seven poorest regions\u2014with the exception of almost completely decarbonized India in 2050. The carbon tax revenue is very high in the carbon-intensive economies of the Middle East and the former Soviet Union.\n\nThe bottom panel of "]}, {"edit": ["proximate entities share similarities, specifically the K-nearest neighbors. This approach enables the algorithm to efficiently navigate the space and identify the most comparable items. The algorithm's methodology is based on the square root of the N technique, which involves searching the entire volume of points in the training dataset. Given a specific point \\(x_{0}\\) that needs to be classified into one of the K groups, a possible approach is to determine the \\(k\\) observed data points closest to \\(x_{0}\\). According to [35], the sample with the most observed data points among the k-nearest neighbors is assigned \\(x_{0}\\) in the classification form. Up to this juncture, the degree of similarity is contingent upon a specific distance metric, thereby rendering the efficacy of classifiers significantly reliant on the integrated distance metric [36]. The methodology involves two distinct procedures, whereby the initial step entails the creation of an adjacency matrix and, subsequently, the estimation of edge weights is performed [37].\n\n#### 3.2.5 Forecasting Performance Measures\n\nWe report on a number of robustness tests. In addition to the mean absolute percentage error (MAPE), despite its widespread application in forecasting literature, we present forecasting results for the following loss functions: the mean absolute error (MAE) and the root mean square error (RMSE). The results for each accuracy measure are reported in Table 3.\n\n\\[\\text{MAE}=\\frac{1}{h}\\sum_{j=1}^{h}|y_{t+j}-\\hat{y}_{t+j}|, \\tag{5}\\]\n\n\\[\\text{RMSE}=\\sqrt{\\frac{1}{h}\\sum_{j=1}^{h}\\left(y_{t+j}-\\hat{y}_{t+j}\\right)^ {2}}, \\tag{6}\\]\n\n\\[\\text{MAPE}=\\frac{1}{h}\\sum_{j=1}^{h}\\frac{y_{t+j}-\\hat{y}_{t+j}}{y_{t+j}}, \\tag{7}\\]\n\nwhere \\(\\hat{y}_{t+j}\\) refers to the model's forecast at time \\(t\\). \\(y_{t+j}\\) is the associated actual values, \\(h\\) is the forecasting horizon, and \\(j\\) is the number of historical observations.\n\nHowever, despite its widespread application in forecasting literature, the mean absolute percentage error (MAPE) has attracted criticism as it exhibits significant shortcomings in generating undefined or infinite outcomes for actual zero or near-zero values [38]. Therefore, to assess the reviewed forecasting techniques and verify that the conclusions drawn from the study are indicative, we employed AvgRelMAE, considering the ARIMA model as a benchmark:\n\n\\[\\text{AvgRelMAE}=\\text{exp}\\left(\\frac{1}{N}\\sum_{i=1}^{N}\\text{ln}\\bigg{(} \\frac{MAE_{\\text{Model},i}}{MAE_{\\text{ARIMA},i}}\\bigg{)}\\right) \\tag{8}\\]\n\nwhere \\(N\\) represents the total number of series used for the evaluation of forecasting methods.\n\n## 4 Discussion\n\nTable 3 reports the accuracy metrics for three different time series forecasting models, namely, ARIMA, ETS-ANN, and \\(k\\)NN, for different SSMI financial market assets, i.e., BTC-USD, GDAXI, FTSE, N100, FCHI, and SSMI, from 2018 to 2021. The accuracy of the models is evaluated based on the following metrics: mean absolute error (MAE), root mean square error (RMSE), and the mean absolute percentage error (MAPE).\n\nThe findings indicate that the ranking of the models' performance remains consistent across each examined subperiod. Based on all accuracy metrics, it can be observed that the hybrid ETS-ANN model is the prevailing model for studying the specific dataset of financial market assets. According to the MAPE metric, the ETS-ANN model had the lowest value for almost all indices and subperiods except for the entire period results. The \\(k\\)NN model displayed some "], "nougat": ["proximate entities share similarities, specifically the K-nearest neighbors. This approach enables the algorithm to efficiently navigate the space and identify the most comparable items. The algorithm\u2019s methodology is based on the square root of the N technique, which involves searching the entire volume of points in the training dataset. Given a specific point \\(x_{0}\\) that needs to be classified into one of the K groups, a possible approach is to determine the \\(k\\) observed data points among the k-nearest neighbors is assigned \\(x_{0}\\) in the classification form. Up to this juncture, the degree of similarity is contingent upon a specific distance metric, thereby rendering the efficacy of classifiers significantly reliant on the integrated distance metric [ 36 ]. The methodology involves two distinct procedures, whereby the initial step entails the creation of an adjacency matrix and, subsequently, the estimation of edge weights is performed [ 37 ].\n\n#### 3.2.5. Forecasting Performance Measures\n\nWe report on a number of robustness tests. In addition to the mean absolute percentage error (MAPE), despite its widespread application in forecasting literature, we present forecasting results for the following loss functions: the mean absolute error (MAE) and the root mean square error (RMSE). The results for each accuracy measure are reported in Table 3 .\n\n\\[\\text{MAE}=\\frac{1}{h}\\sum_{j=1}^{h}|y_{t+j}-\\hat{y}_{t+j}|, \\tag{5}\\]\n\n\\[\\text{RMSE}=\\sqrt{\\frac{1}{h}\\sum_{j=1}^{h}\\left(y_{t+j}-\\hat{y}_{t+j}\\right)^ {2}}, \\tag{6}\\]\n\n\\[\\text{MAPE}=\\frac{1}{h}\\sum_{j=1}^{h}\\frac{y_{t+j}-\\hat{y}_{t+j}}{y_{t+j}}, \\tag{7}\\]\n\nwhere \\(\\hat{y}_{t+j}\\) refers to the model\u2019s forecast at time \\(t\\). \\(y_{t+j}\\) is the associated actual values, \\(h\\) is the forecasting horizon, and \\(j\\) is the number of historical observations.\n\nHowever, despite its widespread application in forecasting literature, the mean absolute percentage error (MAPE) has attracted criticism as it exhibits significant shortcomings in generating undefined or infinite outcomes for actual zero or near-zero values [ 38 ]. Therefore, to assess the reviewed forecasting techniques and verify that the conclusions drawn from the study are indicative, we employed AvgRelMAE, considering the ARIMA model as a benchmark:\n\n\\[\\text{AvgRelMAE}=\\text{exp}\\left(\\frac{1}{N}\\sum_{i=1}^{N}\\text{ln}\\bigg{(} \\frac{MAE_{\\text{Model},i}}{MAE_{\\text{ARIMA},i}}\\bigg{)}\\right) \\tag{8}\\]\n\nwhere \\(N\\) represents the total number of series used for the evaluation of forecasting methods.\n\n## 4. Discussion\n\nTable 3 reports the accuracy metrics for three different time series forecasting models, namely, ARIMA, ETS-ANN, and \\(k\\)NN, for different SSMI financial market assets, i.e., BTCUSD, GDAXI, FTSE, N100, FCHI, and SSMI, from 2018 to 2021. The accuracy of the models is evaluated based on the following metrics: mean absolute error (MAE), root mean square error (RMSE), and the mean absolute percentage error (MAPE).\n\nThe findings indicate that the ranking of the models\u2019 performance remains consistent across each examined subperiod. Based on all accuracy metrics, it can be observed that the hybrid ETS-ANN model is the prevailing model for studying the specific dataset of financial market assets. According to the MAPE metric, the ETS-ANN model had the lowest value for almost all indices and subperiods except for the entire period results. The \\(k\\)NN model displayed some lower results during the pro-COVID-19 period of 2018 and "]}, {"edit": ["private so that agent \\(i\\) knows only his own bonus \\(v_{i}\\) but not those of his opponents. In Halac et al. (2021), the private nature of bonus offers creates what these authors call _rank uncertainty_. Specifically, let \\(t_{i}\\) denote the rank of agent \\(i\\), all \\(i\\in\\mathscr{N}\\), and let \\(T\\) denote the set of all permutations on the player set \\(\\mathscr{N}\\). A ranking \\(t=(t_{i})\\in T\\) assigns a rank to each agent \\(i\\in\\mathscr{N}\\).\n\nIn a _ranking scheme_, no two players are assigned the same rank and the bonus to agent \\(i\\) makes him indifferent between working and shirking is he believes that all agents \\(j\\) ranked lower than \\(i\\) work while those ranked higher do not work. The expected payoff to work for agent \\(i\\) with rank \\(t_{i}\\) offered a bonus \\(v_{i}\\) in a ranking scheme is\n\n\\[v_{i}\\cdot[q\\left(\\{j\\in\\mathscr{N}:t_{j}\\leq t_{i}\\}\\right)-q\\left(\\{j\\in \\mathscr{N}:t_{j}<t_{i}\\}\\right)]-c_{i},\\]\n\nwhere \\(q\\left(\\{J\\}\\right)\\) denotes the probability of project success when all agents \\(j\\in J\\subseteq\\mathscr{N}\\) work while agents \\(j\\notin J\\) do not work. Now let \\(\\mu_{i}\\) be agent \\(i\\)'s ranking belief, i.e. his distribution on the rankings \\(t\\in T\\). Given his belief \\(\\mu_{i}\\), the bonus \\(v_{i}\\) that is necessary to make player \\(i\\) indifferent between working and not working is given by\n\n\\[v_{i}=\\frac{c_{i}}{\\sum_{t\\in T}\\mu_{i}(t)\\left[q\\left(\\{j\\in\\mathscr{N}:t_{j} \\leq t_{i}\\}\\right)-q\\left(\\{j\\in\\mathscr{N}:t_{j}<t_{i}\\}\\right)\\right]}.\\]\n\nThe principal in Halac et al. (2021) tries to find those bonuses and ranking distribution that induce work as the unique equilibrium of the game at minimal cost. They show that, for symmetric agents, an optimal policy is a ranking scheme that induces uniform beliefs over all possible rankings. This convergence to uniform beliefs bears interesting resemblance to the policy derived in Proposition 1, which establishes that in a global game (with or without symmetric agents) an optimal bonus makes agent \\(i\\) who observes signal \\(x_{i}^{\\varepsilon}=\\tilde{x}\\) exactly indifferent between working and shirking under a uniform belief about the number of agents that work.\n\n "], "nougat": ["private so that agent \\(i\\) knows only his own bonus \\(v_{i}\\) but not those of his opponents. In Halac et al. (2021), the private nature of bonus offers creates what these authors call _rank uncertainty_. Specifically, let \\(t_{i}\\) denote the rank of agent \\(i\\), all \\(i\\in\\mathscr{N}\\), and let \\(T\\) denote the set of all permutations on the player set \\(\\mathscr{N}\\). A ranking \\(t=(t_{i})\\in T\\) assigns a rank to each agent \\(i\\in\\mathscr{N}\\).\n\nIn a _ranking scheme_, no two players are assigned the same rank and the bonus to agent \\(i\\) makes him indifferent between working and shirking is he believes that all agents \\(j\\) ranked lower than \\(i\\) work while those ranked higher do not work. The expected payoff to work for agent \\(i\\) with rank \\(t_{i}\\) offered a bonus \\(v_{i}\\) in a ranking scheme is\n\n\\[v_{i}\\cdot[q\\left(\\{j\\in\\mathscr{N}:t_{j}\\leq t_{i}\\}\\right)-q\\left(\\{j\\in \\mathscr{N}:t_{j}<t_{i}\\}\\right)]-c_{i},\\]\n\nwhere \\(q\\left(\\{J\\}\\right)\\) denotes the probability of project success when all agents \\(j\\in J\\subseteq\\mathscr{N}\\) work while agents \\(j\\notin J\\) do not work. Now let \\(\\mu_{i}\\) be agent \\(i\\)'s ranking belief, i.e. his distribution on the rankings \\(t\\in T\\). Given his belief \\(\\mu_{i}\\), the bonus \\(v_{i}\\) that is necessary to make player \\(i\\) indifferent between working and not working is given by\n\n\\[v_{i}=\\frac{c_{i}}{\\sum_{t\\in T}\\mu_{i}(t)\\left[q\\left(\\{j\\in\\mathscr{N}:t_{j }\\leq t_{i}\\}\\right)-q\\left(\\{j\\in\\mathscr{N}:t_{j}<t_{i}\\}\\right)\\right]}.\\]\n\nThe principal in Halac et al. (2021) tries to find those bonuses and ranking distribution that induce work as the unique equilibrium of the game at minimal cost. They show that, for symmetric agents, an optimal policy is a ranking scheme that induces uniform beliefs over all possible rankings. This convergence to uniform beliefs bears interesting resemblance to the policy derived in Proposition 1, which establishes that in a global game (with or without symmetric agents) an optimal bonus makes agent \\(i\\) who observes signal \\(x_{i}^{\\varepsilon}=\\tilde{x}\\) exactly indifferent between working and shirking under a uniform belief about the number of agents that work.\n\n "]}, {"edit": ["Introduction\n\n_Distributionally robust optimization_ (DRO) has emerged as a powerful tool for hedging against model misspecification and distributional shifts. It minimizes _distributional model risk_ (DMR) defined as the worst risk over a class of distributions lying in a _distributional uncertainty set_, see Blanchet and Murthy (2019). Among many different choices of uncertainty sets, Wasserstein DRO (W-DRO) with distributional uncertainty sets based on optimal transport costs has gained much popularity, see Kuhn et al. (2019) and Blanchet et al. (2021) for recent reviews. W-DRO has found successful applications in robust decision making in all disciplines including economics, finance, machine learning, and operations research. Its success is largely credited to the strong duality and other nice properties of the Wasserstein DMR (W-DMR). The objective of this paper is to propose and study W-DMR in _marginal problems where only some marginal measures of a reference measure are given_, see e.g., Kellerer (1984), Rachev and Ruschendorf (1998), Villani (2009), and Villani (2021), and Ruschendorf (1991).\n\nIn practice, _marginal problems_ arise from either the lack of complete data or an incomplete model. In insurance and risk management, computing model-free measures of aggregate risks such as Value-at-Risk and Expected Short-Fall is of utmost importance and routinely done. When the exact dependence structure between individual risks is lacking, researchers and policy makers rely on the worst risk measures defined as the maximum value of aggregate risk measures over all joint measures of the individual risks with some fixed marginal measures, see Embrechts and Puccetti (2010) and Embrechts et al. (2013); In causal inference, distributional treatment effects such as the variance and the proportion of participants who benefit from the treatment depend on the joint distribution of the potential outcomes. Even with ideal randomized experiments such as double-blind clinical trials, the joint distribution of potential outcomes is not identified and as a result, only the lower and upper bounds on distributional treatment effects are identified from the sample information, see Fan and Wu (2009), Fan and Park (2010) and Fan and Park (2012), Fan et al. (2017), Ridder and Moffitt (2007), Firpo and Ridder (2019); In algorithmic fairness when the sensitive group variable is not observed in the main data set, assessment of unfairness measures must be done using multiple data sets, see Kallus et al. (2022). Abstracting away from estimation, all these problems involve optimizing the expected value of a functional of multiple random variables with fixed marginals and thus belong to the class of marginal problems for which optimal transport related tools are important.1 "], "nougat": ["Introduction\n\n_Distributionally robust optimization_ (DRO) has emerged as a powerful tool for hedging against model misspecification and distributional shifts. It minimizes _distributional model risk_ (DMR) defined as the worst risk over a class of distributions lying in a _distributional uncertainty set_, see Blanchet and Murthy ( 2019 ). Among many different choices of uncertainty sets, Wasserstein DRO (W-DRO) with distributional uncertainty sets based on optimal transport costs has gained much popularity, see Kuhn et al. ( 2019 ) and Blanchet et al. ( 2021 ) for recent reviews. W-DRO has found successful applications in robust decision making in all disciplines including economics, finance, machine learning, and operations research. Its success is largely credited to the strong duality and other nice properties of the Wasserstein DMR (W-DMR). The objective of this paper is to propose and study W-DMR in _marginal problems where only some marginal measures of a reference measure are given_, see e.g., Kellerer ( 1984 ), Rachev and R \u0308uschendorf ( 1998 ), Villani ( 2009 ), and Villani ( 2021 ), and R \u0308uschendorf ( 1991 ).\n\nIn practice, _marginal problems_ arise from either the lack of complete data or an incomplete model. In insurance and risk management, computing model-free measures of aggregate risks such as Value-at-Risk and Expected Short-Fall is of utmost importance and routinely done. When the exact dependence structure between individual risks is lacking, researchers and policy makers rely on the worst risk measures defined as the maximum value of aggregate risk measures over all joint measures of the individual risks with some fixed marginal measures, see Embrechts and Puccetti ( 2010 ) and Embrechts et al. ( 2013 ); In causal inference, distributional treatment effects such as the variance and the proportion of participants who benefit from the treatment depend on the joint distribution of the potential outcomes. Even with ideal randomized experiments such as double-blind clinical trials, the joint distribution of potential outcomes is not identified and as a result, only the lower and upper bounds on distributional treatment effects are identified from the sample information, see Fan and Wu ( 2009 ), Fan and Park ( 2010 ) and Fan and Park ( 2012 ), Fan et al. ( 2017 ), Ridder and Moffitt ( 2007 ), Firpo and Ridder ( 2019 ); In algorithmic fairness when the sensitive group variable is not observed in the main data set, assessment of unfairness measures must be done using multiple data sets, see Kallus et al. ( 2022 ). Abstracting away from estimation, all these problems involve optimizing the expected value of a functional of multiple random variables with fixed marginals and thus belong to the class of marginal problems for which optimal transport related tools are important. 1 "]}, {"edit": ["Let \\(\\phi_{\\tau}\\left(F_{X}\\right):=\\inf\\left\\{a\\in\\mathbb{R}:F_{X}\\left(a\\right)\\geq \\tau\\right\\}\\) be the \\(\\tau\\)-th quantile function of a random variable \\(X\\) associated with a c.d.f. \\(F_{X}\\). The von Mises expansion of \\(\\phi_{\\tau}\\left(F_{X}\\right)\\) (p.292 in Vaart (1998)) is given by:\n\n\\[\\phi_{\\tau}\\left(E_{n}\\right)-\\phi_{\\tau}\\left(E\\right)=\\frac{1}{\\sqrt{n}}\\phi _{\\tau,E}^{\\prime}\\left(\\mathbb{G}_{n}\\right)+\\ldots+\\frac{1}{m!}\\frac{1}{n^{ m/2}}\\phi_{\\tau,E}^{\\left(k\\right)}\\left(\\mathbb{G}_{n}\\right)+\\ldots,\\]\n\nwhere \\(\\phi_{\\tau,E}^{\\prime}\\left(.\\right)\\) is a linear derivative map and \\(\\mathbb{G}_{n}\\) denotes an empirical process \\(\\mathbb{G}_{n}f\\left(W\\right)=\\sqrt{n}\\left(E_{n}f\\left(W\\right)-E\\left[f \\left(W\\right)\\right]\\right)\\). Let \\(\\phi_{\\mathbf{\\theta}}^{\\prime}:=\\left(\\phi_{\\tau,\\mathbf{\\theta}}^{\\prime}\\right)_{ \\tau\\in\\mathcal{T}}\\), where \\(\\mathbf{\\theta}=\\left(\\mathbf{\\theta}_{a}\\right)_{a\\in\\mathcal{A}}\\). Let \\(Q_{Y\\left(d,M\\left(d^{\\prime}\\right)\\right)}^{0}\\left(\\tau\\right):=\\inf\\left\\{ a\\in\\mathbb{R}:\\theta_{d,d^{\\prime},a}^{0}\\geq\\tau\\right\\}\\), \\(\\hat{Q}_{Y\\left(d,M\\left(d^{\\prime}\\right)\\right)}\\left(\\tau\\right):=\\inf\\left\\{ a\\in\\mathbb{R}:\\hat{\\theta}_{d,d^{\\prime},a}\\geq\\tau\\right\\}\\) and \\(\\hat{Q}_{Y\\left(d,M\\left(d^{\\prime}\\right)\\right)}^{*}\\left(\\tau\\right):=\\inf \\left\\{a\\in\\mathbb{R}:\\hat{\\theta}_{d,d^{\\prime},a}^{*}\\geq\\tau\\right\\}\\). Let \\(\\mathbf{Q}_{\\tau}^{0}\\), \\(\\mathbf{\\hat{Q}_{\\tau}}\\) and \\(\\mathbf{\\hat{Q}_{\\tau}}^{*}\\) denote the corresponding vectors containing \\(Q_{Y\\left(d,M\\left(d^{\\prime}\\right)\\right)}^{0}\\left(\\tau\\right)\\), \\(\\hat{Q}_{Y\\left(d,M\\left(d^{\\prime}\\right)\\right)}\\left(\\tau\\right)\\) and \\(\\hat{Q}_{Y\\left(d,M\\left(d^{\\prime}\\right)\\right)}^{*}\\left(\\tau\\right)\\) for different \\(\\left(d,d^{\\prime}\\right)\\in "], "nougat": ["Let \\(\\phi_{\\tau}\\left(F_{X}\\right):=\\inf\\left\\{a\\in\\mathbb{R}:F_{X}\\left(a\\right)\\geq \\tau\\right\\}\\) be the \\(\\tau\\)-th quantile function of a random variable \\(X\\) associated with a c.d.f. \\(F_{X}\\). The von Mises expansion of \\(\\phi_{\\tau}\\left(F_{X}\\right)\\) (p.292 in Vaart (1998)) is given by:\n\n\\[\\phi_{\\tau}\\left(E_{n}\\right)-\\phi_{\\tau}\\left(E\\right)=\\frac{1}{\\sqrt{n}}\\phi _{\\tau,E}^{\\prime}\\left(\\mathbb{G}_{n}\\right)+\\ldots+\\frac{1}{m!}\\frac{1}{n^{ m/2}}\\phi_{\\tau,E}^{\\left(k\\right)}\\left(\\mathbb{G}_{n}\\right)+\\ldots,\\]\n\nwhere \\(\\phi_{\\tau,E}^{\\prime}\\left(.\\right)\\) is a linear derivative map and \\(\\mathbb{G}_{n}\\) denotes an empirical process \\(\\mathbb{G}_{n}f\\left(W\\right)=\\sqrt{n}\\left(E_{n}f\\left(W\\right)-E\\left[f \\left(W\\right)\\right]\\right)\\). Let \\(\\phi_{\\mathbf{\\theta}}^{\\prime}:=\\left(\\phi_{\\tau,\\mathbf{\\theta}}^{\\prime}\\right)_{ \\tau\\in\\mathcal{T}}\\), where \\(\\mathbf{\\theta}=\\left(\\mathbf{\\theta}_{a}\\right)_{a\\in\\mathcal{A}}\\). Let \\(Q_{Y\\left(d,M\\left(d^{\\prime}\\right)\\right)}^{0}\\left(\\tau\\right):=\\inf\\left\\{ a\\in\\mathbb{R}:\\theta_{d,d^{\\prime},a}^{0}\\geq\\tau\\right\\}\\), \\(\\hat{Q}_{Y\\left(d,M\\left(d^{\\prime}\\right)\\right)}\\left(\\tau\\right):=\\inf\\left\\{ a\\in\\mathbb{R}:\\hat{\\theta}_{d,d^{\\prime},a}\\geq\\tau\\right\\}\\) and \\(\\hat{Q}_{Y\\left(d,M\\left(d^{\\prime}\\right)\\right)}^{*}\\left(\\tau\\right):=\\inf \\left\\{a\\in\\mathbb{R}:\\hat{\\theta}_{d,d^{\\prime},a}^{*}\\geq\\tau\\right\\}\\). Let \\(\\mathbf{Q}_{\\tau}^{0}\\), \\(\\mathbf{\\hat{Q}_{\\tau}}\\) and \\(\\mathbf{\\hat{Q}_{\\tau}}^{*}\\) denote the corresponding vectors containing \\(Q_{Y\\left(d,M\\left(d^{\\prime}\\right)\\right)}^{0}\\left(\\tau\\right)\\), \\(\\hat{Q}_{Y\\left(d,M\\left(d^{\\prime}\\right)\\right)}\\left(\\tau\\right)\\) and \\(\\hat{Q}_{Y\\left(d,M\\left(d^{\\prime}\\right)\\right)}^{*}\\left(\\tau\\right)\\) for different \\(\\left(d,d^{\\prime}\\right)\\in "]}, {"edit": ["promised epistemic benefits of democracy, we would need to make the decision-procedures used in actual democracies a lot more inclusive and a lot more egalitarian than they are at present. Institutional reforms that the argument points toward include the replacement of elected representatives with randomly selected ones and a greater use of simple majoritarian decision-making.\n\nWhile the argument is not explicitly stated6, a crucial hypothesis needed for the theorem assumes the following forms:\n\nFootnote 6: Despite claiming that \u2018The argument presented here is based on a simple model of democracy and is entirely deductive,\u2019 the precise premises, intermediary steps, and conclusions are never explicitly stated. This should be the first step in constructing the argument and possible replies.\n\n* _Hypothesis_, \\(H\\): Cognitive diversity, defined as individuals seeing problems and making predictions based on different models of the world, is a group property that improves group performance in various contexts.\n* _Hypothesis'_, \\(H^{\\prime}\\): Greater cognitive diversity within a group correlates with better problem-solving and prediction abilities.\n\nTo justify this, Landemore relies on the results of Hong and Page as described above [5]:\n\nTo make that claim, I essentially rely on Hong and Page's formal results about the centrality of cognitive diversity to the emergent property of collective intelligence.\n\nWe aim to demonstrate that this hypothesis is unjustified, which subsequently renders the argument both logically unsound and inapplicable to real-world scenarios. Additionally, we will highlight instances where she incorrectly deduces propositions from these mathematical theorems, leading to a logically invalid argument.\n\n**Remark 6.1**.: Some of the critiques presented in the previous section also apply to Landemore. For instance, when she informally discusses the theorem, she falls into the same misrepresentation as Hong and Page, as discussed in Section 5.2. For instance, she stated in a public debate:\n\nThere are multiple Hong-Page theorems. The one that I use mostly is the 'Diversity Trumps Ability' theorem. It's basically a formalization of the idea that under certain conditions, you're better off solving problems with a group of average people who think differently than with a group of experts or very smart people.\n\nAs we have previously illustrated, this assertion is entirely false, see Section 5.2 and below for more details. \\(\\diamond\\)\n\n### Basic misunderstanding of the mathematical theorems\n\nLandemore says (about the Theorem 1.7):\n\nLet me pause here to emphasize what a remarkably **counterintuitive**, indeed amazing, result his is. Where the conditions apply, you are better off with a **random group of people who think differently than with a bunch of Einsteins**! Who would have thought? In my view, this result should truly change our perspective on what makes groups smart to begin with; I believe it has huge implications for the way we should think about political bodies making decisions on our behalf. _(emphasis added)_\n\nAlso [5],\n\nThat theorem was sufficiently counterintuitive that they provided a computational example to provide intuition.\n\n "], "nougat": ["promised epistemic benefits of democracy, we would need to make the decision-procedures used in actual democracies a lot more inclusive and a lot more egalitarian than they are at present. Institutional reforms that the argument points toward include the replacement of elected representatives with randomly selected ones and a greater use of simple majoritarian decision-making.\n\nWhile the argument is not explicitly stated6, a crucial hypothesis needed for the theorem assumes the following forms:\n\nFootnote 6: Despite claiming that \u2018The argument presented here is based on a simple model of democracy and is entirely deductive,\u2019 the precise premises, intermediary steps, and conclusions are never explicitly stated. This should be the first step in constructing the argument and possible replies.\n\n* _Hypothesis_, \\(H\\): Cognitive diversity, defined as individuals seeing problems and making predictions based on different models of the world, is a group property that improves group performance in various contexts.\n* _Hypothesis'_, \\(H^{\\prime}\\): Greater cognitive diversity within a group correlates with better problem-solving and prediction abilities.\n\nTo justify this, Landemore relies on the results of Hong and Page as described above [5]:\n\nTo make that claim, I essentially rely on Hong and Page's formal results about the centrality of cognitive diversity to the emergent property of collective intelligence.\n\nWe aim to demonstrate that this hypothesis is unjustified, which subsequently renders the argument both logically unsound and inapplicable to real-world scenarios. Additionally, we will highlight instances where she incorrectly deduces propositions from these mathematical theorems, leading to a logically invalid argument.\n\n**Remark 6.1**.: Some of the critiques presented in the previous section also apply to Landemore. For instance, when she informally discusses the theorem, she falls into the same misrepresentation as Hong and Page, as discussed in Section 5.2. For instance, she stated in a public debate:\n\nThere are multiple Hong-Page theorems. The one that I use mostly is the 'Diversity Trumps Ability' theorem. It's basically a formalization of the idea that under certain conditions, you're better off solving problems with a group of average people who think differently than with a group of experts or very smart people.\n\nAs we have previously illustrated, this assertion is entirely false, see Section 5.2 and below for more details. \\(\\diamond\\)\n\n### Basic misunderstanding of the mathematical theorems\n\nLandemore says (about the Theorem 1.7):\n\nLet me pause here to emphasize what a remarkably **counterintuitive**, indeed amazing, result his is. Where the conditions apply, you are better off with a **random group of people who think differently than with a bunch of Einsteins**! Who would have thought? In my view, this result should truly change our perspective on what makes groups smart to begin with; I believe it has huge implications for the way we should think about political bodies making decisions on our behalf. _(emphasis added)_\n\nAlso [5],\n\nThat theorem was sufficiently counterintuitive that they provided a computational example to provide intuition.\n\n "]}, {"edit": ["Proof: Let a proxy vote profile \\((\\mathbf{P},\\mathbf{S},\\mathbf{D})\\) be arbitrary. Pick some bijection \\(\\psi:N\\to N\\). Then we must have\n\n\\[f(\\psi(\\mathbf{P})_{\\mathbf{\\pi},\\psi(\\mathbf{S}),\\psi(\\mathbf{D})}) =f(\\psi(\\mathbf{P}_{\\mathbf{\\pi},\\mathbf{S},\\mathbf{D}}))\\] (since\n\n\\[g\\]\n\n is anonymous) \\[=f(\\mathbf{P}_{\\mathbf{\\pi},\\mathbf{S},\\mathbf{D}})\\] (since\n\n\\[f\\]\n\n is anonymous)\n\nLemma 3: _If \\(f\\) is neutral and \\(g\\) is proxy mechanism neutral, then \\((f,g)\\) is proxy vote neutral._\n\nProof: Let a proxy vote profile \\((\\mathbf{P},\\mathbf{S},\\mathbf{D})\\) be arbitrary. Pick some bijection \\(\\psi:A\\to A\\). Then we must have\n\n\\[f(\\psi(\\mathbf{P})_{\\mathbf{\\pi},\\mathbf{S},\\psi(\\mathbf{D})}) =f(\\psi(\\mathbf{P}_{\\mathbf{\\pi},\\mathbf{S},\\mathbf{D}}))\\] (since\n\n\\[g\\]\n\n is neutral) \\[=\\psi(f(\\mathbf{P}_{\\mathbf{\\pi},\\mathbf{S},\\mathbf{D}}))\\] (since\n\n\\[f\\]\n\n is neutral)\n\nNote that, in general, the other direction of Lemmas 2 and 3 won't hold.\n\n### A Proxy Vote Analogue of May's Theorem\n\nWhen \\(|A|=2\\), the _majority rule_ selects the alternative which receives the most first choice votes. When \\(|N|\\) is odd (meaning the majority rule is resolute), May (May 1952) shows that we can characterise the majority rule as the unique rule satisfying anonymity, neutrality and weak monotonicity.\n\nWe can use the proxy vote analogues of these properties to achieve the same characterisation result. The key point is that setting \\(|A|=2\\) fully specifies the proxy mechanism \\(g\\), by the definition of a proxy mechanism (since either a voter submits an empty order, meaning she is allowed to delegate to any other voter, or she submits a linear order, meaning she casts her vote directly). In effect, we are close to a classical vote; the only voters who delegate their votes are the voters who submit empty orders. So it is unsurprising that the following result holds irrespective of the choice of \\(g\\).\n\nTheorem 5.1.: _Suppose \\(|A|=2\\) and \\(|N|\\) is odd. Then a pair \\((f,g)\\) satisfies_\n\n* _Proxy Vote Anonymity_\n* _Proxy Vote Neutrality_\n* _Proxy Vote Addition Monotonicity (PVAM), and_\n* _Proxy Vote Deletion Monotonicity (PVDM)_\n\n_iff \\(f\\) is the majority rule._ "], "nougat": ["Proof: Let a proxy vote profile \\((\\mathbf{P},\\mathbf{S},\\mathbf{D})\\) be arbitrary. Pick some bijection \\(\\psi:N\\to N\\). Then we must have\n\n\\[f(\\psi(\\mathbf{P})_{\\mathbf{\\pi},\\psi(\\mathbf{S}),\\psi(\\mathbf{D})}) =f(\\psi(\\mathbf{P}_{\\mathbf{\\pi},\\mathbf{S},\\mathbf{D}}))\\] (since\n\n\\[g\\]\n\n is anonymous) \\[=f(\\mathbf{P}_{\\mathbf{\\pi},\\mathbf{S},\\mathbf{D}})\\] (since\n\n\\[f\\]\n\n is anonymous)\n\nLemma 3: _If \\(f\\) is neutral and \\(g\\) is proxy mechanism neutral, then \\((f,g)\\) is proxy vote neutral._\n\nProof: Let a proxy vote profile \\((\\mathbf{P},\\mathbf{S},\\mathbf{D})\\) be arbitrary. Pick some bijection \\(\\psi:A\\to A\\). Then we must have\n\n\\[f(\\psi(\\mathbf{P})_{\\mathbf{\\pi},\\mathbf{S},\\psi(\\mathbf{D})}) =f(\\psi(\\mathbf{P}_{\\mathbf{\\pi},\\mathbf{S},\\mathbf{D}}))\\] (since\n\n\\[g\\]\n\n is neutral) \\[=\\psi(f(\\mathbf{P}_{\\mathbf{\\pi},\\mathbf{S},\\mathbf{D}}))\\] (since\n\n\\[f\\]\n\n is neutral)\n\nNote that, in general, the other direction of Lemmas 2 and 3 won\u2019t hold.\n\n### A Proxy Vote Analogue of May\u2019s Theorem\n\nWhen \\(|A|=2\\), the _majority rule_ selects the alternative which receives the most first choice votes. When \\(|N|\\) is odd (meaning the majority rule is resolute), May (May 1952) shows that we can characterise the majority rule as the unique rule satisfying anonymity, neutrality and weak monotonicity.\n\nWe can use the proxy vote analogues of these properties to achieve the same characterisation result. The key point is that setting \\(|A|=2\\) fully specifies the proxy mechanism (since either a voter submits an empty order, meaning she is allowed to delegate to any other voter, or she submits a linear order, meaning she casts her vote directly). In effect, we are close to a classical vote; the only voters who delegate their votes are the voters who submit empty orders. So it is unsurprising that the following result holds irrespective of the choice of \\(g\\).\n\nTheorem 5.1.: _Suppose \\(|A|=2\\) and \\(|N|\\) is odd. Then a pair \\((f,g)\\) satisfies_\n\n* _Proxy Vote Anonymity_\n* _Proxy Vote Neutrality_\n* _Proxy Vote Addition Monotonicity (PVAM), and_\n* _Proxy Vote Deletion Monotonicity (PVDM)_\n\n_iff \\(f\\) is the majority rule._ "]}, {"edit": ["To investigate the possibility of stockouts at the warehouse level and their potential impact on store-level stockouts, we also analyze aggregate daily deliveries from the warehouse to all 634 stores. Given that the products in our working sample are popular items, we interpret a zero value in aggregate daily deliveries as a stockout event at the warehouse. The results of our analysis, presented in Section A.4 in the Appendix, reveal that warehouse stockouts are negligible for our working sample. For each product within the working sample, warehouse stockout events occur on no more than 3 out of the 677 days in the sample, which accounts for less than 0.5% of the observed period. These findings suggest that stockouts observed at the store level are primarily driven by factors other than warehouse-level stockouts.\n\n### Data on store managers' education and experience\n\nIn addition to the main dataset, we enhance our analysis by incorporating information on store managers' human capital. Leveraging the professional social networking platform _LinkedIn_, we gather data on the education and experience of store managers from their public profiles. Out of the 634 store managers in our dataset, 600 are identifiable by name in the LCBO's records.7 Within this subset, we were able to locate public _LinkedIn_ profiles for 143 managers, allowing us to retrieve valuable information about their educational background and work experience.\n\nFootnote 7: During our sample period, some stores are overseen by interim managers who are not identified by name in our main dataset.\n\nTable 4 presents summary statistics for the variables related to store managers' educational background and work experience. Notably, we observe a pattern in which managers with greater experience and higher educational attainment tend to be assigned to higher-classified stores. This finding suggests a positive correlation between manager characteristics and store classification, indicating that the LCBO may allocate more experienced and highly educated managers to stores of higher importance or larger scale. In Appendix A.2, we provide more detailed information on the positive relationship between manager characteristics and the classification of stores within the LCBO retail chain.\n\n "], "nougat": ["To investigate the possibility of stockouts at the warehouse level and their potential impact on store-level stockouts, we also analyze aggregate daily deliveries from the warehouse to all 634 stores. Given that the products in our working sample are popular items, we interpret a zero value in aggregate daily deliveries as a stockout event at the warehouse. The results of our analysis, presented in Section A.4 in the Appendix, reveal that warehouse stockouts are negligible for our working sample. For each product within the working sample, warehouse stockout events occur on no more than 3 out of the 677 days in the sample, which accounts for less than 0.5% of the observed period. These findings suggest that stockouts observed at the store level are primarily driven by factors other than warehouse-level stockouts.\n\n### Data on store managers\u2019 education and experience\n\nIn addition to the main dataset, we enhance our analysis by incorporating information on store managers\u2019 human capital. Leveraging the professional social networking platform _LinkedIn_, we gather data on the education and experience of store managers from their public profiles. Out of the 634 store managers in our dataset, 600 are identifiable by name in the LCBO\u2019s records. 7 Within this subset, we were able to locate public _LinkedIn_ profiles for 143 managers, allowing us to retrieve valuable information about their educational background and work experience.\n\nFootnote 7: During our sample period, some stores are overseen by interim managers who are not identified by name in our main dataset.\n\nTable 4 presents summary statistics for the variables related to store managers' educational background and work experience. Notably, we observe a pattern in which managers with greater experience and higher educational attainment tend to be assigned to higher-classified stores. This finding suggests a positive correlation between manager characteristics and store classification, indicating that the LCBO may allocate more experienced and highly educated managers to stores of higher importance or larger scale. In Appendix A.2, we provide more detailed information on the positive relationship between manager characteristics and the classification of stores within the LCBO retail chain.\n\n "]}, {"edit": ["is in everyone's best interest to comply to it. Yet bottom-up processes can also lead to norm change. In particular, the so-called critical-mass theory states that when a minority of the population formed by individuals committed to overturn the existing norm reaches a critical group size--commonly referred to as a \"critical mass\"--the social system crosses a tipping point [34, 35, 36]. Once the tipping point is reached, the actions of a minority group trigger a cascade of behaviour change that rapidly increases the acceptance of a minority view. The range of cases in which critical mass theory may work is somehow fuzzy, and the framework has been used to account for changes concerning spontaneous conventions as well as formal regulations. Experimental results, obtained in the framework of the Naming Game model [32, 37], suggest that a critical mass of 25% of the population [38], while modelling indicated a lower threshold [36]. In the case of AI, if the current trend of (informal, so far) regulation continues, one may expect spontaneous attempts to promote new norms. The case of students demanding better regulation for the use of AI in higher education seems an easy first guess. Analogously, non-native English speakers could try and overturn regulations that prevent them from using AI as a tool to level the playing field.\n\n### Spontaneously emerging norms on AI, as seen by ChatGPT\n\nWithin this framework, gathering data on the spontaneous processes currently shaping the norms around AI is difficult. But there's a privileged point of view to observe how our behaviour has evolved in the first months of public LLM chats, namely the one of ChatGPT. Therefore, I report here two extracts of a conversation I had with the freely accessible chatbot on May 22, 2023. If this article can only hope to highlight potential scenarios for the ongoing yet still-in-its-infancy process of norm formation, the text below may perhaps acquire more interest for the future reader, as a testimony of what the machine was 'thinking' in these early days. It is evident that some of the points exposed by ChatGPT correspond to the desiderata of the programmers. Yet it is also possible to accept that some of the answers come from a genuine elaboration of the AI experience.\n\nAfter the the LLM clarified that its ('its'? what is the norm here?) definition of norm corresponds the the one used throughout this piece, i.e. that a norm \"is a widely accepted standard or expectation of behavior in a society or group\", it replied to my question on what emerging norms it has noted during the first months of its activity as reported below.\n\nWhile social norms may vary depending on cultural and individual factors, here are some general trends that can be observed:\n\n1. Politeness and Respect: Many people tend to interact with LLMs in a polite and respectful manner, treating them as intelligent entities. This includes using appropriate language, expressing gratitude, and avoiding derogatory or offensive remarks.\n2. Acknowledging Limitations: Social norms often involve recognizing that LLMs have limitations. Users understand that LLMs provide responses based on patterns and data but may not always possess contextual understanding or have access to real-time information. It is important for users to critically evaluate and verify information obtained from LLMs.\n3. Contextual Understanding: Users have started to recognize that LLMs may not fully grasp the nuances and complexities of certain topics or cultural contexts. Consequently, users are encouraged to provide additional context or clarification to ensure accurate and relevant responses.\n4. Responsible Use: As LLMs become more prevalent, there is a growing awareness of the importance of responsible use. This includes not relying solely on LLMs for critical decisions, being cautious about potential biases in the training data, and understanding the ethical implications of using LLMs.\n5. Human-Machine Collaboration: Social norms are evolving to promote collaboration between humans and LLMs. Users understand that LLMs can be valuable tools for information retrieval, idea generation, and learning. The norm is to view LLMs as partners rather than replacements, leveraging their strengths while recognizing the irreplaceable role of human creativity, empathy, and critical thinking.\n6. Privacy and Data Protection: Norms around privacy and data protection apply to interactions with LLMs as well. Users expect that their personal information and interactions "], "nougat": ["is in everyone's best interest to comply to it. Yet bottom-up processes can also lead to norm change. In particular, the so-called critical-mass theory states that when a minority of the population formed by individuals committed to overturn the existing norm reaches a critical group size\u2014commonly referred to as a \u201ccritical mass\u201d\u2014the social system crosses a tipping point [ 34 \u2013 36 ]. Once the tipping point is reached, the actions of a minority group trigger a cascade of behaviour change that rapidly increases the acceptance of a minority view. The range of cases in which critical mass theory may work is somehow fuzzy, and the framework has been used to account for changes concerning spontaneous conventions as well as formal regulations. Experimental results, obtained in the framework of the Naming Game model [ 32 , 37 ], suggest that a critical mass of 25% of the population [ 38 ], while modelling indicated a lower threshold [ 36 ]. In the case of AI, if the current trend of (informal, so far) regulation continues, one may expect spontaneous attempts to promote new norms. The case of students demanding better regulation for the use of AI in higher education seems an easy first guess. Analogously, non-native English speakers could try and overturn regulations that prevent them from using AI as a tool to level the playing field.\n\n### Spontaneously emerging norms on AI, as seen by ChatGPT\n\nWithin this framework, gathering data on the spontaneous processes currently shaping the norms around AI is difficult. But there\u2019s a privileged point of view to observe how our behaviour has evolved in the first months of public LLM chats, namely the one of ChatGPT. Therefore, I report here two extracts of a conversation I had with the freely accessible chatbot on May 22, 2023. If this article can only hope to highlight potential scenarios for the ongoing yet still-in-its-infancy process of norm formation, the text below may perhaps acquire more interest for the future reader, as a testimony of what the machine was \u2018thinking\u2019 in these early days. It is evident that some of the points exposed by ChatGPT correspond to the desiderata of the programmers. Yet it is also possible to accept that some of the answers come from a genuine elaboration of the AI experience.\n\nAfter the the LLM clarified that its (\u2018its\u2019? what is the norm here?) definition of norm corresponds the the one used throughout this piece, i.e. that a norm \u201cis a widely accepted standard or expectation of behavior in a society or group\u201d, it replied to my question on what emerging norms it has noted during the first months of its activity as reported below.\n\nWhile social norms may vary depending on cultural and individual factors, here are some general trends that can be observed:\n\n1. Politeness and Respect: Many people tend to interact with LLMs in a polite and respectful manner, treating them as intelligent entities. This includes using appropriate language, expressing gratitude, and avoiding derogatory or offensive remarks.\n2. Acknowledging Limitations: Social norms often involve recognizing that LLMs have limitations. Users understand that LLMs provide responses based on patterns and data but may not always possess contextual understanding or have access to real-time information. It is important for users to critically evaluate and verify information obtained from LLMs. 3. Contextual Understanding: Users have started to recognize that LLMs may not fully grasp the nuances and complexities of certain topics or cultural contexts. Consequently, users are encouraged to provide additional context or clarification to ensure accurate and relevant responses. 4. Responsible Use: As LLMs become more prevalent, there is a growing awareness of the importance of responsible use. This includes not relying solely on LLMs for critical decisions, being cautious about potential biases in the training data, and understanding the ethical implications of using LLMs. 5. Human-Machine Collaboration: Social norms are evolving to promote collaboration between humans and LLMs. Users understand that LLMs can be valuable tools for information retrieval, idea generation, and learning. The norm is to view LLMs as partners rather than replacements, leveraging their strengths while recognizing the irreplaceable role of human creativity, empathy, and critical thinking. 6. Privacy and Data Protection: Norms around privacy and data protection apply to interactions with LLMs as well. Users expect that their personal information and interactions "]}, {"edit": ["\n\n## References\n\n* Abdulkadiroglu et al. (2020) Abdulkadiroglu, A., Che, Y.-K. Pathak, P.A., Roth, A. E., Tercieux, O. 2020. Efficiency, justified envy, and incentives in priority-based matching. American Economic Review Insights 2, 425-442.\n* Abdulkadiroglu and Grigoryan (2022) Abdulkadiroglu, A., Grigoryan, A., 2022. Priority-Based Assignment with Reserves and Quotas, Mimeo Available at [https://www.nber.org/papers/w28689](https://www.nber.org/papers/w28689)\n* Abdulkadiroglu et al. (2005) Abdulkadiroglu, A., Pathak, P.A., Roth, A.E., Sonmez, T. 2005. \"The Boston Public School Match.\" American Economic Review 95(2), 368-371.\n* Abdulkadiroglu et al. (2006) Abdulkadiroglu, A., Pathak, P.A., Roth, A.E., Sonmez, T. 2006. \"Changing the Boston Public School Mechanism: Strategy-proofness as equal access,\" NBER Working paper, 11965.\n* Abdulkadiroglu and Sonmez (2003) Abdulkadiroglu A., Sonmez, T. 2003. School choice: A mechanism design approach. American Economic Review 93(3), 729-747.\n* Afacan and Salman (2016) Afacan, M.O., Salman, U. 2016. Affirmative actions: The Boston mechanism case. Economics Letters 141, 95-97\n* Alcalde and Romero-Medina (2017) Alcalde, J., Romero-Medina, A. Fair student placement. Theory and Decision 83, 293-307 (2017).\n* Balinski and Sonmez (1999) Balinski, M., Sonmez, T. 1999. A tale of two mechanisms: student placement, Journal of Economic Theory 84(1), 73-94.\n* Bossert and Suzumura (2010) Bossert, W., Suzumura, K. (2010) Consistency, Choice, and Rationality, Harvard University Press.\n* Che et al. (2019a) Che, Y-K., Kim, J., Kojima, F. 2019a. Stable matching in large economies. Econometrica, 65-110.\n* Che et al. (2019b) Che, Y-K., Kim, J., Kojima, F. 2019b. Weak monotone comparative statics, Mimeo Available at arXiv:1911.06442\n* Dogan (2016) Dogan B. 2016. Responsive affirmative action in school choice. Journal of Economic Theory 165, 69-105\n* Dogan and Ehlers (2021) Dogan B., Ehlers L. 2021. Minimally unstable Pareto improvements over deferred acceptance. Theoretical Economics 16, 1249-1279\n* Dogan and Ehlers (2016)"], "nougat": ["\n\n## References\n\n* Abdulkadiro \u0306glu, A., Che, Y.-K. Pathak, P.A., Roth, A. E., Tercieux, O. 2020. Efficiency, justified envy, and incentives in priority-based matching. American Economic Review Insights 2, 425\u2013442. Abdulkadiro \u0306glu, A., Grigoryan, A., 2022. Priority-Based Assignment with Reserves and Quotas, Mimeo Available at [https://www.nber.org/papers/w28689](https://www.nber.org/papers/w28689) Abdulkadiro \u0306glu, A., Pathak, P.A., Roth, A.E., S \u0308onmez, T. 2005. \u201cThe Boston Public School Match.\u201d American Economic Review 95(2), 368371. Abdulkadiro \u0306glu, A., Pathak, P.A., Roth, A.E., S \u0308onmez, T. 2006. \u201cChanging the Boston Public School Mechanism: Strategy-proofness as equal access,\u201d NBER Working paper, 11965. Abdulkadiro \u0306glu A., S \u0308onmez, T. 2003. School choice: A mechanism design approach. American Economic Review 93(3), 729\u2013747. Afacan, M.O., Salman, U. 2016. Affirmative actions: The Boston mechanism case. Economics Letters 141, 95\u201397 Alcalde, J., Romero-Medina, A. Fair student placement. Theory and Decision 83, 293\u2013307 (2017). Balinski, M., S \u0308onmez, T. 1999. A tale of two mechanisms: student placement, Journal of Economic Theory 84(1), 73\u201394. Bossert, W., Suzumura, K. (2010) Consistency, Choice, and Rationality, Harvard University Press. Che, Y-K., Kim, J., Kojima, F. 2019a. Stable matching in large economies. Econometrica, 65-110. Che, Y-K., Kim, J., Kojima, F. 2019b. Weak monotone comparative statics, Mimeo Available at arXiv:1911.06442 Do \u0306gan B. 2016. Responsive affirmative action in school choice. Journal of Economic Theory 165, 69\u2013105 Do \u0306gan B., Ehlers L. 2021. Minimally unstable Pareto improvements over deferred acceptance. Theoretical Economics 16, 1249\u20131279"]}, {"edit": ["the final assignment. The prescribed collusive ranking model in this article demonstrates that if the platform acts as an agent and redefines a new assignment, the final assignment will be changed to a collusive or joint profit-maximizing assignment based on the criteria. It demonstrates that if any of the following criteria are proposed, namely _products related to this item_, _more to consider from made for Amazon,compare with similar items,customers who viewed this item also viewed_, etc... then the allocation will be collusive. By minimizing the customer's mismatch problem, the next level ranking or shortlisting vendors is to be done out of the first level shortlisting vendors. This mismatch can be reduced by updating online reviews with a Bayesian approach. Because online reviews are nothing more than sequential voting, they contain a history of customer preferences. As a result, relying solely on the current review comments is insufficient to assess the object's quality. This information must also be updated to determine the quality of future services.\n\n### Model Setup: Problem Formulation\n\nImitation of third-party products and self-preferencing are important tools used to direct customers to the platform's products[11]. As a result, it should maintain equal access from both sides, namely the customer and the firms. On E-Commerce platforms, there are currently two types of seller ranking. The first is sponsored, while the second is not. Sellers who chose the sponsored ranking mechanism must participate in the auction bidding process to win the slot. The remainder is managed by manipulating keywords, review ratings, review comments, and so on. Furthermore, platforms have some hidden mechanisms in place to push their products. The current article proposes a general model for retrieving the query so that every seller can benefit concerning their strength, particularly those who do not participate in the sponsored ranking mechanism. The proposed model is the development of an algorithm that solves the preceding two problems so that the search algorithm is not collusive. Otherwise, the search algorithm will be collusive, and the platform will have the power to control the market in favor of certain interest groups.\n\n#### 3.2.1 Analysis: Assortment Planning and the Revenue Maximization Problem\n\nWe consider a marketplace (platform) where a set of \\(K\\) substitutable goods or services offered by the \\(K\\) individual vendors, henceforth, called the _products_-are offered to a market of customers who decide whether to buy one of them or to choose a no-purchase option. Let \\([k]\\triangleq\\{1,...,k\\}\\) denote the universal set of products. The online platform (he) chooses an assortment \\(S\\subset[k]\\) to display and rank them in order within \\(M\\) slots. This has to be done in tow stage process; \\(S_{1}\\&S_{2}(=S)\\). First \\(S_{1}\\subset[k]\\) thereafter \\(S_{2}(=S)\\subseteq S_{1}\\). The platform sells this single product over a horizon of length \\(T\\) and each vendor is holding more than one product. It will be convenient to think of this planning horizon in terms of "], "nougat": ["the final assignment. The prescribed collusive ranking model in this article demonstrates that if the platform acts as an agent and redefines a new assignment, the final assignment will be changed to a collusive or joint profit-maximizing assignment based on the criteria. It demonstrates that if any of the following criteria are proposed, namely _products related to this item_, _more to consider from made for Amazon,compare with similar items,customers who viewed this item also viewed_, etc... then the allocation will be collusive. By minimizing the customer\u2019s mismatch problem, the next level ranking or shortlisting vendors is to be done out of the first level shortlisting vendors. This mismatch can be reduced by updating online reviews with a Bayesian approach. Because online reviews are nothing more than sequential voting, they contain a history of customer preferences. As a result, relying solely on the current review comments is insufficient to assess the object\u2019s quality. This information must also be updated to determine the quality of future services.\n\n### Model Setup: Problem Formulation\n\nImitation of third-party products and self-preferencing are important tools used to direct customers to the platform\u2019s products[Hagiu et al., 2022]. As a result, it should maintain equal access from both sides, namely the customer and the firms. On E-Commerce platforms, there are currently two types of seller ranking. The first is sponsored, while the second is not. Sellers who chose the sponsored ranking mechanism must participate in the auction bidding process to win the slot. The remainder is managed by manipulating keywords, review ratings, review comments, and so on. Furthermore, platforms have some hidden mechanisms in place to push their products. The current article proposes a general model for retrieving the query so that every seller can benefit concerning their strength, particularly those who do not participate in the sponsored ranking mechanism. The proposed model is the development of an algorithm that solves the preceding two problems so that the search algorithm is not collusive. Otherwise, the search algorithm will be collusive, and the platform will have the power to control the market in favor of certain interest groups.\n\n#### 3.2.1 Analysis: Assortment Planning and the Revenue Maximization Problem\n\nWe consider a marketplace (platform) where a set of \\(K\\) substitutable goods or services offered by the \\(K\\) individual vendors, henceforth, called the _products_-are offered to a market of customers who decide whether to buy one of them or to choose a no-purchase option. Let \\([k]\\triangleq\\{1,...,k\\}\\) denote the universal set of products. The online platform (he) chooses an assortment \\(S\\subset[k]\\) to display and rank them in order within \\(M\\) slots. This has to be done in tow stage process; \\(S_{1}\\&S_{2}(=S)\\). First \\(S_{1}\\subset[k]\\) thereafter \\(S_{2}(=S)\\subseteq S_{1}\\). The platform sells this single product over a horizon of length \\(T\\) and each vendor is holding more than one product. It will be convenient to think of this planning horizon in terms of "]}, {"edit": ["ongoing efforts are being made to enhance this practice. A notable example of these initiatives is led by Doctors Without Borders, also known as Medecins Sans Frontieres (Barel et al., 2020).\n\n### Results-based payments without screening\n\nIn this subsection, the principal is restricted to using payments that can only depend on the realized posterior. Following the terminology in Yoder (2022), we define a _results-based contract_ as a contingent payment function \\(t:\\Delta\\left(\\Omega\\right)\\rightarrow\\mathbb{R}\\). We say that \\(\\left(\\mathcal{X},t\\right)\\) is incentive-compatible if\n\n\\[\\mathbb{E}_{p\\sim\\mathcal{X}(\\theta)}[t(p)]-\\theta\\mathcal{C}(\\mathcal{X}( \\theta))\\geq\\mathbb{E}_{p\\sim\\tau}[t(p)]-\\theta\\mathcal{C}(\\tau)\\quad\\forall \\theta\\Theta,\\forall\\tau X. \\tag{6}\\]\n\nBecause the principal cannot price discriminate according to different reports, a results-based contract must provide incentives for all researcher types.\n\nWe aim to investigate whether \\(\\left(\\mathcal{X}^{*},T^{*}\\right)\\) can be implemented using a results-based contract. In the case of two states, Yoder (2022) demonstrates that the optimal choice function \\(\\mathcal{X}^{*}\\) must be Blackwell-monotone. In such a case, a binary, Blackwell-monotone choice function is implementable by a results-based contract that induces the payment rule \\(T^{*}\\). However, as shown in Example 1, this may not be the case when there are more than two states, as the optimal choice function need not be Blackwell-monotone. Thus, it is important to understand how Blackwell monotonicity relates to implementability in a broader context.\n\nWe will introduce an additional assumption on \\(\\mathcal{X}^{*}\\) going forward to simplify the analysis. This assumption requires that every experiment has the same number of posteriors as \\(\\left|\\Omega\\right|\\). While this assumption is restrictive, it allows for a clear characterization of implementable experiment choice functions.3 "], "nougat": ["ongoing efforts are being made to enhance this practice. A notable example of these initiatives is led by Doctors Without Borders, also known as M \u0301edecins Sans Fronti`eres (Barel et al., 2020).\n\n### Results-based payments without screening\n\nIn this subsection, the principal is restricted to using payments that can only depend on the realized posterior. Following the terminology in Yoder (2022), we define a _results-based contract must provide incentives for all researcher types._\n\nWe aim to investigate whether \\((\\mathcal{X}^{*},T^{*})\\) can be implemented using a results-based contract. In the case of two states, Yoder (2022) demonstrates that the optimal choice function \\(\\mathcal{X}^{*}\\) must be Blackwell-monotone. In such a case, a binary, Blackwell-monotone choice function is implementable by a results-based contract that induces the payment rule \\(T^{*}\\). However, as shown in Example 1, this may not be the case when there are more than two states, as the optimal choice function need not be Blackwell-monotone. Thus, it is important to understand how Blackwell monotonicity relates to implementability in a broader context.\n\nWe will introduce an additional assumption on \\(\\mathcal{X}^{*}\\) going forward to simplify the analysis. This assumption requires that every experiment has the same number of posteriors as \\(|\\Omega|\\). While this assumption is restrictive, it allows for a clear characterization of implementable experiment choice functions. 3 "]}, {"edit": ["frequently in smaller stores than in larger ones within our working sample. The average \\(D\\) store experiences 0.56 stockout events per week, while the average _AAA_ store encounters 0.14 stockout events per week. Delivery patterns in our working sample follow a similar trend to Table 1, with larger stores placing larger and more frequent orders. The average _AAA_ store receives deliveries 4 days per week, totaling 272 units per week, whereas the average \\(D\\) store only receives deliveries 0.6 days per week, amounting to 15 units per week.\n\nFigure 1 shows strong heterogeneity across stores in several measures related to inventory management of the five products in our working sample. The figures in panels (a) to (f) are inverse cumulative distributions over stores, together with their 95% confidence bands.6\n\nFootnote 6: For every store, the 95% confidence interval is based on the construction of store-product-specific rates. The 95% confidence interval is determined by percentiles 2.5% and 97.5% in this distribution.\n\n\\begin{table}\n\\begin{tabular}{c|c c c c c c c} \\hline \\hline  & \\multicolumn{7}{c}{_Type of store_} \\\\  & _All_ & _AAA_ & _AA_ & \\(A\\) & \\(B\\) & \\(C\\) & \\(D\\) \\\\ \\hline  & _Mean_ & _Mean_ & _Mean_ & _Mean_ & _Mean_ & _Mean_ & _Mean_ \\\\  & _(st.dev)_ & _(st.dev)_ & _(st.dev)_ & _(st.dev)_ & _(st.dev)_ & _(st.dev)_ & _(st.dev)_ \\\\ \\hline\n**Number of Observations** & & & & & & & \\\\ _Number of Stores_ & 634 & 5 & 25 & 148 & 157 & 164 & 135 \\\\ _Number of Unique Products_ & 5 & 5 & 5 & 5 & 5 & 5 & 5 \\\\ _Number of Days_ & 677 & 676 & 676 & 676 & 677 & 676 & 675 \\\\ \\hline\n**Sales \\& Stockouts Per Store** & & & & & & & \\\\ _Revenue per week (\\$)_ & 1,687 & 5,197 & 4,511 & 3,192 & 1,801 & 821 & 302 \\\\  & (1,402) & (1,487) & (1,415) & (835) & (660) & (443) & (200) \\\\ _Units sold per week_ & 91 & 274 & 252 & 173 & 98 & 43 & 15 \\\\  & (75) & (51) & (65) & (39) & (33) & (22) & (10) \\\\ _Stockouts events per week_ & 0.37 & 0.14 & 0.13 & 0.21 & 0.28 & 0.49 & 0.56 \\\\  & (0.39) & (0.07) & (0.11) & (0.15) & (0.23) & (0.35) & (0.60) \\\\ \\hline\n**Inventories Per Store** & "], "nougat": ["frequently in smaller stores than in larger ones within our working sample. The average \\(D\\) store experiences 0.56 stockout events per week, while the average _AAA_ store encounters 0.14 stockout events per week. Delivery patterns in our working sample follow a similar trend to Table 1 , with larger stores placing larger and more frequent orders. The average _AAA_ store receives deliveries 4 days per week, totaling 272 units per week, whereas the average \\(D\\) store only receives deliveries 0.6 days per week, amounting to 15 units per week.\n\nFigure 1 shows strong heterogeneity across stores in several measures related to inventory management of the five products in our working sample. The figures in panels (a) to (f) are inverse cumulative distributions over stores, together with their 95% confidence bands.6\n\nFootnote 6: For every store, the 95% confidence interval is based on the construction of store-product-specific rates. The 95% confidence interval is determined by percentiles 2.5% and 97.5% in this distribution.\n\n\\begin{table}\n\\begin{tabular}{c|c c c c c c c} \\hline \\hline  & \\multicolumn{7}{c}{_Type of store_} \\\\  & _All_ & _AAA_ & _AA_ & \\(A\\) & \\(B\\) & \\(C\\) & \\(D\\) \\\\ \\hline  & _Mean_ & _Mean_ & _Mean_ & _Mean_ & _Mean_ & _Mean_ \\\\  & _(st.dev)_ & _(st.dev)_ & _(st.dev)_ & _(st.dev)_ & _(st.dev)_ & _(st.dev)_ & _(st.dev)_ \\\\ \\hline\n**Number of Observations** & & & & & & & \\\\ _Number of Stores_ & 634 & 5 & 25 & 148 & 157 & 164 & 135 \\\\ _Number of Unique Products_ & 5 & 5 & 5 & 5 & 5 & 5 & 5 \\\\ _Number of Days_ & 677 & 676 & 676 & 676 & 677 & 676 & 675 \\\\ \\hline\n**Sales \\& Stockouts Per Store** & & & & & & & \\\\ _Revenue per week (\\$)_ & 1,687 & 5,197 & 4,511 & 3,192 & 1,801 & 821 & 302 \\\\  & (1,402) & (1,487) & (1,415) & (835) & (660) & (443) & (200) \\\\ _Units sold per week_ & 91 & 274 & 252 & 173 & 98 & 43 & 15 \\\\  & (75) & (51) & (65) & (39) & (33) & (22) & (10) \\\\ _Stockouts events per week_ & 0.37 & 0.14 & 0.13 & 0.21 & 0.28 & 0.49 & 0.56 \\\\  & (0.39) & (0.07) & (0.11) & (0.15) & (0.23) & (0.35) & (0.60) \\\\ \\hline\n**Inventories Per Store** & & & "]}, {"edit": ["impose the UMAS assumption. Reassuringly, estimates show that admission to the case-study program, Business Administration at PUC Santiago, increases students' probability to graduate from that program. The effect is particularly strong for those whose next-preferred alternative is Economics at UChile or Social Sciences and Humanities at PUC Santiago.\n\nAdmission to our case-study program also strongly decreases students' probability to reapply through the centralized system in subsequent years for students whose local next-preferred option is Science and Engineering at UChile, Economics at UChile, or Social Sciences and Humanities at PUC Santiago. Interestingly, for students with the latter two next-best alternatives, it also decreases the probability to graduate from any program. For students whose next-best alternative is Science & Engineering at UChile, the naive estimate indicates a small negative effect on graduation from any program while our bounds show that this effect cannot be signed and may be 50 times larger in magnitude. Bounds do suggest that admission to Business Administration at PUC Santiago has a negative effect on reapplication for these students. While we generally see magnitudes of 0.1 or 0.2 in the effects on graduation from Business Administration or any program, we see much smaller magnitudes on the students' probability to graduate from a STEM program --which is reassuring given that our definition of STEM does not include Business Administration. Finally, we find a strong positive effect on the probability to graduate from a top institution for those whose local next-best alternative is Law or Social Sciences and Humanities at PUC Santiago.\n\n## 6 Conclusion\n\nCentralized mechanisms for the assignment of students to educational programs are growing in popularity across the world. These systems provide a valuable source of exogenous variation through discontinuities around admission cutoffs. This along with individual-level data on admissions, preferences, and future outcomes allow researchers to identify a wide range of causal effects of education on outcomes. The variation is useful for identification as long as we control for students' true preferences; however, true preferences generally differ from what students report to the mechanism given that most implementations of mechanisms generate incentives for students to behave strategically.\n\nThis paper provides a novel approach to partially identify effects from mechanism assignment on future outcomes that is robust to strategic behavior. We illustrate our approach using data from a deferred-acceptance mechanism that assigns roughly 70,000 students to more than 1,000 university-major programs every year in Chile. First, we find substantial evidence of strategic behavior which confirms earlier findings from the literature on Chile. Second, we compute bounds on the average effect of program assignment on graduation outcomes. We do so for students whose scores are marginal to the cutoff of admission into one of the most popular and selective programs in the system: business administration at the Catholic University of Santiago. Although admission into this program increases the likelihood of graduating from that same program, its effect on the likelihood of ever graduating from university varies depending on what students prefer as their "], "nougat": ["impose the UMAS assumption. Reassuringly, estimates show that admission to the case-study program, Business Administration at PUC Santiago, increases students\u2019 probability to graduate from that program. The effect is particularly strong for those whose next-preferred alternative is Economics at UChile or Social Sciences and Humanities at PUC Santiago.\n\nAdmission to our case-study program also strongly decreases students\u2019 probability to reapply through the centralized system in subsequent years for students whose local next-preferred option is Science and Engineering at UChile, Economics at UChile, or Social Sciences and Humanities at PUC Santiago. Interestingly, for students with the latter two next-best alternatives, it also decreases the probability to graduate from any program. For students whose next-best alternative is Science & Engineering at UChile, the naive estimate indicates a small negative effect on graduation from any program while our bounds show that this effect cannot be signed and may be 50 times larger in magnitude. Bounds do suggest that admission to Business Administration at PUC Santiago has a negative effect on reapplication for these students. While we generally see magnitudes of 0.1 or 0.2 in the effects on graduation from Business Administration or any program, we see much smaller magnitudes on the students\u2019 probability to graduate from a STEM program \u2014which is reassuring given that our definition of STEM does not include Business Administration. Finally, we find a strong positive effect on the probability to graduate from a top institution for those whose local next-best alternative is Law or Social Sciences and Humanities at PUC Santiago.\n\n## 6 Conclusion\n\nCentralized mechanisms for the assignment of students to educational programs are growing in popularity across the world. These systems provide a valuable source of exogenous variation through discontinuities around admission cutoffs. This along with individual-level data on admissions, preferences, and future outcomes allow researchers to identify a wide range of causal effects of education on outcomes. The variation is useful for identification as long as we control for students\u2019 true preferences; however, true preferences generally differ from what students report to the mechanism given that most implementations of mechanisms generate incentives for students to behave strategically.\n\nThis paper provides a novel approach to partially identify effects from mechanism assignment on future outcomes that is robust to strategic behavior. We illustrate our approach using data from a deferred-acceptance mechanism that assigns roughly 70,000 students to more than 1,000 university-major programs every year in Chile. First, we find substantial evidence of strategic behavior which confirms earlier findings from the literature on Chile. Second, we compute bounds on the average effect of program assignment on graduation outcomes. We do so for students whose scores are marginal to the cutoff of admission into one of the most popular and selective programs in the system: business administration at the Catholic University of Santiago. Although admission into this program increases the likelihood of graduating from that same program, its effect on the likelihood of ever graduating from university varies depending on what students prefer as their "]}, {"edit": ["to prove Bob wrong is obtaining an early success, Ann is willing to work harder on her idea in the first period than she would do if Bob was like-minded. Clearly, the same holds for Bob. From conditions 2 and ref 3, one can easily see that \\(e^{A}=e^{B}=e^{\\mathcal{H}}(1+\\beta e^{\\mathcal{H}})>e^{\\mathcal{H}}\\).15 Each player is eager to prove the other wrong, hoping that they will join forces to pursue the most promising type of innovation in the future: disagreement makes the group work harder in the first period, pushing Ann and Bob's average output above the level of Sam and Tom.16\n\nFootnote 15: The reader might have noticed that the optimal level of effort is the same as the one obtained when analyzing the single-technology case. In general, this will not always be the case.\n\nFootnote 16: In practice this conclusion does not require us (the analyst) to be convinced that Tom holds the correct views: it is sufficient that both approaches work equally well, or that we believe that Ann and Bob are correct each with probability \\(\\frac{1}{2}\\), as we discuss in section 5.\n\nFinally note that, in the last illustrative example, we also expect both Ann and Bob to earn higher payoffs, throughout the game, than Tom or Sam. The reason is that disagreement pushes players' first-period effort closer to \\(e^{\\mathcal{H}}(1+\\beta)\\) the level of individual effort that is efficient taking externalities into account. Hence, in this specific example, not only does disagreement increase expected output, it also lead to a Pareto improvement in terms of expected payoffs.\n\nOur formal analysis, presented in sections 3-5 of this paper, generalizes, enriches and qualifies the intuitions presented in the previous toy examples. We extend the analysis to a wider class of utility functions, and consider any views \\(\\mathcal{H}\\) and \\(\\mathcal{L}\\), providing an intuitive condition that ensures that disagreement will drive up the effort of a player holding view \\(\\mathcal{H}\\) - as in the case of the views presented in table 1. We consider a more general model-switching rule, which allows for changes in mind after non-zero probability events. We also discuss the main assumption of our set-up and possible variations.\n\nThe remainder of the paper is organized as follows. In Section 2 we review the literature on model change and disagreement. In section 3 we outline the general model. In section 4 we present the results for the single production technology case, while in section 5 we discuss the multiple technology case. In section 6 we discuss the robustness of our findings and changes of our key assumptions. Finally, in section 7 we draw the conclusions.\n\n "], "nougat": ["to prove Bob wrong is obtaining an early success, Ann is willing to work harder on her idea in the first period than she would do if Bob was like-minded. Clearly, the same holds for Bob. From conditions 2 and ref 3, one can easily see that The reader might have noticed that the optimal level of effort is the same as the one obtained when analyzing the single-technology case. In general, this will not always be the case.\n\nEach player is eager to prove the other wrong, hoping that they will join forces to pursue the most promising type of innovation in the future: disagreement makes the group work harder in the first period, pushing Ann and Bob\u2019s average output above the level of Sam and Tom.16\n\nFootnote 16: In practice this conclusion does not require us (the analyst) to be convinced that Tom holds the correct views: it is sufficient that both approaches work equally well, or that we believe that Ann and Bob are correct each with probability \\(\\frac{1}{2}\\), as we discuss in section 5.\n\nFinally note that, in the last illustrative example, we also expect both Ann and Bob to earn higher payoffs, throughout the game, than Tom or Sam. The reason is that disagreement pushes players' first-period effort closer to \\(e^{\\mathcal{H}}(1+\\beta)\\) the level of individual effort that is efficient taking externalities into account. Hence, in this specific example, not only does disagreement increase expected output, it also lead to a Pareto improvement in terms of expected payoffs.\n\nOur formal analysis, presented in sections 3-5 of this paper, generalizes, enriches and qualifies the intuitions presented in the previous toy examples. We extend the analysis to a wider class of utility functions, and consider any views \\(\\mathcal{H}\\) and \\(\\mathcal{L}\\), providing an intuitive condition that ensures that disagreement will drive up the effort of a player holding view \\(\\mathcal{H}\\) - as in the case of the views presented in table 1. We consider a more general model-switching rule, which allows for changes in mind after non-zero probability events. We also discuss the main assumption of our set-up and possible variations.\n\nThe remainder of the paper is organized as follows. In Section 2 we review the literature on model change and disagreement. In section 3 we outline the general model. In section 4 we present the results for the single production technology case, while in section 5.1 we discuss the multiple technology case. In section 6 we discuss the robustness of our findings and changes of our key assumptions. Finally, in section 7 we draw the conclusions.\n\n "]}, {"edit": ["data transfer rates or large data volumes. Finally, vendor pricing models can be complex and difficult to understand, making it challenging for organizations to accurately predict and manage their cloud costs.\n\nCost optimization has become an increasingly important concern for organizations of all sizes as cloud computing adoption continues to grow. According to a Flexera report [4], the top cloud initiative for businesses is cost optimization. Furthermore, Gartner predicts that \"organizations that do not have a cost optimization program in place will overspend by up to 70%\" through 2024 [5]. The consequences of failing to manage cloud costs are obvious: wasted resources, increased expenses, and decreased overall profitability. Understanding and implementing effective cloud cost optimization strategies is therefore critical for businesses to remain competitive in the modern cloud computing landscape.\n\nThis paper provides an in-depth examination of cloud cost optimization strategies and techniques. Section 2 discusses the fundamental concepts of cloud pricing models, Section 3 then delves into some of the most popular and effective cloud cost-cutting techniques, such as resource allocation, instance resizing, and auto-scaling, emphasizing the significance of this issue for businesses. Section 4 also includes case studies of organizations that have successfully implemented cloud cost optimization strategies, highlighting the specific approaches they used to save money. Section 5 showcases the future research considerations when it comes to optimizing the cost of running scalable and highly available services in the cloud. Finally, section 6 wraps up the paper with a discussion of future research directions in cloud cost optimization.\n\n## 2 Understanding Cloud Pricing Models\n\nCloud pricing models are critical components of cloud computing because they govern how cloud services are billed and costs are calculated. Therefore, understanding cloud pricing models is critical for organizations looking to optimize their cloud spending and ensure they are only paying for the resources they require. This section examines the various pricing models employed by cloud providers.\n\n### On-demand Pricing\n\nOn-demand pricing is the most flexible pricing model offered by cloud providers. It enables businesses to pay for resources hourly or per second, with no upfront costs or long-term commitments. This model is best suited for workloads with erratic traffic patterns or short-term projects requiring resources for a limited time. On-demand instance pricing is determined by the size of the instance, the operating system, the region where the instance is launched, and the duration of use. Larger instances and instances in high-demand regions are typically more expensive. One advantage of on-demand pricing is that it gives businesses instant access to resources that can be scaled up or down as needed. This means "], "nougat": ["data transfer rates or large data volumes. Finally, vendor pricing models can be complex and difficult to understand, making it challenging for organizations to accurately predict and manage their cloud costs.\n\nCost optimization has become an increasingly important concern for organizations of all sizes as cloud computing adoption continues to grow. According to a Flexera report [4], the top cloud initiative for businesses is cost optimization. Furthermore, Gartner predicts that \"organizations that do not have a cost optimization program in place will overspend by up to 70%\" through 2024 [5]. The consequences of failing to manage cloud costs are obvious: wasted resources, increased expenses, and decreased overall profitability. Understanding and implementing effective cloud cost optimization strategies is therefore critical for businesses to remain competitive in the modern cloud computing landscape.\n\nThis paper provides an in-depth examination of cloud cost optimization strategies and techniques. Section 2 discusses the fundamental concepts of cloud pricing models, Section 3 then delves into some of the most popular and effective cloud cost-cutting techniques, such as resource allocation, instance resizing, and auto-scaling, emphasizing the significance of this issue for businesses. Section 4 also includes case studies of organizations that have successfully implemented cloud cost optimization strategies, highlighting the specific approaches they used to save money. Section 5 showcases the future research considerations when it comes to optimizing the cost of running scalable and highly available services in the cloud. Finally, section 6 wraps up the paper with a discussion of future research directions in cloud cost optimization.\n\n## 2 Understanding Cloud Pricing Models\n\nCloud pricing models are critical components of cloud computing because they govern how cloud services are billed and costs are calculated. Therefore, understanding cloud pricing models is critical for organizations looking to optimize their cloud spending and ensure they are only paying for the resources they require. This section examines the various pricing models employed by cloud providers.\n\nOn-demand pricing is the most flexible pricing model offered by cloud providers. It enables businesses to pay for resources hourly or per second, with no upfront costs or long-term commitments. This model is best suited for workloads with erratic traffic patterns or short-term projects requiring resources for a limited time. On-demand instance pricing is determined by the size of the instance, the operating system, the region where the instance is launched, and the duration of use. Larger instances and instances in high-demand regions are typically more expensive. One advantage of on-demand pricing is that it gives businesses instant access to resources that can be scaled up or down as needed. This means "]}, {"edit": ["\\(\\succ^{\\prime}\\) is the profile of priority orders that are adjusted by a priority-based affirmative action policy. Since the policy is aim to create more opportunities for minority students, \\(\\succ^{\\prime}\\) must be an improvement of \\(\\succ\\) for the minority students.\n\nWe provide an example to understand the definitions.\n\n#### Example 3\n\nLet \\(I=\\{i_{1},\\cdots,i_{6}\\}\\) and \\(S=\\{s\\}\\). Here, we assume that \\(i_{1},\\cdots,i_{4}\\) are minority students and \\(i_{5},i_{6}\\) are majority students. The score of the entrance exam for school \\(s\\in S\\) of student \\(i\\) is given by \\(\\sigma_{i}\\). Let\n\n\\[(\\sigma_{i_{1}},\\cdots,\\sigma_{i_{6}})=(615,654,635,664,643,701)\\,.\\]\n\nFirst, we let \\(\\succ_{s}\\) be the priority order that simply determined by the scores; that is,\n\n\\[\\succ^{1}_{s}:\\ i_{6}\\ i_{4}\\ i_{2}\\ i_{5}\\ i_{3}\\ i_{1}.\\]\n\nOn the other hand, we consider some bonus points for the minority students. Let \\(\\alpha_{i}\\) be the bonus point for student \\(i\\) and\n\n\\[\\alpha = (\\alpha_{i_{1}},\\cdots,\\alpha_{i_{6}})=(50,50,10,10,0,0)\\,,\\] \\[\\alpha^{\\prime} = \\left(\\alpha^{\\prime}_{i_{1}},\\cdots,\\alpha^{\\prime}_{i_{6}} \\right)=(50,50,30,30,0,0)\\,,\\] \\[\\alpha^{\\prime\\prime} = \\left(\\alpha^{\\prime\\prime}_{i_{1}},\\cdots,\\alpha^{\\prime\\prime}_ {i_{6}}\\right)=(60,60,10,10,0,0)\\,.\\]\n\nWith the bonus points profile \\(\\alpha,\\alpha^{\\prime}\\) and \\(\\alpha^{\\prime\\prime}\\), the priority order \\(\\succ^{2}_{s},\\succ^{3}_{s}\\) and \\(\\succ^{4}_{s}\\) are respectively such that\n\n\\[\\succ^{2}_{s}:\\ i_{2}\\ i_{6}\\ i_{4}\\ i_{1}\\ i_{5}\\ i_{3},\\] \\[\\succ^{3}_{s}:\\ i_{2}\\ i_{6}\\ i_{4}\\ i_{3}\\ i_{1}\\ i_{5},\\] \\[\\succ^{4}_{s}:\\ i_{2}\\ i_{6}\\ i_{1}\\ i_{4}\\ i_{5}\\ i_{3}.\\]\n\nThen, \\(\\succ^{2}\\), \\(\\succ^{3}\\) and \\(\\succ^{4}\\) are improvements of \\(\\succ^{1}\\) for \\(\\{i_{1},\\cdots,i_{4}\\}\\).\n\nIn this section, we fix the profile of original priority orders \\(\\succ\\) and the improved students \\(I^{\\prime}\\subseteq I\\), and let\n\n\\[\\rhd^{\\prime}_{s} = \\left\\{\\succ_{s},\\succ "], "nougat": ["\\(\\succ^{\\prime}\\) is the profile of priority orders that are adjusted by a priority-based affirmative action policy. Since the policy is aim to create more opportunities for minority students, \\(\\succ^{\\prime}\\) must be an improvement of \\(\\succ\\) for the minority students.\n\nWe provide an example to understand the definitions.\n\n#### Example 3\n\nLet \\(I=\\{i_{1},\\cdots,i_{6}\\}\\) and \\(S=\\{s\\}\\). Here, we assume that \\(i_{1},\\cdots,i_{4}\\) are minority students and \\(i_{5},i_{6}\\) are majority students. The score of the entrance exam for school \\(s\\in S\\) of student \\(i\\) is given by \\(\\sigma_{i}\\). Let\n\n\\[(\\sigma_{i_{1}},\\cdots,\\sigma_{i_{6}})=(615,654,635,664,643,701)\\,.\\]\n\nFirst, we let \\(\\succ_{s}\\) be the priority order that simply determined by the scores; that is,\n\n\\[\\succ^{1}_{s}:\\ i_{6}\\ i_{4}\\ i_{2}\\ i_{5}\\ i_{3}\\ i_{1}.\\]\n\nOn the other hand, we consider some bonus points for the minority students. Let \\(\\alpha_{i}\\) be the bonus point for student \\(i\\) and\n\n\\[\\alpha = (\\alpha_{i_{1}},\\cdots,\\alpha_{i_{6}})=(50,50,10,10,0,0)\\,,\\] \\[\\alpha^{\\prime} = \\left(\\alpha^{\\prime}_{i_{1}},\\cdots,\\alpha^{\\prime}_{i_{6}}\\right) =(50,50,30,30,0,0)\\,,\\] \\[\\alpha^{\\prime\\prime} = \\left(\\alpha^{\\prime\\prime}_{i_{1}},\\cdots,\\alpha^{\\prime\\prime}_ {i_{6}}\\right)=(60,60,10,10,0,0)\\,.\\]\n\nWith the bonus points profile \\(\\alpha,\\alpha^{\\prime}\\) and \\(\\alpha^{\\prime\\prime}\\), the priority order \\(\\succ^{2}_{s},\\succ^{3}_{s}\\) and \\(\\succ^{4}_{s}\\) are respectively such that\n\n\\[\\succ^{2}_{s}:\\ i_{2}\\ i_{6}\\ i_{4}\\ i_{1}\\ i_{5}\\ i_{3},\\] \\[\\succ^{3}_{s}:\\ i_{2}\\ i_{6}\\ i_{4}\\ i_{3}\\ i_{1}\\ i_{5},\\] \\[\\succ^{4}_{s}:\\ i_{2}\\ i_{6}\\ i_{1}\\ i_{4}\\ i_{5}\\ i_{3}.\\]\n\nThen, \\(\\succ^{2}\\), \\(\\succ^{3}\\) and \\(\\succ^{4}\\) are improvements of \\(\\succ^{1}\\) for \\(\\{i_{1},\\cdots,i_{4}\\}\\).\n\nIn this section, we fix the profile of original priority orders \\(\\succ\\) and the improved students \\(I^{\\prime}\\subseteq I\\), and let\n\n\\[\\rhd^{\\prime}_{s} = \\left\\{\\succ_{s},\\succ "]}, {"edit": ["ize, one obtains\n\n\\[\\langle n_{\\mathbf{m}r,s+1}\\rangle-\\langle n_{\\mathbf{m}r,s}\\rangle =(1-\\langle n_{\\mathbf{m}r,s}\\rangle)\\sum_{\\mathbf{m}r^{\\prime} \\neq\\mathbf{m}r}\\langle n_{\\mathbf{m}r^{\\prime},s}\\rangle f(u^{s}_{\\mathbf{m}r ^{\\prime}\\rightarrow\\mathbf{m}r}) \\tag{14}\\] \\[-\\langle n_{\\mathbf{m}r,s}\\rangle\\sum_{\\mathbf{m}r^{\\prime}\\neq \\mathbf{m}r}(1-\\langle n_{\\mathbf{m}r^{\\prime},s}\\rangle)f(u^{s}_{\\mathbf{m}r \\rightarrow\\mathbf{m}r^{\\prime}})\\]\n\nwhere \\(u^{s}_{\\mathbf{m}r\\rightarrow\\mathbf{m}r^{\\prime}}\\equiv u(\\langle\\tilde{n}_{ \\mathbf{m}r^{\\prime},s}\\rangle)-u(\\langle\\tilde{n}_{\\mathbf{m}r,s}\\rangle)\\). For convenience, we take the continuous time and continuous space limit, following the common procedure to obtain a mean-field description of exclusion processes on lattices (see e.g. [44]). The average occupation number \\(\\langle n\\rangle\\) is now described by the density \\(\\rho\\), while the spatially smoothed average occupation number \\(\\langle\\tilde{n}\\rangle\\) is described by the field \\(\\varphi\\equiv G_{\\sigma}*\\rho\\). The master equation for the occupation probability then takes the form of a noiseless hydrodynamic equation, in our case:\n\n\\[\\partial_{t}\\rho(x,t)= [1-\\rho(x,t)]\\int\\mathrm{d}y\\,\\rho(y,t)w([\\varphi],y,x,t) \\tag{15}\\] \\[-\\rho(x,t)\\int\\mathrm{d}y\\,[1-\\rho(y,t)]w([\\varphi],x,y,t),\\]\n\nwith the transition rate from \\(y\\) to \\(x\\) explicitly given by\n\n\\[w([\\varphi],y,x,t)=\\omega f\\left[u(\\varphi(x,t))-u(\\varphi(y,t))\\right], \\tag{16}\\]\n\nwhere \\(\\omega\\) is homogeneous to an inverse time scale. Eq. (15) is valid in any dimension, but, for simplicity, we will work out the mean-field computations in dimension 1 in space. This can be justified _a posteriori_ when we compare the mean-field (MF) to the Monte Carlo (MC) simulations. Let us also mention that the dimension does not play a role in determining the phase densities in the steady state of coarse-grained field theories (Allen-Cahn [45], Cahn-Hilliard [46], etc.).\n\nIntegrating Eq. (15) over space, one immediately sees that the total density \\(\\int\\rho\\) is conserved. One can also check that in the very specific case where \\(u(\\varphi)\\) is linear in \\(\\varphi\\), one can build a free-energy functional that is a Lyapunov function of the non-local MF dynamics, ensuring a convergence towards local minima and preventing limit cycles and oscillatory dynamics. This is a natural consequence of the fact that detailed balance is satisfied at microscopic level. In App. C, we construct this free energy and show that the dynamics is relaxational.\n\n### Linear stability analysis\n\nIn the general case, we would like to understand how the homogeneous state becomes unstable. To do so, we consider a small perturbation around the homogeneous state: \\(\\rho(x,t)=\\rho_{0}+\\rho_{1}(x,t)\\), with \\(\\ "], "nougat": ["ize, one obtains\n\n\\[\\langle n_{\\mathbf{m}r,s+1}\\rangle-\\langle n_{\\mathbf{m}r,s}\\rangle =(1-\\langle n_{\\mathbf{m}r,s}\\rangle)\\sum_{\\mathbf{m}r^{\\prime} \\neq\\mathbf{m}r}\\langle n_{\\mathbf{m}r^{\\prime},s}\\rangle f(u^{s}_{\\mathbf{m}r ^{\\prime}\\rightarrow\\mathbf{m}r}) \\tag{14}\\] \\[-\\langle n_{\\mathbf{m}r,s}\\rangle\\sum_{\\mathbf{m}r^{\\prime}\\neq \\mathbf{m}r}(1-\\langle n_{\\mathbf{m}r^{\\prime},s}\\rangle)f(u^{s}_{\\mathbf{m}r \\rightarrow\\mathbf{m}r^{\\prime}})\\]\n\nwhere \\(u^{s}_{\\mathbf{m}r\\rightarrow\\mathbf{m}r^{\\prime}}\\equiv u(\\langle\\tilde{n}_{ \\mathbf{m}r^{\\prime},s}\\rangle)-u(\\langle\\tilde{n}_{\\mathbf{m}r,s}\\rangle)\\). For convenience, we take the continuous time and continuous space limit, following the common procedure to obtain a mean-field description of exclusion processes on lattices (see e.g. [44]). The average occupation number \\(\\langle n\\rangle\\) is now described by the density \\(\\rho\\), while the spatially smoothed average occupation number \\(\\langle\\tilde{n}\\rangle\\) is described by the field \\(\\varphi\\equiv G_{\\sigma}*\\rho\\). The master equation for the occupation probability then takes the form of a noiseless hydrodynamic equation, in our case:\n\n\\[\\partial_{t}\\rho(x,t)= [1-\\rho(x,t)]\\int\\mathrm{d}y\\,\\rho(y,t)w([\\varphi],y,x,t)\\] (15) is valid in any dimension, but, for simplicity, we will work out the mean-field computations in dimension 1 in space. This can be justified _a posteriori_ when we compare the mean-field (MF) to the Monte Carlo (MC) simulations. Let us also mention that the dimension does not play a role in determining the phase densities in the steady state of coarse-grained field theories (Allen-Cahn [45], Cahn-Hilliard [46], etc.).\n\nIntegrating Eq. (15) over space, one immediately sees that the total density \\(\\int\\rho\\) is conserved. One can also check that in the very specific case where \\(u(\\varphi)\\) is linear in \\(\\varphi\\), one can build a free-energy functional that is a Lyapunov function of the non-local MF dynamics, ensuring a convergence towards local minima and preventing limit cycles and oscillatory dynamics. This is a natural consequence of the fact that detailed balance is satisfied at microscopic level. In App. C, we construct this free energy and show that the dynamics is relaxational.\n\n### Linear stability analysis\n\nIn the general case, we would like to understand how the homogeneous state becomes unstable. To do so, we consider a small perturbation around the homogeneous state: \\(\\rho(x,t)=\\rho_{0}+\\rho_{1}(x,t)\\), with \\(\\rho_{1}\\) the perturbation. By linearity of the convolution, one has \\(\\varphi(x,t)=\\rho_{0}+\\varphi_{1}(x,t)\\), with \\(\\varphi_{1}\\equiv G_{\\sigma}*\\rho_{1}\\). A Taylor expansion of Eq. (15) combined with mass conservation (i.e \\(\\int_{D}\\rho_{1}=\\int_{D}\\varphi_{1}=0\\), where \\(D\\) is the full domain), finally yields:\n\n\\[\\ "]}, {"edit": ["decision. More specifically, it takes one day for an order to be delivered to the store and become available to consumers.13 The ordered amount \\(y_{t}\\) is a discrete variable with support set \\(\\mathcal{Y}\\equiv\\{0,1,...,J\\}\\).\n\nFootnote 13: Based on the interviews we conducted with store managers, the most common delivery lag reported is one day. Delivery lags exceeding three days were described as extremely rare.\n\n**Step (ii)**. Demand \\(d_{t}\\) is realized. Demand has a Negative Binomial distribution with log-expected demand and variance:\n\n\\[\\left\\{\\begin{array}{rcl}\\ln d_{t}^{e}&=&\\boldsymbol{\\eta}_{0}^{\\prime}\\ \\mathbf{ seas}_{t}+\\eta_{p}\\ \\ln p_{t}+\\eta_{Q}\\ \\ln Q_{t}^{[-7,-1]}\\\\ \\sigma_{t}^{2}&=&d_{t}^{e}\\ \\left(1+\\alpha\\ d_{t}^{e}\\right)\\end{array}\\right. \\tag{11}\\]\n\nwhere \\(\\boldsymbol{\\eta}_{0}\\), \\(\\eta_{p}\\), \\(\\eta_{Q}\\), and \\(\\alpha\\) are parameters; \\(\\mathbf{ seas}_{t}\\) is a vector of seasonal dummies (i.e., weekend dummy and main holidays dummy); \\(Q_{t}^{[-7,-1]}\\) is the average daily sales of the product in the store during the last seven days; and \\(\\alpha\\) denotes the over-dispersion parameter in the Negative Binomial. We use \\(F_{d_{t}}\\) to represent the distribution of \\(d_{t}\\) conditional on \\((p_{t},Q_{t}^{[-7,-1]},\\mathbf{ seas}_{t})\\). Importantly, the stochastic demand shock \\(u_{t}^{d}\\equiv\\ln d_{t}-\\ln d_{t}^{e}\\) is unknown to the store manager at the beginning of the day when she makes her ordering decision.\n\n**Step (iii)**. The store sells \\(q_{t}\\) units of inventory, which is the minimum of supply and demand:\n\n\\[q_{t}\\ =\\ \\min\\{\\ d_{t}\\ ,\\ k_{t}\\ \\} \\tag{12}\\]\n\nThe store generates flow profits \\(\\Pi_{t}\\). The profit function has the following form:\n\n\\[\\Pi_{t}\\ =\\ \\left(p_{t}-c_{t}\\right)\\ \\min\\{d_{t},k_{t}\\}+\\gamma^{z}\\mathbb{1} \\{d_{t}>k_{t}\\}-\\gamma^{h}\\ k_{t}-\\gamma^{c}\\ y_{t}-\\gamma^{f}\\ \\mathbb{1}\\{y_{t}>0\\}+ \\sigma_{\\varepsilon}\\ \\varepsilon_{t}(y_{t}) \\tag{13}\\]\n\nwhere \\(c_{t}\\) is the wholesale price, and \\(\\gamma^{z}\\), \\(\\gamma^{h}\\), \\(\\gamma^{c}\\), \\(\\gamma^{f}\\), and \\(\\sigma_{\\varepsilon}\\) are store-product-specific structural parameters. When \\(\\gamma^{z}>0\\), the term \\(\\gamma^{z}\\cdot\\mathbb{1}\\{d_{t}>k_{t}\\}\\) captures the situation where the cost of a stockout can be smaller than the revenue loss from excess demand because some consumers "], "nougat": ["decision. More specifically, it takes one day for an order to be delivered to the store and become available to consumers. Based on the interviews we conducted with store managers, the most common delivery lag reported is one day. Delivery lags exceeding three days were described as extremely rare. The ordered amount \\(y_{t}\\) is a discrete variable with support set \\(\\mathcal{Y}\\equiv\\{0,1,...,J\\}\\).\n\n**Step (ii)**. Demand \\(d_{t}\\) is realized. Demand has a Negative Binomial distribution with logexpected demand and variance:\n\n\\[\\left\\{\\begin{array}{rcl}\\ln d_{t}^{e}&=&\\boldsymbol{\\eta}_{0}^{\\prime}\\ \\mathbf{ seas}_{t}+\\eta_{p}\\ \\ln p_{t}+\\eta_{Q}\\ \\ln Q_{t}^{[-7,-1]}\\\\ \\sigma_{t}^{2}&=&d_{t}^{e}\\ \\left(1+\\alpha\\ d_{t}^{e}\\right)\\end{array}\\right. \\tag{11}\\]\n\nwhere \\(\\boldsymbol{\\eta}_{0}\\), \\(\\eta_{p}\\), \\(\\eta_{Q}\\), and \\(\\alpha\\) are parameters; \\(\\mathbf{ seas}_{t}\\) is a vector of seasonal dummies (i.e., weekend dummy and main holidays dummy); \\(Q_{t}^{[-7,-1]}\\) is the average daily sales of the product in the store during the last seven days; and \\(\\alpha\\) denotes the over-dispersion parameter in the Negative Binomial. We use \\(F_{d_{t}}\\) to represent the distribution of \\(d_{t}\\) conditional on \\((p_{t},Q_{t}^{[-7,-1]},\\mathbf{ seas}_{t})\\). Importantly, the stochastic demand shock \\(u_{t}^{d}\\equiv\\ln d_{t}-\\ln d_{t}^{e}\\) is unknown to the store manager at the beginning of the day when she makes her ordering decision.\n\n**Step (iii)**. The store sells \\(q_{t}\\) units of inventory, which is the minimum of supply and demand:\n\n\\[q_{t}\\ =\\ \\min\\{\\ d_{t}\\ ,\\ k_{t}\\ \\} \\tag{12}\\]\n\nThe store generates flow profits \\(\\Pi_{t}\\). The profit function has the following form:\n\n\\[\\Pi_{t}\\ =\\ \\left(p_{t}-c_{t}\\right)\\ \\min\\{d_{t},k_{t}\\}+\\gamma^{z}\\mathbb{1} \\{d_{t}>k_{t}\\}-\\gamma^{h}\\ k_{t}-\\gamma^{c}\\ y_{t}-\\gamma^{f}\\ \\mathbb{1}\\{y_{t}>0\\}+ \\sigma_{\\varepsilon}\\ \\varepsilon_{t}(y_{t}) \\tag{13}\\]\n\nwhere \\(c_{t}\\) is the wholesale price, and \\(\\gamma^{z}\\), \\(\\gamma^{h}\\), \\(\\gamma^{c}\\), \\(\\gamma^{f}\\), and \\(\\sigma_{\\varepsilon}\\) are store-product-specific structural parameters. When \\(\\gamma^{z}>0\\), the term \\(\\gamma^{z}\\cdot\\mathbb{1}\\{d_{t}>k_{t}\\}\\) captures the situation "]}, {"edit": ["size, social welfare is given by\n\n\\[\\begin{split} W=-\\frac{1}{4}E_{0}\\sum_{t=0}^{\\infty}\\beta^{\\prime}& \\bigg{\\{}(1-\\alpha)\\left(x_{Nt}^{2}+\\frac{\\theta_{N}}{\\kappa_{N}}\\pi_{Nt}^{2} \\right)+\\tilde{\\alpha}\\left(x_{Tt}^{2}+\\frac{\\theta_{T}}{\\kappa_{T}}\\pi_{Ht}^{ 2}\\right)\\\\ &+(1-\\alpha^{*})\\left(x_{Nt}^{*2}+\\frac{\\theta^{*}}{\\kappa_{N}^{*} }\\pi_{Nt}^{*2}\\right)+\\tilde{\\alpha}^{*}\\left(x_{Tt}^{*2}+\\frac{\\theta^{*}}{ \\kappa_{T}^{*}}\\pi_{Ft}^{*2}\\right)\\bigg{\\}}\\end{split} \\tag{11}\\]\n\nwhere asterisks denote foreign variables and with \\(\\theta_{j}\\), \\(j=\\{N,T\\}\\), representing the elasticity of substitution between the differentiated goods in sector \\(j\\).7 The weighting placed on each sector is proportional to the sector's size. This is \\((1-\\alpha)\\) for the non-tradable sector while that for tradables is adjusted to take into account that the planner internalises the terms of trade externality so that. Moreover, the more that prices are flexible in a given sector the lower the weight that the planner places on stabilising inflation in that sector.\n\nFootnote 7: Liu and Pappa (2008) also consider policy under a Nash regime. For the purposes of this paper, comparing commitment to discretion, it makes no difference what regime is implemented.\n\nIn parameterising the model, we follow the values used by Liu and Pappa (2008), which are shown in Table 2. Although some parameters are not structural, such as \\(\\kappa_{j}\\), the analysis conducted below takes this into account.\n\nWe again solve the model under discretion, commitment and the stabilisation of an inflation index, \\(\\pi_{t}^{T}\\). For this model, the latter is given by\n\n\\[\\pi_{t}^{T}=\\varphi\\pi_{Nt}+(1-\\varphi)\\pi_{Ht} \\tag{12}\\]\n\nand an analogous index is used for the foreign economy. The losses as we alter the size of the tradable sector in either country are shown in Figure 5 where for comparability with Liu and Pappa (2008), the losses are reported as a proportion of steady state consumption. The first thing to note is that losses under discretion are not reported as they are almost identical to those under commitment and the reason is explained by the right panel. It shows that when the policy maker stabilises an inflation index in each country described by (12), the resulting outcomes are identical to those under commitment, so that the stabilisation bias is zero. Further detail is provided by Figure 6, which shows how the optimal value of \\(\\varphi\\) changes with the values of \\(\\alpha\\) and \\(\\alpha^{*}\\).8 As the size of the tradable sector (\\(\\alpha\\)) increases, the optimal value of \\(\\varphi\\) falls monotonically, albeit non-linearly. These responses are consistent with those obtained from implementing (6) in the model by Woodford (2003) considered earlier and by Benigno (2004).\n\nFootnote 8: As the figure for \\(\\varphi^{*}\\) is analogous, it is not reported.\n\nWe can additionally conduct the analysis in Section 2.2 of altering the degree of relative degrees of price rigidities whilst maintaining the overall aggregate measure intact. To do this, we can re-write the welfare objective (11) as\n\n\\begin{table}\n\\begin{tabular}{c "], "nougat": ["size, social welfare is given by\n\n\\[\\begin{split} W=-\\frac{1}{4}E_{0}\\sum_{t=0}^{\\infty}\\beta^{t}& \\bigg{\\{}(1-\\alpha)\\left(x_{Nt}^{2}+\\frac{\\theta_{N}}{\\kappa_{N}}\\pi_{Nt}^{2} \\right)+\\tilde{\\alpha}\\left(x_{Tt}^{2}+\\frac{\\theta_{T}}{\\kappa_{T}}\\pi_{Ht}^{ 2}\\right)\\\\ &+(1-\\alpha^{*})\\left(x_{Nt}^{*2}+\\frac{\\theta^{*}}{\\kappa_{N}^{*} }\\pi_{Nt}^{*2}\\right)+\\tilde{\\alpha}^{*}\\left(x_{Tt}^{*2}+\\frac{\\theta^{*}}{ \\kappa_{T}^{*}}\\pi_{Ft}^{*2}\\right)\\bigg{\\}}\\end{split} \\tag{11}\\]\n\nwhere asterisks denote foreign variables and with \\(\\theta_{j}\\), \\(j=\\{N,T\\}\\), representing the elasticity of substitution between the differentiated goods in sector \\(j\\).7 The weighting placed on each sector is proportional to the sector\u2019s size. This is \\((1-\\alpha)\\) for the non-tradable sector while that for tradables is adjusted to take into account that the planner internalises the terms of trade externality so that. Moreover, the more that prices are flexible in a given sector the lower the weight that the planner places on stabilising inflation in that sector.\n\nFootnote 7: Liu and Pappa (2008) also consider policy under a Nash regime. For the purposes of this paper, comparing commitment to discretion, it makes no difference what regime is implemented.\n\nIn parameterising the model, we follow the values used by Liu and Pappa (2008), which are shown in Table 2. Although some parameters are not structural, such as \\(\\kappa_{j}\\), the analysis conducted below takes this into account.\n\nWe again solve the model under discretion, commitment and the stabilisation of an inflation index, \\(\\pi_{t}^{T}\\). For this model, the latter is given by\n\n\\[\\pi_{t}^{T}=\\varphi\\pi_{Nt}+(1-\\varphi)\\pi_{Ht} \\tag{12}\\]\n\nand an analogous index is used for the foreign economy. The losses as we alter the size of the tradable sector in either country are shown in Figure 5 where for comparability with Liu and Pappa (2008), the losses are reported as a proportion of steady state consumption. The first thing to note is that losses under discretion are not reported as they are almost identical to those under commitment and the reason is explained by the right panel. It shows that when the policy maker stabilises an inflation index in each country described by (12), the resulting outcomes are identical to those under commitment, so that the stabilisation bias is zero. Further detail is provided by Figure 6, which shows how the optimal value of \\(\\varphi\\) changes with the values of \\(\\alpha\\) and \\(\\alpha^{*}\\).8 As the size of the tradable sector (\\(\\alpha\\)) increases, the optimal value of \\(\\varphi\\) falls monotonically, albeit non-linearly. These responses are consistent with those obtained from implementing (6) in the model by Woodford (2003) considered earlier and by Benigno (2004).\n\nFootnote 8: As the figure for \\(\\varphi^{*}\\) is analogous, it is not reported.\n\nWe can additionally conduct the analysis in Section 2.2 of altering the degree of relative degrees of price rigidities whilst maintaining the overall aggregate measure intact. To do this, we can re-write the welfare objective (11) as\n\n\\begin{table}\n\\begin{tabular}{c c "]}, {"edit": ["true. In this sense, as in Sandholm (2002, 2005), we need not impose that players play an equilibrium of the game but could depart from more primitive assumptions on players' strategic sophistication by requiring that none play a strategy that is iteratively dominated. Equilibrium play would then be obtained as a result, rather than an assumption, of the analysis.\n\nSection 5 presents the building blocks of this result and develops some of the intuition. It also discusses the properties of \\(\\tilde{s}\\) and compares those to other proposals in the literature.\n\n## 5 Analysis\n\nThe plan for this section is as follows. We first show that for any vector of subsidies \\(s\\) there exists a unique vector of real numbers \\(x(s)=(x_{i}(s))\\) such that the increasing strategy vector \\(p^{x(s)}\\) is the unique strategy vector that survives IESDS in \\({}^{\\varepsilon}(s)\\). Then we demonstrate that the strategy vector \\(p^{x(s)}\\) is also the unique Bayesian Nash equilibrium of \\({}^{\\varepsilon}(s)\\). We use this, and some minor technical results, to derive the unique subsidy scheme \\(\\tilde{s}\\) that implements \\(p^{\\tilde{x}}\\).\n\nThe analysis relies on two monotonicity properties of players' expected incentives for much of the heavy lifting.\n\n**Lemma 1**.: _Given is a vector of real numbers \\(y=(y_{i})\\) and the associated increasing strategy vector \\(p^{y}=(p_{i}^{y_{i}})\\). Then,_\n\n1. \\(u_{i}^{\\varepsilon}(p_{-i}^{y}\\mid x_{i}^{\\varepsilon})\\) _is monotone increasing in_ \\(x_{i}^{\\varepsilon}\\)_;_\n2. \\(u_{i}^{\\varepsilon}(p_{-i}^{y}\\mid x_{i}^{\\varepsilon})\\) _is monotone decreasing in_ \\(y_{j}\\)_, all_ \\(j\\in\\mathscr{N}\\setminus\\{i\\}\\)_._\n\nPart (i) of Lemma 1 says that a player's incentive to play 1 is increasing in his type \\(x_{i}^{\\varepsilon}\\) when his opponents play increasing strategies. There are two sides to this. First, taking as given the vector of actions \\(a_{-i}\\), a player's expected payoff to playing 1 is linearly increasing in \\(x_{i}^{\\varepsilon}\\); hence, his expected incentive is increasing in his signal \\(x_{i}^{\\varepsilon}\\). Second, as \\(x_{i}^{\\varepsilon}\\) increases player \\(i\\)'s posterior distribution on the hidden state \\(x\\) and, therefore, the signals of his opponents "], "nougat": ["true. In this sense, as in Sandholm (2002, 2005), we need not impose that players play an equilibrium of the game but could depart from more primitive assumptions on players\u2019 strategic sophistication by requiring that none play a strategy that is iteratively dominated. Equilibrium play would then be obtained as a result, rather than an assumption, of the analysis.\n\nSection 5 presents the building blocks of this result and develops some of the intuition. It also discusses the properties of \\(\\tilde{s}\\) and compares those to other proposals in the literature.\n\n## 5 Analysis\n\nThe plan for this section is as follows. We first show that for any vector of subsidies \\(s\\) there exists a unique vector of real numbers \\(x(s)=(x_{i}(s))\\) such that the increasing strategy vector \\(p^{x(s)}\\) is the unique strategy vector that survives IESDS in \\({}^{\\varepsilon}(s)\\). Then we demonstrate that the strategy vector \\(p^{x(s)}\\) is also the unique Bayesian Nash equilibrium of \\({}^{\\varepsilon}(s)\\). We use this, and some minor technical results, to derive the unique subsidy scheme \\(\\tilde{s}\\) that implements \\(p^{\\tilde{x}}\\).\n\nThe analysis relies on two monotonicity properties of players\u2019 expected incentives for much of the heavy lifting.\n\n**Lemma 1**.: _Given is a vector of real numbers \\(y=(y_{i})\\) and the associated increasing strategy vector \\(p^{y}=(p_{i}^{y_{i}})\\). Then,_\n\n1. \\(u_{i}^{\\varepsilon}(p_{-i}^{y}\\mid x_{i}^{\\varepsilon})\\) _is monotone increasing in_ \\(x_{i}^{\\varepsilon}\\)_;_\n2. \\(u_{i}^{\\varepsilon}(p_{-i}^{y}\\mid x_{i}^{\\varepsilon})\\) _is monotone decreasing in_ \\(y_{j}\\)_, all_ \\(j\\in\\mathscr{N}\\setminus\\{i\\}\\)_._\n\nPart (i) of Lemma 1 says that a player\u2019s incentive to play 1 is increasing in his type \\(x_{i}^{\\varepsilon}\\) when his opponents play increasing strategies. There are two sides to this. First, taking as given the vector of actions \\(a_{-i}\\), a player\u2019s expected payoff to playing 1 is linearly increasing in \\(x_{i}^{\\varepsilon}\\); hence, his expected incentive is increasing in his signal \\(x_{i}^{\\varepsilon}\\). Second, as \\(x_{i}^{\\varepsilon}\\) increases player \\(i\\)'s posterior distribution on the hidden state \\(x\\) and, therefore, the signals of his opponents "]}, {"edit": ["Continuing the division with this new reminder, the next table reveals again the following pattern:\n\n\\begin{tabular}{|c|l|} \\hline\n**Step** & **Remainder** \\\\ \\hline\n1 & \\([B+(m+1)\\alpha^{m}A]x^{n-m}-m\\alpha^{m+1}Ax^{n-m-1}+Cx^{m}+D\\) \\\\\n2 & \\([2\\alpha B+(m+2)\\alpha^{m+1}A]x^{n-m-1}-[\\alpha^{2}B+(m+1)\\alpha^{m+2}A]x^{n-m -2}+Cx^{m}+D\\) \\\\\n3 & \\([3\\alpha^{2}B+(m+3)\\alpha^{m+2}A]x^{n-m-2}-[2\\alpha^{3}B+(m+2)\\alpha^{m+3}A]x^{ n-m-3}+Cx^{m}+D\\) \\\\ \\(\\vdots\\) & \\(\\vdots\\) \\\\ \\(k\\) & \\(k\\alpha^{k-1}B\\ +\\ (m\\ +\\ k)\\alpha^{m+k-1}A]x^{n-m-k+1}\\ -\\ [(k\\ -\\ 1)\\alpha^{k}B +\\ (m\\ +\\ k\\ -\\ 1)\\alpha^{m+k}A)]x^{n-m-k}+Cx^{m}+D\\) \\\\ \\hline \\end{tabular}\n\nFrom \\(n-m-k=m\\), we get \\(k=n-2m\\). We can rewrite the reminder as follows:\n\n\\[[(n-2m)\\alpha^{n-2m-1}B+(n-m)\\alpha^{n-m-1}A]x^{m+1}-\\] \\[-[(n-2m-1)\\alpha^{n-2m}B+(n-m-1)\\alpha^{n-m}A]x^{m}+Cx^{m}+D,\\]\n\nthat, reordering terms, becomes\n\n\\[[(n-2m)\\alpha^{n-2m-1}B+(n-m)\\alpha^{n-m-1}A]x^{m+1}+\\] \\[+[C-(n-2m-1)\\alpha^{n-2m}B-(n-m-1)\\alpha^{n-m}A]x^{m}+D.\\]\n\nStarting with this new reminder, the last pattern is suggested by the following table:\n\n\\begin{tabular}{|c|l|} \\hline\n**Step** & **Remainder** \\\\ \\hline\n1 & \\([C+(n-2m+1)\\alpha^{n-2m}B+(n-m+1)\\alpha^{n-m}A]x^{m}-[(n-2m)\\alpha^{n-2m+1}B+(n -m)\\alpha^{n-m+1}A]x^{m-1}+D\\) \\\\\n2 & \\([2\\alpha C+(n-2m+2)]\\alpha^{n-2m+1}B+(n-m+2)\\alpha^{n-m+1}A]x^{m-1}-[\\alpha^{2 }C+(n-2m+1)\\alpha^{n-2m+2}B+(n-m+1)\\alpha^{n-m+2}A]x^{m-2}+D\\) \\\\\n3 & \\([3\\alpha^{2}C+(n-2m+3)]\\alpha^{n-2m+2}B+(n "], "nougat": ["Continuing the division with this new reminder, the next table reveals again the following pattern:\n\n\\begin{tabular}{|c|l|} \\hline\n**Step** & **Remainder** \\\\ \\hline\n1 & \\([B+(m+1)\\alpha^{m}A]x^{n-m}-m\\alpha^{m+1}Ax^{n-m-1}+Cx^{m}+D\\) \\\\\n2 & \\([2\\alpha B+(m+2)\\alpha^{m+1}A]x^{n-m-1}-[\\alpha^{2}B+(m+1)\\alpha^{m+2}A]x^{n-m -2}+Cx^{m}+D\\) \\\\\n3 & \\([3\\alpha^{2}B+(m+3)\\alpha^{m+2}A]x^{n-m-2}-[2\\alpha^{3}B+(m+2)\\alpha^{m+3}A]x^{ n-m-3}+Cx^{m}+D\\) \\\\ \\(\\vdots\\) & \\(\\vdots\\) \\\\ \\(k\\) & \\(k\\alpha^{k-1}B\\ +\\ (m\\ +\\ k)\\alpha^{m+k-1}A]x^{n-m-k+1}\\ -\\ [(k\\ -\\ 1)\\alpha^{k}B +\\ (m\\ +\\ k\\ -\\ 1)\\alpha^{m+k}A)]x^{n-m-k}+Cx^{m}+D\\) \\\\ \\hline \\end{tabular}\n\nFrom \\(n-m-k=m\\), we get \\(k=n-2m\\). We can rewrite the reminder as follows:\n\n\\[[(n-2m)\\alpha^{n-2m-1}B+(n-m)\\alpha^{n-m-1}A]x^{m+1}-\\] \\[-[(n-2m-1)\\alpha^{n-2m}B+(n-m-1)\\alpha^{n-m}A]x^{m}+Cx^{m}+D,\\]\n\nthat, reordering terms, becomes\n\n\\[[(n-2m)\\alpha^{n-2m-1}B+(n-m)\\alpha^{n-m-1}A]x^{m+1}+\\] \\[+[C-(n-2m-1)\\alpha^{n-2m}B-(n-m-1)\\alpha^{n-m}A]x^{m}+D.\\]\n\nStarting with this new reminder, the last pattern is suggested by the following table:\n\n\\begin{tabular}{|c|l|} \\hline\n**Step** & **Remainder** \\\\ \\hline\n1 & \\([C+(n-2m+1)\\alpha^{n-2m}B+(n-m+1)\\alpha^{n-m}A]x^{m}-[(n-2m)\\alpha^{n-2m+1}B+(n -m)\\alpha^{n-m+1}A]x^{m-1}+D\\) \\\\\n2 & \\([2\\alpha C+(n-2m+2)]\\alpha^{n-2m+1}B+(n-m+2)\\alpha^{n-m+1}A]x^{m-1}-[\\alpha^{2 }C+(n-2m+1)\\alpha^{n-2m+2}B+(n-m+1)\\alpha^{n-m+2}A]x^{m-2}+D\\) \\\\\n3 & \\([3\\alpha^{2}C+(n-2m+3)]\\alpha^{n-2m+2}B+(n "]}, {"edit": ["We construct Figure 1 to provide additional evidence of strategic behavior. Consider a program \\(j\\) with cutoff \\(c_{j}\\). Suppose, (consistently with the general finding of the empirical literature on college choice), that students tend to prefer programs of higher quality. If applicants behave strategically, one would expect applications to program \\(j\\) to peak among students with application score close to \\(c_{j}\\). If cutoffs tend to remain in the same neighborhood across years, students with application scores much higher than \\(c_{j}\\) can expect to be admissible to more selective, higher-quality programs than \\(j\\), which they prefer over \\(j\\). Hence, we expect very few of these students to include \\(j\\) in their list. As application scores get lower and closer to \\(c_{j}\\), students' admission chances to the most selective programs decrease, and program \\(j\\) becomes one of the most selective (desirable) program among those they still have a high admission probability. Hence, we expect applications to \\(c_{j}\\) to increase as application scores decrease and get closer to \\(c_{j}\\). As application scores decrease below \\(c_{j}\\), students realize that their admission probability to program \\(j\\) gets lower, and while program \\(j\\) remains a relatively desirable (selective) alternative, we expect these expectations to drive applications down. This application pattern, expected if students behave strategically, is exactly what we observe on the top panel of Figure 1. Pooling all programs \\(j\\) together, the top panel of Figure 1 shows the fraction of students listing program \\(j\\) in their rank-order lists (ROL or \\(P\\) in terms of our notation), as a function of the distance between their application for program \\(j\\) and the cutoff \\(c_{j}\\).\n\nIt may be difficult to disentangle the role of preferences from the role of expectations about admission probabilities when both may enter students' choice of which programs to include in their ROLs (Manski (2004); Agarwal and Somaini (2018)). The pattern observed in the top panel of Figure 1 could, alternatively, be consistent with students not behaving strategically but preferring programs that \"are a good fit in terms of quality\", that is, programs in which their skill level would be close to the average skill level. If that is the case, applications should peak among students whose skills (proxied by application score) are close to the mean skill level in the program. This is not what we observe in the bottom panel of Figure 1. Pooling all programs \\(j\\) together, the bottom panel of Figure 1 shows the fraction of students including program \\(j\\) in their list, as a function of the distance between their application for program \\(j\\) and the mean application score among students admitted to \\(j\\). Conditional on application score, the share of students applying to a program \\(j\\) is not the highest for application scores close to the mean score among students admitted to \\(j\\). It reaches its highest point well below this level, showing that students do not systematically prefer programs in which they would be the \"average\" student. This further confirms the hypothesis that students behave strategically.\n\n### Results\n\nRegardless of strategic behavior, the aggregation of treatment effects across different programs brings additional challenges not tackled in this paper; therefore, to more clearly illustrate the importance of accounting for strategic behavior, we focus on a single program. We are interested in identifying the effects of assignment to a given post-secondary program on college graduation.\n\n "], "nougat": ["We construct Figure 1 to provide additional evidence of strategic behavior. Consider a program \\(j\\) with cutoff \\(c_{j}\\). Suppose, (consistently with the general finding of the empirical literature on college choice), that students tend to prefer programs of higher quality. If applicants behave strategically, one would expect applications to program \\(j\\) to peak among students with application score close to \\(c_{j}\\). If cutoffs tend to remain in the same neighborhood across years, students with application scores much higher than \\(c_{j}\\) can expect to be admissible to more selective, higher-quality programs than \\(j\\), which they prefer over \\(j\\). Hence, we expect very few of these students to include \\(j\\) in their list. As application scores get lower and closer to \\(c_{j}\\), students' admission chances to the most selective programs decrease, and program \\(j\\) becomes one of the most selective (desirable) program among those they still have a high admission probability. Hence, we expect applications to \\(c_{j}\\) to increase as application scores decrease and get closer to \\(c_{j}\\). As application scores decrease below \\(c_{j}\\), students realize that their admission probability to program \\(j\\) gets lower, and while program \\(j\\) remains a relatively desirable (selective) alternative, we expect these expectations to drive applications down. This application pattern, expected if students behave strategically, is exactly what we observe on the top panel of Figure 1. Pooling all programs \\(j\\) together, the top panel of Figure 1 shows the fraction of students listing program \\(j\\) in their rank-order lists (ROL or \\(P\\) in terms of our notation), as a function of the distance between their application for program \\(j\\) and the cutoff \\(c_{j}\\).\n\nIt may be difficult to disentangle the role of preferences from the role of expectations about admission probabilities when both may enter students\u2019 choice of which programs to include in their ROLs (Manski (2004); Agarwal and Somaini (2018)). The pattern observed in the top panel of Figure 1 could, alternatively, be consistent with students not behaving strategically but preferring programs that \u201care a good fit in terms of quality\u201d, that is, programs in which their skill level would be close to the average skill level. If that is the case, applications should peak among students whose skills (proxied by application score) are close to the mean skill level in the program. This is not what we observe in the bottom panel of Figure 1. Pooling all programs \\(j\\) together, the bottom panel of Figure 1 shows the fraction of students including program \\(j\\) in their list, as a function of the distance between their application for program \\(j\\) and the mean application score among students admitted to \\(j\\). Conditional on application score, the share of students applying to a program \\(j\\) is not the highest for application scores close to the mean score among students admitted to \\(j\\). It reaches its highest point well below this level, showing that students do not systematically prefer programs in which they would be the \u201caverage\u201d student. This further confirms the hypothesis that students behave strategically.\n\n### Results\n\nRegardless of strategic behavior, the aggregation of treatment effects across different programs brings additional challenges not tackled in this paper; therefore, to more clearly illustrate the importance of accounting for strategic behavior, we focus on a single program. We are interested in identifying the effects of assignment to a given post-secondary program on college graduation.\n\n "]}, {"edit": ["Introduction\n\nUnderstanding the behavior of commodity prices is of critical importance given their role in many countries as determinants of balance of payments, exchange rates, tax revenue, output fluctuations, and inflation (see, e.g., Byrne et al., 2013; Gospodinov and Ng, 2013; Eberhardt and Presbitero, 2021; Peersman, 2022). While some commodity price movements are driven by idiosyncratic shocks to supply and demand,1 other commodity price movements are driven by aggregate factors, such as aggregate demand and monetary conditions. The role of aggregate factors in driving commodity prices is well recognized in the literature. Alquist et al. (2020) show that up to 80% of the variance of commodity prices is explained by common factors (see also Byrne et al., 2013). However, the precise channels of transmission are less well identified.\n\nFootnote 1: For example, the price of lithium spiked in early 2021 when US President Biden announced a plan to build 500,000 charging stations for electric vehicles. In the same period, the price of rubber fell as a shortage in microchip supply for vehicle makers drove forecasts for a reduction in tire demand.\n\nIn this paper, we contribute to the debate on the role of aggregate factors in commodity prices through a focus on interest rate shocks and various important channels. Empirical analysis of this question confront profound challenges, because of endogeneity and the general equilibrium nature of some of the mechanisms: interest rate movements are determined by several macroeconomic variables that also affect commodity markets,2 and conversely, pure monetary shocks affect commodity markets through various channels (e.g., speculation, aggregate demand, and supply reaction) that may not be disentangled empirically.3 A structural model is required to isolate the direct effect of interest rates on commodity prices through each of these channels.\n\nFootnote 2: One of these confounding factors is the output. For example, a low aggregate output (recession) might cause both falling commodity prices and a reduction in interest rates by the monetary authority. Even if we fully control for changes in output and demand, rising commodity prices might themselves trigger a tightening of monetary policy, without any change in output (see, e.g., Cody and Mills, 1991).\n\nFootnote 3: For example, a decline in the US interest rate can stimulate both global demand (see, e.g., Ramey, 2016) and firms\u2019 incentive to hold inventories (see, e.g., Frankel, 1986, 2008, 2014), which then increase commodity prices. An increase in interest rates works in the opposite direction.\n\nThe obvious candidate to provide the required structure is the rational expectations competitive storage model, which has been developed, among others, by Samuelson (1971), Newbery and Stiglitz (1982), Wright and Williams (1982), Scheinkman and Schechtman (1983), Deaton and Laroque (1992, 1996), and Chambers and Bailey (1996).4 In this model, commodities are assets that also have intrinsic value, separate from future cash flows. The "], "nougat": ["Introduction\n\nUnderstanding the behavior of commodity prices is of critical importance given their role in many countries as determinants of balance of payments, exchange rates, tax revenue, output fluctuations, and inflation (see, e.g., Byrne et al. , 2013 ; Gospodinov and Ng , 2013 ; Eberhardt and Presbitero , 2021 ; Peersman , 2022 ). While some commodity price movements are driven by idiosyncratic shocks to supply and demand, 1 other commodity price movements are driven by aggregate factors, such as aggregate demand and monetary conditions. The role of aggregate factors in driving commodity prices is well recognized build 500,000 charging stations for electric vehicles. In the same period, the price of rubber fell as a shortage in the literature. Alquist et al. ( 2020 ) show that up to 80% of the variance of commodity in microchip supply for vehicle makers drove forecasts for a reduction in tire demand. prices is explained by common factors (see also Byrne et al. , 2013 ). However, the precise channels of transmission are less well identified.\n\nFootnote 1: For example, a low aggregate output (recession) might and cause both falling commodity prices and a reduction in interest rates by the monetary authority. Even if conversely, pure monetary shocks affect commodity markets through various channels (e.g., speculation, aggregate demand, and supply reaction) that may not be disentangled we fully control for changes in output and demand, rising commodity prices might themselves trigger a empirically. 3 tightening of monetary policy, without any change in output (see, e.g., Cody and Mills , 1991 ).\n\nIn this paper, we contribute to the debate on the role of aggregate factors in commodity prices through a focus on interest rate shocks and various important channels. Empirical analysis of this question confront profound challenges, because of endogeneity and the general equilibrium nature of some of the mechanisms: interest rate movements are determined by several macroeconomic variables that also affect commodity markets,2 and conversely, pure monetary shocks affect commodity markets through various channels (e.g., speculation, aggregate demand, and supply reaction) that may not be disentangled empirically.3 A structural model is required to isolate the direct effect of interest rates on commodity prices through each of these channels. Ramey , 2016 ) The obvious candidate to provide the required structure is the rational expectations comand firms\u2019 incentive to hold inventories (see, e.g., Frankel , 1986 , 2008 , 2014 petitive storage model, which has been developed, among others, by ), which then increase commodity Samuelson ( 1971 ), Newbery and Stiglitz prices. An increase in interest rates works in the opposite direction. ( 1982 ), Wright and Williams ( 1982 ), Scheinkman and Schechtman ( 1983 ), Deaton and Laroque ( 1992 , 1996 ), and Chambers and Bailey ( 1996 ). 4 4 Samuelson ( 1971 ) proved that the equilibrium quantities produced by this model are welfare maximizIn this model, ing for a representative consumer with marginal utility equal to the inverse demand function. commodities are assets that also have intrinsic value, separate from future cash flows. The Deaton and Laroque ( 1996 ) and Cafiero et al. ( 2011 ) showed that the model can replicate several important features of the data.\n\n "]}, {"edit": ["for the \\(i\\)th unit under treatment \\(a\\in\\{0,1\\}\\), and let \\(Y_{i}=R_{i}\\) denote the observed outcome. Let\n\n\\[\\theta_{0}=E_{Q}\\left[\\frac{\\omega(X_{i})}{E_{Q}[\\omega(X_{i})]}\\left(Y_{i}(1)-Y_ {i}(0)\\right)\\right]\\ ,\\]\n\nfor some known function \\(\\omega:\\mathbf{R}^{d_{x}}\\rightarrow\\mathbf{R}\\). It then follows from Assumption 2.1 that \\(\\theta_{0}\\) solves the moment condition in (3) with \\(m(\\cdot)\\) given by\n\n\\[m(X_{i},A_{i},R_{i},\\theta)=\\omega(X_{i})\\left(\\frac{Y_{i}A_{i}}{\\eta}-\\frac{Y_ {i}(1-A_{i})}{1-\\eta}\\right)-\\omega(X_{i})\\theta\\ .\\]\n\nNote that \\(\\theta_{0}\\) defined in this way can accommodate the (cluster) size-weighted and equally-weighted average treatment effects considered in Bugni et al. (2022) and Bai et al. (2023c) in the context of cluster-level randomized controlled trials.\n\n**Example 2.5**.: [Log-Odds Ratio] Let \\(Y_{i}(a)=R_{i}(a)\\in\\{0,1\\}\\) denote a binary potential outcome for the \\(i\\)th unit under treatment \\(a\\in\\{0,1\\}\\), and let \\(Y_{i}=R_{i}\\) denote the observed outcome. Suppose \\(0<P\\{Y_{i}(a)=0\\}<1\\) for \\(a\\in\\{0,1\\}\\), and let \\(\\theta_{0}=(\\theta_{0}(1),\\theta_{0}(2))^{\\prime}\\), where\n\n\\[\\theta_{0}(1)=\\mathrm{logit}(E_{Q}[Y_{i}(0)])\\ ,\\]\n\n\\[\\theta_{0}(2)=\\mathrm{logit}(E_{Q}[Y_{i}(1)])-\\mathrm{logit}(E_{Q}[Y_{i}(0)])\\ ,\\]\n\nwith \\(\\mathrm{logit}(z)=\\log(\\frac{z}{1-z})\\), so that \\(\\theta_{0}(2)\\) denotes the log-odds ratio of treatment \\(1\\) relative to treatment \\(0\\). It follows from Assumption 2.1 that \\(\\theta_{0}\\) solves the moment condition in (3) with \\(m(\\cdot)\\) given by\n\n\\[m(X_{i},A_{i},R_{i},\\theta)=\\begin{pmatrix}1-A_{i}\\\\ A_{i}\\end{pmatrix}\\left(Y_{i}-\\mathrm{expit}(\\theta(1)+\\theta(2)A_{i})\\right)\\ ,\\]\n\nwhere \\(\\mathrm{expit}(z)=\\frac{\\exp(z)}{1+\\exp(z)}\\). The log-odds ratio can then be defined as \\(h(\\theta_{0})\\) where \\(h:\\mathbf{R}^{2}\\rightarrow\\mathbf{R}\\) is given by \\(h(s,t)=t\\).\n\nAdditional examples could be obtained by considering combinations of Examples 2.1-2.5. For instance, "], "nougat": ["for the \\(i\\)th unit under treatment \\(a\\in\\{0,1\\}\\), and let \\(Y_{i}=R_{i}\\) denote the observed outcome. Let\n\n\\[\\theta_{0}=E_{Q}\\left[\\frac{\\omega(X_{i})}{E_{Q}[\\omega(X_{i})]}\\left(Y_{i}(1)-Y_ {i}(0)\\right)\\right]\\ ,\\]\n\nfor some known function \\(\\omega:\\mathbf{R}^{d_{x}}\\rightarrow\\mathbf{R}\\). It then follows from Assumption 2.1 that \\(\\theta_{0}\\) solves the moment condition in ( 3 ) with \\(m(\\cdot)\\) given by\n\n\\[m(X_{i},A_{i},R_{i},\\theta)=\\omega(X_{i})\\left(\\frac{Y_{i}A_{i}}{\\eta}-\\frac{Y_ {i}(1-A_{i})}{1-\\eta}\\right)-\\omega(X_{i})\\theta\\ .\\]\n\nNote that \\(\\theta_{0}\\) defined in this way can accommodate the (cluster) size-weighted and equally-weighted average treatment effects considered in Bugni et al. ( 2022 ) and Bai et al. ( 2023c ) in the context of cluster-level randomized controlled trials.\n\n**Example 2.5**.: [Log-Odds Ratio] Let \\(Y_{i}(a)=R_{i}(a)\\in\\{0,1\\}\\) denote a binary potential outcome for the \\(i\\)th unit under treatment \\(a\\in\\{0,1\\}\\), and let \\(Y_{i}=R_{i}\\) denote the observed outcome. Suppose \\(0<P\\{Y_{i}(a)=0\\}<1\\) for \\(a\\in\\{0,1\\}\\), and let \\(\\theta_{0}=(\\theta_{0}(1),\\theta_{0}(2))^{\\prime}\\), where\n\n\\[\\theta_{0}(1)=\\mathrm{logit}(E_{Q}[Y_{i}(0)])\\ ,\\]\n\n\\[\\theta_{0}(2)=\\mathrm{logit}(E_{Q}[Y_{i}(1)])-\\mathrm{logit}(E_{Q}[Y_{i}(0)])\\ ,\\]\n\nwith \\(\\mathrm{logit}(z)=\\log(\\frac{z}{1-z})\\), so that \\(\\theta_{0}(2)\\) denotes the log-odds ratio of treatment \\(1\\) relative to treatment \\(0\\). It follows from Assumption 2.1 that \\(\\theta_{0}\\) solves the moment condition in ( 3 ) with \\(m(\\cdot)\\) given by\n\n\\[m(X_{i},A_{i},R_{i},\\theta)=\\begin{pmatrix}1-A_{i}\\\\ A_{i}\\end{pmatrix}\\left(Y_{i}-\\mathrm{expit}(\\theta(1)+\\theta(2)A_{i})\\right)\\ ,\\]\n\nwhere \\(\\mathrm{expit}(z)=\\frac{\\mathrm{exp}(z)}{1+\\mathrm{exp}(z)}\\). The log-odds ratio can then be defined as \\(h(\\theta_{0})\\) where \\(h:\\mathbf{R}^{2}\\rightarrow\\mathbf{R}\\) is given by \\(h(s,t)=t\\).\n\nAdditional examples could be "]}, {"edit": ["due to its structural constraints on the input space. We use two data generation processes - Multinomial Logit (MNL) and Random-Coefficients Logit (RCL). The observable characteristics of the new product are simulated using the same distribution as other products. In Table 10, we present the estimated market share of the new product. Our model outperforms the MNL when the underlying data generation process is RCL, and produces results comparable to the true model.\n\n### Inference and Coverage Analysis\n\nFinally, we demonstrate the performance of the inference procedure. The objective is to demonstrate the validity of the estimated confidence intervals. To this end, we estimate the effect of a 1% change in price on demand and compute the corresponding confidence intervals of this effect. To simulate the data, we consider a random-coefficients logit model of demand with 3 products across 100 markets. We set the true model parameters to be \\(\\beta_{ik}\\sim\\mathcal{N}(1,0.5),\\alpha_{i}\\sim\\mathcal{N}(-1,0.5)\\). The effect of a 1% increase in a product's price is given by\n\n\\[\\theta_{0}=\\mathbb{E}[m(w_{i},\\pi]=\\mathbb{E}[\\pi(p_{jt}*(1.01),x_{jt},\\{x_{kt }\\}_{k\\neq j})-\\pi(p_{jt},x_{jt},\\{x_{kt}\\}_{k\\neq j})],\\]\n\nAs discussed earlier, one way to estimate this effect is to compute the sample analog of this\n\n\\begin{table}\n\\begin{tabular}{c c c c c c c c c} \\hline \\hline True Model & \\multicolumn{2}{c}{Our Model} & \\multicolumn{2}{c}{MNL} & \\multicolumn{2}{c}{RCL} & No. Obs. \\\\  & MAE & RMSE & MAE & RMSE & MAE & RMSE & \\\\ \\hline MNL & 0.0234 & 0.0644 & 0.0041 & 0.0095 & 0.0023 & 0.0045 & 22,000 \\\\ RCL & 0.0186 & 0.0145 & 0.0265 & 0.0331 & 0.0023 & 0.0031 & 22,000 \\\\ \\hline \\hline \\end{tabular}\n**Note:** This table presents the bias and variance of predicted market shares of all products in the market when a new product enters. We simulate market shares as in our baseline scenario (10 products, 100 markets, 10 features) when a new product is introduced.\n\n\\end{table}\nTable 10: New Product Demand Estimation - Predicted Market Share\n\n\\begin{table}\n\\begin{tabular}{c c c c c c c c c c} \\hline \\hline \\# & J & \\multicolumn{2}{c}{Our Model} & \\multicolumn{2}{c}{MNL} & \\multicolumn{2}{c}{RCL} & \\multicolumn{2}{c}{NP} & No. Obs \\\\ \\hline  & & MAE & RMSE & MAE & RMSE & MAE & RMSE & MAE & RMSE & \\\\ \\hline\n0 & 2 & 0.0419 & 4.3553 & 0.1911 & 8.8197 & 0.3350 & 8.5791 & 0.0378 & 5.6814 & 3200 \\\\\n1 & 5 & 0.0486 & 4.7745 & 0.0827 & "], "nougat": ["due to its structural constraints on the input space. We use two data generation processes - Multinomial Logit (MNL) and Random-Coefficients Logit (RCL). The observable characteristics of the new product are simulated using the same distribution as other products. In Table 10, we present the estimated market share of each product in each market by assuming there is a portion of consumers who ignore the product with the highest price. We assume the portion is \\(1-\\frac{1}{1+Price_{j}}\\). We consider 3 scenarios with 2, 5, and 10 products respectively. We fix the number of markets to 100 and the number of features to 1 (with only price). Other parts are the same as RCL in our baseline.\n\n### Inference and Coverage Analysis\n\nFinally, we demonstrate the performance of the inference procedure. The objective is to demonstrate the validity of the estimated confidence intervals. To this end, we estimate the effect of a 1% change in price on demand and compute the corresponding confidence intervals of this effect. To simulate the data, we consider a random-coefficients logit model of demand with 3 products across 10 , we present the estimated market share of the new product. Our model outperforms the MNL when the underlying data generation process is RCL, and produces results comparable to the true model.\n\n### Inference and Coverage Analysis\n\nFinally, we demonstrate the performance of the inference procedure. The objective is to demonstrate the validity of the estimated confidence intervals. To this end, we estimate the effect of a 1% change in price on demand and compute the corresponding confidence intervals of this effect. To simulate the data, we consider a random-coefficients logit model of demand with 3 products across 100 markets, 10 features) when a new product is introduced. We set the true model parameters to be \\(\\beta_{ik}\\sim\\mathcal{N}(1,0.5),\\alpha_{i}\\sim\\mathcal{N}(-1,0.5)\\). The effect of a 1% increase in a product\u2019s price is given by\n\n\\[\\theta_{0}=\\mathbb{E}[m(w_{i},\\pi]=\\mathbb{E}[\\pi(p_{jt}*(1.01),x_{jt},\\{x_{kt }\\}_{k\\neq j})-\\pi(p_{jt},x_{jt},\\{x_{kt}\\}_{k\\neq j})],\\]\n\nAs discussed earlier, one way to estimate this effect is to compute the sample analog of this\n\n\\begin{table}\n\\begin{tabular}{c c c c c c c c c} \\hline \\hline True Model & \\multicolumn{2}{c}{Our Model} & \\multicolumn{2}{c}{MNL} & \\multicolumn{2}{c}{RCL} & No. Obs. \\\\  & MAE & RMSE & MAE & RMSE & MAE & RMSE & & \\\\ \\hline MNL & 0.0234 & 0.0644 & 0.0041 & 0.0095 & 0.0023 & 0.0045 & 22,000 \\\\ RCL & 0.0186 & 0.0145 & 0.0265 & 0.0331 & 0.0023 & 0.0031 & 22,000 \\\\ \\hline \\hline \\end{tabular}\n**Note:** This table presents the bias and variance of predicted market shares of all products in the market when a new product enters. We simulate market shares as in our baseline scenario (10 products, 100 markets, 10 features) when a new product is introduced.\n\n\\end{table}\nTable 10: New Product Demand Estimation - Predicted Market Share\n\n\\begin{table}\n\\begin{tabular}{c c c c c c c c c c} \\hline \\hline \\# & J & \\multicolumn{2}{c}{Our Model} & \\ "]}, {"edit": ["distributions for modelling incomes (see, e.g., Kleiber and Kotz, 2003), and one of them is the Dagum distribution. For statistical inference for the ratio of any two quantiles of this distribution - and the three equality curves \\(\\psi_{k}(p)\\) are such ratios - we refer to Jedrzejczak et al. (2023).\n\nFigure 3.3: The income-equality curve \\(\\psi_{3}\\) and the shaded-in area (i.e., \\(\\Psi_{3}\\)) above it for the distributions of Table 3.1, with the dash-dotted line depicting \\(\\psi_{3}\\) of the uniform distribution.\n\n "], "nougat": ["distributions for modelling incomes (see, e.g., Kleiber and Kotz , 2003 ), and one of them is the Dagum distribution. For statistical inference for the ratio of any two quantiles of this distribution \u2013 and the three equality curves \\(\\psi_{k}(p)\\) are such ratios \u2013 we refer to J \u0327edrzejczak et al. ( 2023 ).\n\nFigure 3.3: The income-equality curve \\(\\psi_{3}\\) and the shaded-in area (i.e., \\(\\Psi_{3}\\)) above it for the distributions of Table 3.1, with the dash-dotted line depicting \\(\\psi_{3}\\) of the uniform distribution.\n\n "]}, {"edit": ["Considering the expression within the integral of (A.3),\n\n\\[\\left(f_{X}^{t}\\left(x\\right)\\frac{d}{dt}\\frac{f_{Y,D,M,X}^{t}\\left( y,d,m.x\\right)}{f_{D,M,X}^{t}\\left(d,m,x\\right)}\\frac{f_{M,D,X}^{t}\\left(m,d^{ \\prime},x\\right)}{f_{D,M,X}^{t}\\left(d^{\\prime},x\\right)}\\right)\\Bigg{|}_{t=0} = \\left(f_{X}^{t}\\left(x\\right)\\frac{f_{Y,D,M,X}^{t}\\left(y,d,m.x \\right)}{f_{D,M,X}^{t}\\left(d,m,x\\right)}\\frac{d}{dt}\\frac{f_{M,D,X}^{t}\\left( m,d^{\\prime},x\\right)}{f_{D,X}^{t}\\left(d^{\\prime},x\\right)}\\right)\\Bigg{|}_{t=0}\\] \\[+\\left(f_{X}^{t}\\left(x\\right)\\frac{f_{M,D,X}^{t}\\left(m,d^{ \\prime},x\\right)}{f_{D,X}^{t}\\left(d^{\\prime},x\\right)}\\frac{d}{dt}\\frac{f_{Y ,D,M,X}^{t}\\left(y,d,m.x\\right)}{f_{D,M,X}^{t}\\left(d,m,x\\right)}\\right)\\Bigg{|} _{t=0}\\,.\\]\n\nConsidering (A.5),\n\n\\[\\left.\\frac{d}{dt}\\frac{f_{M,D,X}^{t}\\left(m,d^{\\prime},x\\right)} {f_{D,X}^{t}\\left(d^{\\prime},x\\right)}\\right|_{t=0} = \\left(\\frac{1}{f_{D,X}^{t}\\left(d^{\\prime},x\\right)}\\frac{d}{dt}f _{M,D,X}^{t}\\left(m,d^{\\prime},x\\right)\\right)\\Bigg{|}_{t=0}\\] \\[-\\left.\\left(\\frac{f_{M,D,X}^{t}\\left(m,d^{\\prime},x\\right)}{ \\left(f_{D,X}^{t}\\left(d^{\\prime},x\\right)\\right)^{2}}\\frac{d}{dt}f_{D,X}^{t} \\left(d^{\\prime},x\\right)\\right)\\Bigg{|}_{t=0}\\right.\\] \\[= \\frac{1\\left\\{D=d^{\\prime}\\right\\}}{f_{D,X}\\left(d^{\\prime},x \\right)}\\left[1_{\\left(\\tilde{m},\\tilde{x}\\right)}\\left(m,x\\right)-f_{M\\mid D, X}\\left(m\\mid d^{\\prime},x\\right)1_{\\tilde{x}}\\left(x\\right)\\right].\\]\n\nUsing some algebra, the part of (A.3) appearing in (A.5) can be expressed as\n\n\\[\\left.\\frac{1\\left\\{D=d^{\\prime}\\right\\ "], "nougat": ["Considering the expression within the integral of (A.3),\n\n\\[\\left(f_{X}^{t}\\left(x\\right)\\frac{d}{dt}\\frac{f_{Y,D,M,X}^{t}\\left( y,d,m.x\\right)}{f_{D,M,X}^{t}\\left(d,m,x\\right)}\\frac{f_{M,D,X}^{t}\\left(m,d^{ \\prime},x\\right)}{f_{D,M,X}^{t}\\left(d^{\\prime},x\\right)}\\right)\\Bigg{|}_{t=0} = \\left(f_{X}^{t}\\left(x\\right)\\frac{f_{Y,D,M,X}^{t}\\left(y,d,m.x \\right)}{f_{D,M,X}^{t}\\left(d,m,x\\right)}\\frac{d}{dt}\\frac{f_{M,D,X}^{t}\\left( m,d^{\\prime},x\\right)}{f_{D,X}^{t}\\left(d^{\\prime},x\\right)}\\right)\\Bigg{|}_{t=0}\\] \\[+\\left.\\left(f_{X}^{t}\\left(x\\right)\\frac{f_{M,D,X}^{t}\\left(m,d^{ \\prime},x\\right)}{f_{D,X}^{t}\\left(d^{\\prime},x\\right)}\\frac{d}{dt}\\frac{f_{Y, D,M,X}^{t}\\left(y,d,m.x\\right)}{f_{D,M,X}^{t}\\left(d,m,x\\right)}\\right)\\right|_{t=0}.\\]\n\nConsidering (A.5),\n\n\\[\\left.\\frac{d}{dt}\\frac{f_{M,D,X}^{t}\\left(m,d^{\\prime},x\\right)} {f_{D,X}^{t}\\left(d^{\\prime},x\\right)}\\right|_{t=0} = \\left(\\frac{1}{f_{D,X}^{t}\\left(d^{\\prime},x\\right)}\\frac{d}{dt}f_ {M,D,X}^{t}\\left(m,d^{\\prime},x\\right)\\right)\\Bigg{|}_{t=0}\\] \\[-\\left.\\left(\\frac{f_{M,D,X}^{t}\\left(m,d^{\\prime},x\\right)}{ \\left(f_{D,X}^{t}\\left(d^{\\prime},x\\right)\\right)^{2}}\\frac{d}{dt}f_{D,X}^{t} \\left(d^{\\prime},x\\right)\\right)\\right|_{t=0}\\] \\[= \\frac{1\\left\\{D=d^{\\prime}\\right\\}}{f_{D,X}\\left(d^{\\prime},x \\right)}\\left[1_{\\left(\\tilde{m},\\tilde{x}\\right)}\\left(m,x\\right)-f_{M\\mid D,X }\\left(m\\middle|d^{\\prime},x\\right)1_{\\tilde{x}}\\left(x\\right)\\right].\\]\n\nUsing some algebra, the part of (A.3) appearing in (A.5) can be expressed as\n\n\\[\\frac{1\\left\\{D=d^{\\prime}\\right\\}}{f_{D\\mid "]}, {"edit": ["To explore the sensitivity of IRFs to states, Figure 7 draws the IRFs calculated for different realized values of \\((X_{t-1},R_{t-1})\\). We use \\((X_{t-1}^{p},R_{t-1}^{p})\\) to denote the percentile points of the realized \\((X_{t-1},R_{t-1})\\) states on the stationary distribution. The top left panel shows that price responses are stronger when availability becomes larger. The immediate responses of price are respectively 1.72 and 2.17 times larger when availability increases from the 25% percentile to the 75% and 95% percentiles. This is because when availability is lower, inventory tends to be lower and there is less room for stock adjustment, hence prices react much less in response to the interest rate shock. This intuition is partially verified by the top middle panel, which shows that a higher availability causes stock accumulation to last longer, yielding a larger decline in inventory in the medium to long run (in spite of a slightly lower immediate decline).\n\nThe bottom left panel in Figure 7 shows that price responses to a 25 bp interest rate shock\n\nFigure 7: Impulse response functions to a 25 bp real interest rate shock conditional on different states (fixing \\(\\delta=0.02\\) and \\(\\lambda=-0.06\\))\n\n"], "nougat": ["To explore the sensitivity of IRFs to states, Figure 7 draws the IRFs calculated for different realized values of \\((X_{t-1},R_{t-1})\\). We use \\((X_{t-1}^{p},R_{t-1}^{p})\\) to denote the percentile points of the realized \\((X_{t-1},R_{t-1})\\) states on the stationary distribution. The top left panel shows that price responses are stronger when availability becomes larger. The immediate responses of price are respectively 1.72 and 2.17 times larger when availability increases from the 25% percentile to the 75% and 95% percentiles. This is because when availability is lower, inventory tends to be lower and there is less room for stock adjustment, hence prices react much less in response to the interest rate shock. This intuition is partially verified by the top middle panel, which shows that a higher availability causes stock decumulation to last longer, yielding a larger decline in inventory in the medium to long run (in spite of a slightly lower immediate decline).\n\nThe bottom left panel in Figure 7 shows that price responses to a 25 bp interest rate shock\n\nFigure 7: Impulse response functions to a 25 bp real interest rate shock conditional on different states (fixing \\(\\delta=0.02\\) and \\(\\lambda=-0.06\\))\n\n"]}, {"edit": ["\n\n# Amortized Neural Networks for Agent-Based Model Forecasting+\nFootnote \u2020: The views expressed herein are solely those of the authors. The content and results of this research should not be considered or referred to in any publications as the Bank of Russia\u2019s official position, official policy, or decisions. Any errors in this document are the responsibility of the authors.\n\nDenis Koshelev, Alexey Ponomarenko, Sergei Seleznev\n\nBank of RussiaBank of Russia\n\n**Abstract**\n\nIn this paper, we propose a new procedure for unconditional and conditional forecasting in agent-based models. The proposed algorithm is based on the application of amortized neural networks and consists of two steps. The first step simulates artificial datasets from the model. In the second step, a neural network is trained to predict the future values of the variables using the history of observations. The main advantage of the proposed algorithm is its speed. This is due to the fact that, after the training procedure, it can be used to yield predictions for almost any data without additional simulations or the re-estimation of the neural network.\n\n**JEL-classification: C11, C15, C32, C45, C53, C63.**\n\n**Keywords: agent-based models, amortized simulation-based inference, Bayesian models, forecasting, neural networks.**"], "nougat": ["\n\n# Amortized Neural Networks for Agent-Based Model Forecasting+\nFootnote \u2020: The views expressed herein are solely those of the authors. The content and results of this research should not be considered or referred to in any publications as the Bank of Russia\u2019s official position, official policy, or decisions. Any errors in this document are the responsibility of the authors.\n\nDenis Koshelev, Alexey Ponomarenko In this paper, we propose a new procedure for unconditional and conditional forecasting in agent-based models. The proposed algorithm is based on the application of amortized neural networks and consists of two steps. The first step simulates artificial datasets from the model. In the second step, a neural network is trained to predict the future values of the variables using the history of observations. The main advantage of the proposed algorithm is its speed. This is due to the fact that, after the training procedure, it can be used to yield predictions for almost any data without additional simulations or the re-estimation of the neural network.\n\n**JEL-classification: C11, C15, C32, C45, C53, C63. Keywords: agent-based models, amortized simulation-based inference, Bayesian models, forecasting, neural networks.**"]}, {"edit": ["in which \\(s_{1}=(y_{1},x_{1})\\) and \\(s_{2}=(y_{2},x_{2})\\). The dual problem \\(\\mathcal{J}(\\delta)\\) corresponding to the primal problem \\(\\mathcal{I}(\\delta)\\) is defined as follows:\n\n\\[\\mathcal{J}(\\delta)=\\begin{cases}\\inf_{\\lambda\\in\\mathbb{R}_{+}^{2}}\\big{\\{} \\langle\\lambda,\\delta\\rangle+\\sup_{\\varpi\\in\\Pi(\\mu_{13},\\mu_{23})}\\int_{ \\mathcal{V}}f_{\\lambda}d\\varpi\\big{\\}}&\\text{ if }\\delta\\in\\mathbb{R}_{++}^{2},\\\\ \\inf_{\\lambda_{1}\\in\\mathbb{R}_{+}}\\big{\\{}\\lambda_{1}\\delta_{1}+\\sup_{\\varpi \\in\\Pi(\\mu_{13},\\mu_{23})}\\int_{\\mathcal{V}}f_{\\lambda_{1},1}d\\varpi\\big{\\}}& \\text{ if }\\delta_{1}>0\\text{ and }\\delta_{2}=0,\\\\ \\inf_{\\lambda_{2}\\in\\mathbb{R}_{+}}\\big{\\{}\\lambda_{2}\\delta_{2}+\\sup_{\\varpi \\in\\Pi(\\mu_{13},\\mu_{23})}\\int_{\\mathcal{V}}f_{\\lambda_{2},2}d\\varpi\\big{\\}} &\\text{ if }\\delta_{1}=0\\text{ and }\\delta_{2}>0.\\end{cases} \\tag{3.2}\\]\n\n**Theorem 3.2**.: _Suppose that Assumptions 2.1 and 2.3 hold. Then, \\(\\mathcal{I}(\\delta)=\\mathcal{J}(\\delta)\\) for all \\(\\delta\\in\\mathbb{R}_{+}^{2}\\setminus\\{0\\}\\)._\n\nAn interesting feature of the dual for overlapping marginals is that it involves marginal problems with non-overlapping marginals, i.e., \\(\\sup_{\\varpi\\in\\Pi(\\mu_{13},\\mu_{23})}\\int_{\\mathcal{V}}f_{\\lambda}(v)d\\varpi(v)\\), although the uncertainty set in the primal problem involves overlapping marginals. Compared with the non-overlapping marginals case, overlapping marginals in the uncertainty set make the relevant consistent product marginal system in the verification of the existence of a joint measure more complicated, see the proof of Lemma C.5. Nonetheless, the non-overlapping marginals in the dual allow us to apply Theorem A.1 to the marginal problem involving \\(f_{\\lambda}\\), \\(f_{\\lambda,1}\\) and \\(f_{\\lambda,2}\\), see corollary A.2 in Appendix A.2.\n\nUnder the assumptions in Theorem 5.2, we have\n\n\\[\\mathcal{I}(\\delta)=\\inf_{\\lambda\\in\\mathbb{R}_{+}^{2}}\\left[\\langle\\lambda, \\delta\\rangle+\\sup_{\\varpi\\in\\Pi(\\mu_{13},\\mu_{23})}\\int_{\\mathcal{V}}f_{ \\lambda}\\,d\\varpi\\right]\\]\n\nfor all \\(\\delta\\in\\mathbb{R}_{+}^{2}\\).\n\n**Remark 3.3**.: _Similar to the non-overlapping case, we can define an alternative W-DMR with overlapping marginals through linear penalty terms, i.e.,_\n\n\\[\\sup_{\\gamma\\in\\mathcal{P}(\\ "], "nougat": ["in which \\(s_{1}=(y_{1},x_{1})\\) and \\(s_{2}=(y_{2},x_{2})\\). The dual problem \\(\\mathcal{J}(\\delta)\\) corresponding to the primal problem \\(\\mathcal{I}(\\delta)\\) is defined as follows:\n\n\\[\\mathcal{J}(\\delta)=\\begin{cases}\\inf_{\\lambda\\in\\mathbb{R}_{+}^{2}}\\big{\\{} \\langle\\lambda,\\delta\\rangle+\\sup_{\\varpi\\in\\Pi(\\mu_{13},\\mu_{23})}\\int_{ \\mathcal{V}}f_{\\lambda}d\\varpi\\big{\\}}&\\text{ if }\\delta\\in\\mathbb{R}_{++}^{2},\\\\ \\inf_{\\lambda_{1}\\in\\mathbb{R}_{+}}\\big{\\{}\\lambda_{1}\\delta_{1}+\\sup_{\\varpi \\in\\Pi(\\mu_{13},\\mu_{23})}\\int_{\\mathcal{V}}f_{\\lambda_{1},1}d\\varpi\\big{\\}}& \\text{ if }\\delta_{1}>0\\text{ and }\\delta_{2}=0,\\\\ \\inf_{\\lambda_{2}\\in\\mathbb{R}_{+}}\\big{\\{}\\lambda_{2}\\delta_{2}+\\sup_{\\varpi \\in\\Pi(\\mu_{13},\\mu_{23})}\\int_{\\mathcal{V}}f_{\\lambda_{2},2}d\\varpi\\big{\\}} &\\text{ if }\\delta_{1}=0\\text{ and }\\delta_{2}>0.\\end{cases} \\tag{3.2}\\]\n\n**Theorem 3.2**.: _Suppose that Assumptions 2.1 and 2.3 hold. Then, \\(\\mathcal{I}(\\delta)=\\mathcal{J}(\\delta)\\) for all \\(\\delta\\in\\mathbb{R}_{+}^{2}\\setminus\\{0\\}\\)._\n\nAn interesting feature of the dual for overlapping marginals is that it involves marginal problems with non-overlapping marginals, i.e., \\(\\sup_{\\varpi\\in\\Pi(\\mu_{13},\\mu_{23})}\\int_{\\mathcal{V}}f_{\\lambda}(v)d\\varpi(v)\\), although the uncertainty set in the primal problem involves overlapping marginals. Compared with the non-overlapping marginals case, overlapping marginals in the uncertainty set make the relevant consistent product marginal system in the verification of the existence of a joint measure more complicated, see the proof of Lemma C.5 . Nonetheless, the non-overlapping marginals in the dual allow us to apply Theorem A.1 to the marginal problem involving \\(f_{\\lambda}\\), \\(f_{\\lambda,1}\\) and \\(f_{\\lambda,2}\\), see corollary A.2 in Appendix A.2 .\n\nUnder the assumptions in Theorem 5.2 , we have\n\n\\[\\mathcal{I}(\\delta)=\\inf_{\\lambda\\in\\mathbb{R}_{+}^{2}}\\left[\\langle\\lambda, \\delta\\rangle+\\sup_{\\varpi\\in\\Pi(\\mu_{13},\\mu_{23})}\\int_{\\mathcal{V}}f_{ \\lambda}\\,d\\varpi\\right]\\]\n\nfor all \\(\\delta\\in\\mathbb{R}_{+}^{2}\\).\n\n**Remark 3.3.**_Similar to the non-overlapping case, we can define an alternative W-DMR with overlapping marginals through linear penalty terms, i.e.,_\n\n\\[\\sup_{\\gamma\\in\\mathcal{P}( "]}, {"edit": ["coins are \\(n_{k,0}\\) with \\(\\sum_{k=1}^{K}n_{k,0}=N\\). We define the _share_ as the fraction of coins each miner owns. So the initial shares (\\(\\pi_{k,0}\\), \\(k\\in[K]\\)) are given by\n\n\\[\\pi_{k,0}:=\\frac{n_{k,0}}{N},\\quad k\\in[K]. \\tag{2.1}\\]\n\nSimilarly, denote by \\(n_{k,t}\\) the number of coins owned by miner \\(k\\) at time \\(t\\), and the corresponding share is\n\n\\[\\pi_{k,t}:=\\frac{n_{k,t}}{N_{t}},\\quad k\\in[K],\\quad\\text{with }N_{t}:=\\sum_{k=1} ^{K}n_{k,t}. \\tag{2.2}\\]\n\nHere \\(N_{t}\\) is the total number of coins at time \\(t\\), with \\(N_{0}=N\\). (We shall often refer to \\(N_{t}\\) as the \"volume of coins\", or simply \"volume\".)\n\nAt time \\(t\\), miner \\(k\\) is selected at random with probability \\(\\pi_{k,t-1}\\). Once selected, the miner receives a deterministic reward of \\(R_{t}\\in\\mathbb{R}_{+}\\) coins (which may include transaction fees and block rewards). Denote by \\(S_{k,t}\\) the random event that miner \\(k\\) is selected at time \\(t\\). So the number of coins owned by each miner evolves as\n\n\\[n_{k,t}=n_{k,t-1}+R_{t}1_{S_{k,t}},\\quad k\\in[K]. \\tag{2.3}\\]\n\nNote that the volume satisfies \\(N_{t}=N_{t-1}+R_{t}\\). Combining (2.2) and (2.3) yields a recursion of the shares:\n\n\\[\\pi_{k,t}=\\frac{N_{t-1}}{N_{t}}\\pi_{k,t-1}+\\frac{R_{t}}{N_{t}}1_{S_{k,t}}, \\quad k\\in[K]. \\tag{2.4}\\]\n\nwhich is a (time-dependent) Polya urn model [26].\n\nWe consider the long-time evolution of the shares (\\(\\pi_{k,t}\\), \\(k\\in[K]\\)). Let \\(\\mathcal{F}_{t}\\) be the filtration generated by the random events (\\(S_{k,r}:k\\in[K],r\\leqslant t\\)). Observe that for each \\(k\\in[K]\\), the process (\\(\\pi_{k,t}\\), \\(t\\geqslant 0\\)) is an \\(\\mathcal{F}_{t}\\)-martingale. By the martingale convergence theorem (see [15, Theorem 4.2.11]),\n\n\\[(\\pi_{1,t},\\ldots,\\pi_{K,t})\\longrightarrow(\\pi_{1,\\infty},\\ldots,\\pi_{K, \\infty})\\quad\\text{as }t\\rightarrow\\infty\\text{ with probability }1, \\tag{2.5}\\]\n\nwhere (\\(\\pi_{1,\\infty},\\ldots,\\pi_{K,\\infty}\\)) is some random probability distribution on \\([K]\\).\n\nTo quantify the wealth evolution of miner \\( "], "nougat": ["coins are \\(n_{k,0}\\) with \\(\\sum_{k=1}^{K}n_{k,0}=N\\). We define the _share_ as the fraction of coins each miner owns. So the initial shares (\\(\\pi_{k,0}\\), \\(k\\in[K]\\)) are given by\n\n\\[\\pi_{k,0}:=\\frac{n_{k,0}}{N},\\quad k\\in[K]. \\tag{2.1}\\]\n\nSimilarly, denote by \\(n_{k,t}\\) the number of coins owned by miner \\(k\\) at time \\(t\\), and the corresponding share is\n\n\\[\\pi_{k,t}:=\\frac{n_{k,t}}{N_{t}},\\quad k\\in[K],\\quad\\text{with }N_{t}:=\\sum_{k=1} ^{K}n_{k,t}. \\tag{2.2}\\]\n\nHere \\(N_{t}\\) is the total number of coins at time \\(t\\), with \\(N_{0}=N\\). (We shall often refer to \\(N_{t}\\) as the \"volume of coins\u201d, or simply \u201cvolume\u201d.)\n\nAt time \\(t\\), miner \\(k\\) is selected at random with probability \\(\\pi_{k,t-1}\\). Once selected, the miner receives a deterministic reward of \\(R_{t}\\in\\mathbb{R}_{+}\\) coins (which may include transaction fees and block rewards). Denote by \\(S_{k,t}\\) the random event that miner \\(k\\) is selected at time \\(t\\). So the number of coins owned by each miner evolves as\n\n\\[n_{k,t}=n_{k,t-1}+R_{t}1_{S_{k,t}},\\quad k\\in[K].\\] (2.2) and (2.3) yields a recursion of the shares: \\[\\pi_{k,t}=\\frac{N_{t-1}}{N_{t}}\\pi_{k,t-1}+\\frac{R_{t}}{N_{t}}1_{S_{k,t}}, \\quad k\\in[K]. \\tag{2.4}\\]\n\nwhich is a (time-dependent) P \u0301olya urn model [26].\n\nWe consider the long-time evolution of the shares (\\(\\pi_{k,t}\\), \\(k\\in[K]\\)). Let \\(\\mathcal{F}_{t}\\) be the filtration generated by the random events (\\(S_{k,r}:k\\in[K],r\\leqslant t\\)). Observe that for each \\(k\\in[K]\\), the process (\\(\\pi_{k,t}\\), \\(t\\geqslant 0\\)) is an \\(\\mathcal{F}_{t}\\)-martingale. By the martingale convergence theorem (see [15, Theorem 4.2.11]),\n\n\\[(\\pi_{1,t},\\ldots,\\pi_{K,t})\\longrightarrow(\\pi_{1,\\infty},\\ldots,\\pi_{K, \\infty})\\quad\\text{as }t\\rightarrow\\infty\\text{ with probability }1, \\tag{2.5}\\]\n\nwhere (\\(\\pi_{1,\\infty},\\ldots,\\pi_{K,\\infty}\\)) is some random probability distribution on \\([K]\\).\n\nTo quantify the wealth evolution of miner \\(k\\), there are two obvious metrics:\n\n\\[|\\pi_{k,t}-\\pi_{k,0}|\\text{ (difference "]}, {"edit": ["shortly) coordination on \\(\\overline{a}\\) whenever \\(x>\\tilde{x}\\), where \\(\\tilde{x}\\in\\mathbb{R}\\) is the _critical state_ which she - the planner - chooses. The planner faces two constraints. First, she cannot condition her policy on the realization of \\(x\\) or players' signals thereof. One interpretation is that the planner must commit to her policy before Nature draws a true \\(x\\) and cannot change her policy afterward.\n\nThe second constraint upon the planner's problem has to do with the kinds of policies she can use. We assume that the planner cannot coordinate players on her preferred equilibrium in a multiple equilibria setting. Instead, she has to rely on simple subsidies (or taxes) to create the appropriate incentives. The focus on simple instruments also means that policies cannot condition directly upon other players' actions. These are standard assumptions in the literature on policy or contract design for coordination problems (Segal, 2003; Winter, 2004; Bernstein and Winter, 2012; Sakovics and Steiner, 2012; Halac et al., 2020).\n\nTo streamline the narrative, we henceforth focus on subsidies as the planner's policy instrument. Let \\(s_{i}\\) denote the subsidy paid to a player \\(i\\) who chooses \\(a_{i}=1\\). Conditional on the subsidy \\(s_{i}\\), player \\(i\\)'s incentive to choose 1 becomes\n\n\\[u_{i}(a_{-i}\\mid x,s_{i})=u_{i}(a_{-i}\\mid x)+s_{i}=x+w_{i}(a_{-i})-c_{i}+s_{i},\\]\n\nand the expected incentive, given the signal \\(x_{i}^{\\varepsilon}\\) and a strategy vector \\(p_{-i}\\), is\n\n\\[\\begin{split} u_{i}^{\\varepsilon}(p_{-i}\\mid x_{i}^{\\varepsilon} ,s_{i})&=\\int u_{i}(p_{-i}(x_{-i}^{\\varepsilon})\\mid x,s_{i})\\, \\mathrm{d}F_{i}^{\\varepsilon}(x,x_{-i}^{\\varepsilon}\\mid x_{i}^{\\varepsilon}) \\\\ &=\\int\\left[u_{i}(p_{-i}(x_{-i}^{\\varepsilon})\\mid x)+s_{i}\\right] \\mathrm{d}F_{i}^{\\varepsilon}(x,x_{-i}^{\\varepsilon}\\mid x_{i}^{\\varepsilon}) \\\\ &=u_{i}^{\\varepsilon}(p_{-i}\\mid x_{i}^{\\varepsilon})+s_{i}.\\end{split} \\tag{4}\\]\n\nIt is clear that a tax equal to \\(s_{i}\\) on playing 0 has the same effect on incentives. Note that (4) assumes observability of \\(a_{i}\\); this assumption is maintained throughout most of the analysis. Section 6.1 considers principal-agent problems in which the vector of actions \\(a\\) is unobserved.\n\n "], "nougat": ["shortly) coordination on \\(\\overline{a}\\) whenever \\(x>\\tilde{x}\\), where \\(\\tilde{x}\\in\\mathbb{R}\\) is the _critical state_ which she - the planner - chooses. The planner faces two constraints. First, she cannot condition her policy on the realization of \\(x\\) or players' signals thereof. One interpretation is that the planner must commit to her policy before Nature draws a true \\(x\\) and cannot change her policy afterward.\n\nThe second constraint upon the planner\u2019s problem has to do with the kinds of policies she can use. We assume that the planner cannot coordinate players on her preferred equilibrium in a multiple equilibria setting. Instead, she has to rely on simple subsidies (or taxes) to create the appropriate incentives. The focus on simple instruments also means that policies cannot condition directly upon other players\u2019 actions. These are standard assumptions in the literature on policy or contract design for coordination problems (Segal, 2003; Winter, 2004; Bernstein and Winter, 2012; Sakovics and Steiner, 2012; Halac et al., 2020).\n\nTo streamline the narrative, we henceforth focus on subsidies as the planner\u2019s policy instrument. Let \\(s_{i}\\) denote the subsidy paid to a player \\(i\\) who chooses \\(a_{i}=1\\). Conditional on the subsidy \\(s_{i}\\), player \\(i\\)'s incentive to choose 1 becomes\n\n\\[u_{i}(a_{-i}\\mid x,s_{i})=u_{i}(a_{-i}\\mid x)+s_{i}=x+w_{i}(a_{-i})-c_{i}+s_{i},\\]\n\nand the expected incentive, given the signal \\(x_{i}^{\\varepsilon}\\) and a strategy vector \\(p_{-i}\\), is\n\n\\[\\begin{split} u_{i}^{\\varepsilon}(p_{-i}\\mid x_{i}^{\\varepsilon} ,s_{i})&=\\int u_{i}(p_{-i}(x_{-i}^{\\varepsilon})\\mid x,s_{i})\\, \\mathrm{d}F_{i}^{\\varepsilon}(x,x_{-i}^{\\varepsilon}\\mid x_{i}^{\\varepsilon}) \\\\ &=\\int\\left[u_{i}(p_{-i}(x_{-i}^{\\varepsilon})\\mid x)+s_{i}\\right] \\mathrm{d}F_{i}^{\\varepsilon}(x,x_{-i}^{\\varepsilon}\\mid x_{i}^{\\varepsilon}) \\\\ &=u_{i}^{\\varepsilon}(p_{-i}\\mid x_{i}^{\\varepsilon})+s_{i}.\\end{split} \\tag{4}\\]\n\nIt is clear that a tax equal to \\(s_{i}\\) on playing 0 has the same effect on incentives. Note that (4) assumes observability of \\(a_{i}\\); this assumption is maintained throughout most of the analysis. Section 6.1 considers principal-agent problems in which the vector of actions \\(a\\) is unobserved.\n\n "]}, {"edit": ["there exists suitable \\(\\rho\\), \\(\\phi_{1}\\) and \\(\\phi_{2}\\) such that\n\n\\[\\pi_{jt}=\\rho(\\phi_{1}(X_{jt},p_{jt},\\mu_{jt}(\\gamma_{0}))+\\sum_{k\\neq j,k\\in \\mathcal{S}}\\phi_{2}(X_{kt},p_{jt},\\mu_{kt}(\\gamma_{0}))).\\]\n\nThe result follows straightforwardly from the observation that after controlling for \\(CF(\\mu_{jt})\\) the unobservable component \\(\\tilde{\\varepsilon}\\) is exogenous. This implies the aggregate demand function is invariant under any permutation applied to competitors of product \\(j\\). The result demonstrates that endogeneity can be addressed by using the residuals from Equation 5 along with product observable characteristics simply as an additional set of features.\n\n### Inference\n\nThe aim of this paper is to estimate choice functions flexibly using non-parametric estimators. However, often in social science contexts, one is also interested in conducting inference over some economic objects. Now, given non-parametric regression functions are estimated at a slower rate compared to parametric regressions it is often infeasible to construct confidence intervals directly on the estimated \\(\\hat{\\pi}\\). Fortunately, it is generally possible to perform inference and construct valid confidence intervals for specific economic objects that are functions of \\(\\pi\\). In this section, we will provide example of one such important economic object and demonstrate how to construct valid confidence intervals for it. This will be done by leveraging the recent advances in automatic debiased machine learning as shown in the works of Ichimura and Newey (2022); Chernozhukov et al. (2022); Chernozhukov et al. (2022); Chernozhukov et al. (2021), and others. The only difference between our setup and existing automatic debiased machine learning is that we have to account for an additional first-stage estimator \\(\\hat{\\gamma}\\). However, as we will show later in this section if \\(\\hat{\\gamma}\\) is estimable at a fast enough rate the same analysis of Chernozhukov et al. (2022) applies.\n\n**Average Effect of Price Change**: Let \\(w_{jt}=(y_{jt},p_{jt},x_{jt},\\{p_{kt},x_{kt}\\}_{k\\neq j})\\) and \\(z_{jt}=(p_{jt},x_{jt},\\{p_{kt},x_{kt}\\}_{k\\neq j})\\) represent the variables associated with product \\(j\\) in market \\(t\\). Here, \\(p_{jt}\\in\\mathbb{C}\\) denotes the observed prices, \\(x_{jt}\\in\\mathbb{C}^{d}\\) represents other exogenous product characteristics and \\(y_{jt}\\in\\mathbb{R}\\) refers to the observed demand for product \\(j\\) in market \\(t\\), such as market shares or log shares. In this context, researchers are often interested in estimating the average effect of a price change on the demand for a product.\n\nThe average effect of a price change can be expressed as the difference between the demand function \\(\\pi_{jt}(\\cdot;\\gamma)\\) evaluated at the original price \\(p_{jt}\\) and at the price incremented by "], "nougat": ["there exists suitable \\(\\rho\\), \\(\\phi_{1}\\) and \\(\\phi_{2}\\) such that\n\n\\[\\pi_{jt}=\\rho(\\phi_{1}(X_{jt},p_{jt},\\mu_{jt}(\\gamma_{0}))+\\sum_{k\\neq j,k\\in \\mathcal{S}}\\phi_{2}(X_{kt},p_{jt},\\mu_{kt}(\\gamma_{0}))).\\]\n\nThe result follows straightforwardly from the observation that after controlling for \\(CF(\\mu_{jt})\\) the unobservable component \\(\\tilde{\\varepsilon}\\) is exogenous. This implies the aggregate demand function is invariant under any permutation applied to competitors of product \\(j\\). The result demonstrates that endogeneity can be addressed by using the residuals from Equation 5 along with product observable characteristics simply as an additional set of features.\n\n### Inference\n\nThe aim of this paper is to estimate choice functions flexibly using non-parametric estimators. However, often in social science contexts, one is also interested in conducting inference over some economic objects. Now, given non-parametric regression functions are estimated at a slower rate compared to parametric regressions it is often infeasible to construct confidence intervals directly on the estimated \\(\\hat{\\pi}\\). Fortunately, it is generally possible to perform inference and construct valid confidence intervals for specific economic objects that are functions of \\(\\pi\\). In this section, we will provide example of one such important economic object and demonstrate how to construct valid confidence intervals for it. This will be done by leveraging the recent advances in automatic debiased machine learning as shown in the works of Ichimura and Newey (2022 ); Chernozhukov, Newey, and R. Singh (2022 ); Chernozhukov, Escanciano, et al. (2022 ); Chernozhukov, Newey, Quintas-Martinez, et al. (2021 ), and others. The only difference between our setup and existing automatic debiased machine learning is that we have to account for an additional first-stage estimator \\(\\hat{\\gamma}\\). However, as we will show later in this section if \\(\\hat{\\gamma}\\) is estimable at a fast enough rate the same analysis of Chernozhukov, Escanciano, et al. (2022 ) applies.\n\n**Average Effect of Price Change**: Let \\(w_{jt}=(y_{jt},p_{jt},x_{jt},\\{p_{kt},x_{kt}\\}_{k\\neq j})\\) and \\(z_{jt}=(p_{jt},x_{jt},\\{p_{kt},x_{kt}\\}_{k\\neq j})\\) represent the variables associated with product \\(j\\) in market \\(t\\). Here, \\(p_{jt}\\in\\mathbb{C}\\) denotes the observed prices, \\(x_{jt}\\in\\mathbb{C}^{d}\\) represents other exogenous product characteristics and \\(y_{jt}\\in\\mathbb{R}\\) refers to the observed demand for product \\(j\\) in market \\(t\\), such as market shares or log shares. In this context, researchers are often interested in estimating the average effect of a price change on the demand for a product.\n\nThe average effect of a price change can be expressed as the difference between the demand function \\(\\pi_{jt}(\\cdot;\\gamma)\\) evaluated at the original price \\(p_{jt}\\) and at the price incremented by "]}, {"edit": ["into the potential outcomes model. Our distinct goal is to provide statistical methods to credibly infer heterogeneous treatment effects (HTEs) using experimental network data sets. The knowledge of HTEs is useful in the design of welfare-maximizing policies as it allows for the targeting of specific subgroups that would benefit the most from a particular intervention. For instance, a study by Viviano (2019) demonstrates how to use HTE estimates to improve a weather insurance policy take-up among rice farmers in rural China. Similarly, Han et al. (2022) use HTE estimates to design multinomial success rules in populations where interactions occur within non-overlapping groups. To achieve our goal, we develop randomization testing methods that are valid asymptotically for three useful notions of HTEs in the presence of network interference: (i) the null hypothesis of constant treatment effects across the population; (ii) heterogeneous treatment effects across network exposure values only; and (iii) heterogeneous treatment effects across network exposure values and covariate-defined discrete groups only.\n\nOur reliance on a randomization-based testing method for HTEs in the current paper is motivated by two main reasons. First, since units are linked through social networks, we cannot assume that the variables of units are independent. Therefore, the traditional asymptotic-based inferential methods are not directly applicable. The second advantage of the randomization-based testing method is that it is fully parametric and provides exact p-values without imposing restrictive conditions on the data generating process (DGP) for _sharp_ null hypotheses.1 Moreover, recent studies show that even for non-sharp null hypotheses, there are conditional randomization-based methods that can generate exact conditional p-values without any assumptions on the DGP.\n\nFootnote 1: Under a sharp null hypothesis, all potential outcomes for each unit can be imputed.\n\nThe null hypotheses we consider in this paper are _not_ sharp due to two reasons. First, our null hypotheses contain nuisance parameters which are unknown values in the science tables one will construct under the null. Hence, one can only partially impute the potential outcomes that depend on these nuisance parameters under the null hypotheses. The problem of nuisance parameters in null hypotheses is not exclusive to network interference, but there may be more of these parameters present in this setting. (See Ding et al. (2016) for a hypothesis of constant treatment effects under no interference). Second, under network interference, the number of potential outcomes depends on the exposure mapping one imposes. Therefore, without additional restrictions on the underlying DGP, we cannot impute all missing potential outcomes under the null hypotheses. As a result "], "nougat": ["into the potential outcomes model. Our distinct goal is to provide statistical methods to credibly infer heterogeneous treatment effects (HTEs) using experimental network data sets. The knowledge of HTEs is useful in the design of welfare-maximizing policies as it allows for the targeting of specific subgroups that would benefit the most from a particular intervention. For instance, a study by Viviano (2019) demonstrates how to use HTE estimates to improve a weather insurance policy take-up among rice farmers in rural China. Similarly, Han et al. (2022) use HTE estimates to design multinomial success rules in populations where interactions occur within non-overlapping groups. To achieve our goal, we develop randomization testing methods that are valid asymptotically for three useful notions of HTEs in the presence of network interference: (i) the null hypothesis of constant treatment effects across the population; (ii) heterogeneous treatment effects across network exposure values only; and (iii) heterogeneous treatment effects across network exposure values and covariate-defined discrete groups only.\n\nOur reliance on a randomization-based testing method for HTEs in the current paper is motivated by two main reasons. First, since units are linked through social networks, we cannot assume that the variables of units are independent. Therefore, the traditional asymptotic-based inferential methods are not directly applicable. The second advantage of the randomization-based testing method is that it is fully parametric and provides exact p-values without imposing restrictive conditions on the data generating process (DGP) for _sharp_ null hypotheses.1 Moreover, recent studies show that even for non-sharp null hypotheses, there are conditional randomization-based methods that can generate exact conditional p-values without any assumptions on the DGP.\n\nFootnote 1: Under a sharp null hypothesis, all potential outcomes for each unit can be imputed.\n\nThe null hypotheses we consider in this paper are _not_ sharp due to two reasons. First, our null hypotheses contain nuisance parameters which are unknown values in the science tables one will construct under the null. Hence, one can only partially impute the potential outcomes that depend on these nuisance parameters under the null hypotheses. The problem of nuisance parameters in null hypotheses is not exclusive to network interference, but there may be more of these parameters present in this setting. (See Ding et al. (2016) for a hypothesis of constant treatment effects under no interference). Second, under network interference, the number of potential outcomes depends on the exposure mapping one imposes. Therefore, without additional restrictions on the underlying DGP, we cannot impute all missing potential outcomes under the null hypotheses. As a result "]}, {"edit": ["\n\n## 3 Methodology\n\n### Data Overview\n\nThe Features of the data which has been provided below\n\n1. Date (DD/MM/YYYY)\n2. Max.Demand met during the day (MW)\n3. Shortage during maximum Demand (MW)\n4. Energy Met (MU)\n5. Drawal Schedule (MU)\n6. OD(+)/UD(-) (MU)\n7. Max OD (MW)\n8. Energy Shortage (MU)\n\nEnergy Shortage (MU) feature is not available every day. This feature is recorded from 2017-05-09 as per the reports generated by the PSP by POSOCO. The data is available here [0]. This paper focuses on Univariant Analysis not to make it as complex, but to consider Max Demand met during the day (MW) as a single column.\n\nAs the column \"Max Demand met During the day\" is the major feature that we consider. The data starts from 2013-04-01 to 2023-05-31 which consists of 3713 days but the data points only consist of 3640 with the missing data we use the Imputation Techniques dataset and Non-Imputation Techniques dataset (no_null).\n\nFor Imputation Techniques, this paper considers Mean, Median, Mode, and Linear Interpolation Imputation data that has been imputed. So, as combined this generates 5 datasets where the models are applied to compare the performance of which Imputation is good.\n\nTable 1 depicts the datasets which are created from the univariant data taken from one of the features in the dataset. (\"Max. Demand met during the day (MW) \"). As explained in the Methodology section the dataset is divided into train and test to take the MAPE score. For ARIMA models we generate train MAPE and test MAPE to check the overfitting criteria also with AIC, BIC, log(p), etc...\n\n## 4 Forecasting Models\n\nThis paper develops a Time series forecasting model ARIMA which is known to be an Auto-Regressive Integrated Moving Average. the models which include AR, MA and ARMA, and ARIMA, and develop a model list from these regression types using the parameters. The major parameters included in the arima model are p, q, and d where p is the parameter for Auto-regressive co-efficient which says about how many days have the co-relation between today's date.\n\nThe real-world data tends to be always non-Stationary. A signal is said to be stationary if its statistical properties like mean, standard deviation, trend, etc... doesn't change over time. To check if the time series is stationary or not, we use\n\n\\begin{table}\n\\begin{tabular}{|c|c|} \\hline\n1 & dropna-dataset \\\\ \\hline\n2 & mean Imputation dataset \\\\ \\hline\n3 & median Imputation dataset \\\\ \\hline\n4 & mode Imputation dataset \\\\ \\hline\n5 & linear-Interpolation Imputation dataset \\\\ \\hline \\end{tabular}\n\\end{table}\nTable 1: Datasets names which are created from the reports"], "nougat": ["\n\n## 3 Methodology\n\n### Data Overview\n\nThe Features of the data which has been provided below\n\n1. Date (DD/MM/YYYY)\n2. Max.Demand met during the day (MW) as a single column.\n3. Shortage during maximum Demand (MW)\n4. Energy Met (MU)\n5. Drawal Schedule (MU)\n6. OD(+)/UD(-) (MU)\n7. Max OD (MW)\n8. Energy Shortage (MU)\n\nEnergy Shortage (MU) feature is not available every day. This feature is recorded from 2013-04-01 to 2023-05-31 which consists of 3713 days but the data points only consist of 3640 with the missing data we use the Imputation Techniques dataset and Non-Imputation Techniques dataset (no_null).\n\nFor Imputation Techniques, this paper considers Mean, Median, Mode, and Linear Interpolation Imputation data that has been imputed. So, as combined this generates 5 datasets where the models are applied to compare the performance of which Imputation is good.\n\nTable 1 depicts the datasets which are created from the univariant data taken from one of the features in the dataset. (\"Max. Demand met during the day (MW) \"). As explained in the Methodology section the dataset is divided into train and test to take the MAPE score. For ARIMA models we generate train MAPE and test MAPE to check the overfitting criteria also with AIC, BIC, log(p), etc...\n\n## 4 Forecasting Models\n\nThis paper develops a Time series forecasting model ARIMA which is known to be an Auto-Regressive Integrated Moving Average. the models which include AR, MA and ARMA, and ARIMA, and develop a model list from these regression types using the parameters. The major parameters included in the arima model are p, q, and d where p is the parameter for Auto-regressive co-efficient which says about how many days have the co-relation between today\u2019s date.\n\nThe real-world data tends to be always non-Stationary. A signal is said to be stationary if its statistical properties like mean, standard deviation, trend, etc... doesn\u2019t change over time. To check if the time series is stationary or not, we use\n\n\\begin{table}\n\\begin{tabular}{|c|c|} \\hline\n1 & dropna-dataset \\\\ \\hline\n2 & mean Imputation dataset \\\\ \\hline\n3 & median Imputation dataset \\\\ \\hline\n4 & mode Imputation dataset \\\\ \\hline\n5 & linear-Interpolation Imputation dataset \\\\ \\hline \\end{tabular}\n\\end{table}\nTable 1: Datasets names which are created from the reports\n\n\\begin{table}\n\\begin{tabular}{|c|c|} \\hline\n**Clusters** & **Paper Citations** \\\\ \\hline Machine Learning & [0] [0] [0] [0] \\\\ \\hline Deep Learning & [0] [0] [0] [0] [0] [0] \\\\ \\hline Auto-Regressive Models & [0] [0] [0] \\\\ \\hline \\end{tabular}\n\\end{table}\nTable 2: Datasets names which are created from the reports"]}, {"edit": ["However, if the approximation error is not a major concern (which is, for example, the case if the order of approximation can be chosen sufficiently large) then one can choose \\(s=t\\) in which case \\(E_{t}f\\left(x\\right)=e^{\\left(B-\\partial_{t}\\right)t}u_{0,t}\\left(x\\right)\\) and the following special case of (2.15) can be employed,\n\n\\[\\hat{E}_{t}f\\left(x\\right)=\\sum_{m=0}^{M}\\frac{t^{m}}{m!}\\left(B-\\partial_{t} \\right)^{m}u_{0,t}\\left(x\\right). \\tag{2.16}\\]\n\nWe show in Appendix A that (2.16) is a generalized version of the proposal of Kristensen and Mele (2011) which in turn contains as special cases the expansions of Yang et al. (2019) and Wan and Yang (2021).\n\n## 3 Implementation of expansion for jump-diffusion models\n\nThis section provides details regarding the practical implementation of the proposed approximation in the jump-diffusion case. We here focus on the special case of \\(r\\left(x\\right)=0\\) and \\(s=t\\), in which case \\(u_{t}\\left(x\\right)=E_{t}f\\left(x\\right)=\\mathbb{E}\\left[f\\left(x_{t}\\right) \\left|x_{0}=x\\right|\\right.\\) and\n\n\\[\\hat{u}_{t}\\left(x\\right)=\\sum_{m=0}^{M}\\frac{t^{m}}{m!}\\left(A-\\partial_{t} \\right)^{m}u_{0,t}\\left(x\\right). \\tag{3.1}\\]\n\nThis is done to avoid overly complicated notation. Most of the ideas and arguments extend to the general case.\n\n### Choice of smoothing function for irregular moments\n\nFollowing Kristensen and Mele (2011), a simple choice of \\(u_{0,s}\\left(x\\right)\\) that satisfies A.1 is \\(u_{0,s}\\left(x\\right)=E_{0,s}f\\left(x\\right)=\\mathbb{E}\\left[f\\left(x_{0,s} \\right)\\left|x_{0,0}=x\\right]\\) where \\(x_{0,s}\\) is chosen as the solution to an auxiliary jump-diffusion model,\n\n\\[dx_{0,t}=\\mu_{0}\\left(x_{0,t}\\right)dt+\\sigma_{0}\\left(x_{0,t}\\right)dW_{t}+J_ {0,t}dN_{0,t}, \\tag{3.2}\\]\n\nwhere \\(N_{0,t}\\) is a Poisson process with jump intensity \\(\\lambda_{0}\\left(x\\right)\\) and \\(J_{0,t}\\) has density \\(\\nu_{0}\\left(\\cdot|x\\right)\\). The auxiliary model should be chosen so that \\(u_{0,t}\\left(x\\right)\\) is available on closed form. One such model is the multivariate Brownian motion with drift model,\n\n\\[dx_{0,t}=\\mu_{0}dt+\\sigma_{0}dW_{t} \\tag{3.3}\\]\n\nwhere \\(\\mu_{0}\\in\\mathbb{R}^{d}\\) and \\(\\sigma_{0}\\in\\mathbb{R}^{d\\times d}\\) are constants, or the multivariate Vasicek (Ornstein-Uhlenbeck) model,\n\n\\[dx_{0,t}=\\left(\\mu_{0}+Ax_{0t}\n\n "], "nougat": ["However, if the approximation error is not a major concern (which is, for example, the case if the order of approximation can be chosen sufficiently large) then one can choose \\(s=t\\) in which case \\(E_{t}f\\left(x\\right)=e^{\\left(B-\\partial_{t}\\right)t}u_{0,t}\\left(x\\right)\\) and the following special case of ( 2.15 ) can be employed,\n\n\\[\\hat{E}_{t}f\\left(x\\right)=\\sum_{m=0}^{M}\\frac{t^{m}}{m!}\\left(B-\\partial_{t} \\right)^{m}u_{0,t}\\left(x\\right).\\] (2.16 ) is a generalized version of the proposal of Kristensen and Mele ( 2011 ) which in turn contains as special cases the expansions of Yang et al. ( 2019 ) and Wan and Yang ( 2021 ).\n\n## 3 Implementation of expansion for jump-diffusion models\n\nThis section provides details regarding the practical implementation of the proposed approximation in the jump\u2013diffusion case. We here focus on the special case of \\(r\\left(x\\right)=0\\) and \\(s=t\\), in which case \\(u_{t}\\left(x\\right)=E_{t}f\\left(x\\right)=\\mathbb{E}\\left[f\\left(x_{t}\\right) \\left|x_{0}=x\\right|\\right.\\) and\n\n\\[\\hat{u}_{t}\\left(x\\right)=\\sum_{m=0}^{M}\\frac{t^{m}}{m!}\\left(A-\\partial_{t} \\right)^{m}u_{0,t}\\left(x\\right). \\tag{3.1}\\]\n\nThis is done to avoid overly complicated notation. Most of the ideas and arguments extend to the general case.\n\n### Choice of smoothing function for irregular moments\n\nFollowing Kristensen and Mele ( 2011 ), a simple choice of \\(u_{0,s}\\left(x\\right)\\) that satisfies A.1 is \\(u_{0,s}\\left(x\\right)=E_{0,s}f\\left(x\\right)=\\mathbb{E}\\left[f\\left(x_{0,s} \\right)\\left|x_{0,0}=x\\right|\\right.\\) where \\(x_{0,s}\\) is chosen as the solution to an auxiliary jump\u2013diffusion model,\n\n\\[dx_{0,t}=\\mu_{0}\\left(x_{0,t}\\right)dt+\\sigma_{0}\\left(x_{0,t}\\right)dW_{t}+J _{0,t}dN_{0,t}, \\tag{3.2}\\]\n\nwhere \\(N_{0,t}\\) is a Poisson process with jump intensity \\(\\lambda_{0}\\left(x\\right)\\) and \\(J_{0,t}\\) has density \\(\\nu_{0}\\left(\\cdot|x\\right)\\). The auxiliary model should be chosen so that \\(u_{0,t}\\left(x\\right)\\) is available on closed form. One such model is the multivariate Brownian motion with drift model,\n\n\\[dx_{0,t}=\\mu_{0}dt+\\sigma_{0}dW_{t} \\tag{3.3}\\]\n\nwhere \\(\\mu_{0}\\in\\mathbb{R}^{d}\\) and \\(\\sigma_{0}\\in\\mathbb{R}^{d\\times d}\\) are constants, or the multivariate Vasicek (Ornstein-Uhlenbeck) model,\n\n\\[dx_{0,t}=\\left(\\mu_{0}+Ax_{0t}\\ "]}, {"edit": ["We extend the network coordination game to include global effects as follows. Given player \\(i\\), and a profile \\(a_{-i}\\) of actions of other players in the network, let \\(p_{i}(a_{-i})=\\frac{1}{I-d_{i}-1}\\sum_{j\\not\\in N_{i}\\cup\\{i\\}}a_{j}\\) be the fraction of the network (not including \\(i\\) or neighbors of \\(i\\)) that plays 1. Player \\(i\\) gets some additional benefit from playing 1 that depends positively on this aggregate proportion. This is formalized by a (weakly) increasing function \\(\\varphi_{i}:[0,1]\\rightarrow\\mathbb{R}_{+}\\) such that payoff to player \\(i\\) from playing 1 is given by \\(u_{i}(1,a_{-i})=\\sum_{j\\in N_{i}}w_{i,j}\\hat{u}_{i}(1,a_{j})+\\varphi_{i}(p_{i}( a_{-i}))\\) with the natural normalization that \\(\\varphi_{i}(0)=0\\). When convenient, we suppress the notation \\(a_{-i}\\) in \\(p_{i}(a_{-i})\\).\n\nIn a _network coordination game with local and global effects_ payoff of each player \\(i\\) is given by \\(u_{i}(1,a_{-i})=\\sum_{j\\in N_{i}}w_{i,j}\\hat{u}_{i}(1,a_{j})+\\varphi_{i}(p_{i} (a_{-i}))\\), and \\(u_{i}(0,a_{-i})=0\\), where \\(\\varphi_{i}:[0,1]\\rightarrow\\mathbb{R}_{+}\\) is a (weakly) increasing function with \\(\\varphi_{i}(0)=0\\). Notice that a network coordination game with local and global effects is a game with strategic complements, because payoff of each player satisfies increasing differences: \\(u_{i}(1,a_{-i})-u_{i}(0,a_{-i})\\) is (weakly) increasing in \\(a_{-i}\\). Indeed, this holds for a (weakly) increasing function of the form \\(\\varphi_{i}(a_{-i})\\), without composing with \\(p_{i}(a_{-i})\\), and our results hold for this generalization. We use \\(p_{i}(a_{-i})\\) to aid intuition for global effects. Moreover, it follows immediately that a network coordination game with local effects is the special case when each \\(\\varphi_{i}\\) is identically zero. Our formulation is more general than assuming a weight \\(w_{i,j}\\geq 0\\) for each non-neighbor \\(j\\) of \\(i\\).\n\nWith heterogeneous local and global effects, 1 **is a best response of player \\(i\\)**, if, and only if, \\(u_{i}(1,a_{-i})\\geq u_{i}(0,a_{-i})\\), which holds exactly when \\(\\frac{s_{i}}{w_{i}}\\geq q-\\frac{\\varphi_{i}(p_{i})}{w_{i}(b+c)}\\), where \\(s_{i}\\) and \\(q\\) are as above. As \\(\\varphi_{i}\\) is (weakly) increasing, the threshold (weighted) fraction of neighbors who play 1 before \\(i\\) switches to 1 decreases in \\(p_{i}\\). In other words, with global effects, a smaller fraction of a "], "nougat": ["We extend the network coordination game to include global effects as follows. Given player \\(i\\), and a profile \\(a_{-i}\\) of actions of other players in the network, let \\(p_{i}(a_{-i})=\\frac{1}{I-d_{i}-1}\\sum_{j\\not\\in N_{i}\\cup\\{i\\}}a_{j}\\) be the fraction of the network (not including \\(i\\) or neighbors of \\(i\\)) that plays 1. Player \\(i\\) gets some additional benefit from playing 1 that depends positively on this aggregate proportion. This is formalized by a (weakly) increasing function \\(\\varphi_{i}:[0,1]\\rightarrow\\mathbb{R}_{+}\\) such that payoff to player \\(i\\) from playing 1 is given by \\(u_{i}(1,a_{-i})=\\sum_{j\\in N_{i}}w_{i,j}\\hat{u}_{i}(1,a_{j})+\\varphi_{i}(p_{i}( a_{-i}))\\) with the natural normalization that When convenient, we suppress the notation \\(a_{-i}\\) in \\(p_{i}(a_{-i})\\).\n\nIn a _network coordination game with local and global effects_ payoff of each player \\(i\\) is given by \\(u_{i}(1,a_{-i})=\\sum_{j\\in N_{i}}w_{i,j}\\hat{u}_{i}(1,a_{j})+\\varphi_{i}(p_{i} (a_{-i}))\\), and \\(u_{i}(0,a_{-i})=0\\), where \\(\\varphi_{i}:[0,1]\\rightarrow\\mathbb{R}_{+}\\) is a (weakly) increasing function with \\(\\varphi_{i}(0)=0\\). Notice that a network coordination game with local and global effects is a game with strategic complements, because payoff of each player satisfies increasing differences: \\(u_{i}(1,a_{-i})-u_{i}(0,a_{-i})\\) is (weakly) increasing in \\(a_{-i}\\). Indeed, this holds for a (weakly) increasing function of the form \\(\\varphi_{i}(a_{-i})\\), without composing with \\(p_{i}(a_{-i})\\), and our results hold for this generalization. We use \\(p_{i}(a_{-i})\\) to aid intuition for global effects. Moreover, it follows immediately that a network coordination game with local effects is the special case when each \\(\\varphi_{i}\\) is identically zero. Our formulation is more general than assuming a weight \\(w_{i,j}\\geq 0\\) for each non-neighbor \\(j\\) of \\(i\\).\n\nWith heterogeneous local and global effects, 1 is a best response of player \\(i\\), if, and only if, \\(u_{i}(1,a_{-i})\\geq u_{i}(0,a_{-i})\\), which holds exactly when \\(\\frac{s_{i}}{w_{i}}\\geq q-\\frac{\\varphi_{i}(p_{i})}{w_{i}(b+c)}\\), where \\(s_{i}\\) and \\(q\\) are as above. As \\(\\varphi_{i}\\) is (weakly) increasing, the threshold (weighted) fraction of neighbors who play 1 before \\(i\\) switches to 1 decreases in \\(p_{i}\\). In other words, with global effects, a smaller fraction of a given player\u2019s neighbors need to play 1 as compared to "]}, {"edit": ["outcomes. Next we show that under Assumption 5.1 and Assumption 5.2, \\(\\mathbb{E}\\bar{Y}_{n,post}^{(a)}\\) may be written as a linear combination of unit \\(n\\)'s _pre_-intervention outcomes. We note that similar observations have previously been made in the panel data literature (e.g. [46]), but we include the following lemma for completeness' sake.\n\n**Lemma 5.4** (**Reformulation of average expected post-intervention outcome**).: _Under Assumption 5.1 and Assumption 5.2, there exists slope vector \\(\\theta(a)\\in\\mathbb{R}^{T_{0}}\\), such that the average expected post-intervention outcome of unit \\(n\\) under intervention \\(a\\) is expressible as_\n\n\\[\\mathbb{E}\\bar{Y}_{n,post}^{(a)}=\\frac{1}{T-T_{0}}\\langle\\theta(a),\\mathbb{E}Y _{n,pre}\\rangle.\\]\n\n\\(\\theta(a)\\) may be interpreted as a unit-independent measure of the causal relationship between pre- and post-intervention outcomes. Using this reformulation, adaptive guarantees for the estimation of causal effects over time may now be obtained by applying our online PCR results of Section 4. Overloading the notation of Section 3, we let \\(\\mathbf{X}_{n}=(\\mathbb{E}Y_{1,pre},\\ldots,\\mathbb{E}Y_{n,pre})^{T}\\), \\(\\mathbf{Z}_{n}=(Y_{1,pre},\\ldots,Y_{n,pre})^{T}\\), \\(\\epsilon_{n,pre}=(\\epsilon_{n,1}^{(0)},\\ldots,\\epsilon_{n,T_{0}}^{(0)})^{T}\\), \\(\\mathcal{E}_{n}=(\\epsilon_{1,pre},\\ldots,\\epsilon_{n,pre})^{T}\\)\\(\\xi_{n}=\\sum_{t=T_{0}+1}^{T}\\epsilon_{n,t}^{(a_{n})}\\), \\(\\Xi_{n}=(\\xi_{1},\\ldots,\\xi_{n})^{T}\\), and\n\n\\[\\mathbf{Y}_{n}=\\left(\\frac{1}{T-T_{0}}\\sum_{t=1}^{T_{0}}Y_{1,t}^{(a_{1})}, \\ldots,\\frac{1}{T-T_{0}}\\sum_{t=1}^{T_{0}}Y_{n,t}^{(a_{n})}\\right)^{T}.\\]\n\nFinally, we define quantities such as \\(\\mathbf{Z}_{n}(a)\\), \\(\\mathbf{X}_{n}(a)\\), \\(\\mathbf{Y}_{n}(a)\\) analogously to Section 3. We now turn to bounding our primary quantity of interest in the panel data setting: prediction error for the average expected post-intervention outcome.\n\n**Theorem 5.5** (**Prediction error of average expected post-intervention outcome**).: _Let \\(\\delta\\in(0,1)\\) be an arbitrary confidence parameter and \\(\\rho>0\\) be chosen to be sufficiently small, as detailed in Appendix F. Further, assume that Assumptions 5.1 and 5.2 are satisfied, there is some \\(n_{0}\\geq 1\\) such that \\(\\operatorname{rank}(\\mathbf{X}_{n_{0}}(a))=r\\), and \\(\\operatorname{snr}_{n}(a)\\geq 2\\) for all \\(n\\geq n_{0}\\). If "], "nougat": ["outcomes. Next we show that under Assumption 5.1 and Assumption 5.2 , \\(\\mathbb{E}\\bar{Y}_{n,post}^{(a)}\\) may be written as a linear combination of unit \\(n\\)'s _pre_-intervention outcomes. We note that similar observations have previously been made in the panel data literature (e.g. [46]), but we include the following lemma for completeness' sake.\n\n**Lemma 5.4** (**Reformulation of average expected post-intervention outcome**).: _Under Assumption 5.1 and Assumption 5.2, there exists slope vector \\(\\theta(a)\\in\\mathbb{R}^{T_{0}}\\), such that the average expected post-intervention outcome of unit \\(n\\) under intervention \\(a\\) is expressible as_\n\n\\[\\mathbb{E}\\bar{Y}_{n,post}^{(a)}=\\frac{1}{T-T_{0}}\\langle\\theta(a),\\mathbb{E}Y _{n,pre}\\rangle.\\]\n\n\\(\\theta(a)\\) may be interpreted as a unit-independent measure of the causal relationship between of Section 4 f               y\n. Overloading the notation of Section 3, we let \\(\\mathbf{X}_{n}=(\\mathbb{E}Y_{1,pre},\\ldots,\\mathbb{E}Y_{n,pre})^{T}\\), \\(\\mathbf{Z}_{n}=(Y_{1,pre},\\ldots,Y_{n,pre})^{T}\\), \\(\\epsilon_{n,pre}=(\\epsilon_{n,1}^{(0)},\\ldots,\\epsilon_{n,T_{0}}^{(0)})^{T}\\), \\(\\mathcal{E}_{n}=(\\epsilon_{1,pre},\\ldots,\\epsilon_{n,pre})^{T}\\)\\(\\xi_{n}=\\sum_{t=T_{0}+1}^{T}\\epsilon_{n,t}^{(a_{n})}\\), \\(\\Xi_{n}=(\\xi_{1},\\ldots,\\xi_{n})^{T}\\), and\n\n\\[\\mathbf{Y}_{n}=\\left(\\frac{1}{T-T_{0}}\\sum_{t=1}^{T_{0}}Y_{1,t}^{(a_{1})}, \\ldots,\\frac{1}{T-T_{0}}\\sum_{t=1}^{T_{0}}Y_{n,t}^{(a_{n})}\\right)^{T}.\\]\n\nFinally, we define quantities such as \\(\\mathbf{Z}_{n}(a)\\), \\(\\mathbf{X}_{n}(a)\\), \\(\\mathbf{Y}_{n}(a)\\) analogously to Section 3. We now turn to bounding our primary quantity of interest in the panel data setting: prediction error for the average expected post-intervention outcome.\n\n**Theorem 5.5** (**Prediction error of average expected post-intervention outcome**).: _Let \\(\\delta\\in(0,1)\\) be an arbitrary confidence parameter and \\(\\rho>0\\) be chosen to be sufficiently small, as some \\(n_{0}\\geq 1\\) such that \\(\\mathrm{rank}(\\mathbf{X}_{n_{0}}(a))=r\\), and \\(\\mathrm{snr}_{n}(a)\\geq 2\\) for all \\(n\\geq n_{0}\\). If \\(T_{0}\\leq\\frac{1}{2}T\\), then under Assumption 4.2 with probability at least \\(1-O(A\\delta)\\), simultaneously for "]}, {"edit": ["\\(i\\leqslant j\\). By assumption, before the modification we have \\(|T^{*}\\cap C_{i,j}|\\geqslant\\delta_{i,j}(f)\\). If \\(j\\geqslant k_{t}\\), then \\(|T^{*}\\cap C_{i,j}|\\) does not decrease after the modification, and is thus at least \\(\\delta_{i,j}(f)\\). So assume \\(j<k_{t}\\). In this case, \\(T\\cap C_{i,j}=\\{c_{k_{1}},\\ldots,c_{k_{t-1}}\\}\\cap C_{i,j}\\subseteq T^{*}\\cap C _{i,j}\\). Since \\(|T\\cap C_{i,j}|\\geqslant\\delta_{i,j}(f)\\) by Lemma 3, we have \\(|T^{*}\\cap C_{i,j}|\\geqslant\\delta_{i,j}(f)\\). \n\nWe use binary search to find the smallest \\(r\\) such that the reduced instance has an \\(f\\)-tolerant hitting set of size at most \\(k\\). Therefore, the following theorem holds.\n\nOptimal Fault-Tolerant Committee can be solved in time \\(\\mathcal{O}(n^{2}m^{3}f\\log(nm))\\) for one-dimensional Euclidean elections.\n\nOur dynamic programming algorithm works as long as either the set \\(V\\) or the set \\(C\\) is embedded in \\(\\mathbb{R}\\) (i.e., has a linear ordering), while the other set can have an arbitrary \\(d\\)-dimensional embedding. Moreover, we can also extend our algorithms to ordinal elections with (widely studied) single-peaked preferences [2, 26] to compute an optimal fault-tolerant Chamberlin-Courant committee.\n\n## 3 Fault-Tolerant Committees in Multidimensional Space\n\nWe now consider fault tolerance in multidimensional elections. Unsurprisingly, the optimal committee design problem is intractable -- it is similar to facility location -- but it turns out that the seemingly simpler variants ORP and FTS are also intractable.\n\n### Hardness Results\n\nAll three problems (Optimal Replacement, Fault-Tolerance Score, and Optimal Fault-Tolerant Committee) are NP-hard, in any dimension \\(d\\geqslant 2\\) under the Euclidean norm, where size of the committee \\(k\\) and the failure parameter \\(f\\) are part of the input.\n\nWe will use a single construction to show NP-hardness all three problems. Our proof uses a reduction from the NP-complete problem Planar Monotone 3-SAT (PM-3SAT) [7]. An input to this problem is a _monotone_ 3-CNF formula \\(\\varphi\\) where each clause contains either three positive literals or three negative literals, and whose variable-clause incidence graph has a planar embedding which is given as a part of the input. Given an instance \\(\\varphi\\) of PM-3SAT, our reduction constructs a 2-dimensional Euclidean election. The general outline follows a scheme used in [32] to show the hardness of committee selection under _ordinal_ preferences, but generalizing the proof to _fault-tolerant committees_ requires several technical modifications and a new proof of correctness.\n\nFor ease of referencing, we adapt the terminologies from [32]. In the planar embedding of the formula \\(\\varphi\\), each variable/clause is drawn as an (axis-parallel) rectangle in the plane, and so this is called a _rectangular embedding_. See Figure 1(a) for an illustration. The rectangles for the variables are drawn along the \\(x\\)-axis, while the rectangles for the positive (resp., negative) clauses lie above (resp., below) the \\(x\\)-axis. If a clause contains a variable, then there is a vertical segment connecting the clause rectangle and the variable rectangle. Each such vertical segment is disjoint from all the rectangles except the two it connects.\n\nThe rectangular embedding of \\(\\varphi\\) can be modified to another embedding which is easier to work with called _ "], "nougat": ["\\(i\\leqslant j\\). By assumption, before the modification we have \\(|T^{*}\\cap C_{i,j}|\\geqslant\\delta_{i,j}(f)\\). If \\(j\\geqslant k_{t}\\), then \\(|T^{*}\\cap C_{i,j}|\\) does not decrease after the modification, and is thus at least \\(\\delta_{i,j}(f)\\). So assume \\(j<k_{t}\\). In this case, \\(T\\cap C_{i,j}=\\{c_{k_{1}},\\ldots,c_{k_{t-1}}\\}\\cap C_{i,j}\\subseteq T^{*}\\cap C _{i,j}\\). Since \\(|T\\cap C_{i,j}|\\geqslant\\delta_{i,j}(f)\\) by Lemma 3, we have \\(|T^{*}\\cap C_{i,j}|\\geqslant\\delta_{i,j}(f)\\). \n\nWe use binary search to find the smallest \\(r\\) such that the reduced instance has an \\(f\\)-tolerant hitting set of size at most \\(k\\). Therefore, the following theorem holds.\n\nOptimal Fault-Tolerant Committee can be solved in time \\(\\mathcal{O}(n^{2}m^{3}f\\log(nm))\\) for one-dimensional Euclidean elections.\n\nOur dynamic programming algorithm works as long as either the set \\(V\\) or the set \\(C\\) is embedded in \\(\\mathbb{R}\\) (i.e., has a linear ordering), while the other set can have an arbitrary \\(d\\)-dimensional embedding. Moreover, we can also extend our algorithms to ordinal elections with (widely studied) single-peaked preferences [2, 26] to compute an optimal fault-tolerant Chamberlin-Courant committee.\n\n## 3 Fault-Tolerant Committees in Multidimensional Space\n\nWe now consider fault tolerance in multidimensional elections. Unsurprisingly, the optimal committee design problem is intractable -- it is similar to facility location -- but it turns out that the seemingly simpler variants ORP and FTS are also intractable.\n\n### Hardness Results\n\nAll three problems (Optimal Replacement, Fault-Tolerance Score, and Optimal Fault-Tolerant Committee) are NP-hard, in any dimension \\(d\\geqslant 2\\) under the Euclidean norm, where size of the committee \\(k\\) and the failure parameter \\(f\\) are part of the input.\n\nWe will use a single construction to show NP-hardness all three problems. Our proof uses a reduction from the NP-complete problem Planar Monotone 3-SAT (PM-3SAT) [7]. An input to this problem is a _monotone_ 3-CNF formula \\(\\varphi\\) where each clause contains either three positive literals or three negative literals, and whose variable-clause incidence graph has a planar embedding which is given as a part of the input. Given an instance \\(\\varphi\\) of PM-3SAT, our reduction constructs a 2-dimensional Euclidean election. The general outline follows a scheme used in [32] to show the hardness of committee selection under _ordinal_ preferences, but generalizing the proof to _fault-tolerant committees_ requires several technical modifications and a new proof of correctness.\n\nFor ease of referencing, we adapt the terminologies from [32]. In the planar embedding of the formula \\(\\varphi\\), each variable/clause is drawn as an (axis-parallel) rectangle in the plane, and so this is called a _rectangular embedding_. See Figure 1(a) for an illustration. The rectangles for the variables are drawn along the \\(x\\)-axis, while the rectangles for the positive (resp., negative) clauses lie above (resp., below) the \\(x\\)-axis. If a clause contains a variable, then there is a vertical segment connecting the clause rectangle and the variable rectangle. Each such vertical segment is disjoint from all the rectangles except the two it connects.\n\nThe rectangular embedding of \\(\\varphi\\) can be modified to another embedding which is easier to work with called _ "]}, {"edit": ["\n\n# Robust Impulse Responses using External Instruments: the Role of Information+\nFootnote \u2020: This paper has benefited from discussions with Mario Forni and participants at the UEA Time Series Workshop, the Workshop in Empirical and Theoretical Macroeconomics and various research seminars.\n\nDavide Brignone\n\nUniversita di Roma Tor Vergata, email: davide.brignone@uniroma2.it.\n\nAlessandro Franconi\n\nUniversita di Pavia, email: alessandro.franconi@unipv.it\n\nMarco Mazzali\n\nUniversita di Bologna, email: marco.mazzali5@unibo.it.\n\nThis version: May 30, 2024\n\n###### Abstract\n\nExternal-instrument identification leads to biased responses when the shock is not invertible and the measurement error is present. We propose to use this identification strategy in a structural Dynamic Factor Model, which we call Proxy DFM. In a simulation analysis, we show that the Proxy DFM always successfully retrieves the true impulse responses, while the Proxy SVAR systematically fails to do so when the model is either misspecified, does not include all relevant information, or the measurement error is present. In an application to US monetary policy, the Proxy DFM shows that a tightening shock is unequivocally contractionary, with deteriorations in domestic demand, labor, credit, housing, exchange, and financial markets. This holds true for all raw instruments available in the literature. The variance decomposition analysis highlights the importance of monetary policy shocks in explaining economic fluctuations, albeit at different horizons.\n\n**Keywords**: Proxy Dynamic Factor Model, Monetary Policy, Fundamentalness, Impulse Response Functions, Variance Decomposition.\n\n**JEL codes**: C32, C38, E52.\n\n"], "nougat": ["\n\n# Robust Impulse Responses using External Instruments: the Role of Information \u2217\nFootnote *: This paper has benefited from discussions with Mario Forni and participants at the UEA Time Series Workshop, the Workshop in Empirical and Theoretical Macroeconomics and various research seminars.\n\nDavide Brignone\n\nUniversita di Roma Tor Vergata, email: davide.brignone@uniroma2.it.\n\nAlessandro Franconi\n\nUniversita di Pavia, email: alessandro.franconi@unipv.it\n\nMarco Mazzali\n\nUniversita di Bologna, email: marco.mazzali5@unibo.it.\n\nThis version: May 30, 2024\n\n###### Abstract\n\nExternal-instrument identification leads to biased responses when the shock is not invertible and the measurement error is present. We propose to use this identification strategy in a structural Dynamic Factor Model, which we call Proxy DFM. In a simulation analysis, we show that the Proxy DFM always successfully retrieves the true impulse responses, while the Proxy SVAR systematically fails to do so when the model is either misspecified, does not include all relevant information, or the measurement error is present. In an application to US monetary policy, the Proxy DFM shows that a tightening shock is unequivocally contractionary, with deteriorations in domestic demand, labor, credit, housing, exchange, and financial markets. This holds true for all raw instruments available in the literature. The variance decomposition analysis highlights the importance of monetary policy shocks in explaining economic fluctuations, albeit at different horizons.\n\n**Keywords**: Proxy Dynamic Factor Model, Monetary Policy, Fundamentalness, Impulse Response Functions, Variance Decomposition.\n\n**JEL codes**: C32, C38, E52.\n\n"]}, {"edit": ["_Assumption 7_.: \\(m(w,\\pi)\\) is linear in \\(\\pi\\) and there is \\(C>0\\) such that\n\n\\[\\left|E\\left[m(w,\\pi)-\\theta_{0}+\\alpha_{0}(z)(y-\\pi(z;\\gamma))\\right]\\right|\\leq C \\left\\|\\pi-\\pi_{0}\\right\\|^{2}\\]\n\n**Proposition 1**.: _If Assumptions 1-3 are satisfied then for \\(V=E[\\{m(w,\\pi_{0}(z;\\gamma_{0}))-\\theta_{0}\\)\\(+\\alpha_{0}(z)(y-\\pi_{0}(z;\\gamma_{0}))\\}^{2}]\\),_\n\n\\[\\sqrt{n}(\\hat{\\theta}-\\theta_{0})\\xrightarrow{D}N(0,V),\\hat{V} \\xrightarrow{p}V.\\]\n\nThis result follows immediately following arguments in Chernozhukov, Escanciano, et al. (2022). The only point of difference is that we have to account for the first-stage estimator \\(\\hat{\\gamma}\\). However, since it is estimable at a \\(n^{-1/2}\\) rate, the analysis of Chernozhukov, Escanciano, et al. (2022) still applies. We show the proof in Appendix 6.\n\n### Estimation Outline\n\n* Consider the dataset \\(\\{y_{t},z_{t}\\}_{t=1}^{n}\\) is independently and identically distributed. Now we randomly split the data into L folds such that the data \\(D_{l}:=\\{y_{t},z_{t}\\}_{t\\in I_{l}}\\), where \\(I_{l}\\) denotes the \\(l^{th}\\) partition.\n* In the second stage for each fold \\(I_{l}\\), we estimate both the choice function and the Riesz estimator on the left out data \\(D_{l^{c}}:=\\{y_{t},x_{t}\\}_{t\\notin I_{l}}\\) \\[\\hat{\\pi}_{l}=\\operatorname*{arg\\,min}_{f\\in\\mathcal{F}}\\frac{1}{\\sum_{t\\in D _{l^{c}}}J_{t}}\\sum_{t\\in D_{l^{c}}}\\sum_{j\\in J_{t}}[(y_{jt}-\\pi(z_{jt};\\gamma ))^{2}]\\] (8) \\[\\hat{\\alpha}_{l}=\\operatorname*{arg\\,min}_{\\alpha\\in\\mathcal{A}} \\frac{1}{\\sum_{t\\in D_{l^{c}}}J_{t}}\\sum_{t\\in D_{l^{c}}}\\sum_{j\\in J_{t}}\\left[ \\alpha(z_{jt})^{2}-2m(w_{jt};\\alpha)\\right]\\] (9)\n* Now for every \\(I_{l}\\), we will estimate \\(\\hat{\\theta}_{l}\\) by using the estimators estimated on \\(D_{l}^{c}\\). And finally, to estimate \\(\\theta\\), we average it out across all folds. Thus the estimator for \\(\\theta_{0}\\) and its variance can be given as follows - "], "nougat": ["_Assumption 7_.: \\(m(w,\\pi)\\) is linear in \\(\\pi\\) and there is \\(C>0\\) such that\n\n\\[\\left|E\\left[m(w,\\pi)-\\theta_{0}+\\alpha_{0}(z)(y-\\pi(z;\\gamma))\\right]\\right|\\leq C \\left\\|\\pi-\\pi_{0}\\right\\|^{2}\\]\n\n**Proposition 1**.: _If Assumptions 1-3 are satisfied then for \\(V=E[\\{m(w,\\pi_{0}(z;\\gamma_{0}))-\\theta_{0}\\)\\(+\\alpha_{0}(z)(y-\\pi_{0}(z;\\gamma_{0}))\\}^{2}]\\),_\n\n\\[\\sqrt{n}(\\hat{\\theta}-\\theta_{0})\\xrightarrow{D}N(0,V),\\hat{V} \\xrightarrow{p}V.\\]\n\nThis result follows immediately following arguments in Chernozhukov, Escanciano, et al. (2022 ). The only point of difference is that we have to account for the first-stage estimator \\(\\hat{\\gamma}\\). However, since it is estimable at a \\(n^{-1/2}\\) rate, the analysis of Chernozhukov, Escanciano, et al. (2022 ) still applies. We show the proof in Appendix 6 .\n\n### Estimation Outline\n\n* Consider the dataset \\(\\{y_{t},z_{t}\\}_{t=1}^{n}\\) is independently and identically distributed. Now we randomly split the data into L folds such that the data \\(D_{l}:=\\{y_{t},z_{t}\\}_{t\\in I_{l}}\\), where \\(I_{l}\\) denotes the \\(l^{th}\\) partition.\n* In the second stage for each fold \\(I_{l}\\), we estimate both the choice function and the Riesz estimator on the left out data \\(D_{l^{c}}:=\\{y_{t},x_{t}\\}_{t\\notin I_{l}}\\) \\[\\hat{\\pi}_{l}=\\operatorname*{arg\\,min}_{f\\in\\mathcal{F}}\\frac{1}{\\sum_{t\\in D _{l^{c}}}J_{t}}\\sum_{t\\in D_{l^{c}}}\\sum_{j\\in J_{t}}[(y_{jt}-\\pi(z_{jt};\\gamma) )^{2}]\\] (8) \\[\\hat{\\alpha}_{l}=\\operatorname*{arg\\,min}_{\\alpha\\in\\mathcal{A}} \\frac{1}{\\sum_{t\\in D_{l^{c}}}J_{t}}\\sum_{t\\in D_{l^{c}}}\\sum_{j\\in J_{t}}\\left[ \\alpha(z_{jt})^{2}-2m(w_{jt};\\alpha)\\right]\\] (9)\n* Now for every \\(I_{l}\\), we will estimate \\(\\hat{\\theta}_{l}\\) by using the estimators estimated on \\(D_{l}^{c}\\). And finally, to estimate \\(\\theta\\), we average it out across all folds. Thus the estimator for \\(\\theta_{0}\\) and its variance can be given as follows \u2013 "]}, {"edit": ["\n\n# Endowments, Patience Types, and Uniqueness in Two-Good Hara Utility Economies\n\nAndrea Loi\n\nDepartment of Mathematics, University of California, Riverside, Riverside, CA 92521, USA. loi@math.ucr.edu\n\nStefano Matta\n\nDepartment of Mathematics, University of California, Riverside, CA 92521, USA. matta@math.ucr.edu\n\n###### Abstract.\n\nThis paper establishes a link between endowments, patience types, and the parameters of the HARA Bernoulli utility function that ensure equilibrium uniqueness in an economy with two goods and two impatience types with additive separable preferences. We provide sufficient conditions that guarantee uniqueness of equilibrium for any possible value of \\(\\gamma\\) in the HARA utility function \\(\\frac{\\gamma}{1-\\gamma}\\left(b+\\frac{a}{\\gamma}x\\right)^{1-\\gamma}\\). The analysis contributes to the literature on uniqueness in pure exchange economies with two-goods and two agent types and extends the result in [4].\n\nThe first author was supported by INdAM. GNSAGA - Gruppo Nazionale per le Strutture Algebriche, Geometriche e le loro Applicazioni and by KASBA, funded by Regione Autonoma della Sardegna. Both authors were supported by STAGE, funded by Fondazione di Sardegna.\n\n"], "nougat": ["\n\n# Endowments, Patience Types, and Uniqueness in Two-Good Hara Utility Economies\n\nAndrea Loi\n\nDepartment of Mathematics, University of California, Riverside, Riverside, CA 92521, USA. loi@math.ucr.edu\n\nStefano Matta\n\nDepartment of Mathematics, University of California, Riverside, CA 92521, USA. matta@math.ucr.edu\n\n###### Abstract.\n\nThis paper establishes a link between endowments, patience types, and the parameters of the HARA Bernoulli utility function that ensure equilibrium uniqueness in an economy with two goods and two impatience types with additive separable preferences. We provide sufficient conditions that guarantee uniqueness of equilibrium for any possible value of \\(\\gamma\\) in the HARA utility function \\(\\frac{\\gamma}{1-\\gamma}\\left(b+\\frac{a}{\\gamma}x\\right)^{1-\\gamma}\\). The analysis contributes to the literature on uniqueness in pure exchange economies with two-goods and two agent types and extends the result in [4].\n\nThe first author was supported by INdAM. GNSAGA - Gruppo Nazionale per le Strutture Algebriche, Geometriche e le loro Applicazioni and by KASBA, funded by Regione Autonoma della Sardegna. Both authors were supported by STAGE, funded by Fondazione di Sardegna.\n\n"]}, {"edit": ["\n\n## Introduction\n\nIn response to the 2022 cost-of-living crisis in Europe, the German government introduced a three-month fuel excise tax cut and a public transport season ticket for 9 Euro per month, valid on all local and regional services, the so-called \"9-Euro-Ticket\". The latter can be considered almost fare-free public transport. The selected months were June, July, and August, i.e., a period characterized by summer holidays. Given the critical role of travel costs in mode choice [1], this natural experiment was expected to lead to a modal shift to public transport because the season ticket price cut was so substantial. In addition, many expected that the almost fare-free aspect of the \"9-Euro-Ticket\" leads to high levels of induced demand. The success of the \"9-Euro-Ticket\" prompted an immediate discussion as well as public and political demand for introducing a successor ticket to the \"9-Euro-Ticket\" as soon as possible. This ticket, the so-called \"Deutschlandticket\", was finally introduced in May 2023. We set up the \"Mobilitat.Leben\" study to observe this fare policy innovation using questionnaires and GPS tracking to generate travel diaries [2, 3].\n\nThis behavioral intervention, which could be one of the largest public transport pricing travel behavior experiments, has been studied by many: all of them reported a substantial increase in public transport usage during the validity period of the \"9-Euro-Ticket\" and a return to almost pre-ticket levels after the \"9-Euro-Ticket\" validity period [2, 4, 5, 6, 7]. The official and main study was conducted by Association of German Transport Companies, which surveyed more than 200,000 people in Germany [4]: around 20% of all \"9-Euro-Ticket\" customers were new customers to public transport. Out of all public transport trips in the months of June, July, and August 2022, 17% of trips have been shifted from other transport modes, and 10% of trips have been shifted from the car to public transport, in rural areas, even 13 to 16%. 16% of all trips correspond to induced demand. In addition, trip distances increased by 38% during the \"9-Euro-Ticket\" period. Another survey showed that 11% of all trips conducted during the \"9-Euro-Ticket\" period shifted from other modes of transport, while 6% of all trips were induced [6]. Using surveys and GPS tracking, another study concluded that the \"9-Euro-Ticket\" did not lead to a change in daily mobility but instead increased leisure travel at the beginning and the end of the ticket's validity period, leaving monetary savings as the main effect of the \"9-Euro-Ticket\" [5]. Generally, the summer months of June, July, and August usually see less ridership due to the summer holidays, but in 2022 this trend was reversed [7]. For the \"Deutschlandticket\", first results for the Hamburg metropolitan area suggest that season-ticket ownership increased by 22 % and ridership increased from 89.3 % to 95.4 % of the pre-pandemic levels from April to May 2023 [8].\n\nThe dominant methodological approach was a cross-sectional survey with online questionnaires and interviews [4, 6, 7], and panel studies [9, 10]. Both are established survey techniques in transport research, also for nationwide household travel surveys [11, 12, 13, 14]. The complexity of the fare-policy innovation with the \"9-Euro-Ticket\" and the \"Deutschlandticket\", however, means that travel behavior changes can result, which are difficult to observe using traditional paper-based or CATI-bases travel diary surveys. For example, one can expect that self-assessments of how individuals travel across transit district zones and district borders can only be answered reliably by individuals with some knowledge of the fare system, not everyone. Here, using GPS tracking can be considered an appropriate supplement or substitute to questionnaires to measure travel behavior and its changes over a long time period. Such tracking-based survey approaches generally work in creating travel diaries [15"], "nougat": ["\n\n## Introduction\n\nIn response to the 2022 cost-of-living crisis in Europe, the German government introduced a threemonth fuel excise tax cut and a public transport season ticket for 9 Euro per month, valid on all local and regional services, the so-called \u201c9-Euro-Ticket\u201d. The latter can be considered almost fare-free public transport. The selected months were June, July, and August, i.e., a period characterized by summer holidays. Given the critical role of travel costs in mode choice ( 1 ), this natural experiment was expected to lead to a modal shift to public transport because the season ticket price cut was so substantial. In addition, many expected that the almost fare-free aspect of the \u201c9-EuroTicket\u201d leads to high levels of induced demand. The success of the \u201c9-Euro-Ticket\u201d prompted an immediate discussion as well as public and political demand for introducing a successor ticket to the \u201c9-Euro-Ticket\u201d as soon as possible. This ticket, the so-called \u201cDeutschlandticket\u201d, was finally introduced in May 2023. We set up the \u201cMobilit\u00e4t.Leben\u201d study to observe this fare policy innovation using questionnaires and GPS tracking to generate travel diaries ( 2 , 3 ).\n\nThis behavioral intervention, which could be one of the largest public transport pricing travel behavior experiments, has been studied by many: all of them reported a substantial increase in public transport usage during the validity period of the \u201c9-Euro-Ticket\u201d and a return to almost pre-ticket levels after the \u201c9-Euro-Ticket\u201d validity period ( 2 , 4 \u2013 7 ). The official and main study was conducted by Association of German Transport Companies, which surveyed more than 200,000 people in Germany ( 4 ): around 20% of all \u201c9-Euro-Ticket\u201d customers were new customers to public transport. Out of all public transport trips in the months of June, July, and August 2022, 17% of trips have been shifted from other transport modes, and 10% of trips have been shifted from the car to public transport, in rural areas, even 13 to 16%. 16% of all trips correspond to induced demand. In addition, trip distances increased by 38% during the \u201c9-Euro-Ticket\u201d period. Another survey showed that 11% of all trips conducted during the \u201c9-Euro-Ticket\u201d period shifted from other modes of transport, while 6% of all trips were induced ( 6 ). Using surveys and GPS tracking, another study concluded that the \u201c9-Euro-Ticket\u201d did not lead to a change in daily mobility but instead increased leisure travel at the beginning and the end of the ticket\u2019s validity period, leaving monetary savings as the main effect of the \u201c9-Euro-Ticket\u201d ( 5 ). Generally, the summer months of June, July, and August usually see less ridership due to the summer holidays, but in 2022 this trend was reversed ( 7 ). For the \u201cDeutschlandticket\u201d, first results for the Hamburg metropolitan area suggest that season-ticket ownership increased by 22 % and ridership increased from 89.3 % to 95.4 % of the pre-pandemic levels from April to May 2023 ( 8 ).\n\nThe dominant methodological approach was a cross-sectional survey with online questionnaires and interviews ( 4 , 6 , 7 ),"]}, {"edit": ["and define a robust welfare function as\n\n\\[\\mathrm{RW}(d)=\\sup_{\\gamma\\in\\Sigma(\\delta)}\\mathbb{E}_{\\gamma}\\left[\\sum_{\\ell=1}^ {L}Y_{\\ell}I(d(X)=\\ell)\\right].\\]\n\n**Proposition 7.1**.: _For \\(\\ell\\in[L]\\), let_\n\n\\[c_{\\ell}(s_{\\ell},s^{\\prime}_{\\ell})=|y_{\\ell}-y^{\\prime}_{\\ell}|+\\|x_{\\ell}- x^{\\prime}_{\\ell}\\|_{2}.\\]\n\n_Assume that \\(Y_{\\ell}\\) is unbounded, \\(\\mathbb{E}[\\|X\\|_{2}^{2}]<\\infty\\) and \\(\\mathbb{E}[|Y_{\\ell}|]<\\infty\\). Then_\n\n\\[\\mathrm{RW}(d)=\\sup_{\\lambda\\geq 1}\\left\\{\\inf_{\\pi\\in\\Pi(\\mu_{1,L+1}, \\ldots,\\mu_{L,L+1})}\\int_{\\mathcal{V}}\\min_{\\ell\\in[L]}\\{y_{\\ell}+\\phi_{ \\lambda,\\ell}(x_{1},\\ldots,x_{L})\\}d\\pi(s)-\\langle\\lambda,\\delta\\rangle\\right\\},\\]\n\n_where_\n\n\\[\\varphi_{\\lambda,\\ell}(x_{1},\\ldots,x_{L})=\\min_{x^{\\prime},d(x^{\\prime})=\\ell }\\sum_{\\ell=1}^{L}\\lambda_{\\ell}\\|x_{\\ell}-x^{\\prime}\\|_{2}.\\]\n\nProposition 7.1 is an extension of Proposition 6.5.\n\n## 8 Concluding Remarks\n\nIn this paper, we have introduced W-DMR in marginal problems for both non-overlapping and overlapping marginals and established fundamental results including strong duality, finiteness of the proposed W-DMR, and existence of an optimizer at each radius. We have also shown continuity of the W-DMR-MP as a function of the radius. Applicability of the proposed W-DMR in marginal problems and established properties is demonstrated via distinct applications when the sample information comes from multiple data sources and only some marginal reference measures are identified. To the best of the authors' knowledge, this paper is the first systematic study of W-DMR in marginal problems. Many open questions remain including the structure of optimizers of W-DMR for both non-overlapping and overlapping marginals, efficient numerical algorithms, and estimation and inference in each motivating example. Another useful extension is to consider objective functions that are nonlinear in the joint probability measure such as the Value-at-Risk of a linear portfolio of risks in Puccetti and Ruschendorf (2012) and robust spectral measures of risk in Ghossoub et al. (2023) and Ennaji et al. (2022).\n\n "], "nougat": ["and define a robust welfare function as\n\n\\[\\mathrm{RW}(d)=\\sup_{\\gamma\\in\\Sigma(\\delta)}\\mathbb{E}_{\\gamma}\\left[\\sum_{\\ell=1}^ {L}Y_{\\ell}I(d(X)=\\ell)\\right].\\]\n\n**Proposition 7.1**.: _For \\(\\ell\\in[L]\\), let_\n\n\\[c_{\\ell}(s_{\\ell},s^{\\prime}_{\\ell})=|y_{\\ell}-y^{\\prime}_{\\ell}|+\\|x_{\\ell}- x^{\\prime}_{\\ell}\\|_{2}.\\]\n\n_Assume that \\(Y_{\\ell}\\) is unbounded, \\(\\mathbb{E}[\\|X\\|_{2}^{2}]<\\infty\\) and \\(\\mathbb{E}[|Y_{\\ell}|]<\\infty\\). Then_\n\n\\[\\mathrm{RW}(d)=\\sup_{\\lambda\\geq 1}\\left\\{\\inf_{\\pi\\in\\Pi(\\mu_{1,L+1}, \\ldots,\\mu_{L,L+1})}\\int_{\\mathcal{V}}\\min_{\\ell\\in[L]}\\{y_{\\ell}+\\phi_{ \\lambda,\\ell}(x_{1},\\ldots,x_{L})\\}d\\pi(s)-\\langle\\lambda,\\delta\\rangle\\right\\},\\]\n\n_where_\n\n\\[\\varphi_{\\lambda,\\ell}(x_{1},\\ldots,x_{L})=\\min_{x^{\\prime},d(x^{\\prime})=\\ell }\\sum_{\\ell=1}^{L}\\lambda_{\\ell}\\|x_{\\ell}-x^{\\prime}\\|_{2}.\\]\n\nProposition 7.1 is an extension of Proposition 6.5.\n\n## 8 Concluding Remarks\n\nIn this paper, we have introduced W-DMR in marginal problems for both nonoverlapping and overlapping marginals and established fundamental results including strong duality, finiteness of the proposed W-DMR, and existence of an optimizer at each radius. We have also shown continuity of the W-DMR-MP as a function of the radius. Applicability of the proposed W-DMR in marginal problems and established properties is demonstrated via distinct applications when the sample information comes from multiple data sources and only some marginal reference measures are identified. To the best of the authors\u2019 knowledge, this paper is the first systematic study of W-DMR in marginal problems. Many open questions remain including the structure of optimizers of W-DMR for both non-overlapping and overlapping marginals, efficient numerical algorithms, and estimation and inference in each motivating example. Another useful extension is to consider objective functions that are nonlinear in the joint probability measure such as the Value-at-Risk of a linear portfolio of risks in Puccetti and R \u0308uschendorf ( 2012 ) and robust spectral measures of risk in Ghossoub et al. ( 2023 ) and Ennaji et al. ( 2022 ).\n\n "]}, {"edit": ["summation of column (4) on the shorted objects in stage one. Thereafter, the objects will be arranged in ascending order concerning price from highest to lowest. And finally, we got the first ranked object as A. We may have got more than one object for the subsequent positions. As we have got only one object here so, the next places will be calculated using the same way by eliminating object A. The second-ranked object will be B. And the third object will be F. Here it has been assumed that the customer has a fixed attention span i.e. 3. Hence, we have compared the expected revenue by comparing with another collusive ranking i.e. A-D-F. The article has proved in Proposition 1,2,&3 that the purchase probability for product F in the second list will be more than the actual one. i.e. A-B-F. But this will not guarantee that the expected revenue will be maximum. Because the price of product D may be lower than product B because the demand is lower for product D compared to B. This has been proved in the article and especially in Proposition 1, 2, and 3.\n\nFigure 1: Product Rank of Food Items-Example from amazon.in\n\n"], "nougat": ["summation of column (4) on the shorted objects in stage one. Thereafter, the objects will be arranged in ascending order concerning price from highest to lowest. And finally, we got the first ranked object as A. We may have got more than one object for the subsequent positions. As we have got only one object here so, the next places will be calculated using the same way by eliminating object A. The secondranked object will be B. And the third object will be F. Here it has been assumed that the customer has a fixed attention span i.e. 3. Hence, we have compared the expected revenue by comparing with another collusive ranking i.e. A-D-F. The article has proved in Proposition 1,2,&3 that the purchase probability for product F in the second list will be more than the actual one. i.e. A-B-F. But this will not guarantee that the expected revenue will be maximum. Because the price of product D may be lower than product B because the demand is lower for product D compared to B. This has been proved in the article and especially in Proposition 1, 2, and 3.\n\nFigure 1: Product Rank of Food Items-Example from amazon.in\n\n"]}, {"edit": ["\n\n**Lemma 3**.: _Let \\(\\Gamma=(S,A,\\Omega,p,u)\\) be an information aggregation game with \\(\\Omega=\\{\\omega^{1},\\ldots,\\omega^{m}\\}\\). Let \\(E\\) a system of equations over \\(x_{1},\\ldots,x_{m}\\) such that equation \\(x_{i}\\leq x_{j}\\) appears in \\(E\\) if and only if \\(\\bar{\\omega}^{i}\\prec_{k}\\bar{\\omega}_{j}\\) or if there exists an input \\(\\vec{m}\\) such that \\(\\bar{\\omega}^{i}\\prec_{k}\\vec{m}\\prec_{k}\\bar{\\omega}^{j}\\). Then, \\(y_{1},\\ldots,y_{m}\\) is a solution of \\(E\\) if and only if_\n\n\\[\\omega^{i}\\leq_{k}\\omega^{j}\\Longrightarrow y_{i}\\leq y_{j}\\]\n\n_for all \\(i,j\\in[m]\\)._\n\nIntuitively, Lemma 3 says that the inequalities obtained from chains of length 2 or 3 _span_ the partial order over \\(\\Omega\\) defined by \\(\\leq_{k}\\), and thus that we can take the system of equations \\(E\\) of Theorem 1 to be the one in the lemma above. Therefore, given two states \\(\\omega\\) and \\(\\omega^{\\prime}\\), it only remains to show that we can check in polynomial time if \\(\\bar{\\omega}\\prec_{k}\\bar{\\omega}^{\\prime}\\) or if there exists a state \\(\\vec{m}\\) such that \\(\\bar{\\omega}\\prec_{k}\\vec{m}\\prec_{k}\\bar{\\omega}^{\\prime}\\). Checking if \\(\\bar{\\omega}\\prec_{k}\\bar{\\omega}^{\\prime}\\) is equivalent to checking if \\(k=n\\) and either all senders prefer 1 in \\(\\omega\\) or all senders prefer 0 in \\(\\omega^{\\prime}\\). Finding an input \\(\\vec{m}\\) such that \\(\\bar{\\omega}\\prec_{k}\\vec{m}\\prec_{k}\\bar{\\omega}^{\\prime}\\) reduces to finding an input \\(\\vec{m}\\) such that\n\n1. the set \\(C_{\\omega}\\) of senders such that their message is not \\(\\omega\\) in \\(\\vec{m}\\) has size at most \\(k\\), and all senders in \\(C_{\\omega}\\) strictly prefer 1 to 0 in \\(\\omega\\).\n2. the set \\(C_{\\omega^{\\prime}}\\) of senders such that their message is not \\(\\omega^{\\prime}\\) in \\(\\vec{m}\\) has size at most \\(k\\), and all of them strictly prefer 0 to 1 in \\(\\omega^{\\prime}\\).\n\nThe high level idea of the algorithm is that, if \\(\\vec{m}\\) satisfies the above properties, all senders \\(i\\) that prefer 0 to 1 in \\(\\omega\\) must satisfy that \\(m_{i}=\\omega\\) (otherwise, it breaks property (a)), and all senders \\(i\\) that prefer 1 to 0 in \\(\\omega^{\\prime}\\) must satisfy that \\(m_{i}=\\omega^{\\prime}\\) (otherwise, it breaks property (b)). If there is a sender \\(i\\) that prefers 0 to 1 in \\(\\omega\\) and 1 to 0 in"], "nougat": ["\n\n**Lemma 3**.: _Let \\(\\Gamma=(S,A,\\Omega,p,u)\\) be an information aggregation game with \\(\\Omega=\\{\\omega^{1},\\ldots,\\omega^{m}\\}\\). Let \\(E\\) a system of equations over \\(x_{1},\\ldots,x_{m}\\) such that equation \\(x_{i}\\leq x_{j}\\) appears in \\(E\\) if and only if \\(\\bar{\\omega}^{i}\\prec_{k}\\bar{\\omega}_{j}\\) or if there exists an input \\(\\vec{m}\\) such that \\(\\bar{\\omega}^{i}\\prec_{k}\\vec{m}\\prec_{k}\\bar{\\omega}^{j}\\). Then, \\(y_{1},\\ldots,y_{m}\\) is a solution of \\(E\\) if and only if_\n\n\\[\\omega^{i}\\leq_{k}\\omega^{j}\\Longrightarrow y_{i}\\leq y_{j}\\]\n\n_for all \\(i,j\\in[m]\\)._\n\nIntuitively, Lemma 3 says that the inequalities obtained from chains of length 2 or 3 _span_ the partial order over \\(\\Omega\\) defined by \\(\\leq_{k}\\), and thus that we can take the system of equations \\(E\\) of Theorem 1 to be the one in the lemma above. Therefore, given two states \\(\\omega\\) and \\(\\omega^{\\prime}\\), it only remains to show that we can check in polynomial time if \\(\\bar{\\omega}\\prec_{k}\\bar{\\omega}^{\\prime}\\) or if there exists a state \\(\\vec{m}\\) such that (a) the set \\(\\vec{m}\\prec_{k}\\vec{m}\\prec_{k}\\bar{\\omega}^{\\prime}\\). Checking if \\(\\bar{\\omega}\\prec_{k}\\bar{\\omega}^{\\prime}\\) is equivalent to checking if \\(k=n\\) and either all senders prefer 1 in \\(\\omega\\) or all senders prefer 0 in \\(\\omega^{\\prime}\\). Finding an input \\(\\vec{m}\\) such that (a) the set \\(C_{\\omega}\\) of senders such that their message is not \\(\\omega\\) in \\(\\vec{m}\\) has size at most \\(k\\), and all senders in \\(C_{\\omega}\\) strictly prefer 1 to 0 in \\(\\omega\\).\n\n(b) the set \\(C_{\\omega^{\\prime}}\\) of senders such that their message is not \\(\\omega^{\\prime}\\) in \\(\\vec{m}\\) has size at most \\(k\\), and all of them strictly prefer 0 to 1 in \\(\\omega^{\\prime}\\).\n\nThe high level idea of the algorithm is that, if \\(\\vec{m}\\) satisfies the above properties, all senders \\(i\\) that prefer 0 to 1 in \\(\\omega\\) must satisfy that \\(m_{i}=\\omega\\) (otherwise, it breaks property (a)), and all senders \\(i\\) that prefer 1 to 0 in \\(\\omega^{\\prime}\\) must satisfy that \\(m_{i}=\\omega^{\\prime}\\) (otherwise, it breaks property (b)). If there is a sender \\(i\\) that prefers 0 to 1 in \\(\\omega\\) and 1 to 0 in \\(\\omega^{\\prime}\\) then such an input \\(\\vec{m}\\) does not exist, and if there is a sender \\(i\\) that strictly prefers 1 to"]}, {"edit": ["Augmented-Dickey Fuller Test where the null hypothesis is \"the time series contains a unit root and is non-stationary\". The results of the Augmented Dickey-Fuller Test for each of the Imputation datasets are given in the below sections.\n\n### Datasets Analysis and ADF test results\n\nThe Figure below gives a plot of the whole dataset without dividing into train and test.\n\nThis figure below depicts the Mean Imputation dataset Plot.\n\nThe ADF test for Mean imputation dataset test-statistic = -5.393 p-value = 3.49e-06 for first difference to try to change the time-series to stationary. test-statistic = -10.073 p-value = 1.23e-17 for Second Difference, which is not suggested as the p-value is zero (over-differencing) test-statistic = -21.617 p-value = 0.0 "], "nougat": ["Augmented-Dickey Fuller Test where the null hypothesis is \"the time series contains a unit root and is non-stationary\". The results of the Augmented Dickey-Fuller Test for each of the Imputation datasets are given in the below sections.\n\n### Datasets Analysis and ADF test results\n\nThe Figure below gives plot of the whole dataset without dividing into train and test. The ADF test for no-imputation dataset test-statistic = -5.45 p-value = 2.55e-06 for no-imputation dataset for first difference to try to change the time-series to stationary. test-statistic = -10.403 p-value = 1.88e-18 for Second Difference, which is not suggested as the p-value is zero (over-differencing) test-statistic = -21.152 p-value = 0.0\n\nThis figure below depicts the Mean Imputation dataset Plot.\n\nThe ADF test for Mean imputation dataset test-statistic = -5.393 p-value = 3.49e-06 for first difference to try to change the time-series to stationary. test-statistic = -10.073 p-value = 1.23e-17 for Second Difference, which is not suggested as the p-value is zero (over-differencing) test-statistic = -21.617 p-value = 0.0 "]}, {"edit": ["strongly Markovian, a likely candidate for the promotion time is the first hitting time of a threshold as high as possible. In particular, if the cost of effort is zero, the principal promotes the worker when his type reaches the upper boundary of \\(\\mathcal{X}^{i}\\). However, when effort is costly, this threshold is too high. So, the principal chooses the highest threshold for which the worker is willing to exert effort instead. If the agent's type increases, the promotion threshold stays constant: the principal needs to keep her promises. On the other hand, when the worker's type decreases, the worker becomes more pessimistic about his promotion chances. The principal then has to lower the promotion threshold to motivate the worker. The logic is the same as in McClellan (2017): the promotion threshold becomes laxer when the participation constraint binds.26 Because of the monotonicity of the problem, this constraint binds precisely when the worker's type decreases.\n\nFootnote 26: See also Harris and Holmstrom (1982), Thomas and Worrall (1988), or Grochulski and Zhang (2011).\n\nFormally, the proof of Theorem 4.2 is based on the idea of the proof of Theorem 1 in McClellan (2017). It follows from the five steps below:\n\n* First consider a relaxation of problem (RP\\({}^{i}\\)) for which the constraint (DPC\\({}^{i}\\)) only needs to hold for on a finite set of (stopping) times.\n* Lemma 1.5 derives the Lagrangian associated with the relaxed problem as an application of Theorem 1 in Balzer and Janssen (2002).\n* In the third step, useful properties of the solution of the relaxed problem introduced in step 1 are established.\n* The fourth step identifies a promotion contest that guarantees the principal a payoff of at least the value of the relaxed problem introduced in the first step. It is enough to focus on promotion contests that promote worker \\(i\\) after good performances (as \\(X^{i}\\) crosses an upper threshold from bellow) and take the outside option after bad outcomes (when \\(X^{i}\\) crosses a lower threshold from above).\n* Putting everything together and letting the set of times at which (DPC\\({}^{i}\\)) holds grow dense yields Theorem 4.2.\n\nSteps 1, 2, and 5 are essentially the same as in the proof of Theorem 1 in McClellan (2017). Steps 3 and 4 are new and specific to our setting. The details are in Appendix 1.6.1. Supporting Lemmas are in Appendix 1.6.2.\n\n "], "nougat": ["strongly Markovian, a likely candidate for the promotion time is the first hitting time of a threshold as high as possible. In particular, if the cost of effort is zero, the principal promotes the worker when his type reaches the upper boundary of \\(\\mathcal{X}^{i}\\). However, when becomes laxer when the participation constraint binds. 26 Because of the monotonicity of the problem, this constraint binds precisely when the worker's type decreases.\n\nFootnote 26: See also Harris and Holmstrom (1982), Thomas and Worrall (1988), or Grochulski and Zhang (2011).\n\nFormally, the proof of Theorem 4.2 is based on the idea of the proof of Theorem 1 in McClellan (2017). It follows from the five steps below:\n\n* First consider a relaxation of problem (RP\\({}^{i}\\)) for which the constraint (DPC\\({}^{i}\\)) only needs to hold for on a finite set of (stopping) times.\n* Lemma 1.5 derives the Lagrangian associated with the relaxed problem as an application of Theorem 1 in Balzer and Janssen (2002).\n* In the third step, useful properties of the solution of the relaxed problem introduced in step 1 are established.\n* The fourth step identifies a promotion contest that guarantees the principal a payoff of at least the value of the relaxed problem introduced in the first step. It is enough to focus on promotion contests that promote worker \\(i\\) after good performances (as \\(X^{i}\\) crosses an upper threshold from bellow) and take the outside option after bad outcomes (when \\(X^{i}\\) crosses a lower threshold from above).\n* Putting everything together and letting the set of times at which (DPC\\({}^{i}\\)) holds grow dense yields Theorem 4.2.\n\nSteps 1, 2, and 5 are essentially the same as in the proof of Theorem 1 in McClellan (2017). Steps 3 and 4 are new and specific to our setting. The details are in Appendix 1.6.1. Supporting Lemmas are in Appendix 1.6.2.\n\n "]}, {"edit": ["where \\(e_{h}\\) is the distribution that puts all its weight on \\(h=(\\ldots,x_{-2},x_{-1})\\in\\mathcal{X}^{\\infty}\\), and \\(e_{h}\\otimes p(\\cdot|x_{-1})\\) encodes the fact that the receivers' belief about \\(\\bar{\\omega}_{t}\\) comes from the resulting Markovian transition \\(p(\\cdot|x_{-1})\\). (Here, \\(\\mathbf{P}^{\\sigma}\\) denotes the probability measure induced by the signaling mechanism \\(\\sigma\\) together with the underlying Markovian dynamics, assuming that the receivers adopt the sender's recommendations.) When the preceding condition holds, we denote the resulting belief sequence \\(\\{\\phi_{t}:t\\in\\mathbb{Z}\\}\\) by \\(\\Phi_{\\mathsf{full}}\\) and call it the _full-history information model_.\n2. **No-history information model:** At the other extreme, consider the case where the receivers have no information about the history of the process. Then, at any time \\(t\\), the receiver's belief \\(\\phi_{t}\\) must be independent of the realized history. A natural approach, motivated by the requirement of consistency,2 is to let each belief \\(\\phi_{t}\\) equal the invariant distribution \\(\\mathsf{Inv}(\\sigma)\\). Specifically, we have for each \\(t\\in\\mathbb{Z}\\), Footnote 2: In certain cases, this modeling assumption can be established formally. For instance, if time periods denote the Poisson arrival times of receivers to a stochastic system, then the receivers observe the system distributed as the time-average [Wolff, 1982], which equals the expectation w.r.t. the invariant distribution when the latter is unique. \\[\\mathbf{P}^{\\sigma}\\big{(}\\phi_{t}=\\mathsf{Inv}(\\sigma)\\otimes P\\,\\big{|}\\,\\, \\bar{h}_{t}=h,\\bar{\\omega}_{t}=\\omega\\,\\big{)}=1,\\text{for all }h\\in\\mathcal{X}^{\\infty}\\text{ and }\\omega\\in\\Omega.\\] Here, \\(\\mathsf{Inv}(\\sigma)\\otimes P\\) encodes the distribution of \\((\\bar{h}_{t},\\bar{\\omega}_{t})\\) where the history \\(\\bar{h}_{t}\\) is distributed as \\(\\mathsf{Inv}(\\sigma)\\), and the state \\(\\bar{\\omega}_{t}\\) is obtained from a subsequent transition from the Markov kernel \\(P\\). For the setting where the preceding condition holds, we denote the belief sequence \\(\\{\\phi_{t}:t\\in\\mathbb{Z}\\}\\) by \\(\\Phi_{\\mathsf{no}}\\) and call it the _no-history information model_.\n3. **Partial-history information models:** Between the two extremes described above lie a multitude of information models where receivers possess partial information about the process history. In such partial-history models, the belief sequence \\(\\phi_{t}\\) would have a complex dependence on the history \\(\\bar{h}_{t}\\). Although a comprehensive analysis of all such models is beyond the scope of this paper, we focus on a particular sequence of information models to capture realistic scenarios where the receivers may have some stale information about the process.3 Specifically, for a fixed \\(\\ell\\geq 0\\), consider the setting where the receivers observe the process with an \\(\\ell\\)-period lag. In other words, at each time \\(t\\), the receiver observes the history \\(\\bar{h}_{t-\\ell}\\), i.e., all the state-action pairs before time \\(t-\\ell\\). Then, we have for each \\(t\\in\\mathbb{Z}\\), Footnote 3: Such stale information about the process "], "nougat": ["where \\(e_{h}\\) is the distribution that puts all its weight on \\(h=(\\ldots,x_{-2},x_{-1})\\in\\mathcal{X}^{\\infty}\\), and \\(e_{h}\\otimes p(\\cdot|x_{-1})\\) encodes the fact that the receivers' belief about \\(\\bar{\\omega}_{t}\\) comes from the resulting Markovian transition \\(p(\\cdot|x_{-1})\\). (Here, \\(\\mathbf{P}^{\\sigma}\\) denotes the probability measure induced by the signaling mechanism \\(\\sigma\\) together with the underlying Markovian dynamics, assuming that the receivers adopt the sender's recommendations.) When the preceding condition holds, we denote the resulting belief sequence \\(\\{\\phi_{t}:t\\in\\mathbb{Z}\\}\\) by \\(\\Phi_{\\mathsf{full}}\\) and call it the _full-history information model_.\n2. **No-history information model:** At the other extreme, consider the case where the receivers have no information about the history of the process. Then, at any time \\(t\\), the receiver's belief \\(\\phi_{t}\\) must be independent of the realized history. A natural approach, motivated by the requirement of consistency,2 is to let each belief \\(\\phi_{t}\\) equal the invariant distribution \\(\\mathsf{Inv}(\\sigma)\\). Specifically, we have for each \\(t\\in\\mathbb{Z}\\), Footnote 2: In certain cases, this modeling assumption can be established formally. For instance, if time periods denote the Poisson arrival times of receivers to a stochastic system, then the receivers observe the system distributed as the time-average [Wolff, 1982], which equals the expectation w.r.t. the invariant distribution when the latter is unique. \\[\\mathbf{P}^{\\sigma}\\big{(}\\phi_{t}=\\mathsf{Inv}(\\sigma)\\otimes P\\,\\big{|}\\,\\, \\bar{h}_{t}=h,\\bar{\\omega}_{t}=\\omega\\,\\big{)}=1,\\text{for all }h\\in\\mathcal{X}^{\\infty}\\text{ and }\\omega\\in\\Omega.\\] Here, \\(\\mathsf{Inv}(\\sigma)\\otimes P\\) encodes the distribution of \\((\\bar{h}_{t},\\bar{\\omega}_{t})\\) where the history \\(\\bar{h}_{t}\\) is distributed as \\(\\mathsf{Inv}(\\sigma)\\), and the state \\(\\bar{\\omega}_{t}\\) is obtained from a subsequent transition from the Markov kernel \\(P\\). For the setting where the preceding condition holds, we denote the belief sequence \\(\\{\\phi_{t}:t\\in\\mathbb{Z}\\}\\) by \\(\\Phi_{\\mathsf{no}}\\) and call it the _no-history information model_.\n3. **Partial-history information models:** Between the two extremes described above lie a multitude of information models where receivers possess partial information about the process history. In such partial-history models, the belief sequence \\(\\phi_{t}\\) would have a complex dependence on the history \\(\\bar{h}_{t}\\). Although a comprehensive analysis of all such models is beyond the scope of this paper, we focus on a particular sequence of information models to capture realistic scenarios where the receivers may have some stale information about the process.3 Specifically, for a fixed \\(\\ell\\geq 0\\), consider the setting where the receivers observe the process with an \\(\\ell\\)-period lag. In other words, at each time \\(t\\), the receiver observes the history \\(\\bar{h}_{t-\\ell}\\), i.e., all the state-action pairs before time \\(t-\\ell\\). Then, we have for each \\(t\\in\\mathbb{Z}\\), Footnote 3: Such stale information about the process "]}, {"edit": ["regularly put forward but whose analysis has been neglected thus far. To properly capture the nonlinear dynamics of the competitive storage model, we follow the approach of Koop et al. (1996) who define the IRFs as state-and-history-dependent random variables. The simulated IRFs show that prices immediately decline after a positive interest rate shock and converge slowly to their long-run value. Moreover, an interest rate increase depresses storage and increases price volatility. Inventories decrease immediately with the shock but reach their lowest value after a long time, and convergence to the long-run average is even slower compared with price. Price volatility mostly follows stock dynamics: a larger response in inventory causes an oppositely larger response in price volatility. Finally, the magnitude and overall pattern of the IRFs depend substantially on the market supply and interest rate regimes.8\n\nFootnote 8: These results suggest that postulating an invariant effect of monetary shocks under different market supply and interest rate regimes may cause biases in empirical analysis.\n\nOur work joins a large literature on the relationship between interest rates and commodity prices. Jeffrey Frankel made numerous empirical and theoretical contributions on the topic, with an emphasis on how commodity prices overshoot their long-run target following a shock because of their inherent price flexibility (Frankel and Hardouvelis, 1985; Frankel, 1986, 2008, 2014). This literature has tended to find a negative effect of interest rate hikes on commodity prices in the short (Rosa, 2014; Basistha and Kurov, 2015; Scrimgeour, 2015) and medium run (Akram, 2009; Anzuini et al., 2013; Harvey et al., 2017), with the exception of Kilian and Zhou (2022) who find no effect of real interest rate movements on oil prices. The negative relationship of interest rates and commodity prices has also been detected by Christiano et al. (1999) and Bernanke et al. (2005) in other contexts. In addition, interest rates not only affect commodity price levels, but also their cross-correlation and their volatility as shown by Gruber and Vigfusson (2018). Compared with these studies, the method developed here allows for a systematic analysis of the transmission mechanisms. In particular, our quantitative analysis clarifies the role of the speculative channel in the observed negative relationship.\n\nOur work contributes also to the studies of the theoretical and empirical properties of the competitive storage model. Theoretically, we derive new results on the existence and uniqueness of a rational expectations equilibrium in a general competitive storage model with state-dependent and time-varying discount factors, which extend the results of Scheinkman and Schechtman (1983), Deaton and Laroque (1992), Cafiero et al. (2015) and is crucial for studying the dynamic causal effect of these factors on commodity price fluctuations.9 In addition, we provide a sharp characterization of the analytical properties of the equilibrium solution, based on which we develop a suitable endogenous grid "], "nougat": ["regularly put forward but whose analysis has been neglected thus far. To properly capture the nonlinear dynamics of the competitive storage model, we follow the approach of Koop et al. ( 1996 ) who define the IRFs as state-and-history-dependent random variables. The simulated IRFs show that prices immediately decline after a positive interest rate shock and converge slowly to their long-run value. Moreover, an interest rate increase depresses storage and increases price volatility. Inventories decrease immediately with the shock but reach their lowest value after a long time, and convergence to the long-run average is even slower compared with price. Price volatility mostly follows stock dynamics: a larger response in inventory causes an oppositely larger response in price volatility. Finally, the magnitude and overall pattern of the IRFs depend substantially on the market supply and interest rate regimes. 8\n\nFootnote 8: These results suggest that postulating an invariant effect of monetary shocks under different market supply and interest rate regimes may cause biases in empirical analysis.\n\nOur work joins a large literature on the relationship between interest rates and commodity prices. Jeffrey Frankel made numerous empirical and theoretical contributions on the topic, with an emphasis on how commodity prices overshoot their long-run target following a shock because of their inherent price flexibility ( Frankel and Hardouvelis , 1985 ; Frankel , 1986 , 2008 , 2014 ). This literature has tended to find a negative effect of inter-est rate hikes on commodity prices in the short ( Rosa , 2014 ; Basistha and Kurov , 2015 ; Scrimgeour , 2015 ) and medium run ( Akram , 2009 ; Anzuini et al. , 2013 ; Harvey et al. , 2017 ), with the exception of Kilian and Zhou ( 2022 ) who find no effect of real interest rate movements on oil prices. The negative relationship of interest rates and commodity prices has also been detected by Christiano et al. ( 1999 ) and Bernanke et al. ( 2005 ) in other contexts. In addition, interest rates not only affect commodity price levels, but also their cross-correlation and their volatility as shown by Gruber and Vigfusson ( 2018 ). Compared with these studies, the method developed here allows for a systematic analysis of the transmission mechanisms. In particular, our quantitative analysis clarifies the role of the speculative channel in the observed negative relationship.\n\nOur work contributes also to the studies of the theoretical and empirical properties of the competitive storage model. Theoretically, we derive new results on the existence and uniqueness of a rational expectations equilibrium in a general competitive storage model with state-dependent and time-varying discount factors, which extend the results of Scheinkman and Schechtman ( 1983 ), Deaton and Laroque ( 1992 ), Cafiero et al. ( 2015 ) and is crucial for studying the dynamic causal effect of these factors on commodity price fluctuations. 9 9 See also Basak and Pavlova ( 2016 ) for another storage model with stochastic discount factor but which abstracts from the non-negative constraint on storage.\n\nFootnote 9: See also Basak and Pavlova ( 2016 ) for another storage model with stochastic discount factor but which abstracts from the non-negative constraint on storage.\n\nIn addition, we provide a sharp characterization of the analytical properties of the equilibrium solution, based on which we develop a suitable endogenous grid "]}, {"edit": ["using numerical approximations. To motivate our proposed approximation of \\(u_{t}\\left(x\\right)\\), observe that an equivalent representation of it is the solution to a partial integro-differential equation (PIDE). An important component of this PIDE is the so-called (infinitesimal) generator \\(A\\) of \\(x_{t}\\) which fully characterizes the dynamics. The generator is given by, for any sufficiently regular function \\(f\\left(x\\right)\\),\n\n\\[Af\\left(x\\right)=A_{D}f\\left(x\\right)+A_{J}f\\left(x\\right), \\tag{2.4}\\]\n\nwhere, with \\(\\sigma^{2}\\left(x\\right):=\\sigma\\left(x\\right)\\sigma\\left(x\\right)^{\\top}\\in \\mathbb{R}^{d\\times d}\\),\n\n\\[A_{D}f\\left(x\\right)=\\sum_{i=1}^{d}\\mu_{i}\\left(x\\right)\\partial_{x_{i}}f\\left( x\\right)+\\frac{1}{2}\\sum_{i,j=1}^{d}\\sigma_{ij,t}^{2}\\left(x\\right)\\partial_{x_{i} ,x_{j}}^{2}f\\left(x\\right) \\tag{2.5}\\]\n\nand\n\n\\[A_{J}f\\left(x\\right)=\\lambda\\left(x\\right)\\int_{\\mathbb{R}^{d}}\\left[f\\left(x +c\\right)-f\\left(x\\right)\\right]\\nu\\left(c|x\\right)dc \\tag{2.6}\\]\n\nare the generators of the diffusive and jump component of \\(x_{t}\\), respectively. Here, \\(\\partial_{x_{i}}f\\left(x\\right)=\\partial f\\left(x\\right)/\\left(\\partial x_{i}\\right)\\), \\(\\partial_{x_{i},x_{j}}^{2}f\\left(x\\right)=\\partial^{2}f\\left(x\\right)/\\left( \\partial x_{i}\\partial x_{j}\\right)\\) and similar for other partial derivatives.\n\nIt can then be shown, c.f. Section 4, that \\(u_{t}\\left(x\\right)\\) solves the following PIDE:\n\n\\[\\partial_{t}u_{t}\\left(x\\right)=\\left[A-r\\left(x\\right)\\right]u_{t}\\left(x \\right),\\quad t\\geq 0,x\\in\\mathcal{X}, \\tag{2.7}\\]\n\nwith initial condition \\(u_{0}\\left(x\\right)=f\\left(x\\right)\\) for all \\(x\\in\\mathcal{X}\\). In the case of pure diffusions (\\(A_{J}=0\\)), the reader may recognize (2.2) as the celebrated Feynman-Kac representation of the solution to (2.7) which also holds for the general case of jump-diffusions. The solution to this PIDE can be represented in the following abstract manner: \\(u_{t}\\left(x\\right)=e^{\\left(A-r\\right)t}f\\left(x\\right)\\), where \\(e^{\\left(A-r\\right)t}\\) is the exponential of the operator \\(A-r\\) in the sense that\n\n\\[\\frac{\\partial e^{\\left(A-r\\right)t}}{\\partial t}=\\left(A-r\\right)e^{\\left(A-r \\right)t}. \\tag{2.8}\\]\n\nWe are now interested in obtaining an approximation of \\(u_{t}\\left( "], "nougat": ["using numerical approximations. To motivate our proposed approximation of \\(u_{t}\\left(x\\right)\\), observe that an equivalent representation of it is the solution to a partial integro-differential equation (PIDE). An important component of this PIDE is the so\u2013called (infinitesimal) generator \\(A\\) of \\(x_{t}\\) which fully characterizes the dynamics. The generator is given by, for any sufficiently regular function \\(f\\left(x\\right)\\),\n\n\\[Af\\left(x\\right)=A_{D}f\\left(x\\right)+A_{J}f\\left(x\\right), \\tag{2.4}\\]\n\nwhere, with \\(\\sigma^{2}\\left(x\\right):=\\sigma\\left(x\\right)\\sigma\\left(x\\right)^{\\top}\\in \\mathbb{R}^{d\\times d}\\),\n\n\\[A_{D}f\\left(x\\right)=\\sum_{i=1}^{d}\\mu_{i}\\left(x\\right)\\partial_{x_{i}}f\\left( x\\right)+\\frac{1}{2}\\sum_{i,j=1}^{d}\\sigma_{ij,t}^{2}\\left(x\\right)\\partial_{x_{i} ,x_{j}}^{2}f\\left(x\\right) \\tag{2.5}\\]\n\nand\n\n\\[A_{J}f\\left(x\\right)=\\lambda\\left(x\\right)\\int_{\\mathbb{R}^{d}}\\left[f\\left(x+ c\\right)-f\\left(x\\right)\\right]\\nu\\left(c|x\\right)dc \\tag{2.6}\\]\n\nare the generators of the diffusive and jump component of \\(x_{t}\\), respectively. Here, \\(\\partial_{x_{i}}f\\left(x\\right)=\\partial f\\left(x\\right)/\\left(\\partial x_{i}\\right)\\), \\(\\partial_{x_{i},x_{j}}^{2}f\\left(x\\right)=\\partial^{2}f\\left(x\\right)/\\left( \\partial x_{i}\\partial x_{j}\\right)\\) and similar for other partial derivatives.\n\nIt can then be shown, c.f. Section 4 , that \\(u_{t}\\left(x\\right)\\) solves the following PIDE:\n\n\\[\\partial_{t}u_{t}\\left(x\\right)=\\left[A-r\\left(x\\right)\\right]u_{t}\\left(x \\right),\\quad t\\geq 0,x\\in\\mathcal{X}, \\tag{2.7}\\]\n\nwith initial condition \\(u_{0}\\left(x\\right)=f\\left(x\\right)\\) for all \\(x\\in\\mathcal{X}\\). In the case of pure diffusions (\\(A_{J}=0\\)), the reader may recognize ( 2.2 ) as the celebrated Feynman\u2013Kac representation of the solution to ( 2.7 ) which also holds for the general case of jump\u2013diffusions. The solution to this PIDE can be represented in the following abstract manner: \\(u_{t}\\left(x\\right)=e^{\\left(A-r\\right)t}f\\left(x\\right)\\), where \\(e^{\\left(A-r\\right)t}\\) is the exponential of the operator \\(A-r\\) in the sense that\n\n\\[\\frac{\\partial e^{\\left(A-r\\right)t}}{\\partial t}=\\left(A-r\\right)e^{\\left(A-r \\right)t}. \\tag{2.8}\\]\n\nWe are now interested in obtaining an approximation of \\(u_{t "]}, {"edit": ["Table 3 adds regional differences in the initial location parameter, as well as dispersion parameters. That generates some bias. Finally, Panel E makes regions more similar in terms of their location parameters. That leads to large biases.\n\nThe fundamental problem is similar to that in the case of the fraction affected design: the treatment intensity variable used in the regression does not neatly map into the thought experiment that intuitively identifies the causal effect of interest. Random shocks to location, uncorrelated with dispersion parameters, provide good variation because they closely correspond to what would happen if all regions where the same, but subject to random changes in minimum wage bindingness. But suppose that regions differ exclusively in their initial dispersion parameters and are subject to a national increase in the minimum wage. One can show that the effective minimum wages changes differentially across regions, and so do outcomes such as employment and wages. However, that scenario does not correspond to the thought experiment of interest. Correspondingly, the estimated effects of the minimum wage\n\n\\begin{table}\n\\begin{tabular}{c c c c c c} \\hline \\hline  & & \\multicolumn{4}{c}{Outcome} \\\\  & Emp. & p10 & p25 & p50 & p90 \\\\ \\hline _Panel A: Baseline_ & & & & & \\\\ Mean causal effect & 0.000 & 0.022 & 0.002 & 0.000 & 0.000 \\\\ Effective min. wage & 0.000 & 0.025 & 0.002 & 0.000 & -0.000 \\\\  & (0.000) & (0.019) & (0.010) & (0.000) & (0.019) \\\\ _Panel B: Reduction in average dispersion_ & & & & & \\\\ Mean causal effect & 0.000 & 0.025 & 0.001 & 0.000 & 0.000 \\\\ Effective min. wage & 0.000 & 0.016 & 0.001 & 0.000 & -0.000 \\\\  & (0.000) & (0.018) & (0.010) & (0.000) & (0.019) \\\\ _Panel C: Dispersion changes correlated with location changes_ & & & & & \\\\ Mean causal effect & 0.000 & 0.019 & 0.001 & 0.000 & 0.000 \\\\ Effective min. wage & 0.000 & 0.076 & 0.034 & 0.000 & -0.064 \\\\  & (0.000) & (0.016) & (0.009) & (0.000) & (0.016) \\\\ _Panel D: Dispersion changes correlated with initial location_ & & & & & \\\\ Mean causal effect & 0.000 & 0.019 & 0.001 & 0.000 & 0.000 \\\\ Effective min. wage & 0.000 & 0.011 & -0.006 & 0.000 & 0.014 \\\\  & (0.000) & (0.019) & (0.010) & (0.000) & (0.019) \\\\ _Panel E: Regional variation in location is small_ & & & & & \\\\ Mean causal effect & 0.000 & 0.008 & 0.000 & 0.000 & 0.000 \\\\ Effective min. wage & 0.00 "], "nougat": ["Table 3 adds regional differences in the initial location parameter, as well as dispersion parameters. That generates some bias. Finally, Panel E makes regions more similar in terms of their location parameters. That leads to large biases.\n\nThe fundamental problem is similar to that in the case of the fraction affected design: the treatment intensity variable used in the regression does not neatly map into the thought experiment that intuitively identifies the causal effect of interest. Random shocks to location, uncorrelated with dispersion parameters, provide good variation because they closely correspond to what would happen if all regions where the same, but subject to random changes in minimum wage bindingness. But suppose that regions differ exclusively in their initial dispersion parameters and are subject to a national increase in the minimum wage. One can show that the effective minimum wages changes differentially across regions, and so do outcomes such as employment and wages. However, that scenario does not correspond to the thought experiment of interest. Correspondingly, the estimated effects of the minimum wage\n\n\\begin{table}\n\\begin{tabular}{c c c c c c} \\hline \\hline  & & \\multicolumn{4}{c}{Outcome} \\\\  & Emp. & p10 & p25 & p50 & p90 \\\\ \\hline _Panel A: Baseline_ & & & & & \\\\ Mean causal effect & 0.000 simulations, each with 50 regions.\n\n "]}, {"edit": ["_Recall that \\(\\mathfrak{r}^{(\\ell)}_{it}\\) is the \\(t\\)-th row of \\(M_{F^{0,(\\ell)}}X^{(\\ell)}_{i}\\) and let \\(\\hat{\\mathfrak{r}}^{(\\ell)}_{it}\\) be the \\(t\\)-th row of \\(M_{\\hat{F}^{(\\ell)}}X^{(\\ell)}_{i}\\), respectively. Under Assumption 1\\({}^{\\ast}\\) (iii), we have \\(\\hat{\\Omega}^{(\\ell)}_{i}=\\frac{1}{T_{\\ell}}\\sum_{t\\in\\mathcal{T}_{t}}\\hat{ \\mathfrak{r}}^{(\\ell)}_{it}\\hat{\\mathfrak{r}}^{(\\ell)\\prime}_{it}\\hat{e}^{2}_{it}\\) and \\(\\Omega^{0,(\\ell)}_{i}=\\frac{1}{T_{\\ell}}\\sum_{t\\in\\mathcal{T}_{t}}\\mathbb{E}( \\mathfrak{r}^{(\\ell)}_{it}\\hat{\\mathfrak{r}}^{(\\ell)\\prime}_{it}e^{2}_{it})\\). It remains to show_\n\n\\[\\max_{i\\in\\mathcal{N}}\\left\\|\\frac{1}{T_{\\ell}}\\sum_{t\\in\\mathcal{T}_{\\ell}} \\left[\\hat{\\mathfrak{r}}^{(\\ell)}_{it}\\hat{\\mathfrak{r}}^{(\\ell)\\prime}_{it} \\hat{e}^{2}_{it}-\\mathbb{E}(\\mathfrak{r}^{(\\ell)}_{it}\\mathfrak{r}^{(\\ell)\\prime }_{it}e^{2}_{it})\\right]\\right\\|=o_{p}(1).\\] (D.23)\n\n_From the definitions of \\(\\mathfrak{r}^{(\\ell)}_{it}\\) and \\(\\hat{\\mathfrak{r}}^{(\\ell)}_{it}\\), we notice that \\(\\mathfrak{r}^{(\\ell)}_{it}=X_{it}-\\frac{1}{T_{\\ell}}X^{(\\ell)\\prime}_{i}F^{0,( \\ell)}f^{0}_{t}\\) and \\(\\hat{\\mathfrak{r}}^{(\\ell)}_{it}=X_{it}-\\frac{1}{T_{\\ell}}X^{(\\ell)\\prime}_{i }\\hat{F}^{(\\ell)}\\hat{f}^{0}_{t}\\), which gives_\n\n\\[\\mathfrak{r}^{(\\ell)}_{it}-\\hat{\\mathfrak{r}}^{(\\ell)}_{it}=\\frac{1}{T_{\\ell}}X ^{(\\ell)\\prime}_{i}(\\hat{F}^{(\\ell)}\\hat{f}^{0}_{t}-F^{0,(\\ell)}f^{0}_{t}).\\] (D.24)\n\n_Note that_\n\n\\[\\max_{t\\in\\mathcal{T}_{\\ell}}\\frac{1}{\\sqrt{T_{\\ell}}}\\left\\|\\hat {F}^{(\\ell)}\\hat{f}^{0}_{t}-F^{0,(\\ell)}f^{0}_{t}\\right "], "nougat": ["_Recall that \\(\\mathfrak{r}^{(\\ell)}_{it}\\) is the \\(t\\)-th row of \\(M_{F^{0,(\\ell)}}X^{(\\ell)}_{i}\\) and let \\(\\hat{\\mathfrak{r}}^{(\\ell)}_{it}\\) be the \\(t\\)-th row of \\(M_{\\hat{F}^{(\\ell)}}X^{(\\ell)}_{i}\\), respectively. Under Assumption 1\\({}^{\\ast}\\) (iii), we have \\(\\hat{\\Omega}^{(\\ell)}_{i}=\\frac{1}{T_{\\ell}}\\sum_{t\\in\\mathcal{T}_{t}}\\hat{ \\mathfrak{r}}^{(\\ell)}_{it}\\hat{\\mathfrak{r}}^{(\\ell)\\prime}_{it}\\hat{e}^{2}_{it}\\) and (iv) and Lemma B.7 (i). Then by ( D.24 ) and Assumption 8 (ii), we have_\n\n\\[\\max_{i\\in\\mathcal{N},t\\in\\mathcal{T}_{t}}\\left\\|\\hat{\\mathfrak{r}}^{(\\ell)}_{ it}-\\hat{\\mathfrak{r}}^{(\\ell)}_{it}\\right\\|_{2}=e_{it}\\left(\\sqrt{\\frac{\\log N \\lor T}{N\\wedge T}}\\right),\\quad\\text{and}\\] (D.25 )\n\n_Then_\n\n\\[\\max_{i\\in\\mathcal{N},t\\in\\mathcal{T}_{t}}\\left|\\hat{e}_{it}-e_{ it}\\right|=O_{p}\\left(\\sqrt{\\frac{\\log N}{T}}(NT)^{1/q}\\right)+O_{p}\\left(\\sqrt{ \\frac{\\log N\\lor T}{N\\wedge T}}\\right),\\quad\\text{and}\\] (D.26 ) \\[\\hat{e}^{2}_{it}-e^{2}_{it}=e_{it}\\left(\\hat{e}_{it}-e_{it}\\right) +\\left(\\hat{e}_{it}-e_{it}\\right)^{2}=e_{it}X^{\\prime}_{it}R_{1,it}+R_{2,it}\\] \\[s.t.\\max_{i\\in\\mathcal{N},t\\in\\mathcal{T}_{t}}\\left\\|R_{1,it} \\right\\|_{2}=O_{p}\\left(\\sqrt{\\frac{\\log N}{T}}\\right),\\max_{i\\in[N],t\\in \\mathcal{T}_{t}}\\left|R_{2,it}\\right|=O_{p}\\left(\\sqrt{\\frac{\\log N\\lor T}{N \\wedge T}}\\right)\\]\n\n_by Lemmas D.5 (iii), D.6 (iii), and D.6(iv). It follows that_\n\n\\[\\frac{1}{T_{\\ell}}\\sum_{t\\in\\mathcal{T}_{t}}\\hat{\\mathfrak{r}}^{ (\\ell)}_{it}\\hat{\\mathfrak{r}}^{(\\ell)\\prime}_{it}\\hat{e}^{2}_{it} =\\frac{1}{T_{\\ell}}\\sum_{t\\in\\mathcal{T}_{t}}\\mathfrak{r}^{(\\ell) \\prime}_ "]}, {"edit": ["equilibrium is unique by Proposition 3, under the conditions in Theorem 1, there are no equilibria in which the land price equals its fundamental value.\n\nTheorem 1 has three important implications. First, it clarifies the role of unbalanced growth and elasticity of substitution for generating land overvaluation, which was previously overlooked. Regarding the assumption of elasticity of substitution between land and non-land exceeding 1, we justify it on empirical and theoretical grounds as discussed in Section 3.2.\n\nSecond, we can derive a new insight on the long-run behavior of land prices in a modern economy. The conventional view is that on the long-run trend, the land price should reflect its fundamental value, even if it may deviate from the fundamental value temporarily. In sharp contrast with this widely-held view, Theorem 1 implies that during the process of economic development characterized by unbalanced productivity growth, land overvaluation will naturally and necessarily arise.6\n\nFootnote 6: Of course, if \\(G_{H}=G_{X}\\) holds in the long run, which is often assumed to ensure a balanced growth path in the growth literature, there will be no overvaluation in land prices but this is obviously a knife-edge case.\n\nBefore discussing the third implication in Section 3.4, we show that all examples in Section 2 are special cases of Theorem 1.\n\n**Corollary 2**.: _Proposition 2 is true._\n\nProof.: Define the aggregate production function by\n\n\\[F_{t}(H,X)=A_{1t}H_{1}H+A_{2t}(H_{2}H)^{\\alpha}X^{1-\\alpha},\\]\n\nwhere \\(H_{1},H_{2}>0\\) are constants. Define\n\n\\[F(H,X) =H_{2}H+H_{2}^{\\alpha}H^{\\alpha}X^{1-\\alpha},\\] \\[(A_{Ht},A_{Xt}) =(A_{1t},(A_{2t}/A_{1t}^{\\alpha})^{\\frac{1}{1-\\alpha}}).\\]\n\nThen clearly \\(F_{t}(H,X)=F(A_{Ht}H,A_{Xt}X)\\) and Assumption 1 holds. Assumption 2 holds by Example 3. If \\(A_{jt}=G_{j}^{t}\\) with \\(G_{1}>G_{2}\\), then\n\n\\[A_{Ht}/A_{Xt}=(A_{1t}/A_{2t})^{\\frac{1}{1-\\alpha}}=(G_{1}/G_{2})^{\\frac{t}{1- \\alpha}},\\]\n\nso (3.7) holds. \n\n**Corollary 3**.: _Land is overvalued in Example 1 if \\(G_{1}>G_{2}\\)._ "], "nougat": ["equilibrium is unique by Proposition 3 , under the conditions in Theorem 1 , there are no equilibria in which the land price equals its fundamental value.\n\nTheorem 1 has three important implications. First, it clarifies the role of unbalanced growth and elasticity of substitution for generating land overvaluation, which was previously overlooked. Regarding the assumption of elasticity of substitution between land and non-land exceeding 1, we justify it on empirical and theoretical grounds as discussed in Section 3.2 .\n\nSecond, we can derive a new insight on the long-run behavior of land prices in a modern economy. The conventional view is that on the long-run trend, the land price should reflect its fundamental value, even if it may deviate from the fundamental value temporarily. In sharp contrast with this widely-held view, Theorem 1 implies that during the process of economic development characterized by unbalanced productivity growth, land overvaluation will naturally and necessarily arise. 6\n\nFootnote 6: Of course, if \\(G_{H}=G_{X}\\) holds in the long run, which is often assumed to ensure a balanced growth path in the growth literature, there will be no overvaluation in land prices but this is 3.4 , we show that all examples in Section obviously a knife-edge case. 2 are special cases of Theorem 1 .\n\n**Corollary 2**.: _Proposition 2 is true._\n\nProof.: Define the aggregate production function by\n\n\\[F_{t}(H,X)=A_{1t}H_{1}H+A_{2t}(H_{2}H)^{\\alpha}X^{1-\\alpha},\\]\n\nwhere \\(H_{1},H_{2}>0\\) are constants. Define\n\n\\[F(H,X) =H_{2}H+H_{2}^{\\alpha}H^{\\alpha}X^{1-\\alpha},\\] \\[(A_{Ht},A_{Xt}) =(A_{1t},(A_{2t}/A_{1t}^{\\alpha})^{\\frac{1}{1-\\alpha}}).\\]\n\nThen clearly \\(F_{t}(H,X)=F(A_{Ht}H,A_{Xt}X)\\) and Assumption 1 holds. Assumption 2 holds by Example 3. If \\(A_{jt}=G_{j}^{t}\\) with \\(G_{1}>G_{2}\\), then\n\n\\[A_{Ht}/A_{Xt}=(A_{1t}/A_{2t})^{\\frac{1}{1-\\alpha}}=(G_{1}/G_{2})^{\\frac{t}{1- \\alpha}},\\]\n\nso (3.7) holds. \n\n**Corollary 3**.: _Land is overvalued in Example 1 if \\(G_{1}>G_{2}\\)._ "]}, {"edit": ["Whenever the context is clear, we will omit the arguments of \\(k_{0}^{*}\\), \\(k_{1}^{*}\\), \\(r_{0}^{*}\\), and \\(r_{1}^{*}\\). With the optimal \\(r_{i}^{*}\\) derived, it can be shown that a voter with costs \\(\\gamma_{i}\\) receives an expected payoff from \\(r=r_{1}^{*}\\) that is at least as great as from \\(r=r_{0}^{*}\\) if and only if\n\n\\[\\gamma_{i}\\leq k_{0}\\cdot F(k_{0})+k_{1}\\cdot(1-F(k_{1}))+t\\cdot(F(k_{1})-F(k_ {0})).\\]\n\nBecause the optimal rewards, \\(r_{1}^{*}\\) and \\(r_{0}^{*}\\), are characterized by the terms \\(k_{1}\\) and \\(k_{0}\\), we define the following term \\(k_{i}^{*}\\), which we refer to as individual \\(i\\)'s \"optimal \\(k\\).\"\n\n\\[k_{i}^{*}=\\begin{cases}k_{1}&\\text{if }\\gamma_{i}\\leq k_{0}\\cdot F(k_{0})+k_{1 }\\cdot(1-F(k_{1}))+t\\cdot(F(k_{1})-F(k_{0})),\\\\ k_{0}&\\text{otherwise}.\\end{cases} \\tag{11}\\]\n\nThe value \\(k_{i}^{*}\\) can be interpreted essentially as the optimal level of compliance, given \\(i\\)'s type, \\(\\gamma_{i}\\), because \\(F(k_{i}^{*})\\) represents \\(i\\)'s optimal _equilibrium prevalence_. Individuals with \\(k_{i}^{*}=k_{1}\\) prefer higher prevalence (in equilibrium, _i.e._, after taking transfers and the distribution of others' costs into account) than individuals with \\(k_{i}^{*}=k_{0}\\). We shall see that, in all equilibria with non-null algorithms, some individuals will \"vote for\" high compliance but ultimately not comply, or vice-versa. We will return to this, but the point to note is that this seeming preference reversal will be solely a function of the individual in question being on the \"losing side\" of the majority vote over the ultimate reward.\n\nFor any given pair \\((k_{0}^{*},k_{1}^{*})\\), Equation (11) defines a cut-point that divides individuals (in terms of their types) into \"low cost\" and \"high cost\" individuals -- individuals with low enough costs will support the higher reward level, \\(r_{1}^{*}\\), and individuals with high costs will support the lower reward level, \\(r_{0}^{*}\\). Equation (11) also demonstrates, as claimed earlier, that support for the higher reward increases in the marginal value of the externality, \\(t\\). This is stated formally in the following proposition.\n\n**Proposition 6**.: _For any \\(t\\), \\(F\\), and \\(\\phi\\), and any voter \\(i\\in N\\),_\n\n\\[\\left[\\begin{array}{l}k_{i}^{*}(t,F)=k_{1}^{*}(t,F)\\;\\;\\text{and}\\;\\;t^{ \\prime}>t\\end{array}\\right]\\Rightarrow\\;\\;k_{i}^{*}(t "], "nougat": ["Whenever the context is clear, we will omit the arguments of \\(k_{0}^{*}\\), \\(k_{1}^{*}\\), \\(r_{0}^{*}\\), and \\(r_{1}^{*}\\). With the optimal \\(r_{i}^{*}\\) derived, it can be shown that a voter with costs \\(\\gamma_{i}\\) receives an expected payoff from \\(r=r_{1}^{*}\\) that is at least as great as from \\(r=r_{0}^{*}\\) if and only if\n\n\\[\\gamma_{i}\\leq k_{0}\\cdot F(k_{0})+k_{1}\\cdot(1-F(k_{1}))+t\\cdot(F(k_{1})-F(k_ {0})).\\]\n\nBecause the optimal rewards, \\(r_{1}^{*}\\) and \\(r_{0}^{*}\\), are characterized by the terms \\(k_{1}\\) and \\(k_{0}\\), we define the following term \\(k_{i}^{*}\\), which we refer to as individual \\(i\\)'s \"optimal \\(k\\).\"\n\n\\[k_{i}^{*}=\\begin{cases}k_{1}&\\text{if }\\gamma_{i}\\leq k_{0}\\cdot F(k_{0})+k_{1 }\\cdot(1-F(k_{1}))+t\\cdot(F(k_{1})-F(k_{0})),\\\\ k_{0}&\\text{otherwise}.\\end{cases} \\tag{11}\\]\n\nThe value \\(k_{i}^{*}\\) can be interpreted essentially as the optimal level of compliance, given \\(i\\)'s type, \\(\\gamma_{i}\\), because \\(F(k_{i}^{*})\\) represents \\(i\\)'s optimal _equilibrium prevalence_. Individuals with \\(k_{i}^{*}=k_{1}\\) prefer higher prevalence (in equilibrium, _i.e._, after taking transfers and the distribution of others\u2019 costs into account) than individuals with \\(k_{i}^{*}=k_{0}\\). We shall see that, in all equilibria with non-null algorithms, some individuals will \u201cvote for\u201d high compliance but ultimately not comply, or viceversa. We will return to this, but the point to note is that this seeming preference reversal will be solely a function of the individual in question being on the \u201closing side\u201d of the majority vote over the ultimate reward.\n\nFor any given pair \\((k_{0}^{*},k_{1}^{*})\\), Equation (11) defines a cut-point that divides individuals (in terms of their types) into \u201clow cost\u201d and \u201chigh cost\u201d individuals \u2014 individuals with low enough costs will support the higher reward level, \\(r_{1}^{*}\\), and individuals with high costs will support the lower reward level, \\(r_{0}^{*}\\). Equation (11) also demonstrates, as claimed earlier, that support for the higher reward increases in the marginal value of the externality, \\(t\\). This is stated formally in the following proposition.\n\n**Proposition 6**.: _For any \\(t\\), \\(F\\), and \\(\\phi\\), and any voter \\(i\\in N\\),_\n\n\\[\\left[\\begin{array}{l}k_{i}^{*}(t,F)=k_{1}^{*}(t,F)\\;\\;\\text{and}\\;\\;t^{ \\prime}>t\\end{array}\\right]\\Rightarrow\\;\\;k_{i}^{*}(t^{\\prime}, "]}, {"edit": ["de Finetti and many other authors who wrote on bookmaking, assumed additivity more or less implicitly4 but emphasized the importance of monotonicity. They used the above result, without the antimonotonicity restriction, and several variations, to argue that it is rational to use subjective probabilities in the context of uncertainty. Linearity of utility, as implied here, is reasonable for moderate stakes (l'Haridon & Vieider, 2019, p. 189; Savage, 1954, p. 91). de Finetti's result was historically important as a foundation of Bayesianism. Our result shows, again, that the most critical case of \"bookmaking\" occurs when there are maximal possibilities of hedging (antimonotonicity). That is, de Finetti needed to defend his condition only for antimonotonic cases.\n\nFootnote 4: The bookmaking argument usually makes yet stronger assumptions by also incorporating positive scalar multiplications and, thus, linear combinations. We showed that such assumptions are not needed because they are implied by the other conditions.\n\n### Anscombe and Aumann's subjective expected utility\n\nAnscombe & Aumann (1963) presented a famous preference axiomatization of expected utility. 2 (1970, SS13.1) proposed a simplified version of their framework that is most popular today and we will use that. It is important because it is mostly used as point of departure for developing ambiguity theories (SS4.3). The outcome set \\(\\mathcal{C}\\) is nowadays often chosen to be a mixture space, and so will we do. Examples of mixture spaces include money intervals, convex subsets of linear spaces, or, the most common case in the literature on ambiguity, probability distributions over general \"prizes.\" For simplicity, readers not familiar with general mixture spaces may take in mind any such example.\n\nWe call \\(\\mathcal{C}\\) a _mixture space_ if it is endowed with a mixture operation. A _mixture operation_ generalizes convex combinations on linear spaces. It maps \\(\\mathcal{C}\\times[0,1]\\times\\mathcal{C}\\) to \\(\\mathcal{C}\\), and is denoted \\(\\alpha x+(1-\\alpha)y\\). It is required to satisfy the following conditions:\n\n* \\(1x+0y=x\\) (identity);\n* \\(\\alpha x+(1-\\alpha)y=(1-\\alpha)y+\\alpha x\\) (commutativity);\n* \\(\\alpha(\\beta x+(1-\\beta)y)+(1-\\alpha)y=\\alpha\\beta x+(1-\\alpha\\beta)y\\) (associativity).\n\nA function \\(U:\\mathcal{C}\\rightarrow\\mathbb{R}\\) is _affine_ if\n\n\\[\\text{for all outcomes }x,y,\\text{ and }0<\\alpha<1:\\ U(\\alpha x+(1-\\alpha)y)= \\alpha U(x)+(1-\\alpha)U(y).\\] (6) "], "nougat": ["de Finetti and many other authors who wrote on bookmaking, assumed additivity more or less implicitly 4 but emphasized the importance of monotonicity. They used the above result, without the antimonotonicity restriction, and several variations, to argue that it is rational to use subjective probabilities in the context of uncertainty. Linearity of utility, as implied here, is reasonable for moderate stakes (l'Haridon & Vieider, 2019, p. 189; Savage, 1954, p. 91). de Finetti's result was historically important as a foundation of Bayesianism. Our result shows, again, that the most critical case of \"bookmaking\" occurs when there are maximal possibilities of hedging (antimonotonicity). That is, de Finetti needed to defend his condition only for antimonotonic cases.\n\nFootnote 4: The bookmaking argument usually makes yet stronger assumptions by also incorporating positive scalar multiplications and, thus, linear combinations. We showed that such assumptions are not needed because result, without the antimonotonicity restriction, and several variations, to argue that it is they are implied by the other conditions. rational to use subjective probabilities in the context of uncertainty. Linearity of utility, as implied here, is reasonable for moderate stakes ( l\u2019Haridon & Vieider , 2019 , p. 189; Savage , 1954 , p. 91). de Finetti\u2019s result was historically important as a foundation of Bayesianism. Our result shows, again, that the most critical case of \u201cbookmaking\u201d occurs when there are maximal possibilities of hedging (antimonotonicity). That is, de Finetti needed to defend his condition only for antimonotonic cases.\n\n### Anscombe and Aumann\u2019s subjective expected utility\n\nAnscombe & Aumann ( 1963 ) presented a famous preference axiomatization of expected utility. 7 (1970, SS13.1) proposed a simplified version of their framework that is most popular today and we will use that. It is important because it is mostly used as point of departure for developing ambiguity theories ( \u00a7 4.3 ). The outcome set \\(\\mathcal{C}\\) is nowadays often chosen to be a mixture space, and so will we do. Examples of mixture spaces include money intervals, convex subsets of linear spaces, or, the most common case in the literature on ambiguity, probability distributions over general \u201cprizes.\u201d For simplicity, readers not familiar with general mixture spaces may take in mind any such example. We call \\(\\mathcal{C}\\) a _mixture space_ if it is endowed with a mixture operation. A _mixture operation_ generalizes convex combinations on linear spaces. It maps \\(\\mathcal{C}\\times[0,1]\\times\\mathcal{C}\\) to \\(\\mathcal{C}\\), and is denoted \\(\\alpha x+(1-\\alpha)y\\). It is required to satisfy the following conditions:\n\n* \\(1x+0y=x\\) (identity);\n* \\(\\alpha x+(1-\\alpha)y=(1-\\alpha)y+\\alpha x\\) (commutativity);\n* \\(\\alpha(\\beta x+(1-\\beta)y)+(1-\\alpha)y=\\alpha\\beta x+(1-\\alpha\\beta)y\\) (associativity).\n\nA function \\(U:\\mathcal{C}\\rightarrow\\mathbb{R}\\) is _affine_ if\n\n\\[\\text{for all outcomes }x,y,\\text{ and }0<\\alpha<1:\\ U(\\alpha x+(1-\\alpha)y)= \\alpha U(x)+(1-\\alpha)U(y).\\] (6) "]}, {"edit": ["no polynomial algorithm over \\(n\\) and \\(m\\) can compute this mechanism just because of the sheer size of the output. However, given a game \\(\\Gamma\\) and an output \\(o\\), it is not necessary to compute the whole description of the resilient truthful mechanism \\(m_{d}^{*}\\) for \\(\\Gamma\\) that implements \\(o\\), we only need to be able to compute \\(m_{d}^{*}(\\vec{m})\\) in polynomial time for each possible message profile \\(\\vec{m}\\). We state this as follows.\n\n**Theorem 2**.: _There exists an algorithm \\(\\pi\\) that receives as input the description of an information aggregation game \\(\\Gamma=(S,A,\\Omega,p,u)\\), an outcome \\(o\\) for \\(\\Gamma\\) implementable by a \\(k\\)-resilient mechanism (resp., strong \\(k\\)-resilient mechanism), and a message input \\(\\vec{m}\\) for the mediator, and \\(\\pi\\) outputs a value \\(q\\in[0,1]\\) such that the function \\(m_{d}^{*}\\) defined by \\(m_{d}^{*}(\\vec{m}):=A(\\Gamma,o,\\vec{m})\\) determines a \\(k\\)-resilient truthful mechanism (resp., strong \\(k\\)-resilient truthful mechanism) for \\(\\Gamma\\) that implements \\(o\\). Moreover, \\(\\pi\\) runs in polynomial time over \\(|\\Omega|\\) and \\(|S|\\)._\n\nThe proofs of Theorems 1 and 2 are detailed in Sections 4 and 5 respectively. Intuitively, each coalition imposes a constraint over the space of possible messages that the mediator may receive, implying that the mediator should suggest action \\(0\\) more often for some message inputs than others. These constraints induce a partial order over _pure inputs_ (i.e., messages such that all senders report the same state), which is precisely the order defined by \\(E\\) in Theorem 1. It can be shown that, even though there may be exponentially many possible coalitions of size at most \\(k\\), this partial order can be computed in polynomial time over the number of states and senders.\n\n## 4 Proof of Theorem 1\n\nIn this section we prove Theorem 1. Note that, because of Lemma 1, we only have to show that, given a game \\(\\Gamma=(S,A,\\Omega,p,u)\\) with \\(|\\Omega|=m\\) and \\(|S|=n\\), there exists a system of equations \\(E\\) as in Theorem 1 such that an outcome \\(o\\) is implementable by an honest mechanism that is \\(k\\)-resilient incentive-compatible (resp., strong \\(k\\)-resilient) for the senders if and only if \\((o^{*}(\\omega^{1}),\\ldots,o^{*}(\\omega^{m}))\\) is a solution of \\(E\\).\n\nTo understand the key idea, let us start with an example in which \\(\\Omega=\\{\\omega^{1},\\omega^{2}\\}\\), \\(S=\\{1,2,3,4\\}\\), senders \\(1,2\\) and \\(3\\) prefer action \\(0\\) in \\(\\omega^{2}\\), senders \\(2,3\\) and \\(4\\) prefer action \\(1\\) in \\(\\omega^{1}\\), and in which we are trying to characterize all outcomes that could be implemented by a mechanism that is \\(2\\)-resilient incentive-compatible for the senders. If all senders are honest, then the mediator could only receive inputs \\((\\omega^{1},\\omega^{1},\\omega^{1},\\omega^{1 "], "nougat": ["no polynomial algorithm over \\(n\\) and \\(m\\) can compute this mechanism just because of the sheer size of the output. However, given a game \\(\\Gamma\\) and an output \\(o\\), it is not necessary to compute the whole description of the resilient truthful mechanism \\(m_{d}^{*}\\) for \\(\\Gamma\\) that implements \\(o\\), we only need to be able to compute \\(m_{d}^{*}(\\vec{m})\\) in polynomial time for each possible message profile \\(\\vec{m}\\). We state this as follows.\n\n**Theorem 2**.: _There exists an algorithm \\(\\pi\\) that receives as input the description of an information aggre-gation game \\(\\Gamma=(S,A,\\Omega,p,u)\\), an outcome \\(o\\) for \\(\\Gamma\\) implementable by a \\(k\\)-resilient mechanism (resp., strong \\(k\\)-resilient mechanism), and a message input \\(\\vec{m}\\) for the mediator, and \\(\\pi\\) outputs a value \\(q\\in[0,1]\\) such that the function \\(m_{d}^{*}\\) defined by \\(m_{d}^{*}(\\vec{m}):=A(\\Gamma,o,\\vec{m})\\) determines a \\(k\\)-resilient truthful mechanism (resp., strong \\(k\\)-resilient truthful mechanism) for \\(\\Gamma\\) that implements \\(o\\). Moreover, \\(\\pi\\) runs in polynomial time over \\(|\\Omega|\\) and \\(|S|\\)._\n\nThe proofs of Theorems 1 and 2 are detailed in Sections 4 and 5 respectively. Intuitively, each coalition imposes a constraint over the space of possible messages that the mediator may receive, implying that the mediator should suggest action \\(0\\) more often for some message inputs than others. These constraints induce a partial order over _pure inputs_ (i.e., messages such that all senders report the same state), which is precisely the order defined by \\(E\\) in Theorem 1. It can be shown that, even though there may be exponentially many possible coalitions of size at most \\(k\\), this partial order can be computed in polynomial time over the number of states and senders.\n\nProof of Theorem 1. Note that, because of Lemma 1, we only have to show that, given a game \\(\\Gamma=(S,A,\\Omega,p,u)\\) with \\(|\\Omega|=m\\) and \\(|S|=n\\), there exists a system of equations \\(E\\) as in Theorem 1 such that an outcome \\(o\\) is implementable by an honest mechanism that is \\(k\\)-resilient incentive-compatible (resp., strong \\(k\\)-resilient) for the senders if and only if \\((o^{*}(\\omega^{1}),\\ldots,o^{*}(\\omega^{m}))\\) is a solution of \\(E\\).\n\nTo understand the key idea, let us start with an example in which \\(\\Omega=\\{\\omega^{1},\\omega^{2}\\}\\), \\(S=\\{1,2,3,4\\}\\), senders \\(1,2\\) and \\(3\\) prefer action \\(0\\) in \\(\\omega^{2}\\), senders \\(2,3\\) and \\(4\\) prefer action \\(1\\) in \\(\\omega^{1}\\), and in which we are trying to characterize all outcomes that could be implemented by a mechanism that is \\(2\\)-resilient incentive-compatible for the senders. If all senders are honest, then the mediator could only receive inputs \\((\\omega^{1},\\omega^{1},\\omega^{1},\\omega^{1})\\) or "]}]