[{"edit": ["In Figure 9, we show the Leavitt laws for a sample of Cepheids in the LMC (Persson et al. 2004) and in NGC 7250 (Owens et al. 2023). The scatter in the JWST F115W data for NGC 7250 is a factor of two smaller than in the SHoES F160W data; i.e., the improved resolution and higher signal-to-noise ratio of the JWST data results in a lower variance (\\(\\sigma^{2}\\)) for the F115W relation by almost a factor of four. This is all the more remarkable since the \\(J\\)-band data are single-phase observations only, while the HST observations have been corrected to mean light. The HST data exhibit more than three times the scatter of the \\(H\\)-band data for the LMC, the latter of which reflects the expected scatter for that band, as exemplified by the LMC data.\n\n## 12 Summary\n\nThe accuracy of the Cepheid distance scale has continued to improve over the century during which it has been used to measure the distances to nearby galaxies and set the scale for the determination of H\\({}_{0}\\). Still, challenges remain in overcoming systematic uncertainties. Many of these challenges will be overcome with new capabilities provided by the JWST.\n\nNew JWST data for the nearby galaxy NGC 7250 already demonstrate that (1) many of the Cepheids observed with HST/WFC3 are significantly crowded (and biased to brighter apparent magnitudes) by nearby neighbors. A re-analysis of the SH0ES optical data, then coupled with the new high-resolution and higher signal-to-noise JWST F115W data, leads to significantly reduced effects of crowding and smaller photometric uncertainties. (2) These improvements result in a factor of two lower scatter in the near-infrared Leavitt law for JWST F115W compared with HST F160W, even with single-epoch F115W JWST photometry.\n\nThe galaxies in our JWST CCHP program sample have all been selected to have with distances \\(\\lesssim\\)20 Mpc, close enough to minimize crowding effects. As for the case of NGC 7250 presented here, these data will be combined with a re-analysis of the SH0ES HST optical data for the Cepheids. TRGB, carbon star, and Cepheid distances to the same sample of galaxies being observed as part of the CCHP will allow measurement of three independent distances to each\n\nFigure 8: Four Cepheids in NGC 7250 discovered as part of the SH0ES project (bottom row: HST F160W/\\(H\\)-band exposures; top row: JWST F115W/\\(J\\) band). Each postage-stamp image is \\(2\\times 2\\) arcsec on a side. The red circles are centered at the position of the Cepheid determined from the HST optical (F350LP white light) photometry. It is immediately evident that the crowding for these Cepheids is quite severe and the signal-to-noise ratio (SNR) for the \\(H\\)-band data tends to be low, ranging from 1 to 23. In contrast, the SNR for the \\(J\\)-band data ranges from 36 to 121. On average, the JWST data have almost an order of magnitude greater SNR, and a four times better angular resolution, allowing the Cepheids to be distinguished clearly from the background.\n\n "], "nougat": ["In Figure 9, we show the Leavitt laws for a sample of Cepheids in the LMC (Persson et al. 2004) and in NGC 7250 (Owens et al. 2023). The scatter in the JWST F115W data for NGC 7250 is a factor of two smaller than in the SHoES F160W data; i.e., the improved resolution and higher signal-to-noise ratio of the JWST data results in a lower variance (\\(\\sigma^{2}\\)) for the F115W relation by almost a factor of four. This is all the more remarkable since the \\(J\\)-band data are single-phase observations only, while the HST observations have been corrected to mean light. The HST data exhibit more than three times the scatter of the \\(H\\)-band data for the LMC, the latter of which reflects the expected scatter for that band, as exemplified by the LMC data.\n\n## 12 Summary\n\nThe accuracy of the Cepheid distance scale has continued to improve over the century during which it has been used to measure the distances to nearby galaxies and set the scale for the determination of H 0. Still, challenges remain in overcoming systematic uncertainties. Many of these challenges will be overcome with new capabilities provided by the JWST . New JWST data for the nearby galaxy NGC 7250 already demonstrate that (1) many of the Cepheids observed with are significantly crowded (and biased to brighter apparent magnitudes) by nearby neighbors. A re-analysis of the SH0ES optical data, then coupled with the new high-resolution and higher signal-to-noise JWST F115W data, leads to significantly reduced effects of crowding and smaller photometric uncertainties. (2) These improvements result in a factor of two lower scatter in the near-infrared Leavitt law for JWST F115W compared with HST F160W, even with single-epoch F115W JWST photometry.\n\nThe galaxies in our JWST CCHP program sample have all been selected to have with distances \\(\\lesssim\\)20 Mpc, close enough to minimize crowding effects. As for the case of NGC 7250 presented here, these data will be combined with a re-analysis of the SH0ES HST optical data for the Cepheids. TRGB, carbon star, and Cepheid distances to the same sample of galaxies being observed as part of the CCHP will allow measurement of three independent distances to each\n\nFigure 8: Four Cepheids in NGC 7250 discovered as part of the SH0ES project (bottom row: HST F160W/\\(H\\)-band exposures; top row: JWST F115W/\\(J\\) band). Each postage-stamp image is \\(2\\times 2\\) arcsec on a side. The red circles are centered at the position of the Cepheid determined from the HST optical (F350LP white light) photometry. It is immediately evident that the crowding for these Cepheids is quite severe and the signal-to-noise ratio (SNR) for the \\(H\\)-band data tends to be low, ranging from 1 to 23. In contrast, the SNR for the \\(J\\)-band data ranges from 36 to 121. On average, the JWST data have almost an order of magnitude greater SNR, and a four times better angular resolution, allowing the Cepheids to be distinguished clearly from the background.\n\n "]}, {"edit": ["\\[\\alpha_{\\rm eff}=\\beta_{\\rm eff} \\tag{50}\\] \\[\\implies \\alpha\\frac{N_{1}}{L}=\\beta\\bigg{(}1-\\frac{N_{2}}{L}\\bigg{)} \\tag{51}\\]\n\nBelow we obtain the exact location \\(x_{w}\\) and height \\(\\Delta\\) of the LDW in terms of the control parameters.\n\nIn the DW phase, particle number in \\(T\\) can be expressed as\n\n\\[N_{T}=L\\int_{0}^{1}\\rho(x)dx, \\tag{52}\\]\n\nwhere a multiplicative factor \\(L\\) is introduced in the right-hand side to rescale the integration limit of the position variable \\(x\\). Using (48) and (51) in (52), we get:\n\n\\[N_{T}=L\\bigg{[}\\alpha\\frac{N_{1}}{L}(2x_{w}-1)+1-x_{w}\\bigg{]}. \\tag{53}\\]\n\nIdentifying the steady state TASEP current in the DW phase as \\(J_{T}=\\rho_{\\rm LD}(1-\\rho_{\\rm LD})\\) or \\(J_{T}=\\rho_{\\rm HD}(1-\\rho_{\\rm HD})\\) and substituting the expressions of \\(N_{T}\\) [see eq. (53)] and \\(J_{T}\\) in eq. (17a) together with PNC, we obtain the following two equations coupled in \\(N_{1}/L\\) and \\(x_{w}\\):\n\n\\[\\frac{N_{1}}{L}\\bigg{[}1-\\frac{\\alpha}{\\beta}+\\alpha(2x_{w}-1) \\bigg{]}-x_{w}+1=\\mu-1, \\tag{54}\\] \\[\\frac{N_{1}}{L}=\\frac{k_{2}}{k_{1}+k_{2}}\\bigg{[}\\mu-\\alpha\\frac {N_{1}}{L}(2x_{w}-1)-1+x_{w}\\bigg{]}\\] (55) \\[\\qquad\\qquad-\\frac{1}{L(k_{1}+k_{2})}\\alpha\\frac{N_{1}}{L}\\bigg{(} 1-\\alpha\\frac{N_{1}}{L}\\bigg{)}.\\]\n\nWhile solving eqs. (54) and (55) for \\(N_{1}/L\\) and \\(x_{w}\\), we get a quadratic equation for \\(N_{1}/L\\) with two solutions:\n\n\\[\\bigg{(}\\frac{N_{1}}{L}\\bigg{)}^{\\pm}=\\bigg{(}\\frac{1}{2\\alpha}+ \\frac{k_{10}}{2\\alpha^{2}}+\\frac{k_{20}}{2\\alpha\\beta}\\bigg{)} \\tag{56}\\] \\[\\qquad\\qquad\\pm\\bigg{[}\\bigg{(}\\frac{1}{2\\alpha}+\\frac{k_{10}}{2 \\alpha^{2}}+\\frac{k_{20}}{2\\alpha\\beta}\\bigg{)}^{2}-\\frac{k_{20}}{\\alpha^{2}} \\bigg{]}^{\\frac{1}{2}}.\\]\n\nThe density in the LD part of the DW is thus\n\n\\[\\rho_{\\rm LD}=\\alpha\\frac{N_{1}}{L}=\\bigg{(}\\frac{1}{2}\n\n "], "nougat": ["\\[\\alpha_{\\rm eff}=\\beta_{\\rm eff} \\tag{50}\\] \\[\\implies \\alpha\\frac{N_{1}}{L}=\\beta\\bigg{(}1-\\frac{N_{2}}{L}\\bigg{)} \\tag{51}\\]\n\nBelow we obtain the exact location \\(x_{w}\\) and height \\(\\Delta\\) of the LDW in terms of the control parameters.\n\nIn the DW phase, particle number in \\(T\\) can be expressed as\n\n\\[N_{T}=L\\int_{0}^{1}\\rho(x)dx, \\tag{52}\\]\n\nwhere a multiplicative factor \\(L\\) is introduced in the righthand side to rescale the integration limit of the position variable \\(x\\). Using ( 48 ) and ( 51 ) in ( 52 ), we get:\n\n\\[N_{T}=L\\bigg{[}\\alpha\\frac{N_{1}}{L}(2x_{w}-1)+1-x_{w}\\bigg{]}. \\tag{53}\\]\n\nIdentifying the steady state TASEP current in the DW phase as \\(J_{T}=\\rho_{\\rm LD}(1-\\rho_{\\rm LD})\\) or \\(J_{T}=\\rho_{\\rm HD}(1-\\rho_{\\rm HD})\\) and substituting the expressions of \\(N_{T}\\) [see eq. ( 53 )] and \\(J_{T}\\) in eq. ( 17a ) together with PNC, we obtain the following two equations coupled in \\(N_{1}/L\\) and ( 55 ) for \\(N_{1}/L\\) and \\(x_{w}\\), we get a quadratic equation for \\(N_{1}/L\\) with two solutions:\n\n\\[\\begin{split}\\frac{N_{1}}{L}\\bigg{[}& 1-\\frac{\\alpha}{\\beta}+\\alpha(2x_{w}-1)\\bigg{]}-x_{w}+1=\\mu-1,\\\\ \\frac{N_{1}}{L}&=\\frac{k_{2}}{k_{1}+k_{2}}\\bigg{[} \\mu-\\alpha\\frac{N_{1}}{L}(2x_{w}-1)-1+x_{w}\\bigg{]}\\\\ &\\quad-\\frac{1}{L(k_{1}+k_{2})}\\alpha\\frac{N_{1}}{L}\\bigg{(}1- \\alpha\\frac{N_{1}}{L}\\bigg{)}.\\end{split} \\tag{55}\\]\n\nWhile solving eqs. (54) and (55) for \\(N_{1}/L\\) and \\(x_{w}\\), we get a quadratic equation for \\(N_{1}/L\\) with two solutions:\n\n\\[\\begin{split}\\bigg{(}\\frac{N_{1}}{L}\\bigg{)}^{\\pm}& =\\bigg{(}\\frac{1}{2\\alpha}+\\frac{k_{10}}{2\\alpha^{2}}+\\frac{k_{20}}{2 \\alpha\\beta}\\bigg{)}\\\\ &\\quad\\pm\\bigg{[}\\bigg{(}\\frac{1}{2\\alpha}+\\frac{k_{10}}{2\\alpha^ {2}}+\\frac{k_{20}}{2\\alpha\\beta}\\bigg{)}^{2}-\\frac{k_{20}}{\\alpha^{2}}\\bigg{]} ^{\\frac{1}{2}}.\\end{split}\n\n "]}, {"edit": ["of threads/CPU cores. For the FL training we utilized Flower framework 3. We simulate a computer vision IoT training task using dataset CIFAR-10 4 and the computer vision model SqueezeNet which is light-weight and deemed to be suitable for edge computer visions applications. We assign higher kernel priority to baseload process to ensure the FL training doesn't affect the CPU time of baseload in co-running scenario. For the energy consumption measurement, we utilized WLAN power socket switch5. We report mean energy per sample and samples per second values for different power-modes and CPU core baseloads.\n\nFootnote 3: [https://flower.dev](https://flower.dev)\n\nFootnote 4: [https://www.cs.toronto.edu/](https://www.cs.toronto.edu/) kriz/cifar.html\n\nFootnote 5: [https://www.delock.com/produk/11826/merkmale.html](https://www.delock.com/produk/11826/merkmale.html)\n\n\\[EPS :\\text{Energy per Sample}\\] \\[P_{\\text{total}} :\\text{Total power consumption (FL and Baseload)}\\] \\[P_{\\text{BL}} :\\text{Power consumption due to Baseload}\\] \\[N :\\text{Number of Samples}\\]\n\nEnergy per sample values were calculated using Eq. 1.\n\nFigure 0(a) illustrates the mean energy per sample and 95% confidence intervals for each powermode, based on 10 repeated measurements. We observe significant difference in energy per sample and samples per second values when there is no non-FL base load (0 baseload cores) compared to a scenario when non-FL baseload is executing and utilizing all CPU cores. We also observe that while samples per second (Figure 0(b)) doesn't vary significantly when non-FL baseload is co-running with FL, energy per sample values fluctuate for baseloads 3 and 4. For our experiments, ondemand mode with baseload cores 3 has an optimum energy usage when calculating same number of samples compared to other baseload and samples per second combinations.\n\n## IV Conclusion and Future Work\n\nRecent research studies have focused on energy-efficient and carbon-efficient FL scheduling and client selection. However, most of the research assumes simplistic energy consumption models for underlying FL clients. In this work, we showed that how energy per sample values under real-world scenarios such as different power modes and non-FL baseloads at CPU cores can vary and exhibit complex operational behavior patterns.\n\nFor future work, following open research questions and possibilities could be explored further,\n\n* How do current FL systems communicate FL clients' energy related information? How to collect energy per sample, throughput per second and uncertainty related information at runtime?\n* How can we predict the power-performance characteristics, what are the relevant metrics? With more data about real-world impact factors affecting energy footprint of edge devices, can we build predictive models for forecasting?\n* How often do we need to measure before we can be certain? Can we report the uncertainty to be used in scheduling? FL trainings are usually executed multiple times due to data distribution drifts and hyperparameter search. This repetitive FL training execution could be leveraged to collect more data about power-performance behavior patterns of FL clients.\n* What's the impact of hardware accelerated edge devices such as jetson nano on energy related metrics? What are the energy efficiency opportunities in FL and non-FL co-running scenarios?\n\n## References\n\n* [1] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, \"Communication-efficient learning of deep networks from decentralized data,\" in _AISTATS_, 2016.\n* [2] X. Qiu, T. Parcollet, D. J. Beutel, T. Topal, A. Mathur, and N. D. Lane, \"A first look into the carbon footprint of federated learning,\" _CoRR_, vol. abs/2010.06537, 2020.\n* [3] X. Zhou, J. Zhao, H. Han, and C. Guet, \"Joint optimization of energy consumption and completion time in federated learning,\" in _ICDCS_, IEEE, 2022.\n* [4] C. W. Zaw, S. R. Pandey, K. Kim, and C. S. Hong, \"Energy-aware resource management for federated learning in multi-access edge computing systems,\" _IEEE Access_, vol. 9, "], "nougat": ["of threads/CPU cores. For the FL training we utilized Flower framework 3 . We simulate a computer vision IoT training task [https://flower.dev](https://flower.dev) using dataset CIFAR-10 4 and the computer vision model [https://www.cs.toronto.edu/](https://www.cs.toronto.edu/) kriz/cifar.html SqueezeNet which is light-weight and deemed to be suitable for edge computer visions applications. We assign higher kernel priority to baseload process to ensure the FL training doesn\u2019t affect the CPU time of baseload in co-running scenario. For the energy consumption measurement, we utilized WLAN power socket switch 5 . We report mean energy per [https://www.delock.com/produkt/11826/merkmale.html](https://www.delock.com/produkt/11826/merkmale.html) sample and samples per second values for different powermodes and CPU core baseloads.\n\nFootnote 3: [https://flower.dev](https://flower.dev)\n\nFootnote 4: [https://www.cs.toronto.edu/](https://www.cs.toronto.edu/) kriz/cifar.html\n\nFootnote 5: [https://www.delock.com/produkt/11826/merkmale.html](https://www.delock.com/produkt/11826/merkmale.html)\n\n\\[EPS :\\text{Energy per Sample}\\] \\[P_{\\text{total}} :\\text{Total power consumption (FL and Baseload)}\\] \\[P_{\\text{BL}} :\\text{Power consumption due to Baseload}\\] \\[N :\\text{Number of Samples}\\]\n\nEnergy per sample values were calculated using Eq. 1.\n\nFigure 0(a) illustrates the mean energy per sample and 95% confidence intervals for each powermode, based on 10 repeated measurements. We observe significant difference in energy per sample and samples per second values when there is no nonFL base load (0 baseload cores) compared to a scenario when non-FL baseload is executing and utilizing all CPU cores. We also observe that while samples per second (Figure 1b) doesn\u2019t vary significantly when non-FL baseload is co-running with FL, energy per sample values fluctuate for baseloads 3 and 4. For our experiments, ondemand mode with baseload cores 3 has an optimum energy usage when calculating same number of samples compared to other baseload and samples per second combinations.\n\n## IV Conclusion and Future Work\n\nRecent research studies have focused on energy-efficient and carbon-efficient FL scheduling and client selection. However, most of the research assumes simplistic energy consumption models for underlying FL clients. In this work, we showed that how energy per sample values under real-world scenarios such as different power modes and non-FL baseloads at CPU cores can vary and exhibit complex operational behavior patterns.\n\nFor future work, following open research questions and possibilities could be explored further,\n\n* How do current FL systems communicate FL clients\u2019 energy related information? How to collect energy per sample, throughput per second and uncertainty related information at runtime?\n* How can we predict the power-performance characteristics, what are the relevant metrics? With more data about real-world impact factors affecting energy footprint of edge devices, can we build predictive models for forecasting?\n* How often do we need to measure before we can be certain? Can we report the uncertainty to be used in scheduling? FL trainings are usually executed multiple times due to data distribution drifts and hyperparameter search. This repetitive FL training execution could be leveraged to collect more data about power-performance behavior patterns of FL clients.\n* What's the impact of hardware accelerated edge devices such as jetson nano on energy related metrics? What are the energy efficiency opportunities in FL and non-FL corunning scenarios?\n\n## References\n\n* [1] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, \u201cCommunication-efficient learning of deep networks from decentralized data,\u201d in _AISTATS_, 2016. [2] X. Qiu, T. Parcollet, D. J. Beutel, T. Topal, A. Mathur, and N. D. Lane, \u201cA first look into the carbon footprint of federated learning,\u201d _CoRR_, vol. abs/2010.06537, 2020. [3] X. Zhou, J. Zhao, H. Han, and C. Guet, \u201cJoint optimization of energy consumption and completion time in federated learning,\u201d in _ICDCS_, IEEE, "]}, {"edit": ["In the process of scanning the database, the system identifies all the songs that have the word 'good' either in their titles or in their lyrics. The first song that meets this criterion is 'Good Life' by Kehlani and G-Eazy. This song contains the word 'good' not only in its title but also within its lyrical content.\n\nIt's important to note that the search doesn't stop at 'Good Life.' There might be many other songs with the word 'good' in their titles or lyrics within the database. The given information only indicates the first result that the system presents in response to the query 'good.'\n\nTo summarize, the system is a music-centric search tool that uses the provided query to locate songs in its database containing the term 'good' either in their titles or lyrics. The first song it finds is 'Good Life' by Kehlani and G-Eazy, which contains 'good' both in its title and lyrics. The system can provide more results if the user wishes to explore other songs with the word 'good.'\n\nIn Figure 7, the dataset consists of song lyrics from the year 1950 to 2019. This provides a well-spread dataset which allows us to get results from over 60 years, thus making the search engine efficient enough to provide songs from every year. The year 2017 has the most songs amounting upto 660 songs from various different genres and emotions.\n\nIn Figure 8, the dataset has been categorized into 7 major genres namely - pop, country, blues, rock, jazz, reggae and hip hop. There are about 7042 songs under the genre pop. This includes pop\n\nFigure 6: Lyric Query Detection\n\nFigure 7: Distribution of songs throughout the years\n\n"], "nougat": ["In the process of scanning the database, the system identifies all the songs that have the word \u2019good\u2019 either in their titles or in their lyrics. The first song that meets this criterion is \u2019Good Life\u2019 by Kehlani and G-Eazy. This song contains the word \u2019good\u2019 not only in its title but also within its lyrical content.\n\nIt's important to note that the search doesn\u2019t stop at \u2019Good Life.\u2019 There might be many other songs with the word \u2019good\u2019 in their titles or lyrics within the database. The given information only indicates the first result that the system presents in response to the query \u2019good.\u2019\n\nTo summarize, the system is a music-centric search tool that uses the provided query to locate songs in its database containing the term \u2019good\u2019 either in their titles or lyrics. The first song it finds is \u2019Good Life\u2019 by Kehlani and G-Eazy, which contains \u2019good\u2019 both in its title and lyrics. The system can provide more results if the user wishes to explore other songs with the word \u2019good.\u2019\n\nIn Figure 7, the dataset consists of song lyrics from the year 1950 to 2019. This provides a wellspread dataset which allows us to get results from over 60 years, thus making the search engine efficient enough to provide songs from every year. The year 2017 has the most songs amounting upto 660 songs from various different genres and emotions.\n\nIn Figure 8, the dataset has been categorized into 7 major genres namely pop, country, blues, rock, jazz, reggae and hip hop. There are about 7042 songs under the genre pop. This includes pop\n\nFigure 6: Lyric Query Detection\n\nFigure 7: Distribution of songs throughout the years\n\n"]}, {"edit": ["graph embedding size 200, batch size 128, maximum number of batches 10000, number of gradient accumulation steps 8, maximum patience of 50 for C++ data and 20 for Java data.\n\n## 3. Experiments with C++ data\n\nTo answer the first research question, we used the original C++ method-level vulnerability dataset from (Bach et al., 2017). After parsing, we obtained the following statistics of the input graphs: 11788 train graphs (956 vulnerable), 1667 validation graphs (133 vulnerable), 3385 test graphs (286 vulnerable)\n\nTo test each dimension of RQ 1, we performed 10 trials of training the model. In each trial, the dataset was split into train, validation, and test parts anew. The results can be found in Table 1.\n\n### Excluding SMOTE and RL\n\nThe model without SMOTE and RL achieves the worst performance with respect to the F1 score and the best performance with respect the ROC AUC measure.\n\n### AST edges\n\nThe model performs slightly better without including AST edges. This is likely due to including too much of fine-grained information or too many nodes. The model becomes more likely to overfit to irrelevant features in the input and fail to generalize.\n\n### Pruning\n\nThe experiments also showed that the model performs better with pruning at operator nodes. Pruning makes a graph simpler and less entangled for the model to understand.\n\n### Downsampling\n\nTable 1 shows that the model performs worse with balancing the train set by downsampling non-vulnerable methods. We think that a rough balancing of the train part impacts the score negatively since it turns off SMOTE.\n\n## 4. Experiments with Java data\n\nTo answer the rest of research questions, we trained and tested the model on different parts of the Java dataset (1): \\(P_{1}\\), \\(P_{2}\\), and \\(P_{3}\\). In particular, we varied \\(k\\) in the range from 1 to 14. Then, we plotted the resulting ROC AUC scores against \\(k\\), and draw conclusions based on the observed dynamics. To make set \\(P_{3}\\) to be independent of \\(k\\), we fixed it to be the complement of \\(P_{1}\\). That is, \\(P_{3}\\) consisted of functions that remained unchanged in the commits where only one function was changed. Also, in order to balance different parts involved in training and testing, we restricted the size of \\(P_{3}\\):\n\n\\[|P_{3}|=|P_{1}|+|P_{2}|\\]\n\nDuring the data cleaning phase, we ensured that in each experiment, \\(P_{3}\\) did not contain functions that are contained in \\(P_{1}\\cup P_{2}\\). Also, we removed any duplicate functions from each of the parts \\(P_{1}\\), \\(P_{2}\\), and \\(P_{3}\\), and removed methods contained in the training data from the test data.\n\nTable 2 shows the distribution of the collected Java methods after stratification by \\(k\\) and cleaning the data:\n\n### Research question 2\n\nIn this research question, we investigate training on different combinations of sets \\(P_{1}\\), \\(P_{2}\\), and \\(P_{3}\\), and testing on \\(P_{1}\\cup P_{2}\\cup P_{3}\\) or \\(P_{1}\\cup P_{3}\\), which is a stricter test. The results can be found in Figures 2 and 3.\n\nFigures 2 and 3 allow us to conclude that if the test set includes part \\(P_{3}\\), then the inclusion of part \\(P_{3}\\) into training is critical to achieving a high performance. Overall, parts \\(P_{2}\\) and \\(P_{3}\\) contribute the most to the prediction, as seen by the red and blue lines on Figures 2 and 3.\n\nAlso, on Figure 3, we see "], "nougat": ["graph embedding size 200, batch size 128, maximum number of batches 10000, number of gradient accumulation steps 8, maximum patience of 50 for C++ data and 20 for Java data.\n\n## 3. Experiments with C++ data\n\nTo answer the first research question, we used the original C++ method-level vulnerability dataset from (Bach et al., 2017). After parsing, we obtained the following statistics of the input graphs: 11788 train graphs (956 vulnerable), 1667 validation graphs (133 vulnerable), 3385 test graphs (286 vulnerable)\n\nTo test each dimension of RQ 1, we performed 10 trials of training the model. In each trial, the dataset was split into train, validation, and test parts anew. The results can be found in Table 1.\n\n### Excluding SMOTE and RL\n\nThe model without SMOTE and RL achieves the worst performance with respect to the F1 score and the best performance with respect the ROC AUC measure.\n\n### AST edges\n\nThe model performs slightly better without including AST edges. This is likely due to including too much of fine-grained information or too many nodes. The model becomes more likely to overfit to irrelevant features in the input and fail to generalize.\n\n### Pruning\n\nThe experiments also showed that the model performs better with pruning at operator nodes. Pruning makes a graph simpler and less entangled for the model to understand.\n\n### Downsampling\n\nTable 1 shows that the model performs worse with balancing the train set by downsampling non-vulnerable methods. We think that a rough balancing of the train part impacts the score negatively since it turns off SMOTE.\n\n## 4. Experiments with Java data\n\nTo answer the rest of research questions, we trained and tested the model on different parts of the Java dataset (1): \\(P_{1}\\), \\(P_{2}\\), and \\(P_{3}\\). In particular, we varied \\(k\\) in the range from 1 to 14. Then, we plotted the resulting ROC AUC scores against \\(k\\), and draw conclusions based on the observed dynamics. To make set \\(P_{3}\\) to be independent of \\(k\\), we fixed it to be the complement of \\(P_{1}\\). That is, \\(P_{3}\\) consisted of functions that remained unchanged in the commits where only one function was changed. Also, in order to balance different parts involved in training and testing, we restricted the size of \\(P_{3}\\):\n\n\\[|P_{3}|=|P_{1}|+|P_{2}|\\]\n\nDuring the data cleaning phase, we ensured that in each experiment, \\(P_{3}\\) did not contain functions that are contained in \\(P_{1}\\cup P_{2}\\). Also, we removed any duplicate functions from each of the parts \\(P_{1}\\), \\(P_{2}\\), and \\(P_{3}\\), and removed methods contained in the training data from the test data.\n\nTable 2 shows the distribution of the collected Java methods after stratification by \\(k\\) and cleaning the data:\n\n### Research question 2\n\nIn this research question, we investigate training on different combinations of sets \\(P_{1}\\), \\(P_{2}\\), and \\(P_{3}\\), and testing on \\(P_{1}\\cup P_{2}\\cup P_{3}\\) or \\(P_{1}\\cup P_{3}\\), which is a stricter test. The results can be found in Figures 2 and 3.\n\nFigures 2 and 3 allow us to conclude that if the test set includes part \\(P_{3}\\), then the inclusion of part \\(P_{3}\\) into training is critical to achieving a high performance. Overall, parts \\(P_{2}\\) and \\(P_{3}\\) contribute the most to the prediction, as seen by the red and blue lines on Figures 2 and 3.\n\nAlso, on Figure 3, we see "]}, {"edit": ["\\[4\\cdot\\begin{pmatrix}2&-1|3&2\\\\ 0&-2|0&3\\end{pmatrix}-0\\cdot\\begin{pmatrix}2&5|3&1\\\\ 0&3|0&4\\end{pmatrix}+5\\cdot\\begin{pmatrix}5&-1|0&3\\\\ 3&-2|2&5\\end{pmatrix}-1\\cdot\\begin{pmatrix}2&-1|-3&3\\\\ 0&-2|-3&5\\end{pmatrix}+\\]\n\n\\[0\\cdot\\begin{pmatrix}2&5|-3&0\\\\ 0&3|-3&2\\end{pmatrix}=326.\\]\n\n_After expanding further the minors of above determinant based on Theorem 1, we see that this result is the same as the result of Example 2._\n\n_For \\(i=2\\), we have:_\n\n\\[A=\\begin{pmatrix}3&0&-4|-2&4&0|5&1&0\\\\ 2&5&-1|-3&0&3|3&1&2\\\\ 0&3&-2|-3&2&5|0&4&3\\end{pmatrix}\\]\n\n\\[=2\\cdot\\begin{pmatrix}4&0|1&0\\\\ 2&5|4&3\\end{pmatrix}-5\\cdot\\begin{pmatrix}-2&0|5&0\\\\ -3&5|0&3\\end{pmatrix}+(-1)\\cdot\\begin{pmatrix}-2&4|5&1\\\\ -3&2|0&4\\end{pmatrix}-(-3)\\cdot\\begin{pmatrix}0&-4|1&0\\\\ 3&-2|4&3\\end{pmatrix}+\\]\n\n\\[0\\cdot\\begin{pmatrix}3&-4|5&0\\\\ 0&-2|0&3\\end{pmatrix}-3\\cdot\\begin{pmatrix}3&0|5&1\\\\ 0&3|0&4\\end{pmatrix}+5\\cdot\\begin{pmatrix}5&-1|0&3\\\\ 3&-2|2&5\\end{pmatrix}-1\\cdot\\begin{pmatrix}2&-1|-3&3\\\\ 0&-2|-3&5\\end{pmatrix}+\\]\n\n\\[0\\cdot\\begin{pmatrix}2&5|-3&0\\\\ 0&3|-3&2\\end{pmatrix}=326.\\]\n\n_After expanding further the minors of above determinant based on Theorem 1, we see that this result is the same as the result of Example 2._\n\n_For \\(i=3\\), we have:_\n\n\\[A=\\begin{pmatrix}3&0&-4|-2&4&0|5&1&0\\\\ 2&5&-1|-3&0&3|3&1&2\\\\ 0&3&-2|-3&2&5|0&4&3\\end{pmatrix}\\]\n\n\\[=0\\cdot\\begin{pmatrix}4&0&1|1&0\\\\ 0&3|1&2\\end{pmatrix}-5\\cdot\\begin{pmatrix}-2&0|5&0\\\\ -3&3|3&2\\end{pmatrix}+(-2)\\cdot\\begin{pmatrix}-2&4|5&1\\\\ -3&0|3&1\\end{pmatrix}-(-3)\\cdot\\begin{pmatrix}0&-4|1&0\\\\ 5&-1|1&2\\end{pmatrix}+\\]\n\n\\[2\\cdot\\begin{pmatrix}3&-4|5&0\\\\ 2&-1|3&3\\end{pmatrix}-5\\cdot\\begin{pmatrix}3&0|5&1\\\\ 2&5|3&1\\end{pmatrix}+0\\cdot\\begin{pmatrix}5&-1|0&3\\\\ 5&-1|0&3\\end{pmatrix}-4\\cdot\\begin{pmatrix}2&-1|-3&3\\\\ 2&-1|-3&3\\end{pmatrix}+\\ "], "nougat": ["\\[4\\cdot\\begin{pmatrix}2&-1|3&2\\\\ 0&-2|0&3\\end{pmatrix}-0\\cdot\\begin{pmatrix}2&5|3&1\\\\ 0&3|0&4\\end{pmatrix}+5\\cdot\\begin{pmatrix}5&-1|0&3\\\\ 3&-2|2&5\\end{pmatrix}-1\\cdot\\begin{pmatrix}2&-1|-3&3\\\\ 0&-2|-3&5\\end{pmatrix}+\\]\n\n\\[0\\cdot\\begin{pmatrix}2&5|-3&0\\\\ 0&3|-3&2\\end{pmatrix}=326.\\]\n\n_After expanding further the minors of above determinant based on Theorem 1, we see that this result is the same as the result of Example 2._\n\n_For \\(i=2\\), we have:_\n\n\\[A=\\begin{pmatrix}3&0&-4|-2&4&0|5&1&0\\\\ 2&5&-1|-3&0&3|3&1&2\\\\ 0&3&-2|-3&2&5|0&4&3\\end{pmatrix}\\]\n\n\\[=2\\cdot\\begin{pmatrix}4&0|1&0\\\\ 2&5|4&3\\end{pmatrix}-5\\cdot\\begin{pmatrix}-2&0|5&0\\\\ -3&5|0&3\\end{pmatrix}+(-1)\\cdot\\begin{pmatrix}-2&4|5&1\\\\ -3&2|0&4\\end{pmatrix}-(-3)\\cdot\\begin{pmatrix}0&-4|1&0\\\\ 3&-2|4&3\\end{pmatrix}+\\]\n\n\\[0\\cdot\\begin{pmatrix}3&-4|5&0\\\\ 0&-2|0&3\\end{pmatrix}-3\\cdot\\begin{pmatrix}3&0|5&1\\\\ 0&3|0&4\\end{pmatrix}+5\\cdot\\begin{pmatrix}5&-1|0&3\\\\ 3&-2|2&5\\end{pmatrix}-1\\cdot\\begin{pmatrix}2&-1|-3&3\\\\ 0&-2|-3&5\\end{pmatrix}+\\]\n\n\\[0\\cdot\\begin{pmatrix}2&5|-3&0\\\\ 0&3|-3&2\\end{pmatrix}=326.\\]\n\n_After expanding further the minors of above determinant based on Theorem 1, we see that this result is the same as the result of Example 2._\n\n_For \\(i=3\\), we have:_\n\n\\[A=\\begin{pmatrix}3&0&-4|-2&4&0|5&1&0\\\\ 2&5&-1|-3&0&3|3&1&2\\\\ 0&3&-2|-3&2&5|0&4&3\\end{pmatrix}\\]\n\n\\[=0\\cdot\\begin{pmatrix}4&0&1|1&0\\\\ 0&3|1&2\\end{pmatrix}-5\\cdot\\begin{pmatrix}-2&0|5&0\\\\ -3&3|3&2\\end{pmatrix}+(-2)\\cdot\\begin{pmatrix}-2&4|5&1\\\\ -3&0|3&1\\end{pmatrix}-(-3)\\cdot\\begin{pmatrix}0&-4|1&0\\\\ 5&-1|1&2\\end{pmatrix}+\\]\n\n\\[2\\cdot\\begin{pmatrix}3&-4|5&0\\\\ 2&-1|3&3\\end{pmatrix}-5\\cdot\\begin{pmatrix}3&0|5&1\\\\ 2&5|3&1\\end{pmatrix}+0\\cdot\\begin{pmatrix}5&-1|0&3\\\\ 5&-1|0&3\\end{pmatrix}-4\\cdot\\begin{pmatrix}2&-1|-3&3\\\\ 2&-1|-3&3\\end{pmatrix}+\\ "]}, {"edit": ["Let us denote by \\(\\mathcal{T}=(\\kappa\\times\\omega\\times\\operatorname{acc}(\\kappa)\\times\\gamma\\times \\kappa\\times\\kappa\\times\\kappa\\times\\kappa)^{\\leq\\gamma}\\). For every \\(\\xi\\in\\mathcal{T}\\) there are functions \\(\\{\\xi_{i}\\in\\kappa^{\\leq\\omega}\\mid 0<i\\leq 8\\}\\) such that for all \\(i\\leq 8\\), \\(dom(\\xi_{i})=dom(\\xi)\\) and for all \\(n\\in dom(\\xi)\\), \\(\\xi(n)=(\\xi_{1}(n),\\xi_{2}(n),\\xi_{3}(n),\\xi_{4}(n),\\xi_{5}(n),\\xi_{6}(n),\\xi _{7}(n),\\xi_{8}(n))\\). For every \\(\\xi\\in\\mathcal{T}\\) let us denote \\((\\xi_{4},\\xi_{5},\\xi_{6},\\xi_{7},\\xi_{8})\\) by \\(\\overline{\\xi}\\).\n\n**Definition 3.12**.: For all \\(\\alpha\\in\\operatorname{acc}(\\kappa)\\) and \\(\\eta\\in\\mathcal{T}\\) with \\(\\overline{\\eta}\\in J_{f}\\), \\(dom(\\eta)=m<\\gamma\\) define \\(\\Gamma^{\\alpha}_{\\eta}\\) as follows:\n\nIf \\(\\overline{\\eta}\\in J^{\\alpha}_{f}\\), then \\(\\Gamma^{\\alpha}_{\\eta}\\) is the set of elements \\(\\xi\\) of \\(\\mathcal{T}\\) such that:\n\n1. \\(\\xi\\nmid m=\\eta\\),\n2. \\(\\overline{\\xi}\\nmid dom(\\xi)\\backslash m\\in W^{\\alpha}_{\\eta}\\),\n3. \\(\\xi_{3}\\) is constant on \\(dom(\\xi)\\backslash m\\),\n4. \\(\\xi_{3}(m)=\\alpha\\),\n5. for all \\(n\\in dom(\\xi)\\backslash m\\), let \\(\\xi_{2}(n)\\) be the unique \\(r<\\omega\\) such that \\(\\sigma^{\\alpha}_{\\zeta}(r)=\\overline{\\xi}(n)\\), where \\(\\zeta=\\overline{\\xi}\\nmid n\\).\n\nIf \\(\\overline{\\eta}\\notin J^{\\alpha}_{f}\\), then \\(\\Gamma^{\\alpha}_{\\eta}=\\emptyset\\).\n\nFor \\(\\eta\\in\\mathcal{T}\\) with \\(\\overline{\\eta}\\in J_{f}\\), \\(dom(\\eta)=m<\\gamma\\) define\n\n\\[\\Gamma(\\eta)=\\bigcup_{\\alpha\\in\\operatorname{acc}(\\kappa)}\\Gamma^{\\alpha}_{ \\eta}.\\]\n\nFinally we can define \\(A^{f}\\) by induction. Let \\(T_{f}(0)=\\{\\emptyset\\}\\) and for all \\(n<\\gamma\\),\n\n\\[T_{f}(n+1)=T_{f}(n)\\cup\\bigcup_{\\eta\\in T_{f}(n)}\\bigcup_{dom(\\eta)=n}\\Gamma( \\eta).\\]\n\nFor \\(n\\leq\\gamma\\) a limit ordinal,\n\n\\[\\bar{T}_{f}(n)=\\bigcup "], "nougat": ["Let us denote by \\(\\mathcal{T}=(\\kappa\\times\\omega\\times\\operatorname{acc}(\\kappa)\\times\\gamma\\times \\kappa\\times\\kappa\\times\\kappa\\times\\kappa)^{\\leq\\gamma}\\). For every \\(\\xi\\in\\mathcal{T}\\) there are functions \\(\\{\\xi_{i}\\in\\kappa^{\\leq\\omega}\\mid 0<i\\leq 8\\}\\) such that for all \\(i\\leq 8\\), \\(dom(\\xi_{i})=dom(\\xi)\\) and for all \\(n\\in dom(\\xi)\\), \\(\\xi(n)=(\\xi_{1}(n),\\xi_{2}(n),\\xi_{3}(n),\\xi_{4}(n),\\xi_{5}(n),\\xi_{6}(n),\\xi _{7}(n),\\xi_{8}(n))\\). For every \\(\\xi\\in\\mathcal{T}\\) let us denote \\((\\xi_{4},\\xi_{5},\\xi_{6},\\xi_{7},\\xi_{8})\\) by \\(\\overline{\\xi}\\).\n\n**Definition 3.12**.: For all \\(\\alpha\\in\\operatorname{acc}(\\kappa)\\) and \\(\\eta\\in\\mathcal{T}\\) with \\(\\overline{\\eta}\\in J_{f}\\), \\(dom(\\eta)=m<\\gamma\\) define \\(\\Gamma^{\\alpha}_{\\eta}\\) as follows:\n\nIf \\(\\overline{\\eta}\\in J^{\\alpha}_{f}\\), then \\(\\Gamma^{\\alpha}_{\\eta}\\) is the set of elements \\(\\xi\\) of \\(\\mathcal{T}\\) such that:\n\n1. \\(\\xi\\nmid m=\\eta\\),\n2. \\(\\overline{\\xi}\\nmid dom(\\xi)\\backslash m\\in W^{\\alpha}_{\\eta}\\),\n3. \\(\\xi_{3}\\) is constant on \\(dom(\\xi)\\backslash m\\),\n4. \\(\\xi_{3}(m)=\\alpha\\),\n5. for all \\(n\\in dom(\\xi)\\backslash m\\), let \\(\\xi_{2}(n)\\) be the unique \\(r<\\omega\\) such that \\(\\sigma^{\\alpha}_{\\zeta}(r)=\\overline{\\xi}(n)\\), where \\(\\zeta=\\overline{\\xi}\\nmid n\\).\n\nIf \\(\\overline{\\eta}\\notin J^{\\alpha}_{f}\\), then \\(\\Gamma^{\\alpha}_{\\eta}=\\emptyset\\).\n\nFor \\(\\eta\\in\\mathcal{T}\\) with \\(\\overline{\\eta}\\in J_{f}\\), \\(dom(\\eta)=m<\\gamma\\) define\n\n\\[\\Gamma(\\eta)=\\bigcup_{\\alpha\\in\\operatorname{acc}(\\kappa)}\\Gamma^{\\alpha}_{ \\eta}.\\]\n\nFinally we can define \\(A^{f}\\) by induction. Let \\(T_{f}(0)=\\{\\emptyset\\}\\) and for all \\(n<\\gamma\\),\n\n\\[T_{f}(n+1)=T_{f}(n)\\cup\\bigcup_{\\eta\\in T_{f}(n)}\\bigcup_{dom(\\eta)=n}\\Gamma( \\eta).\\]\n\nFor \\(n\\leq\\gamma\\) a limit ordinal,\n\n\\[\\bar{T}_{f}(n)=\\bigcup "]}, {"edit": ["whenever \\(\\operatorname{tr}(\\rho^{2})\\leq 2/d\\). Secondly, Lemma 7 and the vectorization of matrices, Theorem 8 and Theorem 11 give hints to the similarity of the role of \\(\\vartheta(G)\\) in quantum contextuality and joint expectation values.\n\n## IV Selfadjoint unitary representations\n\nThe set of operators, where any pair either commutes or anticommutes, plays an important role as exemplified by the Pauli strings. The commutation and anticommutation relations of such a set \\(\\{S_{i}\\}\\) can be encoded into a so-called frustration graph \\(G\\)[11; 12], where \\(i\\sim j\\) if \\(\\{S_{i},S_{j}\\}=0\\), and \\(i\\not\\sim j\\) if \\([S_{i},S_{j}]=0\\). By checking extensive examples, it is conjectured in Ref. [34] that\n\n\\[q\\left(\\{S_{i}\\}\\right)=\\alpha(G). \\tag{20}\\]\n\nWhether Eq. (20) can be violated is also an open question in Ref. [25]. Conversely, for a given graph \\(G\\), we can consider its representation by a set \\(\\{S_{i}\\}\\) of selfadjoint unitaries, in the sense that \\(\\{S_{i},S_{j}\\}=0\\) if \\(i\\sim j\\) and \\([S_{i},S_{j}]=0\\) if \\(i\\not\\sim j\\). This representation is called selfadjoint unitary representation (SAUR) [41]. By taking the graph-theoretic approach instead of starting from a special set, we denote \\(\\beta(G,w)=q\\left(\\mathcal{S}_{ac}(G),w\\right)\\), where \\(\\mathcal{S}_{ac}(G)\\) is the set of all SAURs of \\(G\\). The conjecture in Eq. (20) is equivalent to \\(\\beta(G)=\\alpha(G)\\). In Ref. [25], no such an example is known that \\(\\beta(G)>\\alpha(G)\\). To continue, we first introduce the standard SAUR of a given graph, which is defined deductively. The standard SAUR can help us to reduce the complexity of considerations, since we only need to focus on the standard SAUR to obtain \\(\\beta(G)\\) as we will see later.\n\n**Definition 12**.: For a given graph \\(G\\) and one of its edges \\((i_{0},j_{0})\\), other vertices except \\(i_{0}\\) and \\(j_{0}\\) can be devided into four groups \\(V_{0},V_{1},V_{2},V_{3}\\), such that\n\n* \\(i\\not\\sim i_{0}\\) and \\(i\\not\\sim j_{0}\\) for any \\(i\\in V_{0}\\);\n* \\(i\\not\\sim i_{0}\\) and \\(i\\sim j_{0}\\) for any \\(i\\in V_{1}\\);\n* \\(i\\sim i_{0}\\) and \\(i\\sim j_{0}\\) for any \\(i\\in V_{2}\\);\n* \\(i\\sim i_{0}\\) and \\(i\\not\\sim j_{0}\\) for any \\(i\\in V_{3}\\).\n\nThe subgraph \\(G^{\\prime}\\) of \\(G\\) with vertices in \\(\\cup_{i=0}^{4}V_{i}\\) is said to be a Pauli-\\((i_{0},j_{0})\\)-induced subgraph of \\( "], "nougat": ["whenever \\(\\operatorname{tr}(\\rho^{2})\\leq 2/d\\). Secondly, Lemma 7 and the vechints to the similarity of the role of \\(\\vartheta(G)\\) in quantum contextuality and joint expectation values.\n\n## IV Selfadjoint unitary representations\n\nThe set of operators, where any pair either commutes or anticommutes, plays an important role as exemplified by the Pauli strings. The commutation and anticommutation relations of such a set \\(\\{S_{i}\\}\\) can be encoded into a so-called frustration graph \\(G\\)[11; 12], where \\(i\\sim j\\) if \\(\\{S_{i},S_{j}\\}=0\\), and \\(i\\not\\sim j\\) if \\([S_{i},S_{j}]=0\\). By checking extensive examples, it is conjectured in Ref. [34] that\n\n\\[q\\left(\\{S_{i}\\}\\right)=\\alpha(G). \\tag{20}\\]\n\nWhether Eq. (20) can be violated is also an open question in Ref. [25]. Conversely, for a given graph \\(G\\), we can consider its representation by a set \\(\\{S_{i}\\}\\) of selfadjoint unitaries, in the sense that \\(\\{S_{i},S_{j}\\}=0\\) if \\(i\\sim j\\) and \\([S_{i},S_{j}]=0\\) if \\(i\\not\\sim j\\). This representation is called selfadjoint unitary representation (SAUR) [41]. By taking the graph-theoretic approach instead of starting from a special set, we denote \\(\\beta(G,w)=q\\left(\\mathcal{S}_{ac}(G),w\\right)\\), where \\(\\mathcal{S}_{ac}(G)\\) is the set of all SAURs of \\(G\\). The conjecture in Eq. (20) is equivalent to \\(\\beta(G)=\\alpha(G)\\). In Ref. [25], no such an example is known that \\(\\beta(G)>\\alpha(G)\\). To continue, we first introduce the standard SAUR of a given graph, which is defined deductively. The standard SAUR can help us to reduce the complexity of considerations, since we only need to focus on the standard SAUR to obtain \\(\\beta(G)\\) as we will see later.\n\n**Definition 12**.: For a given graph \\(G\\) and one of its edges \\((i_{0},j_{0})\\), other vertices except \\(i_{0}\\) and \\(j_{0}\\) can be devided into four groups \\(V_{0},V_{1},V_{2},V_{3}\\), such that\n\n* \\(i\\not\\sim i_{0}\\) and \\(i\\not\\sim j_{0}\\) for any \\(i\\in V_{0}\\);\n* \\(i\\not\\sim i_{0}\\) and \\(i\\sim j_{0}\\) for any \\(i\\in V_{1}\\);\n* \\(i\\sim i_{0}\\) and \\(i\\sim j_{0}\\) for any \\(i\\in V_{2}\\);\n* \\(i\\sim i_{0}\\) and \\(i\\not\\sim j_{0}\\) for any \\(i\\in V_{3}\\).\n\nThe subgraph \\(G^{\\prime}\\) of \\(G\\) with vertices in \\(\\cup_{i=0}^{4}V_{i}\\) is said to be a Pauli-\\((i_{0},j_{0})\\)-induced subgraph of \\(G\\) if\n\n* when \\(i "]}, {"edit": ["\n\n### Checking SUSY for the Fibered Background\n\nHere we aim to compute how many supercharges are preserved by the the backgrounds presented in this paper. For the un-fibered background in eq.(2.1) we refer the reader to [12], [19], where it is shown that this solution preserves 16 Supercharges in an interesting way: the anti-commutator of two supercharges includes the \\(R\\)-Symmetry generators. Now we present the analysis for the fibered background in eq.(2.3). We perform all the analysis in the S-dual system, in terms of NS5 branes, where we only have \\(H_{3}\\) flux.\n\nFirst, note that the dilatino variation is a matrix equation of the form \\(M\\varepsilon=0\\). In order to have non-trivial solutions to this equation, we require \\(M\\) to be non-invertible, for which we need to impose \\(\\det(M)=0\\). It is also possible to obtain a matrix equation from the gravitino variation. Noting that we can write the gravitino variation as a covariant derivative, for which we define the connection\n\n\\[W_{\\mu}=\\frac{1}{4}\\omega_{\\mu}^{\\phantom{\\mu}ab}\\Gamma_{ab}+\\frac{1}{4\\cdot 2 !}H_{\\mu\\nu\\lambda}\\Gamma^{\\nu\\lambda}\\sigma^{3}+\\frac{e^{\\Phi}}{8}\\left(F_{ \\mu}\\Gamma^{\\mu}(i\\sigma_{2})+\\frac{1}{3!}F_{\\mu\\nu\\lambda}\\Gamma^{\\mu\\nu \\lambda}\\sigma^{1}+\\frac{1}{2\\cdot 5!}F_{\\mu\\nu\\lambda\\rho\\sigma}\\Gamma^{\\mu\\nu \\lambda\\rho\\sigma}(i\\sigma_{2})\\right)\\Gamma_{\\mu},\\] (A.10)\n\nthen we can write the gravitino variation as\n\n\\[\\delta\\psi_{\\mu}dx^{\\mu}=\\left(\\partial_{\\mu}\\varepsilon+W_{\\mu}\\varepsilon \\right)dx^{\\mu}\\equiv\\mathcal{D}\\varepsilon.\\] (A.11)\n\nWe can get rid of the partial derivative of the spinor by acting with \\(\\mathcal{D}\\) a second time\n\n\\[\\mathcal{D}\\wedge\\mathcal{D}\\varepsilon=\\left(dW+W\\wedge W\\right)\\varepsilon= \\frac{1}{2}\\Theta_{\\mu\\nu}dx^{\\mu}\\wedge dx^{\\nu}\\varepsilon.\\] (A.12)\n\nEach of the components of \\(\\Theta_{\\mu\\nu}\\) defines a matrix equation, giving a total of 45 independent equations. We need to make sure that \\(\\det(\\Theta_{\\mu\\nu})=0\\) for each of the components. The equations\n\n\\[M\\varepsilon=0,\\quad\\Theta_{\\mu\\nu}\\varepsilon=0,\\] (A.13)\n\nconstrain the number of independent components of the spinor. After this procedure we use the gravitino variation to solve the dependence of the spinor on the spacetime coordinates.\n\nSpecialising to our background, the determinant of the Dilatino variation for the background in eq.(2.3) reads\n\n\\[\\det(M)\\sim\\left(4(e_{B}Q_{A}-e_{A}Q_{B})^{2}+m^{2}\\right)^{8}\\left(4(e_{B}Q_{ A}+e_{A}Q_{B})^{2}+m^{2}\\right)^{8}.\\] (A.14)\n\nIn order to have non-trivial solutions we need to impose the following BPS conditions on the parameters of the background\n\n\\[e_{A}Q_{B}=\\pm e_{B}Q_{A},\\quad m=0.\\] (A.\n\n"], "nougat": ["\n\n### Checking SUSY for the Fibered Background\n\nHere we aim to compute how many supercharges are preserved by the the backgrounds presented in this paper. For the un-fibered background in eq.( 2.1 ) we refer the reader to [ 12 ], [ 19 ], where it is shown that this solution preserves 16 Supercharges in an interesting way: the anti-commutator of two supercharges includes the \\(R\\)-Symmetry generators. Now we present the analysis for the fibered background in eq.( 2.3 ). We perform all the analysis in the S-dual system, in terms of NS5 branes, where we only have \\(H_{3}\\) flux.\n\nFirst, note that the dilatino variation is a matrix equation of the form \\(M\\varepsilon=0\\). In order to have non-trivial solutions to this equation, we require \\(M\\) to be non-invertible, for which we need to impose det(\\(M)=0\\). It is also possible to obtain a matrix equation from the gravitino variation. Noting that we can write the gravitino variation as a covariant derivative, for which we define the connection\n\n\\[W_{\\mu}=\\frac{1}{4}\\omega_{\\mu}^{\\phantom{\\mu}ab}\\Gamma_{ab}+\\frac{1}{4\\cdot 2 !}H_{\\mu\\nu\\lambda}\\Gamma^{\\nu\\lambda}\\sigma^{3}+\\frac{e^{\\Phi}}{8}\\left(F_{ \\mu}\\Gamma^{\\mu}(i\\sigma_{2})+\\frac{1}{3!}F_{\\mu\\nu\\lambda}\\Gamma^{\\mu\\nu \\lambda}\\sigma^{1}+\\frac{1}{2\\cdot 5!}F_{\\mu\\nu\\lambda\\rho\\sigma}\\Gamma^{\\mu\\nu \\lambda\\rho\\sigma}(i\\sigma_{2})\\right)\\Gamma_{\\mu},\\] (A.10)\n\nthen we can write the gravitino variation as\n\n\\[\\delta\\psi_{\\mu}dx^{\\mu}=\\left(\\partial_{\\mu}\\varepsilon+W_{\\mu}\\varepsilon \\right)dx^{\\mu}\\equiv\\mathcal{D}\\varepsilon.\\] (A.11)\n\nWe can get rid of the partial derivative of the spinor by acting with \\(\\mathcal{D}\\) a second time\n\n\\[\\mathcal{D}\\wedge\\mathcal{D}\\varepsilon=\\left(dW+W\\wedge W\\right)\\varepsilon= \\frac{1}{2}\\Theta_{\\mu\\nu}dx^{\\mu}\\wedge dx^{\\nu}\\varepsilon.\\] (A.12)\n\nEach of the components of \\(\\Theta_{\\mu\\nu}\\) defines a matrix equation, giving a total of 45 independent equations. We need to make sure that det(\\(\\Theta_{\\mu\\nu})=0\\) for each of the components. The equations\n\n\\[M\\varepsilon=0,\\quad\\Theta_{\\mu\\nu}\\varepsilon=0,\\] (A.13)\n\nconstrain the number of independent components of the spinor. After this procedure we use the gravitino variation to solve the dependence of the spinor on the spacetime coordinates.\n\nSpecialising to our background, the determinant of the Dilatino variation for the background in eq.( 2.3 ) reads\n\n\\[\\det(M)\\sim\\left(4(e_{B}Q_{A}-e_{A}Q_{B})^{2}+m^{2}\\right)^{8}\\left(4(e_{B}Q_{ A}+e_{A}Q_{B})^{2}+m^{2}\\right)^{8}.\\] (A.14)\n\nIn order to have non-trivial solutions we need to impose the following BPS conditions on the parameters of the background\n\n\\[e_{A}Q_{B}=\\pm e_{B}Q_{A},\\quad m=0.\n\n"]}, {"edit": ["scenes (though they have similar objects such as computers, chairs, or tables, as shown in Figure 1(b)), which may confuse the recognition system. Furthermore, constructing a scene representation that captures crucial semantic information reflecting the complexity of the data is challenging, particularly when dealing with a large number of categories\n\nResearch in scene recognition frameworks can be categorized into ones that are based on hand-crafted engineering and methods that employ automatic feature extraction without human intervention. Hand-engineered features are based on manually designing and selecting features utilizing different techniques to capture spatial characteristics, local features, object-based concepts, and holistic representations of scenes [6; 7; 8; 9]. However, hand-crafted features require notable domain expertise and significant human effort, resulting in inefficiency. Consequently, Deep Convolutional Neural Networks (DCNNs) have largely replaced them due to their superior representation learning ability [10; 11; 12]. They have demonstrated that they attain superior classification performance when trained on extensive datasets. After AlexNet's [13] introduction, deep convolutional neural networks, such as VGG [14], Inception [15], ResNet [16], and DenseNet [17], impressively advanced the field of image classification due to their great ability of capturing locally correlated image values [18; 19; 20] and learning features that enhance efficiency compared to hand-crafted methods. Utilizing deep convolutional neural networks can enhance scene recognition performance, but it remains challenging due to the intricate spatial layout, intra-class variation, and inter-class similarity, which weaken discriminability among scenes [21; 22]. In addition, due to the rise of extensive scene-centric datasets, a simple CNN-generated representation model is inadequate to accurately discriminate large-scale scenes [23]. As a result, research has shifted its focus to developing more representative features by incorporating contextual information, such as objects, or proposing strategies to effectively extract features that facilitate decision-making boundaries. Furthermore, several studies have employed ensemble learning strategies to leverage the complementary strengths of multiple feature levels or recognition models [24; 25; 26].\n\nBesides improving accuracy, recognition systems based on deep neural networks face the challenge of ambiguity in the reasoning behind the model's predictions[27; 28; 29]. This boils down to the black-box nature of deep neural networks, which induces a lack of trust and ethical concerns regarding the model's decisions, especially in high-stakes applications such as medical analysis and self-driving vehicles [30; 31; 32]. Along this direction, numerous efforts have been made to help understand how a model solves a problem and makes decisions. Typically, an explanation should help us get the answers to the following questions: Why did the model predict this category? Is the prediction of the model reliable? Which parts of the input led\n\nFigure 1: Demonstrations of inter-class similarity and intra-class variation. a) Images from the auditorium and movie theater classes have a high degree of similarity (inter-class similarity). b) Images of the office demonstrate a considerable degree of intra-class diversity, suggesting a wide spectrum of visual features within the category.\n\n "], "nougat": ["scenes (though they have similar objects such as computers, chairs, or tables, as shown in Figure 1(b)), which may confuse the recognition system. Furthermore, constructing a scene representation that captures crucial semantic information reflecting the complexity of the data is challenging, particularly when dealing with a large number of categories\n\nResearch in scene recognition frameworks can be categorized into ones that are based on hand-crafted engineering and methods that employ automatic feature extraction without human intervention. Hand-engineered features are based on manually designing and selecting features utilizing different techniques to capture spatial characteristics, local features, object-based concepts, and holistic representations of scenes [6; 7; 8; 9]. However, hand-crafted features require notable domain expertise and significant human effort, resulting in inefficiency. Consequently, Deep Convolutional Neural Networks (DCNNs) have largely replaced them due to their superior representation learning ability [10; 11; 12]. They have demonstrated that they attain superior classification performance when trained on extensive datasets. After AlexNet's [13] introduction, deep convolutional neural networks, such as VGG [14], Inception [15], ResNet [16], and DenseNet [17], impressively advanced the field of image classification due to their great ability of capturing locally correlated image values [18; 19; 20] and learning features that enhance efficiency compared to hand-crafted methods. Utilizing deep convolutional neural networks can enhance scene recognition performance, but it remains challenging due to the intricate spatial layout, intra-class variation, and inter-class similarity, which weaken discriminability among scenes [21; 22]. In addition, due to the rise of extensive scene-centric datasets, a simple CNN-generated representation model is inadequate to accurately discriminate large-scale scenes [23]. As a result, research has shifted its focus to developing more representative features by incorporating contextual information, such as objects, or proposing strategies to effectively extract features that facilitate decision-making boundaries. Furthermore, several studies have employed ensemble learning strategies to leverage the complementary strengths of multiple feature levels or recognition models [24; 25; 26].\n\nBesides improving accuracy, recognition systems based on deep neural networks face the challenge of ambiguity in the reasoning behind the model's predictions[27; 28; 29]. This boils down to the black-box nature of deep neural networks, which induces a lack of trust and ethical concerns regarding the model's decisions, especially in high-stakes applications such as medical analysis and self-driving vehicles [30; 31; 32]. Along this direction, numerous efforts have been made to help understand how a model solves a problem and makes decisions. Typically, an explanation should help us get the answers to the following questions: Why did the model predict this category? Is the prediction of the model reliable? Which parts of the input led\n\nFigure 1: Demonstrations of inter-class similarity and intra-class variation. a) Images from the auditorium and movie theater classes have a high degree of similarity (inter-class similarity). b) Images of the office demonstrate a considerable degree of intra-class diversity, suggesting a wide spectrum of visual features within the category.\n\n "]}, {"edit": ["Database (NVD) by inferring new classes, enriching relations, and expanding conceptual coverage. The ontology is used to search for and query social media threads that contain cybersecurity-related information, and natural language processing techniques are used to relate unstructured information to concepts in the ontology. The paper highlights the advantages of Semantic Web technologies in integrating information from multiple and often heterogeneous sources, without human intervention. Rosa et.al. [8] presented a novel ontology-based approach to utilize ontology to identify and map threats to assets. With the support of formally sound approaches, this process can be streamlined and made more efficient. From an ontology perspective, the authors introduced ThreMA, an ontology-based approach for automating threat modeling in ICT infrastructures. ThreMA provides a standard meta-model that describes the infrastructure and a set of rules for threat modeling. The meta-model consists of three ontologies modules: ICT ontology for modeling the infrastructure, Data Flow ontology for representing data flow diagrams, and threat ontology for characterizing threats. The use of ontology and inference rules allows for a syntactical representation of the problem, mimicking expert thinking. This approach enhances extensibility, maintainability, and integration in a rapidly changing context. The paper emphasizes the importance of using ontologies to address the lack of context and low accuracy in threat modeling. Overall, ThreMA offers a comprehensive ontology-based solution for automating threat modeling in ICT infrastructures.\n\n## III Methodology\n\nThis work presents an ontology for representing various data sources about cloud computing and security. This ontology enables a knowledge presentation framework for all cloud computing and its relationships. The ontology consists of several modules: Cloud Computing and services, Cloud Service underlying components, and CVE module.\n\n### _Cloud Computing Stack and Services Ontology Module_\n\nThis section represents our proposed ontology module that covers the cloud computing stack and services. Our extension ontology is provided as a separate ontology, which is an important design criteria in ontology engine engineering [13, 14]. This ontology can be used to unify and provide a primary baseline for cloud computing stack, threat understanding, and system diagram presentations. Firstly, we start by creating the ontology of the cloud computing stack. Figure 2 depicts the ontology of the three cloud stack: Software as a Service (SaaS), Platform as a Service (PaaS), Infrastructure as a Service (IaaS), Function as a Service (FaaS), Communication as a service (CaaS), and Desktop as a service (DaaS). Figure 1 depicts the cloud service model. Cloud service has nine layers, green colored layers mean these layers are managed by the client while the other color refers to layers managed by the cloud providers.\n\nFig. 1: Cloud computing stack. The green color depicts the layers managed by the used user. While the orange-colored boxes represent the layers managed by the cloud provider.\n\nFig. 2: Cloud computing stack ontology.\n\n "], "nougat": ["Database (NVD) by inferring new classes, enriching relations, and expanding conceptual coverage. The ontology is used to search for and query social media threads that contain cybersecurity-related information, and natural language processing techniques are used to relate unstructured information to concepts in the ontology. The paper highlights the advantages of Semantic Web technologies in integrating information from multiple and often heterogeneous sources, without human intervention. Rosa et.al. [8] presented a novel ontology-based approach to utilize ontology to identify and map threats to assets. With the support of formally sound approaches, this process can be streamlined and made more efficient. From an ontology perspective, the authors introduced ThreMA, an ontology-based approach for automating threat modeling in ICT infrastructures. ThreMA provides a standard metamodel that describes the infrastructure and a set of rules for threat modeling. The meta-model consists of three ontologies modules: ICT ontology for modeling the infrastructure, Data Flow ontology for representing data flow diagrams, and threat ontology for characterizing threats. The use of ontology and inference rules allows for a syntactical representation of the problem, mimicking expert thinking. This approach enhances extensibility, maintainability, and integration in a rapidly changing context. The paper emphasizes the importance of using ontologies to address the lack of context and low accuracy in threat modeling. Overall, ThreMA offers a comprehensive ontology-based solution for automating threat modeling in ICT infrastructures.\n\n## III Methodology\n\nThis work presents an ontology for representing various data sources about cloud computing and security. This ontology enables a knowledge presentation framework for all cloud computing and its relationships. The ontology consists of several modules: Cloud Computing and services, Cloud Service underlying components, and CVE module.\n\n### _Cloud Computing Stack and Services Ontology Module_\n\nThis section represents our proposed ontology module that covers the cloud computing stack and services. Our extension ontology is provided as a separate ontology, which is an important design criteria in ontology engine engineering [13, 14]. This ontology can be used to unify and provide a primary baseline for cloud computing stack, threat understanding, and system diagram presentations. Firstly, we start by creating the ontology of the cloud computing stack. Figure 2 depicts the ontology of the three cloud stack: Software as a Service (SaaS), Platform as a Service (PaaS), Infrastructure as a Service (IaaS), Function as a Service (FaaS), Communication as a service (CaaS), and Desktop as a service (DaaS). Figure 1 depicts the cloud service model. Cloud service has nine layers, green colored layers mean these layers are managed by the client while the other color refers to layers managed by the cloud providers.\n\nFig. 1: Cloud computing stack. The green color depicts the layers managed by the used user. While the orange-colored boxes represent the layers managed by the cloud provider.\n\nFig. 2: Cloud computing stack ontology.\n\n "]}, {"edit": ["for the \\(q\\)-analogue to \\(\\lambda\\), which reduces to\n\n\\[[n]_{q}=1+q+\\cdots+q^{n-1},\\qquad\\text{for $n\\in\\mathbb{N}^{+}$.}\\]\n\nWe have that \\([0]_{q}=0\\) and\n\n\\[[np]_{q}=\\frac{q^{p}-1}{q-1}\\frac{q^{np}-1}{q^{p}-1}=[p]_{q}\\cdot[n]_{q^{p}}, \\qquad n,p\\in\\mathbb{N}^{+}. \\tag{2.1}\\]\n\nFor future use, we note that\n\n\\[|[n]_{q}|\\leqslant[n]_{|q|},\\qquad\\text{ for $n\\in\\mathbb{N}$,}\\]\n\nand also that\n\n\\[\\lim_{n\\to+\\infty}\\frac{[n]_{q}}{q^{n}}=\\frac{1}{q-1}. \\tag{2.2}\\]\n\nThese constants appear naturally while considering Jackson's \\(q\\)-derivative of a function \\(f\\), which is given by\n\n\\[d_{q}(f)(x):=\\frac{f(qx)-f(x)}{qx-x}=\\frac{\\sigma_{q}(f)(x)-f(x)}{qx-x},\\]\n\nwhenever the expression is defined. As before, \\(\\sigma_{q}(f)(x):=f(qx)\\). For analytic functions \\(f\\in\\mathcal{O}(D_{r})\\), we see that\n\n\\[\\sigma_{q}(f),d_{q}(f)\\in\\mathcal{O}(D_{r/|q|}) \\tag{2.3}\\]\n\nand they can be computed term by term using its power series expansion according to the rules\n\n\\[d_{q}(x^{n})=[n]_{q}x^{n-1},\\qquad\\sigma_{q}(x^{n})=q^{n}x^{n},\\qquad n\\in \\mathbb{N}.\\]\n\nOn the other hand, this formula allows to consider \\(d_{q},\\sigma_{d}:\\mathbb{C}[[x]]\\to\\mathbb{C}[[x]]\\), also defined term by term. In this setting, Leibniz rule is replaced by\n\n\\[d_{q}(fg)(x)=d_{q}(f)(x)g(x)+f(qx)d_{q}(g)(x). \\tag{2.4}\\]\n\nWe recall the coefficients\n\n\\[(a;q)_{n}=\\prod_{j=0}^{n-1}(1-aq^{j}),\\qquad(a;q^{-1})_{\\infty}=\\prod_{j=0}^{ \\infty}(1-aq^{-j}),\\qquad a\\in\\mathbb{C}. \\tag{2.5}\\]\n\nThe second one is convergent as we can compare it with a geometric series. The \\(q\\)-factorial is defined accordingly as\n\n\\[[n]_{q}^{!}=[1]_{q}[2]_{q}\\cdots[n]_{q}=\\frac{(q;q)_{n}}{(1-q)^{n}}.\\]\n\nIn general, for \\(|q|>1\\), since \\(\\lambda\\in\\mathbb{R}\\longmapsto[\\lambda]_{|q|}\\) is a strictly increasing function, the same holds for the map \\(n\\in\\mathbb{N}\\longmapsto[n]_{|q|}^{!}\\). Therefore, "], "nougat": ["for the \\(q\\)-analogue to \\(\\lambda\\), which reduces to\n\n\\[[n]_{q}=1+q+\\cdots+q^{n-1},\\qquad\\text{for $n\\in\\mathbb{N}^{+}$.}\\]\n\nWe have that \\([0]_{q}=0\\) and\n\n\\[[np]_{q}=\\frac{q^{p}-1}{q-1}\\frac{q^{np}-1}{q^{p}-1}=[p]_{q}\\cdot[n]_{q^{p}}, \\qquad n,p\\in\\mathbb{N}^{+}. \\tag{2.1}\\]\n\nFor future use, we note that\n\n\\[|[n]_{q}|\\leqslant[n]_{|q|},\\qquad\\text{ for $n\\in\\mathbb{N}$,}\\]\n\nand also that\n\n\\[\\lim_{n\\to+\\infty}\\frac{[n]_{q}}{q^{n}}=\\frac{1}{q-1}. \\tag{2.2}\\]\n\nThese constants appear naturally while considering Jackson\u2019s \\(q\\)-derivative of a function \\(f\\), which is given by\n\n\\[d_{q}(f)(x):=\\frac{f(qx)-f(x)}{qx-x}=\\frac{\\sigma_{q}(f)(x)-f(x)}{qx-x},\\]\n\nwhenever the expression is defined. As before, \\(\\sigma_{q}(f)(x):=f(qx)\\). For analytic functions \\(f\\in\\mathcal{O}(D_{r})\\), we see that\n\n\\[\\sigma_{q}(f),d_{q}(f)\\in\\mathcal{O}(D_{r/|q|}) \\tag{2.3}\\]\n\nand they can be computed term by term using its power series expansion according to the rules\n\n\\[d_{q}(x^{n})=[n]_{q}x^{n-1},\\qquad\\sigma_{q}(x^{n})=q^{n}x^{n},\\qquad n\\in \\mathbb{N}.\\]\n\nOn the other hand, this formula allows to consider \\(d_{q},\\sigma_{d}:\\mathbb{C}[[x]]\\to\\mathbb{C}[[x]]\\), also defined term by term. In this setting, Leibniz rule is replaced by\n\n\\[d_{q}(fg)(x)=d_{q}(f)(x)g(x)+f(qx)d_{q}(g)(x). \\tag{2.4}\\]\n\nWe recall the coefficients\n\n\\[(a;q)_{n}=\\prod_{j=0}^{n-1}(1-aq^{j}),\\qquad(a;q^{-1})_{\\infty}=\\prod_{j=0}^{ \\infty}(1-aq^{-j}),\\qquad a\\in\\mathbb{C}. \\tag{2.5}\\]\n\nThe second one is convergent as we can compare it with a geometric series. The \\(q\\)-factorial is defined accordingly as\n\n\\[[n]_{q}^{!}=[1]_{q}[2]_{q}\\cdots[n]_{q}=\\frac{(q;q)_{n}}{(1-q)^{n}}.\\]\n\nIn general, for \\(|q|>1\\), since \\(\\lambda\\in\\mathbb{R}\\longmapsto[\\lambda]_{|q|}\\) is a strictly increasing function, the same holds for the map \\(n\\in\\mathbb{N}\\longmapsto[n]_{|q|}^{!}\\). Therefore, "]}, {"edit": ["doping. As a result we find the largest angle change in \\(\\delta\\). As the inter-layer distance stays the same, the inner and outer Cr atoms of the bilayer system experience a different change in their local environments, which we interpret as the reason for the intra-layer magnetic symmetry breaking as reflected in \\(\\mu_{B}^{1}\\neq\\mu_{B}^{2}\\) and \\(\\mu_{B}^{3}\\neq\\mu_{B}^{4}\\).\n\n39\n\nN. D. Mermin and H. Wagner, Phys. Rev. Lett. **17**, 1133 (1966).\n\nS. Chakravarty, B. I. Halperin, and D. R. Nelson, Phys. Rev. B **39**, 2344 (1989).\n\nL. J. de Jongh, ed., _Magnetic Properties of Layered Transition Metal Compounds_ (Springer, 1990).\n\nV. Y. Irkhin, A. A. Katanin, and M. I. Katsnelson, Phys. Rev. B **60**, 1082 (1999).\n\nM. Gibertini, M. Koperski, A. F. Morpurgo, and K. S. Novoselov, Nature Nanotechnology **14**, 408 (2019).\n\nK. Zollner, P. E. Faria Junior, and J. Fabian, Phys. Rev. B **100**, 085128 (2019).\n\nB. Huang, G. Clark, E. Navarro-Moratalla, D. R. Klein, R. Cheng, K. L. Seyler, D. Zhong, E. Schmidgall, M. A. McGuire, D. H. Cobden, et al., Nature **546**, 270 (2017).\n\nC. Gong, L. Li, Z. Li, H. Ji, A. Stern, Y. Xia, T. Cao, W. Bao, C. Wang, Y. Wang, et al., Nature **546**, 265 (2017).\n\nY. Deng, Y. Yu, Y. Song, J. Zhang, N. Z. Wang, Z. Sun, Y. Yi, Y. Z. Wu, S. Wu, J. Zhu, et al., Nature **563**, 94 (2018).\n\nO. Goser, W. Paul, and H. Kahle, Journal of Magnetism and Magnetic Materials **92**, 129 (1990).\n\nE. J. Telford, A. H. Dismukes, K. Lee, M. Cheng, A. Wieteska, A. K. Bartholomew, Y.-S. Chen, X. Xu, A. N. Pasupathy, X. Zhu, et al., Advanced Materials **32**, 2003240 (2020).\n\nY. Guo, Y. Zhang, S. Yuan, B. Wang, and J. Wang, Nanoscale **10**, 18036 (2018).\n\nZ. Jiang, P. Wang, J. Xing, X. Jiang, and J. Zhao, ACS Applied Materials and Interfaces **10**, 39032 (2018).\n\nC. Wang, X. Zhou, L. Zhou, N.-H. Tong, Z.-Y. Lu, and W. Ji, Science Bulletin **64**, 293 (2019).\n\nN. P. Wilson, K. Lee, J. Cenker, K. Xie, A. H. Dismukes, E. J. Telford, J. Fonseca, S. Sivakumar, C. Dean, T. Cao, et al., Nature Materials **20**, 1657 (2021).\n\nJ. Klein, B. Pingault, M. Florian, M.-C. Heissenbuttel, A. Steinhoff, Z. Song, K. Torres, F "], "nougat": ["doping. As a result we find the largest angle change in \\(\\delta\\). As the inter-layer distance stays the same, the inner and outer Cr atoms of the bilayer system experience a different change in their local environments, which we interpret as the reason for the intra-layer magnetic symmetry breaking as reflected in \\(\\mu_{B}^{1}\\neq\\mu_{B}^{2}\\) and \\(\\mu_{B}^{3}\\neq\\mu_{B}^{4}\\).\n\n39\n\nN. D. Mermin and H. Wagner, Phys. Rev. Lett. **17**, 1133 (1966). S. Chakravarty, B. I. Halperin, and D. R. Nelson, Phys. Rev. B **39**, 2344 (1989). L. J. de Jongh, ed., _Magnetic Properties of Layered Transition Metal Compounds_ (Springer, 1990). V. Y. Irkhin, A. A. Katanin, and M. I. Katsnelson, Phys. Rev. B **60**, 1082 (1999). M. Gibertini, M. Koperski, A. F. Morpurgo, and K. S. Novoselov, Nature Nanotechnology **14**, 408 (2019). K. Zollner, P. E. Faria Junior, and J. Fabian, Phys. Rev. B **100**, 085128 (2019). B. Huang, G. Clark, E. Navarro-Moratalla, D. R. Klein, R. Cheng, K. L. Seyler, D. Zhong, E. Schmidgall, M. A. McGuire, D. H. Cobden, et al., Nature **546**, 270 (2017). C. Gong, L. Li, Z. Li, H. Ji, A. Stern, Y. Xia, T. Cao, W. Bao, C. Wang, Y. Wang, et al., Nature **546**, 265 (2017). Y. Deng, Y. Yu, Y. Song, J. Zhang, N. Z. Wang, Z. Sun, Y. Yi, Y. Z. Wu, S. Wu, J. Zhu, et al., Nature **563**, 94 (2018). O. G \u0308oser, W. Paul, and H. Kahle, Journal of Magnetism and Magnetic Materials **92**, 129 (1990). E. J. Telford, A. H. Dismukes, K. Lee, M. Cheng, A. Wieteska, A. K. Bartholomew, Y.-S. Chen, X. Xu, A. N. Pasupathy, X. Zhu, et al., Advanced Materials **32**, 2003240 (2020). Y. Guo, Y. Zhang, S. Yuan, B. Wang, and J. Wang, Nanoscale **10**, 18036 (2018). Z. Jiang, P. Wang, J. Xing, X. Jiang, and J. Zhao, ACS Applied Materials and Interfaces **10**, 39032 (2018). C. Wang, X. Zhou, L. Zhou, N.-H. Tong, Z.-Y. Lu, and W. Ji, Science Bulletin **64**, 293 (2019). N. P. Wilson, K. Lee, J. Cenker, K. Xie, A. H. Dismukes, E. J. Telford, J. Fonseca, S. Sivakumar, C. Dean, T. Cao, et al., Nature Materials **20**, 1657 (2021). J. Klein, B. Pingault, M. Florian, M.-C. Heissenbuttel, A. Steinhoff, Z. Song, K. Torres, F. Dirnberger, J. B. Curtis, M. Weile, et al., ACS Nano **17**, "]}, {"edit": ["describes the traveling-wave microwave photon transporting along the transmission line with \\(l=b,c\\) refering to its left, right side, and the relevant bosonic operators satisfy the communication relation: \\([l(\\omega),l^{\\dagger}(\\omega^{\\prime})]=\\delta(\\omega-\\omega^{\\prime})\\). Also, the flux operator of the traveling-wave photon reads [14; 15; 16]:\n\n\\[\\hat{\\phi}_{l}(x)=\\sqrt{\\frac{\\hbar Z_{0}}{4\\pi}}\\int_{0}^{\\infty}\\frac{d \\omega}{\\sqrt{\\omega}}\\left[\\hat{l}(\\omega)e^{ikx}+\\hat{l}^{\\dagger}(\\omega)e^ {-ikx}\\right], \\tag{7}\\]\n\nwith \\(Z_{0}\\) being the characteristic impedance of the transmission line, and thus\n\n\\[\\dot{\\hat{\\phi}}_{l}(x)=(-i)\\sqrt{\\frac{\\hbar Z_{0}}{4\\pi}}\\int_{0}^{\\infty}d \\omega\\sqrt{\\omega}\\left[\\hat{l}(\\omega)e^{ikx}-\\hat{l}^{\\dagger}(\\omega)e^{- ikx}\\right]. \\tag{8}\\]\n\nUnder the sufficiently low current bias, the CBJJ Hamiltonian reads: \\(\\hat{H}_{CBJJ\\prime}\\approx\\hat{H}_{b}\\), shown in Eq. 4. The physical boundary condition at \\(x=0\\), i.e., the location of the device, reads: \\(\\hat{I}\\left(0_{b},t\\right)=\\hat{I}\\left(0_{c},t\\right)\\), \\(V_{J}=\\left(\\Phi_{0}/2\\pi\\right)\\dot{\\delta}+\\left[\\dot{\\phi}(0_{b})-\\dot{\\phi }(0_{c})\\right]\\). Thus, under the low-excitation limit and rotating-wave approximation, i.e., the photon scattering is the desired elastic and any possibly created and annihilated of the photons at \\(x=0\\) is neglected, we have\n\n\\[\\hat{H}_{CBJJ-B}= C_{J}\\hat{p}_{\\theta}\\left[\\dot{\\phi}(0_{b})-\\dot{\\phi}(0_{c})\\right]\\] \\[= i\\hbar\\sqrt{\\frac{\\kappa_{l}}{2\\pi}}\\int d\\omega\\left[a^{\\dagger} l(\\omega)-l^{\\dagger}(\\omega)a\\right],\\]\n\nwhere \\(\\kappa_{l}=Z_{0}/4Z_{J}\\left(l=b,c\\right)\\) describes the interaction between the CBJJ and the left/right traveling-wave photons, \\(Z_{J}=\\sqrt{L_{J}/C_{J}}\\) is the characteristic impedance of the Josephson junction. As a consequence, the Hamiltonian (with \\(\\hbar=1\\)) of the system [16; 17; 18]:\n\n\\[H_{B}= \\left(\\omega_{P}-\\frac{i\\gamma}{2}\\right)a^{\\dagger}a\\] (9) \\[+\\int d\\omega\\left[\\omega b(\\omega)^{\\dagger}b(\\omega)+i\\sqrt{ \\frac{\\kappa_{1}}{2\\pi}}\\left(a^{\\dagger}b(\\omega)-ab(\\omega)^{\\dagger})\\right]\\] \\[+\\int d\\omega\\left[\\omega c(\\omega)^{\\dagger}c(\\omega "], "nougat": ["describes the traveling-wave microwave photon transporting along the transmission line with \\(l=b,c\\) refering to its left, right side, and the relevant bosonic operators satisfy the communication relation: \\([l(\\omega),l^{\\dagger}(\\omega^{\\prime})]=\\delta(\\omega-\\omega^{\\prime})\\). Also, the flux operator of the traveling-wave photon reads [14\u201316]:\n\n\\[\\hat{\\phi}_{l}(x)=\\sqrt{\\frac{\\hbar Z_{0}}{4\\pi}}\\int_{0}^{\\infty}\\frac{d\\omega }{\\sqrt{\\omega}}\\left[\\hat{l}(\\omega)e^{ikx}+\\hat{l}^{\\dagger}(\\omega)e^{-ikx }\\right], \\tag{7}\\]\n\nwith \\(Z_{0}\\) being the characteristic impedance of the transmission line, and thus\n\n\\[\\dot{\\hat{\\phi}}_{l}(x)=(-i)\\sqrt{\\frac{\\hbar Z_{0}}{4\\pi}}\\int_{0}^{\\infty}d \\omega\\sqrt{\\omega}\\left[\\hat{l}(\\omega)e^{ikx}-\\hat{l}^{\\dagger}(\\omega)e^{- ikx}\\right]. \\tag{8}\\]\n\nUnder the sufficiently low current bias, the CBJJ Hamiltonian reads: \\(\\hat{H}_{CBJJ}\\approx\\hat{H}_{b}\\), shown in Eq. 4. The physical boundary condition at \\(x=0\\), i.e., the location of the device, reads: \\(\\hat{I}\\left(0_{b},t\\right)=\\hat{I}\\left(0_{c},t\\right)\\), \\(V_{J}=\\left(\\Phi_{0}/2\\pi\\right)\\dot{\\delta}+\\left[\\dot{\\phi}(0_{b})-\\dot{\\phi }(0_{c})\\right]\\). Thus, under the low-excitation limit and rotating-wave approximation, i.e., the photon scattering is the desired elastic and any possibly created and annihilated of the photons at \\(x=0\\) is neglected, we have\n\n\\[\\hat{H}_{CBJJ-B}= C_{J}\\hat{p}_{\\theta}\\left[\\dot{\\phi}(0_{b})-\\dot{\\phi}(0_{c})\\right]\\] \\[= i\\hbar\\sqrt{\\frac{\\kappa_{l}}{2\\pi}}\\int d\\omega\\left[a^{\\dagger} l(\\omega)-l^{\\dagger}(\\omega)a\\right],\\]\n\nwhere \\(\\kappa_{l}=Z_{0}/4Z_{J}\\left(l=b,c\\right)\\) describes the interaction between the CBJJ and the left/right traveling-wave photons, \\(Z_{J}=\\sqrt{L_{J}/C_{J}}\\) is the characteristic impedance of the Josephson junction. As a consequence, the Hamiltonian (with \\(\\hbar=1\\)) of the system [16\u201318]:\n\n\\[H_{B}= \\left(\\omega_{P}-\\frac{i\\gamma}{2}\\right)a^{\\dagger}a\\] (9) \\[+\\int d\\omega\\left[\\omega b(\\omega)^{\\dagger}b(\\omega)+i\\sqrt{ \\frac{\\kappa_{1}}{2\\pi}}\\left(a^{\\dagger}b(\\omega)-ab(\\omega)^{\\dagger})\\right]\\] \\[+\\int d\\omega\\left[\\omega c(\\omega)^{\\dagger}c(\\omega)+i\\sqrt{ \\frac{\\kappa "]}, {"edit": ["Introduction\n\nVectorlike (VL) fermions are key ingredients in many new physics models beyond the Standard Model (SM) adopted to resolve both theoretical and experimental issues. Since chiral fermions in the fourth family are excluded experimentally [1, 2], these are considered to be vectorlike and their masses are given independently to the Higgs mechanism in the SM. The VL fermions are introduced in, for instance, supersymmetric models [3, 4, 5, 6, 7, 8, 9, 10], gauge mediated supersymmetry breaking scenario [11, 12, 13, 14, 15, 16], composite Higgs models [17, 18, 19], KSVZ axion models [20, 21], axion-like particle models [22, 23, 24, 25], alternative solutions of the strong CP problem [26, 27], two Higgs doublet model augmented by VL fermions [28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38], and models for gauge coupling unification [39, 40].\n\nAmong the fourth family fermions, VL leptons (VLLs) with nonzero lepton number play unique roles in constructing lepton-philic dark matter (DM) models [41, 42, 43, 44, 45, 46], mirror sector models [47, 48, 49], and explanations for the muon anomalies [50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65]1. Interestingly, the lightest VLL is expected to be in the reach of the Large Hadron Collider (LHC) or High-Luminosity (HL)-LHC. Both ATLAS and CMS collaborations search for the pair production of VLLs each of which dominantly decays to a SM boson and a tau lepton [70, 71, 72]. For the doublet VLL 2, the ATLAS search excludes the mass range of \\(130<m_{\\rm VLL}<900\\) GeV [72], and the CMS search excludes the mass up to 1045 GeV [71]. The singlet VLL is less constrained and the limit is less than 150 GeV [71]. Prospects of such VLLs at the future colliders are discussed in Ref. [73]. The pair productions of the VLLs decaying to a SM boson and a muon (neutrino) are studied by the ATLAS [74] and the theorists [75, 76] using the Run-I data. The limit is obtained for \\(m_{\\rm VLL}\\lesssim 500\\) GeV when the neutral component of the lightest doublet VLL dominantly decays to a \\(W\\) boson and a muon [75]. There are also studies for the VLL produced from cascade decays of extra neutral Higgs bosons [28, 29, 30, 31, 35, 36], and signals from the VLLs decaying to a DM particle [77] or \\(Z^{\\prime}\\) boson [78].\n\nFootnote 1: The latest experimental result [66] confirms the previous results [67, 68] which might deviate from the SM prediction [69].\n\nFootnote 2: Throughout this work, doublet (singlet) for VLL means iso-doublet (iso-singlet) under the \\(SU(2)_{L}\\) gauge symmetry.\n\nIn this paper, we study pair-productions of the VLLs, through the Drell-Yan process, decaying to the second generation lepton, namely _muon-philic_ VLL. Such VLL is well motivated to explain the experimental anomalies in the muon \\(g-2\\)[69, 79, 80] and the semi-leptonic \\(B\\) decays [81]. 3 In this work, we obtain the current limits using the Run-2 data at \\(\\sqrt{s}=13\\) TeV "], "nougat": ["Introduction\n\nVectorlike (VL) fermions are key ingredients in many new physics models beyond the Standard Model (SM) adopted to resolve both theoretical and experimental issues. Since chiral fermions in the fourth family are excluded experimentally [1, 2], these are considered to be vectorlike and their masses are given independently to the Higgs mechanism in the SM. The VL fermions are introduced in, for instance, supersymmetric models [3\u201310], gauge mediated supersymmetry breaking scenario [11\u201316], composite Higgs models [17\u201319], KSVZ axion models [20,21], axionlike particle models [22\u201325], alternative solutions of the strong CP problem [26,27], two Higgs doublet model augmented by VL fermions [28\u201338], and models for gauge coupling unification [39,40].\n\nAmong the fourth family fermions, VL leptons (VLLs) with nonzero lepton number play unique roles in constructing lepton-philic dark matter (DM) models [41\u201346], mirror sector models [47\u201349], and explanations for the muon anomalies [50\u201365] 1 . Interestingly, the lightest VLL is expected to be in the reach of the Large Hadron Collider (LHC) or High-Luminosity (HL)-LHC. Both ATLAS and CMS collaborations search for the pair production of VLLs each of which dominantly decays to a SM boson and a tau lepton [70\u201372]. For the doublet VLL 2, the ATLAS search excludes the mass range of \\(130<m_{\\rm VLL}<900\\) GeV [72], and the CMS search excludes the mass up to 1045 GeV [71]. The singlet VLL is less constrained and the limit is less than 150 GeV [71]. Prospects of such VLLs at the future colliders are discussed in Ref. [73]. The pair productions of the VLLs decaying to a SM boson and a muon (neutrino) are studied by the ATLAS [74] and the theorists [75, 76] using the Run-I data. The limit is obtained for \\(m_{\\rm VLL}\\lesssim 500\\) GeV when the neutral component of the lightest doublet VLL dominantly decays to a \\(W\\) boson and a muon [75]. There are also studies for the VLL produced from cascade decays of extra neutral Higgs bosons [28\u201331, 35, 36, 29\u201331], and signals from the VLLs decaying to a DM particle [77] or \\(Z^{\\prime}\\) boson [78].\n\nFootnote 1: The latest experimental result [66] confirms the previous results [67,68] which might deviate from the SM prediction [69].\n\nFootnote 2: Throughout this work, doublet (singlet) for VLL means iso-doublet (iso-singlet) under the \\(SU(2)_{L}\\) gauge symmetry.\n\nIn this paper, we study pair-productions of the VLLs each of which dominantly decays to a SM boson and a tau lepton [70\u201372]. For the doublet VLL 2 , the ATLAS search excludes the mass range of \\(130<m_{\\rm VLL}<900\\) GeV [72], and the CMS search excludes the mass up to 1045 GeV [71]. The singlet VLL is less constrained and the limit is less than 150 GeV [71]. Prospects of such VLLs at the future colliders are discussed in Ref. [73]. The pair productions of the VLLs decaying to a SM boson and a muon (neutrino) are studied by the ATLAS [74] and the theorists [75, 76] using the Run-I data. The limit is obtained for \\(m_{\\rm VLL}\\lesssim 500\\) GeV when the neutral component of the lightest doublet VLL dominantly decays to a \\(W\\) boson and a muon [75]. There are also studies for the VLL produced from cascade decays of extra neutral Higgs bosons [28\u201331, 35, 36, 29\u201331], and signals from the VLLs decaying to "]}, {"edit": ["\n\n#### 3.1.2 Even \\(L\\) with a fermion parity defect\n\nLet us introduce a \\(\\mathsf{G}\\) defect. A concrete Hamiltonian to keep in mind is\n\n\\[H_{\\mathsf{G}}=\\frac{i}{2}\\sum_{\\ell=1}^{L-1}\\chi_{\\ell+1}\\chi_{\\ell}-\\frac{i}{2 }\\chi_{1}\\chi_{L}\\,, \\tag{3.21}\\]\n\nwhere the defect is in the link connecting \\((L,1)\\). We use the subscript \\(\\mathsf{G}\\) for the Hamiltonian and symmetry operators in the system with a \\(\\mathsf{G}\\) defect. However, we emphasize that for most of our discussion the particular form of the Hamiltonian will not matter. Note that the defect can be moved to other links, e.g., to the link \\((1,2)\\), by conjugating \\(H_{\\mathsf{G}}\\) by a local \\(\\mathsf{G}\\) transformation, \\(\\chi_{1}\\) or \\(\\mathsf{g}_{1}\\).\n\nLet us determine the symmetry operators of the theory with the defect. We use the same fermion parity operator \\(\\mathsf{G}\\) as in (3.10), because it commutes with \\(H_{\\mathsf{G}}\\).21 On the other hand, instead of (3.5), the translation operator now acts on the fermion fields as\n\nFootnote 21: We do not write \\(\\mathsf{G}_{\\mathsf{G}}\\) because it is the same as \\(\\mathsf{G}\\).\n\n\\[T_{\\mathsf{G}}:\\qquad\\chi_{\\ell}\\to T_{\\mathsf{G}}\\chi_{\\ell}T_{\\mathsf{G}}^{ -1}=\\sum_{\\ell^{\\prime}}R(T_{\\mathsf{G}})_{\\ell,\\ell^{\\prime}}\\chi_{\\ell^{ \\prime}}=\\begin{cases}\\chi_{\\ell+1}&\\ell=1,2,\\cdots,L-1\\\\ -\\chi_{1}&\\ell=L\\end{cases} \\tag{3.22}\\]\n\n The algebra satisfied by these operators is\n\n\\[R(\\mathsf{G})^{2}=1\\,,\\quad R(T_{\\mathsf{G}})^{L}=R(\\mathsf{G})\\,,\\quad R( \\mathsf{G})\\,R(T_{\\mathsf{G}})=R(T_{\\mathsf{G}})\\,R(\\mathsf{G})\\,. \\tag{3.23}\\]\n\nIn contrast to the case without the defect (3.7),now we have\n\n\\[\\begin{split}&\\det R(T_{\\mathsf{G}})_{\\ell,\\ell^{\\prime}}=+1\\,,\\\\ &\\det R(\\mathsf{G})_{\\ell,\\ell^{\\prime}}=+1\\,.\\end{split} \\tag{3.24}\\]\n\nThis means that the twisted translation operator is an \\(SO(L)\\) transformation and is constructed out of an even number of fermions, i.e., it is bosonic.\n\nLet us write \\(T_{\\mathsf{G}}\\) in terms of the fermion fields. Again, (3.22) does not determine its phase normalization, and we will make an arbitrary choice below. Later, in Section 3.3, we will rescale it to \\(T_{\\text{NSNS}}\\) and compare it to the continuum operators. Its action in (3.22) means that we should multiply \\(T\\) by an operator that maps \\(\\chi_{1}\\to-\\chi_{1}\\) and leaves the other fermions unchanged, i.e., we should multiply it by \\(\\mathsf{g}\n\n"], "nougat": ["\n\n#### 3.1.2 Even \\(L\\) with a fermion parity defect\n\nLet us introduce a \\(\\mathsf{G}\\) defect. A concrete Hamiltonian to keep in mind is\n\n\\[H_{\\mathsf{G}}=\\frac{i}{2}\\sum_{\\ell=1}^{L-1}\\chi_{\\ell+1}\\chi_{\\ell}-\\frac{i}{2 }\\chi_{1}\\chi_{L}\\,, \\tag{3.21}\\]\n\nwhere the defect is in the link connecting \\((L,1)\\). We use the subscript \\(\\mathsf{G}\\) for the Hamiltonian and symmetry operators in the system with a \\(\\mathsf{G}\\) defect. However, we emphasize that for most of our discussion the particular form of the Hamiltonian will not matter. Note that the defect can be moved to other links, e.g., to the link \\((1,2)\\), by conjugating \\(H_{\\mathsf{G}}\\) by a local \\(\\mathsf{G}\\) transformation, \\(\\chi_{1}\\) or \\(\\mathsf{g}_{1}\\).\n\nLet us determine the symmetry operators of the theory with the defect. We use the same fermion parity operator \\(\\mathsf{G}\\) as in ( 3.10 ), because it commutes with \\(H_{\\mathsf{G}}\\).21 On the other hand, instead 21 of ( 3.5 ), the translation operator now acts on the fermion fields as\n\nFootnote 21: We do not write \\(\\mathsf{G}_{\\mathsf{G}}\\) because it is the same as \\(\\mathsf{G}\\).\n\n\\[T_{\\mathsf{G}}:\\qquad\\chi_{\\ell}\\to T_{\\mathsf{G}}\\chi_{\\ell}T_{\\mathsf{G}}^{- 1}=\\sum_{\\ell^{\\prime}}R(T_{\\mathsf{G}})_{\\ell,\\ell^{\\prime}}\\chi_{\\ell^{\\prime }}=\\begin{cases}\\chi_{\\ell+1}&\\ell=1,2,\\cdots,L-1\\\\ -\\chi_{1}&\\ell=L\\end{cases} \\tag{3.22}\\]\n\n The algebra satisfied by these operators is\n\n\\[R(\\mathsf{G})^{2}=1\\,,\\quad R(T_{\\mathsf{G}})^{L}=R(\\mathsf{G})\\,,\\quad R( \\mathsf{G})\\,R(T_{\\mathsf{G}})=R(T_{\\mathsf{G}})\\,R(\\mathsf{G})\\,. \\tag{3.23}\\]\n\n In contrast to the case without the defect ( 3.7 ),now we have\n\n\\[\\begin{split}&\\det R(T_{\\mathsf{G}})_{\\ell,\\ell^{\\prime}}=+1\\,,\\\\ &\\det R(\\mathsf{G})_{\\ell,\\ell^{\\prime}}=+1\\,.\\end{split} \\tag{3.24}\\]\n\n This means that the twisted translation operator is an \\(SO(L)\\) transformation and is constructed out of an even number of fermions, i.e., it is bosonic.\n\nLet us write \\(T_{\\mathsf{G}}\\) in terms of the fermion fields. Again, ( 3.22 ) does not determine its phase normalization, and we will make an arbitrary choice below. Later, in Section 3.3 , we will rescale it to \\(T_{\\text{NSNS}}\\) and compare it to the continuum operators. Its action in ( 3.22 ) means that we should multiply \\(T\\) by an operator that maps \\(\\chi_{1}\\to-\\chi_{1}\\) and leaves the"]}, {"edit": ["when measuring the correlation of cities. Although the correlation of cities can be measured from the aspect of POI distribution, the user behavioral transition pattern is a significant factor in the next POI recommendation task, we thus further explore such correlation from the angle of user sequential behaviors.\n\n**Correlation of Cities w.r.t Behavioral Patterns**. We examine the correlation of cities w.r.t. the categories of users' successive POI visits. In particular, given any two cities, \\(A^{cat}=[A_{1}^{cat},A_{2}^{cat}...A_{|\\mathcal{S}|}^{cat}]\\) and \\(B^{cat}=[B_{1}^{cat},B_{2}^{cat}...B_{|\\mathcal{S}|}^{cat}]\\) refer to the category transition distributions among \\(\\mathcal{S}\\) transition types, e.g., \\(A_{1}^{cat}\\) denotes the ratio of transition type \\(FO\\to SS\\) within city A. Analogously, the similarity among different cities can be calculated via the Pearson correlation coefficient, shown in Fig. 2(b). Interestingly, we observe that the correlation of cities w.r.t behavioral patterns is quite different from that w.r.t POI distribution. Specifically, PHO and CAL still keep higher similarity, whereas NYC shows comparably lower similarity with PHO and CAL. To further dig out how the four cities are correlated and different over the behavioral patterns, we compare the two most correlated cities (i.e., CAL and PHO) and the two least correlated cities (i.e., NYC and SIN). For ease of presentation, we select the 10 most frequent category transitions for comparison as shown in Fig. 2(c-d), where the \\(x\\)-axis denotes the category transitions, e.g., \\(AE\\to CU\\) (AE2CU), and the \\(y-\\)axis shows the proportion of such a transition within a city. We find that the more correlated cities possess consistent distributions over the frequent category transitions and _vice versa_. The above observations depict the various correlations between cities, which inspire us to differentiate their influence when transferring knowledge from auxiliary cities to the target city.\n\n## 4 The Proposed MERec\n\nThis section presents the proposed MERec, which leverages the correlation of behavioral patterns when transferring knowledge from auxiliary cities to the target city, i.e., paying more attention to more correlated knowledge.\n\n**Problem Formulation.** Each city has its unique user set \\(\\mathcal{U}\\) and POI set \\(\\mathcal{P}\\) without sharing any common users and POIs. For user \\(u\\), all his check-in records, i.e., \\(r=(p,c,g,t)\\), are ordered by timestamps as in [22], where \\(p,c,g,t\\) denote POI \\(p\\), category \\(c\\), coordinate \\(g\\) (i.e., longitude and latitude) and timestamp \\(t\\). We then split his historical records into sequences by day and obtain two\n\nFigure 2: (a-b) the correlation of four cities w.r.t POI distribution and behavioral patterns at category level; (c-d) two most correlated and least correlated cities.\n\n "], "nougat": ["when measuring the correlation of cities. Although the correlation of cities can be measured from the aspect of POI distribution, the user behavioral transition pattern is a significant factor in the next POI recommendation task, we thus further explore such correlation from the angle of user sequential behaviors.\n\n**Correlation of Cities w.r.t Behavioral Patterns**. We examine the correlation of cities w.r.t. the categories of users\u2019 successive POI visits. In particular, given any two cities, \\(A^{cat}=[A_{1}^{cat},A_{2}^{cat}...A_{|\\mathcal{S}|}^{cat}]\\) and \\(B^{cat}=[B_{1}^{cat},B_{2}^{cat}...B_{|\\mathcal{S}|}^{cat}]\\) refer to the category transition distributions among \\(\\mathcal{S}\\) transition types, e.g., \\(A_{1}^{cat}\\) denotes the ratio of transition type \\(FO\\to SS\\) within city A. Analogously, the similarity among different cities can be calculated via the Pearson correlation coefficient, shown in Fig. 2(b). Interestingly, we observe that the correlation of cities w.r.t behavioral patterns is quite different from that w.r.t POI distribution. Specifically, PHO and CAL still keep higher similarity, whereas NYC shows comparably lower similarity with PHO and CAL. To further dig out how the four cities are correlated and different over the behavioral patterns, we compare the two most correlated cities (i.e., CAL and PHO) and the two least correlated cities (i.e., NYC and SIN). For ease of presentation, we select the 10 most frequent category transitions for comparison as shown in Fig. 2(c-d), where the \\(x\\)-axis denotes the category transitions, e.g., \\(AE\\to CU\\) (AE2CU), and the \\(y-\\)axis shows the proportion of such a transition within a city. We find that the more correlated cities possess consistent distributions over the frequent category transitions and _vice versa_. The above observations depict the various correlations between cities, which inspire us to differentiate their influence when transferring knowledge from auxiliary cities to the target city.\n\n## 4 The Proposed MERec\n\nThis section presents the proposed MERec, which leverages the correlation of behavioral patterns when transferring knowledge from auxiliary cities to the target city, i.e., paying more attention to more correlated knowledge.\n\n**Problem Formulation.** Each city has its unique user set \\(\\mathcal{U}\\) and POI set \\(\\mathcal{P}\\) without sharing any common users and POIs. For user \\(u\\), all his check-in records, i.e., \\(r=(p,c,g,t)\\), are ordered by timestamps as in [22], where \\(p,c,g,t\\) denote POI \\(p\\), category \\(c\\), coordinate \\(g\\) (i.e., longitude and latitude) and timestamp \\(t\\). We then split his historical records into sequences by day and obtain two\n\nFigure 2: (a-b) the correlation of four cities w.r.t POI distribution and behavioral patterns at category level; (c-d) two most correlated and least correlated cities.\n\n "]}, {"edit": ["\n\n### Symmetric and Schur-positive sets\n\nAs mentioned in Section 1, a set \\(\\mathcal{A}\\) is symmetric with respect to a statistic function \\(D:\\mathcal{A}\\to 2^{[N-1]}\\) if its generating function \\(\\mathcal{Q}_{N,D}(\\mathcal{A})\\) is a symmetric function. Moreover, it is Schur-positive if all Schur coefficients are nonnegative integers.\n\nOne of the fundamental constructions of Schur-positive sets, regarding sets of standard Young tableaux (SYT), is due to Gessel [11]. Let \\(\\mathrm{SYT}(\\lambda)\\) denote the set of standard Young tableaux of shape \\(\\lambda\\). We draw tableaux in English notation, as in Figure 2. The _descent set_ of \\(T\\in\\mathrm{SYT}(\\lambda)\\) is\n\n\\[\\mathrm{Des}(T):=\\{i\\in[N-1]\\mid i+1\\text{ appears in a lower row than }i\\text{ in }T\\}.\\]\n\nFor example, the descent set of the SYT in Figure 2 is \\(\\{2,4,7,8\\}\\).\n\nThe entry in row \\(i\\) and column \\(j\\) of a tableau \\(T\\in\\mathrm{SYT}(\\lambda)\\) is denoted as \\(T_{i,j}\\). In addition, we define \\(\\mathrm{row}_{i}(T):=\\{T_{i,j}\\mid 1\\leq j\\leq\\lambda_{i}\\}\\) as the set of entries in the \\(i\\)-th row of \\(T\\). For example, if we consider the SYT shown in Figure 2, then \\(T_{3,2}=8\\) and \\(\\mathrm{row}_{3}(T)=\\{5,8\\}\\).\n\n**Theorem 2.4** (Gessel [11]).: _For every \\(\\lambda\\vdash N\\), the set \\(\\mathrm{SYT}(\\lambda)\\) is Schur-positive with respect to Des. Moreover, \\(\\mathcal{Q}(\\mathrm{SYT}(\\lambda))=s_{\\lambda}\\)._\n\nIn 2015, Adin and Roichman proved the following criterion.\n\n**Theorem 2.5** ([2, Prop. 9.1]).: _A set \\(\\mathcal{A}\\) is symmetric with respect to \\(D:S\\to 2^{[N-1]}\\) if and only if_\n\n\\[\\sum_{a\\in\\mathcal{A}}\\boldsymbol{t}^{D(a)}=\\sum_{\\lambda\\vdash N}c_{\\lambda} \\sum_{T\\in\\mathrm{SYT}(\\lambda)}\\boldsymbol{t}^{\\mathrm{Des}(T)}\\]\n\n_for some values \\(c_{\\lambda}\\), where \\(\\boldsymbol{t}^{J}:=\\prod_{j\\in J}t_{j}\\) for \\(J\\subseteq[N-1]\\). The coefficients \\(c_{\\lambda}\\) are the Schur-coefficients of \\(\\mathcal{A}\\). Moreover, \\(\\mathcal{A}\\) is Schur-positive if and only if \\(c_{\\lambda}\\in\\mathbb{N}_{0}\\) for all \\(\\lambda\\vdash N\\)._\n\nThis criterion implies that proving the Schur-positivity of a set is achievable by establishing a statistic-preserving bijection between the set and SYTs of shapes corresponding to a specific multiset.\n\nIn this paper, we will also apply a recently formulated criterion for symmetry [19].\n\n**Definition 2.6**.: Let \\(\\mathcal{A}\\) be a finite set with a statistic \\(D:\\mathcal{A}\\to 2^{[N-"], "nougat": ["\n\n### Symmetric and Schur-positive sets\n\nAs mentioned in Section 1, a set \\(\\mathcal{A}\\) is symmetric with respect to a statistic function \\(D:\\mathcal{A}\\to 2^{[N-1]}\\) if its generating function \\(\\mathcal{Q}_{N,D}(\\mathcal{A})\\) is a symmetric function. Moreover, it is Schur-positive if all Schur coefficients are nonnegative integers.\n\nOne of the fundamental constructions of Schur-positive sets, regarding sets of standard Young tableaux (SYT), is due to Gessel [11]. Let \\(\\mathrm{SYT}(\\lambda)\\) denote the set of standard Young tableaux of shape \\(\\lambda\\). We draw tableaux in English notation, as in Figure 2. The _descent set_ of \\(T\\in\\mathrm{SYT}(\\lambda)\\) is\n\n\\[\\mathrm{Des}(T):=\\{i\\in[N-1]\\mid i+1\\text{ appears in a lower row than }i\\text{ in }T\\}.\\]\n\nFor example, the descent set of the SYT in Figure 2 is \\(\\{2,4,7,8\\}\\).\n\nThe entry in row \\(i\\) and column \\(j\\) of a tableau \\(T\\in\\mathrm{SYT}(\\lambda)\\) is denoted as \\(T_{i,j}\\). In addition, we define \\(\\mathrm{row}_{i}(T):=\\{T_{i,j}\\mid 1\\leq j\\leq\\lambda_{i}\\}\\) as the set of entries in the \\(i\\)-th row of \\(T\\). For example, if we consider the SYT shown in Figure 2, then \\(T_{3,2}=8\\) and \\(\\mathrm{row}_{3}(T)=\\{5,8\\}\\).\n\n**Theorem 2.4** (Gessel [11]).: _For every \\(\\lambda\\vdash N\\), the set \\(\\mathrm{SYT}(\\lambda)\\) is Schur-positive with respect to Des. Moreover, \\(\\mathcal{Q}(\\mathrm{SYT}(\\lambda))=s_{\\lambda}\\)._\n\nIn 2015, Adin and Roichman proved the following criterion.\n\n**Theorem 2.5** ([2, Prop. 9.1]).: _A set \\(\\mathcal{A}\\) is symmetric with respect to \\(D:S\\to 2^{[N-1]}\\) if and only if_\n\n\\[\\sum_{a\\in\\mathcal{A}}\\boldsymbol{t}^{D(a)}=\\sum_{\\lambda\\vdash N}c_{\\lambda} \\sum_{T\\in\\mathrm{SYT}(\\lambda)}\\boldsymbol{t}^{\\mathrm{Des}(T)}\\]\n\n_for some values \\(c_{\\lambda}\\), where \\(\\boldsymbol{t}^{J}:=\\prod_{j\\in J}t_{j}\\) for \\(J\\subseteq[N-1]\\). The coefficients \\(c_{\\lambda}\\) are the Schur-coefficients of \\(\\mathcal{A}\\). Moreover, \\(\\mathcal{A}\\) is Schur-positive if and only if \\(c_{\\lambda}\\in\\mathbb{N}_{0}\\) for all \\(\\lambda\\vdash N\\)._\n\nThis criterion implies that proving the Schur-positivity of a set is achievable by establishing a statistic-preserving bijection between the set and SYTs of shapes corresponding to a specific multiset.\n\nIn this paper, we will also apply a recently formulated criterion for symmetry [19].\n\n**Definition 2.6**.: Let \\(\\mathcal{A}\\) be a finite set with a statistic \\(D:\\mathcal{A}\\to 2^{[N-"]}, {"edit": ["where \\(\\delta_{nm}\\) is the Kronecker delta symbol. Such relation leads to the definition of the Laguerre transform of order \\(\\nu\\):\n\n\\[{\\cal T}^{\\nu}[f(x)]=\\left\\{\\int_{0}^{\\infty}e^{-x}x^{\\nu}L_{n}^{\\nu}(x)f(x)dx \\right\\}=\\{c_{n}^{\\nu}\\}, \\tag{11}\\]\n\nit must be emphasize that the Laguerre transform is a sequence of numbers in \\(\\mathbb{C}\\). The inverse Laguerre transform is defined by\n\n\\[f(x)=({\\cal T}^{\\nu})^{-1}[\\{c_{n}^{\\nu}\\}]=\\sum_{k=0}^{\\infty}c_{n}^{\\nu}L_{n }^{\\nu}(x). \\tag{12}\\]\n\n## 3 Examples of applications\n\n### Applications to the Schrodinger equation\n\nIn this section is consider the equation\n\n\\[-\\frac{1}{2}\\frac{d^{2}}{dr^{2}}\\psi(r)-(V(r)+E)\\psi(r)=0 \\tag{13}\\]\n\nwhich in appropriate units (\\(\\hbar\\)=M=1) is the steady state Schodinger equation defined in a one dimensional space, where \\(V(r)\\) is a potential function and \\(E\\) is the energy. Under the change of coordinates (see [0]), given by \\(\\lambda^{-1}\\xi(x)=dx/dr\\), where \\(\\lambda\\geq 0\\) has inverse length units, equation (13) becomes\n\n\\[\\lambda^{2}\\xi^{2}\\left[\\frac{d^{2}}{dx^{2}}\\psi(x)+\\frac{1}{\\xi}\\frac{d\\xi}{ dx}\\frac{d}{dx}\\psi(x)-\\frac{2}{\\lambda^{2}\\xi^{2}}W(x)\\psi(x)\\right]=0, \\tag{14}\\]\n\nwhere \\(W(x)=V(r)-E\\). To obtain a Laguerre-type equation the change of coordinates must satisfy \\(x(r)\\geq 0\\) and setting \\(\\frac{1}{\\xi}\\frac{d\\xi}{dx}=\\frac{a}{x}\\), leads to \\(\\xi(x)=x^{a}e^{bx}\\).\n\nIn this way, equation (14) becomes\n\n\\[\\lambda^{2}\\xi^{2}\\left[\\frac{d^{2}}{dx^{2}}\\psi(x)+\\left(\\frac{a}{x}+b\\right) \\frac{d}{dx}\\psi(x)+\\left(A_{+}+\\frac{A_{-}}{x^{2}}-\\frac{A_{0}}{x}\\right)\\psi (x)\\right]=0, \\tag{15}\\]\n\nwhere \\(A_{\\pm},A_{0},a,b\\) are real parameters determined in terms of \\(V(r)\\) and \\(E\\).\n\nTo solve equation (15) it is proposed a solution of the form\n\n\\[\\psi(x)=x^{\\alpha}e^{-\\beta x}y(x), \\tag{16}\\]\n\nwhere \\(y=\\sum_{k=0}^{\\infty}c_{k}L_{k}^{\\nu}(x)\\), and \\(L_{n}^{\\nu}(x)\\) are the Laguerre polynomials of order \\(\\nu\\), and \\(\\alpha,\\beta,\\nu\\) are dimensionless parameters, free for the moment "], "nougat": ["where \\(\\delta_{nm}\\) is the Kronecker delta symbol. Such relation leads to the definition of the Laguerre transform of order \\(\\nu\\):\n\n\\[{\\cal T}^{\\nu}[f(x)]=\\left\\{\\int_{0}^{\\infty}e^{-x}x^{\\nu}L_{n}^{\\nu}(x)f(x)dx \\right\\}=\\{c_{n}^{\\nu}\\}, \\tag{11}\\]\n\nit must be emphasize that the Laguerre transform is a sequence of numbers in \\(\\mathbb{C}\\). The inverse Laguerre transform is defined by\n\n\\[f(x)=({\\cal T}^{\\nu})^{-1}[\\{c_{n}^{\\nu}\\}]=\\sum_{k=0}^{\\infty}c_{n}^{\\nu}L_{n }^{\\nu}(x). \\tag{12}\\]\n\n## 3 Examples of applications\n\n### Applications to the Schr \u0308odinger equation\n\nIn this section is consider the equation\n\n\\[-\\frac{1}{2}\\frac{d^{2}}{dr^{2}}\\psi(r)-(V(r)+E)\\psi(r)=0 \\tag{13}\\]\n\nwhich in appropriate units (\\(\\hbar\\)=M=1) is the steady state Sch \u0308odinger equation defined in a one dimensional space, where \\(V(r)\\) is a potential function and \\(E\\) is the energy. Under the change of coordinates (see [0]), given by \\(\\lambda^{-1}\\xi(x)=dx/dr\\), where \\(\\lambda\\geq 0\\) has inverse length units, equation (13) becomes\n\n\\[\\lambda^{2}\\xi^{2}\\left[\\frac{d^{2}}{dx^{2}}\\psi(x)+\\frac{1}{\\xi}\\frac{d\\xi}{ dx}\\frac{d}{dx}\\psi(x)-\\frac{2}{\\lambda^{2}\\xi^{2}}W(x)\\psi(x)\\right]=0, \\tag{14}\\]\n\nwhere \\(W(x)=V(r)-E\\). To obtain a Laguerre-type equation the change of coordinates must satisfy \\(x(r)\\geq 0\\) and setting \\(\\frac{1}{\\xi}\\frac{d\\xi}{dx}=\\frac{a}{x}\\), leads to \\(\\xi(x)=x^{a}e^{bx}\\).\n\nIn this way, equation (14) becomes\n\n\\[\\lambda^{2}\\xi^{2}\\left[\\frac{d^{2}}{dx^{2}}\\psi(x)+\\left(\\frac{a}{x}+b\\right) \\frac{d}{dx}\\psi(x)+\\left(A_{+}+\\frac{A_{-}}{x^{2}}-\\frac{A_{0}}{x}\\right)\\psi (x)\\right]=0, \\tag{15}\\]\n\nwhere \\(A_{\\pm},A_{0},a,b\\) are real parameters determined in terms of \\(V(r)\\) and \\(E\\).\n\nTo solve equation (15) it is proposed a solution of the form\n\n\\[\\psi(x)=x^{\\alpha}e^{-\\beta x}y(x), \\tag{16}\\]\n\nwhere \\(y=\\sum_{k=0}^{\\infty}c_{k}L_{k}^{\\nu}(x)\\), and \\(L_{n}^{\\nu}(x)\\) are the Laguerre polynomials of order \\(\\nu\\), and \\(\\alpha,\\beta,\\nu\\) are dimensionless parameters "]}, {"edit": ["Andrei Teimurazov\\({}^{1}\\), Matthew McCormack\\({}^{2,}\\)1, Moritz Linkmann\\({}^{2,}\\)2, and Olga Shishkina\\({}^{1,}\\)3\n\n\\({}^{1}\\)Max Planck Institute for Dynamics and Self-Organization, 37077 GA2{titingen, Germany\n\n\\({}^{2}\\)School of Mathematics and Maxwell Institute for Mathematical Sciences, University of Edinburgh, UK\n\nFootnote 1: A. Teimurazov and M. McCormack contributed equally.\n\nFootnote 2: moritz.linkmann@ed.ac.uk\n\nFootnote 3: olga.shishkina@ds.mpg.de\n\n###### Abstract\n\nIn magnetoconvection, the flow of electromagnetically conductive fluid is driven by a combination of buoyancy forces, which create the fluid motion due to thermal expansion and contraction, and Lorentz forces, which distort the convective flow structure in the presence of a magnetic field. The differences in the global flow structures in the buoyancy-dominated and Lorentz-force-dominated regimes lead to different heat transport properties in these regimes, reflected in distinct dimensionless scaling relations of the global heat flux (Nusselt number Nu) versus the strength of buoyancy (Rayleigh number Ra) and electromagnetic forces (Hartmann number Ha). Here, we propose a theoretical model for the transition between these two regimes for the case of a quasistatic vertical magnetic field applied to a convective fluid layer confined between two isothermal, a lower warmer and an upper colder, horizontal surfaces. The model suggests that the scaling exponents \\(\\gamma\\) in the buoyancy-dominated regime, \\(\\mathrm{Nu}\\sim\\mathrm{Ra}^{\\gamma}\\), and \\(\\xi\\) in the Lorentz-force-dominated regime, \\(\\mathrm{Nu}\\sim(\\mathrm{Ha}^{-2}\\mathrm{Ra})^{\\xi}\\), are related as \\(\\xi=\\gamma/(1-2\\gamma)\\), and the onset of the transition scales with \\(\\mathrm{Ha}^{-1/\\gamma}\\mathrm{Ra}\\). These theoretical results are supported by our Direct Numerical Simulations for \\(10\\leqslant\\mathrm{Ha}\\leqslant 2000\\), Prandtl number \\(\\mathrm{Pr}=0.025\\) and \\(\\mathrm{Ra}\\) up to \\(10^{9}\\) and data from the literature.\n\n## 1 Introduction\n\nMagnetoconvection (MC) governs most astro- and geophysical systems and is relevant to various engineering applications [25, 7]. The former include, for instance, outer layers of stars and liquid metal planetary cores [12], examples of the latter comprise liquid-metal batteries, induction heating, casting, liquid-metal cooling for nuclear fusion reactors and semiconductor crystal growth [6]. MC occurs in an electrically conducting fluid that is subjected both to a magnetic field and an imposed temperature gradient. The buoyancy forces induce convective fluid motion due to thermal expansion and contraction, while the magnetic field affects this motion and distorts the global flow structure through the Lorentz force, which eventually influences the heat transport in the system. The resulting main two control parameters, the strength of the imposed thermal driving and that of the external magnetic field, are encoded in independent dimensionless groups, the Rayleigh number Ra and Hartmann number Ha, respectively.\n\nOne of the key objectives in MC research is to provide scaling relations for the heat transport through the system, represented in dimensionless form by the Nusselt number Nu, as a function of Ra and Ha. However, the heat transport scaling relations also depend on the flow configuration, including the angle between the magnetic field and gravity, the geometry of the container and the boundary conditions (BCs), and on whether the buoyancy forces dominate over the Lorentz forces in the system or vice versa. This inherent complexity results in the need, at least in principle, to derive separate heat transport scaling relations to describe each specific flow regime itself and transitions between distinct regimes. The considerable difficulty of doing so in a coherent manner is exacerbated by non-universal scaling relations even within specific regimes - the scaling relations in the buoyancy-dominated and Lorentz-force-dominated regimes themselves change with the control parameters, and transitions between the different regimes are also non-universal.\n\nThe objective of this paper is to offer a unifying heat transport model for the transition between the buoyancy-dominated and Lorentz-force-dominated regimes in "], "nougat": ["\n\n# Andrei Teimurazov\\({}^{1}\\), Matthew McCormack\\({}^{2,}\\)1, Moritz Linkmann 2, 2\n\nOlga Shishkina A. Teimurazov and M. McCormack contributed equally. 1,2\n\nFootnote 1: A. Teimurazov and M. McCormack contributed equally.\n\nFootnote 2: moritz.linkmann@ed.ac.uk\n\n###### Abstract\n\nIn magnetoconvection, the flow of electromagnetically conductive fluid is driven by a combination of buoyancy forces, which create the fluid motion due to thermal expansion and contraction, and Lorentz forces, which distort the convective flow structure in the presence of a magnetic field. The differences in the global flow structures in the buoyancy-dominated and Lorentz-force-dominated regimes lead to different heat transport properties in these regimes, reflected in distinct dimensionless scaling relations of the global heat flux (Nusselt number Nu) versus the strength of buoyancy (Rayleigh number Ra) and electromagnetic forces (Hartmann number Ha). Here, we propose a theoretical model for the transition between these two regimes for the case of a quasistatic vertical magnetic field applied to a convective fluid layer confined between two isothermal, a lower warmer and an upper colder, horizontal surfaces. The model suggests that the scaling exponents \\(\\gamma\\) in the buoyancy-dominated regime, \\(\\mathrm{Nu}\\sim\\mathrm{Ra}^{\\gamma}\\), and \\(\\xi\\) in the Lorentz-force-dominated regime, \\(\\mathrm{Nu}\\sim(\\mathrm{Ha}^{-2}\\mathrm{Ra})^{\\xi}\\), are related as \\(\\xi=\\gamma/(1-2\\gamma)\\), and the onset of the transition scales with \\(\\mathrm{Ha}^{-1/\\gamma}\\mathrm{Ra}\\). These theoretical results are supported by our Direct Numerical Simulations for \\(10\\leqslant\\mathrm{Ha}\\leqslant 2000\\), Prandtl number \\(\\mathrm{Pr}=0.025\\) and \\(\\mathrm{Ra}\\) up to \\(10^{9}\\) and data from the literature.\n\n\\({}^{1}\\)Max Planck Institute for Dynamics and Self-Organization, 37077 G \u0303A\\(\\,\\)ttingen, Germany\n\n\\({}^{2}\\)School of Mathematics and Maxwell Institute for Mathematical Sciences, University of Edinburgh, UK\n\n## 1 Introduction\n\nMagnetoconvection (MC) governs most astro- and geophysical systems and is relevant to various engineering applications [25, 7]. The former include, for instance, outer layers of stars and liquid metal planetary cores [12], examples of the latter comprise liquid-metal batteries, induction heating, casting, liquid-metal cooling for nuclear fusion reactors and semiconductor crystal growth [6]. MC occurs in an electrically conducting fluid that is subjected both to a magnetic field and an imposed temperature gradient. The buoyancy forces induce convective fluid motion due to thermal expansion and contraction, while the magnetic field affects this motion and distorts the global flow structure through the Lorentz force, which eventually influences the heat transport in the system. The resulting main two control parameters, the strength of the imposed thermal driving and that of the external magnetic field, are encoded in independent dimensionless groups, the Rayleigh number Ra and Hartmann number Ha, respectively.\n\nOne of the key objectives in MC research is to provide scaling relations for the heat transport through the system, represented in dimensionless form by the Nusselt number Nu, as a function of Ra and Ha. However, the heat transport scaling relations also depend on the flow configuration, including the angle between the magnetic field and gravity, the geometry of the container and the boundary conditions (BCs), and on whether the buoyancy forces dominate over the Lorentz forces in the system or vice versa. This inherent complexity results in the need, at least in principle, to derive separate heat transport scaling relations to describe each specific flow regime itself and transitions between distinct regimes. The considerable difficulty of doing so in a coherent manner is exacerbated by non-universal scaling relations even within specific regimes - the scaling relations in the buoyancy-dominated and Lorentz-force-dominated regimes themselves change with the control parameters, and transitions between the different regimes are also non-universal.\n\nThe objective of this paper is to offer a unifying heat transport model for the transition between the buoyancy-dominated and Lorentz-force-dominated regimes in quasistatic MC. We focus on Rayleigh-Benard"]}, {"edit": ["\n\n### _Compared Methods_\n\nThe experiment includes a comparison of different models:\n\n* **I) MHA-LSTM [4]:** This model only takes as inputs the past trajectories of the agents in the scene and outputs \\(L\\) trajectories with their associated probabilities (see the architecture in the red rectangle in Fig. 1). We use \\(L=6\\) attention heads.\n* **II) G-MHA-LSTM [17]:** We add to the previous model a radial grid representation from which we extract potential goals. We predict the goal and then the trajectories conditioned on the predicted goal. (see the architecture in the orange rectangle in Fig. 1).\n* **III) DCM-MHA-LSTM :** To predict the goal of the target agent, we combine the DCM and the neural network using the LMNL framework [15]. This model is described in Section III and the architecture is illustrated in the blue rectangle in Fig. 1.\n* **IV) ODCM-MHA-LSTM :** This model only uses the DCM to predict the goal of the target agent.\n\n**Goal set representations :** We also compare different types of radial grids. For the methods II), III) and IV), we compare our results for two types of radial grid : a **dynamic** grid (d) and a **fixed** one (f). Similar to [12], we build the dynamic grid by considering the target agent's current velocity \\(v_{T}^{t_{obs}}\\). If \\(v_{T}^{t_{obs}}=0\\), we replace it with an arbitrary value equals to \\(0.5\\)\\(m.s^{-1}\\). The fixed grid is built using the value \\(v=5.83m.s^{-1}\\), which corresponds to the mean of the velocities in the INTERACTION training set.\n\n### _Compared DCMs_\n\nWe compare two types of DCMs for modelling the behavior of vehicle motion. For our case, the functions modelling vehicle motion phenomenon which we consider for goal selection in this work are:\n\n1. _occupancy:_ directions containing neighbours in the vicinity are less desirable.\n2. _keep direction:_ vehicles tend to maintain the same direction of motion.\n3. _collision avoidance:_ when a neighbour vehicle's trajectory is head-on towards a potential goal, this goal becomes less desirable due to the chance of a collision.\n* **1) DCM 1 :** For the first DCM configuration, we use a utility function defined as: \\[u_{k}(\\mathbf{X})=\\beta_{dir}dir_{k}+\\beta_{occ}occ_{k}+\\beta_{col}col_{k}\\] (13) Where the functions \\(dir_{k}\\), \\(occ_{k}\\), and \\(col_{k}\\) correspond respectively to _keep direction_, _occupancy_ and _collision avoidance_. These functions are defined in [2] and [6].\n* **2) DCM 2 :** For the second DCM, the utility function is defined as : \\[u_{k}(\\mathbf{X})=\\beta_{dir}dir_{k}+\\beta_{occup}occup_{k}\\] (14) Where the function \\(dir_{k}\\) is the same as in (IV-C). For \\(occup_{k}\\), we use the same mathematical formula as the occupancy function in (IV-C), however, we don't consider the position of the neighbors at time \\(t_{obs}\\). Instead, we consider their predicted position at time \\(t_{obs}+t_{f}\\) using a Constant velocity model. We assume that before predicting his goal, the target agent first predicts the future positions of his surroundings according to their headings and current velocitites, and then avoids the zones that are expected to be crowded. While training this model, we calculate the \\(occup_{k}\\) function using the growth truth positions of the neighbors.\n\n### _Implementation details_\n\nWe use \\(K=15\\) number of potential goals. Similar to [8], our interaction space"], "nougat": ["\n\n### _Compared Methods_\n\nThe experiment includes a comparison of different models:\n\n* **I) MHA-LSTM [4]:** This model only takes as inputs the past trajectories of the agents in the scene and outputs \\(L\\) trajectories with their associated probabilities (see the architecture in the red rectangle in Fig. 1). We use \\(L=6\\) attention heads.\n* **II) G-MHA-LSTM [17]:** We add to the previous model a radial grid representation from which we extract potential goals. We predict the goal and then the trajectories conditioned on the predicted goal. (see the architecture in the orange rectangle in Fig. 1).\n* **III) DCM-MHA-LSTM :** To predict the goal of the target agent, we combine the DCM and the neural network using the LMNL framework [15]. This model is described in Section III and the architecture is illustrated in the blue rectangle in Fig. 1.\n* **IV) ODCM-MHA-LSTM :** This model only uses the DCM to predict the goal of the target agent.\n\n**Goal set representations :** We also compare different types of radial grids. For the methods II), III) and IV), we compare our results for two types of radial grid : a **dynamic** grid (d) and a **fixed** one (f). Similar to [12], we build the dynamic grid by considering the target agent\u2019s current velocity \\(v_{T}^{t_{obs}}\\). If \\(v_{T}^{t_{obs}}=0\\), we replace it with an arbitrary value equals to \\(0.5\\)\\(m.s^{-1}\\). The fixed grid is built using the value \\(v=5.83m.s^{-1}\\), which corresponds to the mean of the velocities in the INTERACTION training set.\n\n### _Compared DCMs_\n\nWe compare two types of DCMs for modelling the behavior of vehicle motion. For our case, the functions modelling vehicle motion phenomenon which we consider for goal selection in this work are:\n\n1. _occupancy:_ directions containing neighbours in the vicinity are less desirable.\n2. _keep direction:_ vehicles tend to maintain the same direction of motion.\n3. _collision avoidance:_ when a neighbour vehicle's trajectory is head-on towards a potential goal, this goal becomes less desirable due to the chance of a collision.\n* **1) DCM 1 :** For the first DCM configuration, we use a utility function defined as: \\[u_{k}(\\mathbf{X})=\\beta_{dir}dir_{k}+\\beta_{occ}occ_{k}+\\beta_{col}col_{k}\\] (13) Where the functions \\(dir_{k}\\), \\(occ_{k}\\), and \\(col_{k}\\) correspond respectively to _keep direction_, _occupancy_ and _collision avoidance_. These functions are defined in [2] and [6].\n* **2) DCM 2 :** For the second DCM, the utility function is defined as : \\[u_{k}(\\mathbf{X})=\\beta_{dir}dir_{k}+\\beta_{occup}occup_{k}\\] (14) Where the function \\(dir_{k}\\) is the same as in (IV-C). For \\(occup_{k}\\), we use the same mathematical formula as the occupancy function in (IV-C), however, we don\u2019t consider the position of the neighbors at time \\(t_{obs}\\). Instead, we consider their predicted position at time \\(t_{obs}+t_{f}\\) using a Constant velocity model. We assume that before predicting his goal, the target agent first predicts the future positions of his surroundings according to their headings and current velocitites, and then avoids the zones that are expected to be crowded. While training this model, we calculate the \\(occup_{k}\\) function using the grouth truth positions of the neighbors.\n\n### _Implementation details_\n\nWe use \\(K=15\\) number of potential"]}, {"edit": ["that local interactions play a subordinate role and the hard-core constraint is largely inactive. As a result, the qualitative behavior of the system is dominated by the hopping processes, which naturally result in the formation of a Fermi surface similar to the one in graphene at high hole doping. The hard-core nature of the fermions and their density-density interactions then enter as corrections to this pure Fermi-gas behavior. By measuring the lattice Green's function \\(G^{\\alpha,\\beta}_{i,j}\\equiv\\langle c^{\\dagger}_{\\beta,j}c_{\\alpha,i}\\rangle\\), we obtain the momentum-space occupation number \\(n(\\mathbf{k})\\) of the ground state wave function as\n\n\\[n(\\mathbf{k})=\\frac{1}{N}\\sum_{\\alpha=A,B}\\sum_{i,j}e^{i\\mathbf{k}\\cdot( \\mathbf{r}_{i}-\\mathbf{r}_{j})}G^{\\alpha,\\alpha}_{i,j}. \\tag{3}\\]\n\nFor non-interacting fermions on the honeycomb lattice, i.e. without density-density interactions or the hard-core constraint, \\(n(\\mathbf{k})\\) should exhibit a clear step (whose magnitude is the quasiparticle residue, \\(Z\\)) as a function of the graphene dispersion \\(\\epsilon_{\\mathrm{G}}(\\mathbf{k})\\). Fig. 3(a) clearly displays a step-like behavior for the occupation at lower fillings \\(\\nu\\lesssim 0.2\\), consistent with the formation of a Fermi liquid. Increasing the fermion density leads to a gradual softening of the quasiparticle residue, so that the Fermi liquid appears to give way to a non-Fermi-liquid phase at \\(\\nu\\gtrsim 0.25\\).\n\nInterestingly, we find that the Fermi liquid regime is interrupted by a charge ordered phase at filling \\(\\nu=1/6\\), which triples the unit cell and spontaneously polarizes into either the \\(A\\) or \\(B\\) sublattice. The expected six-fold ground state degeneracy in the many-body spectrum as well as the clear energetic preference of simulation clusters supporting this form of order strongly points at spontaneous symmetry breaking as the root of this incompressible phase. Additionally, the static structure factor \\(S_{\\mathrm{C}}(\\mathbf{q})\\) obtained from Fourier transformation of the measured density-density correlation function\n\n\\[C^{\\alpha,\\beta}_{i}(a)\\equiv\\langle n^{\\alpha}_{i}n^{\\beta}_{i+a}\\rangle, \\quad\\text{where}\\quad\\alpha=A,B, \\tag{4}\\]\n\nshown in Fig. 3(b), is expected to exhibit pronounced peaks. Indeed, as can be seen in Fig. 3(c), the peak at \\(\\mathbf{q}=\\mathbf{K}\\) extrapolates to finite values in the TDL while other signals vanish, indicative of long-range order.\n\nThese ED results are corroborated by our HF simulations, where the same type of symmetry breaking order prevails at \\(\\nu=1/6\\) and ground state energies per site are relatively close to the ones obtained from ED (cf. Fig. 1 for \\(\\nu\\leq 1/6\\)). HF also finds a charge density wave at \\(\\nu=1/8\\) consisting of a fermion delocalized around a honeycomb for every enlarged \\(2\\times 2\\) unit cell. This suggests a potentially more general instability towards charge order above some critical filling in the dilute fermion regime.\n\n## III Zero-energy state window \\(1/4\\leq\\nu\\lesssim 0.292\\)\n\nStarting from \\(\\nu=1/4\\), i.e. one fermion per two unit cells, the ground state of the SUSY model on the honeycomb lattice from Eq. (2) has _exactly_ zero energy for a finite range of fillings. In fact, as illustrated in Fig. 4, we find robust zero-energy states for (almost2) all rational fillings \\(1/4\\leq\\nu\\lesssim 0.292\\) within our finite-size simulations. This filling window extends beyond the homological predictions of Ref.\n\n "], "nougat": ["that local interactions play a subordinate role and the hard-core constraint is largely inactive. As a result, the qualitative behavior of the system is dominated by the hopping processes, which naturally result in the forma-tion of a Fermi surface similar to the one in graphene at high hole doping. The hard-core nature of the fermions and their density-density interactions then enter as corrections to this pure Fermi-gas behavior. By measuring the lattice Green\u2019s function \\(G^{\\alpha,\\beta}_{i,j}\\equiv\\langle c^{\\dagger}_{\\beta,j}c_{\\alpha,i}\\rangle\\), we obtain the momentum-space occupation number \\(n(\\mathbf{k})\\) of the ground state wave function as\n\n\\[n(\\mathbf{k})=\\frac{1}{N}\\sum_{\\alpha=A,B}\\sum_{i,j}e^{i\\mathbf{k}\\cdot( \\mathbf{r}_{i}-\\mathbf{r}_{j})}G^{\\alpha,\\alpha}_{i,j}. \\tag{3}\\]\n\nFor non-interacting fermions on the honeycomb lattice, i.e. without density-density interactions or the hard-core constraint, \\(n(\\mathbf{k})\\) should exhibit a clear step (whose magnitude is the quasiparticle rssude, \\(Z\\)) as a function of the graphene dispersion \\(\\epsilon_{\\mathrm{G}}(\\mathbf{k})\\). Fig. 3(a) clearly displays a step-like behavior for the occupation at lower fillings \\(\\nu\\lesssim 0.2\\), consistent with the formation of a Fermi liquid. Increasing the fermion density leads to a gradual softening of the quasiparticle residue, so that the Fermi liquid appears to give way to a non-Fermi-liquid phase at \\(\\nu\\gtrsim 0.25\\).\n\nInterestingly, we find that the Fermi liquid regime is interrupted by a charge ordered phase at filling \\(\\nu=1/6\\), which triples the unit cell and spontaneously polarizes into either the \\(A\\) or \\(B\\) sublattice. The expected six-fold ground state degeneracy in the many-body spectrum as well as the clear energetic preference of simulation clusters supporting this form of order strongly points at spontaneous symmetry breaking as the root of this incompressible phase. Additionally, the static structure factor \\(S_{\\mathrm{C}}(\\mathbf{q})\\) obtained from Fourier transformation of the measured density-density correlation function\n\n\\[C^{\\alpha,\\beta}_{i}(a)\\equiv\\langle n^{\\alpha}_{i}n^{\\beta}_{i+a}\\rangle, \\quad\\text{where}\\quad\\alpha=A,B, \\tag{4}\\]\n\nshown in Fig. 3(b), is expected to exhibit pronounced peaks. Indeed, as can be seen in Fig. 3(c), the peak at \\(\\mathbf{q}=\\mathbf{K}\\) extrapolates to finite values in the TDL while other signals vanish, indicative of long-range order.\n\nThese ED results are corroborated by our HF simulations, where the same type of symmetry breaking order prevails at \\(\\nu=1/6\\) and ground state energies per site are relatively close to the ones obtained from ED (cf. Fig. 1 for \\(\\nu\\leq 1/6\\)). HF also finds a charge density wave at \\(\\nu=1/8\\) consisting of a fermion delocalized around a honeycomb for every enlarged \\(2\\times 2\\) unit cell. This suggests a potentially more general instability towards charge order above some critical filling in the dilute fermion regime.\n\n## III Zero-energy state window \\(1/4\\leq\\nu\\lesssim 0.292\\)\n\nStarting from \\(\\nu=1/4\\), i.e. one fermion per two unit cells, the ground state of the SUSY model on the honeycomb lattice from Eq. (2) has _exactly_ zero energy for a finite range of fillings. In fact, as illustrated in Fig. 4, we find robust zero-energy states for (almost2) all rational fillings \\(1/4\\leq\\nu\\lesssim 0.292\\) within our finite-size simulations. This filling window extends beyond the homological "]}, {"edit": ["University, Thailand. We acknowledge the supporting computing infrastructure provided by NSTDA, CU, CUAASC, NSRF via PMUB [B05F650021, B37G660013] (Thailand). URL:www.e-science.in.th. The Computational Materials Physics (CMP) Project, SLRI, Thailand, is acknowledged for providing computational resource.\n\n## References\n\n* Needs and Pickard [2016]R. J. Needs and C. J. Pickard, Perspective: Role of structure prediction in materials discovery and design, APL Materials **4**, 053210 (2016).\n* Kohn and Sham [1965]W. Kohn and L. J. Sham, Phys. Rev. **140**, A1133 (1965).\n* Oganov and Glass [2006]A. R. Oganov and C. W. Glass, Crystal structure prediction using ab initio evolutionary techniques: Principles and applications, The Journal of Chemical Physics **124**, 244704 (2006).\n* Wang _et al._ [2010]Y. Wang, J. Lv, L. Zhu, and Y. Ma, Crystal structure prediction via particle-swarm optimization, Phys. Rev. B **82**, 094116 (2010).\n* Pickard and Needs [2011]C. J. Pickard and R. J. Needs, Ab initio random structure searching, Journal of Physics: Condensed Matter **23**, 053201 (2011).\n* Oganov _et al._ [2019]A. R. Oganov, C. J. Pickard, Q. Zhu, and R. J. Needs, Structure prediction drives materials discovery, Nature Reviews Materials **4**, 331 (2019).\n* Schon _et al._ [2010]J. C. Schon, K. Doll, and M. Jansen, Predicting solid compounds via global exploration of the energy landscape of solids on the ab initio level without recourse to experimental information, physica status solidi (b) **247**, 23 (2010).\n* Xie _et al._ [2022]T. Xie, X. Fu, O.-E. Ganea, R. Barzilay, and T. S. Jaakkola, Crystal diffusion variational autoencoder for periodic material generation, in _International Conference on Learning Representations_ (2022).\n* Shi _et al._ [2021]C. Shi, S. Luo, M. Xu, and J. Tang, Learning gradient fields for molecular conformation generation, in _Proceedings of the 38th International Conference on Machine Learning_, Proceedings of Machine Learning Research, Vol. 139, edited by M. Meila and T. Zhang (PMLR, 2021) pp. 9558-9568.\n* Xu _et al._ [2022]M. Xu, L. Yu, Y. Song, C. Shi, S. Ermon, and J. Tang, Geodiff: A geometric diffusion model for molecular conformation generation, in _International Conference on Learning Representations_ (2022).\n* Guan _et al._ [2023]J. Guan, W. W. Qian, X. Peng, Y. Su, J. Peng, and J. Ma, 3d equivariant diffusion for target-aware molecule generation and affinity prediction, in _The Eleventh International Conference on Learning Representations_ (2023).\n* Kang and Cho [2019]S. Kang and K. Cho, Conditional molecular design with deep generative models, Journal of Chemical Information and Modeling **59**, 43 (2019), pMID: 30016587.\n* Lim _et al._ [2018]J. Lim, S. Ryu, J. W. Kim, and W. Y. Kim, Molecular generative model based on conditional variational autoencoder for de novo molecular design, Journal of Cheminformatics **10**, 31 "], "nougat": ["University, Thailand. We acknowledge the supporting computing infrastructure provided by NSTDA, CU, CUAASC, NSRF via PMUB [B05F650021, B37G660013] (Thailand). URL:www.e-science.in.th. The Computational Materials Physics (CMP) Project, SLRI, Thailand, is acknowledged for providing computational resource.\n\n## References\n\n* Needs and Pickard [2016]R. J. Needs and C. J. Pickard, Perspective: Role of structure prediction in materials discovery and design, APL Materials **4**, 053210 (2016). [2] W. Kohn and L. J. Sham, Phys. Rev. **140**, A1133 (1965). [3] A. R. Oganov and C. W. Glass, Crystal structure prediction using ab initio evolutionary techniques: Principles and applications, The Journal of Chemical Physics **124**, 244704 (2006). [4] Y. Wang, J. Lv, L. Zhu, and Y. Ma, Crystal structure prediction via particle-swarm optimization, Phys. Rev. B **82**, 094116 (2010). [5] C. J. Pickard and R. J. Needs, Ab initio random structure searching, Journal of Physics: Condensed Matter **23**, 053201 (2011). [6] A. R. Oganov, C. J. Pickard, Q. Zhu, and R. J. Needs, Structure prediction drives materials discovery, Nature Reviews Materials **4**, 331 (2019). [7] J. C. Sch \u0308on, K. Doll, and M. Jansen, Predicting solid compounds via global exploration of the energy landscape of solids on the ab initio level without recourse to experimental information, physica status solidi (b) **247**, 23 (2010). [8] T. Xie, X. Fu, O.-E. Ganea, R. Barzilay, and T. S. Jaakkola, Crystal diffusion variational autoencoder for periodic material generation, in _International Conference on Learning Representations_ (2022). [9] C. Shi, S. Luo, M. Xu, and J. Tang, Learning gradient fields for molecular conformation generation, in _Proceedings of the 38th International Conference on Machine Learning_, Proceedings of Machine Learning Research, Vol. 139, edited by M. Meila and T. Zhang (PMLR, 2021) pp. 9558\u20139568. [10] M. Xu, L. Yu, Y. Song, C. Shi, S. Ermon, and J. Tang, Geodiff: A geometric diffusion model for molecular conformation generation, in _International Conference on Learning Representations_ (2022). [11] J. Guan, W. W. Qian, X. Peng, Y. Su, J. Peng, and J. Ma, 3d equivariant diffusion for target-aware molecule generation and affinity prediction, in _The Eleventh Inter-national Conference on Learning Representations_ (2023). [12] S. Kang and K. Cho, Conditional molecular design with deep generative models, Journal of Chemical Information and Modeling **59**, 43 (2019), pMID: 30016587. [13] J. Lim, S. Ryu, J. W. Kim, and W. Y. Kim, Molecular generative model based on conditional variational autoencoder for de novo molecular design, Journal of Cheminformatics **10**, 31 (2018). [14] Y. Song, L. Shen, L. Xing, and S. Ermon, Solving inverse problems in medical imaging with score-based generative models, in _International Conference on Learning Repre- sentations_ (2022). [15] A. Cui, K. Jiang, M. Jiang, L. Shang, L. Zhu, Z. Hu, G. Xu, and J. Chu, Decoding phases of matter by machine-learning raman spectroscopy, Phys. Rev. Appl. **12**, "]}, {"edit": ["which is precisely (1.3) for \\(p=1\\) due to (3.12); the non-normalized case follows in a standard way.\n\nSince the Sobolev inequality (1.3) for \\(p=1\\) is equivalent to the isoperimetric inequality\n\n\\[n(\\omega_{n}\\mathsf{AVR}_{g})^{\\frac{1}{n}}\\mathrm{Vol}_{g}(\\Omega)^{\\frac{n-1}{ n}}\\leq\\mathcal{P}_{g}(\\partial\\Omega) \\tag{3.15}\\]\n\nfor every bounded open domain \\(\\Omega\\subset M\\) with smooth boundary (\\(\\mathcal{P}_{g}\\) being the perimeter), and (3.15) is sharp, see Balogh and Kristaly [2] and Brendle [4], it turns out that (1.3) is also sharp. \\(\\square\\)\n\n## 4. Proof of the sharp \\(L^{p}\\)-logarithmic Sobolev inequality (Theorem 1.2)\n\n### The case \\(p>1\\)\n\nLet \\(p>1\\) and fix \\(f\\in C_{0}^{\\infty}(M)\\) arbitrarily; we may assume that \\(f\\) is nonnegative and\n\n\\[\\int_{M}f^{p}\\mathrm{d}v_{g}=1.\\]\n\nAs before, let \\(\\Omega=\\{x\\in M:f(x)>0\\}\\); since \\(f\\in C_{0}^{\\infty}(M)\\), then \\(\\overline{\\Omega}\\) is compact.\n\nLet \\(x_{0}\\in\\Omega.\\) For every \\(\\lambda>0\\) and \\(k\\in\\mathbb{N}\\), we introduce the truncated Gaussian bubble \\(G_{\\lambda,k}:M\\to\\mathbb{R}\\) given by\n\n\\[G_{\\lambda,k}(x)=P_{k}(d_{g}(x_{0},x))e^{-\\lambda d_{g}^{p^{\\prime}}(x_{0},x)},\\]\n\nwhere \\(P_{k}\\) is defined in (3.2). We observe that the support of \\(G_{\\lambda,k}\\) is the ball \\(\\overline{B_{x_{0}}(k+1)}\\). Let\n\n\\[\\mathcal{J}_{\\lambda,k}=\\int_{M}G_{\\lambda,k}(y)\\mathrm{d}v_{g}(y);\\]\n\nclearly, \\(0<\\mathcal{J}_{\\lambda,k}<\\infty\\) for every \\(\\lambda>0\\) and \\(k\\in\\mathbb{N}\\).\n\nLet \\(\\mathrm{d}\\mu(x)=f^{p}(x)\\mathrm{d}v_{g}(x)\\) and \\(\\mathrm{d}\\nu(y)=\\frac{G_{\\lambda,k}(y)}{\\mathcal{J}_{\\lambda,k}}\\mathrm{d}v_ {g}(y)\\) be two probability measures on \\((M,g)\\) with compact supports; by the theory of OMT one can find a unique map \\(T:\\overline{\\Omega}\\to\\overline{B_{x_{0}}(k+1)}\\subset M\\) pushing \\(\\mu\\) forward to \\(\\nu\\) having the form \\(T(x)=\\exp_{x}(-\\nabla_{g}u(x))\\) for a.e. \\(x\\in\\overline{\\Omega}\\), for some \\(c=d_{g}^{2}/2\\)-concave function \\(u:\\overline{\\Omega "], "nougat": ["which is precisely (1.3) for \\(p=1\\) due to (3.12); the non-normalized case follows in a standard way.\n\nSince the Sobolev inequality (1.3) for \\(p=1\\) is equivalent to the isoperimetric inequality\n\n\\[n(\\omega_{n}\\mathsf{AVR}_{g})^{\\frac{1}{n}}\\mathrm{Vol}_{g}(\\Omega)^{\\frac{n-1}{ n}}\\leq\\mathcal{P}_{g}(\\partial\\Omega) \\tag{3.15}\\]\n\nfor every bounded open domain \\(\\Omega\\subset M\\) with smooth boundary (\\(\\mathcal{P}_{g}\\) being the perimeter), and (3.15) is sharp, see Balogh and Kristaly [2] and Brendle [4], it turns out that (1.3) is also sharp. \\(\\square\\)\n\n## 4. Proof of the sharp \\(L^{p}\\)-logarithmic Sobolev inequality (Theorem 1.2)\n\n### The case \\(p>1\\)\n\nLet \\(p>1\\) and fix \\(f\\in C_{0}^{\\infty}(M)\\) arbitrarily; we may assume that \\(f\\) is nonnegative and\n\n\\[\\int_{M}f^{p}\\mathrm{d}v_{g}=1.\\]\n\nAs before, let \\(\\Omega=\\{x\\in M:f(x)>0\\}\\); since \\(f\\in C_{0}^{\\infty}(M)\\), then \\(\\overline{\\Omega}\\) is compact.\n\nLet \\(x_{0}\\in\\Omega.\\) For every \\(\\lambda>0\\) and \\(k\\in\\mathbb{N}\\), we introduce the truncated Gaussian bubble \\(G_{\\lambda,k}:M\\to\\mathbb{R}\\) given by\n\n\\[G_{\\lambda,k}(x)=P_{k}(d_{g}(x_{0},x))e^{-\\lambda d_{g}^{p^{\\prime}}(x_{0},x)},\\]\n\nwhere \\(P_{k}\\) is defined in (3.2). We observe that the support of \\(G_{\\lambda,k}\\) is the ball \\(\\overline{B_{x_{0}}(k+1)}\\). Let\n\n\\[\\mathcal{J}_{\\lambda,k}=\\int_{M}G_{\\lambda,k}(y)\\mathrm{d}v_{g}(y);\\]\n\nclearly, \\(0<\\mathcal{J}_{\\lambda,k}<\\infty\\) for every \\(\\lambda>0\\) and \\(k\\in\\mathbb{N}\\).\n\nLet \\(\\mathrm{d}\\mu(x)=f^{p}(x)\\mathrm{d}v_{g}(x)\\) and \\(\\mathrm{d}\\nu(y)=\\frac{G_{\\lambda,k}(y)}{\\mathcal{J}_{\\lambda,k}}\\mathrm{d}v_ {g}(y)\\) be two probability measures on \\((M,g)\\) with compact supports; by the theory of OMT one can find a unique map \\(T:\\overline{\\Omega}\\to\\overline{B_{x_{0}}(k+1)}\\subset M\\) pushing \\(\\mu\\) forward to \\(\\nu\\) having the form \\(T(x)=\\exp_{x}(-\\nabla_{g}u(x))\\) for a.e. \\(x\\in\\overline{\\Omega}\\), for some \\(c=d_{g}^{2}/2\\)-concave function \\(u:\\overline{\\Omega "]}, {"edit": ["we confirm the previous detection of He i and, by analysing this He(\\(2^{3}\\)S) signal, we study the hydrodynamical escape of this planet and derive the temperature and mass-loss rate of its upper atmosphere.\n\n## 2 Observations and data analysis\n\n### CARMENES observations and analysis\n\nA single transit of the planet candidate 23.01 was observed with the CARMENES1(Quirrenbach et al., 2014, 2020) spectrograph located at the Calar Alto Observatory, Almeria, Spain, on the night of August 2022. CARMENES has two spectral channels: the optical channel (VIS), which covers the wavelength range of 0.52-0.96 \\(\\mu\\)m with a resolving power of \\(\\mathcal{R}\\) = 94 600, and the near-infrared channel (NIR), which covers 0.96-1.71 \\(\\mu\\)m with a resolving power of \\(\\mathcal{R}\\) = 80 400. We observed the target with both channels simultaneously and collected a total of 44 spectra of 5 min exposure time, with 28 of them between the first (\\(T_{1}\\)) and fourth (\\(T_{4}\\)) transit contacts. We obtained a median S/N of 72 around the H\\(\\alpha\\) line and of 86 around the He i triplet.\n\nFootnote 1: Calar Alto high-Resolution search for M dwarfs with Exoearths with Near-infrared and optical Echelle Spectrographs.\n\nFiber A was used to observe the target star, while fiber B was placed on sky, separated by 88 arcsec in the east-west direction. The observations were reduced using the CARMENES pipeline caracal(Caballero et al., 2016) and both fibers were extracted with the flat-optimized extraction algorithm (Zecchmeister et al., 2014). We also processed the spectra with **serval2**(Zechmeister et al., 2018), which is the standard CARMENES pipeline to derive the radial velocities (RVs) and several activity indicators: the chromatic radial velocity index (CRX) and the differential line width (dLW), as well as the H\\(\\alpha\\), Na i D1 and D2, and Ca ii IRT line indices.\n\nFootnote 2: [https://github.com/mzechemister/serval](https://github.com/mzechemister/serval)\n\nWe corrected the VIS and NIR spectra from telluric absorptions with molecfit(Smette et al., 2015; Kausch et al., 2015). We analyzed the spectroscopic observations via the well-established transmission spectroscopy technique (e.g. Wyttenbach et al., 2015; Casasayas-Barris et al., 2017). We computed the H\\(\\alpha\\) transmission spectra (TS) following the standard procedure. However, because there are OH emission lines from the Earth's atmosphere close to the He i triplet lines, we applied an extra step before computing the He i TS. First, we planned the observations to avoid a complete overlap or superposition of the OH telluric lines and the He i planetary trace. We corrected the fiber A spectra from OH telluric emission using fiber B information, which is used to generate an OH emission model for correcting the science spectra. This methodology is based in previous He i studies with CARMENES(Normman et al., 2018; Salz et al., 2018; Alonso-Floriano et al., 2019; Palle et al., 2020; Casasayas-Barris et al., 2021; Czesla et al., 2022). In particular, we followed the procedure previously applied in Drell-Miquel et al. (2022). Figure 1 compares our prediction for the telluric contamination of the He i triplet lines with the real observations.\n\n### X-ray observations and planetary irradiation\n\nWe used XMM-Newton archival observations of 23 (PI M. Zhang) to calculate the X-ray luminosity of the star. The star was observed on 7 July 2021. We reduced the data following standard procedures and used the "], "nougat": ["we confirm the previous detection of He i and, by analysing this He(\\(2^{3}\\)S) signal, we study the hydrodynamical escape of this planet and derive the temperature and mass-loss rate of its upper atmosphere.\n\n## 2 Observations and data analysis\n\n### CARMENES observations and analysis\n\nA single transit of the planet candidate 23.01 was observed with of 0.52\u20130.96 the CARMENES1(Quirrenbach et al., 2014, 2020) spectrograph located at the Calar Alto Observatory, Almer\u00eda, Spain, on the i the optical channel (VIS), which covers the wavelength range Calar Alto high-Resolution search for M dwarfs with Exoearths with Near-infrared and optical Echelle Spectrographs.\n\nFootnote 1: Calar Alto high-Resolution search for M dwarfs with Exoearths with Near-infrared and optical Echelle Spectrographs.\n\n "]}, {"edit": ["\n\n## 9. Zeros\n\nIt is well known that the zeros of orthogonal polynomials with respect to a positive definite linear functional are real, simple, and located in the interior of the convex hull of the support [15, 20]. With this in mind, let \\(\\{x_{n,k}(z)\\}_{k=1}^{n}\\) be the the zeros of \\(P_{n}(x)\\) in an increasing order, i. e.,\n\n\\[P_{n}(x_{n,k}(z),z)=0 \\tag{9.1}\\]\n\nwith\n\n\\[x_{n,1}(z)<x_{n,2}(z)<\\cdots<x_{n,n}(z).\\]\n\nNext we will focus our attention on an electrostatic interpretation of the zeros of the polynomial \\(P_{n}(x;z)\\) in terms of the energy associated with a logarithmic potential and next we will study the dynamics of them in terms of the parameter \\(z\\).\n\n### Electrostatic interpretation\n\nEvaluating the operator \\(D_{n+1}\\) defined in (5.6) at \\(x=x_{n+1,k}\\), we get\n\n\\[\\left.\\frac{\\partial_{x}^{2}P_{n+1}}{\\partial_{x}P_{n+1}}\\right|_ {x=x_{n+1,k}}=-\\frac{(2x_{n+1,k}-z)}{\\phi(x_{n+1,k})}+\\frac{a_{n+1}}{C_{n}(x_{ n+1,k})}\\\\ +\\frac{(x_{n,k}-b_{n})C_{n-1}(x_{n,k};z)}{a_{n}\\phi(x_{n,k};z)}+ \\frac{\\delta_{n}(x_{n,k};z)+\\delta_{n-1}(x_{n,k};z)}{\\phi(x_{n,k};z)},\\]\n\nwhere \\(C_{n}(x;z)\\) and \\(\\delta_{n}(x;z)\\) were defined in Proposition 5.4. Taking into account (5.4) the above expression reads\n\n\\[\\left.\\frac{\\partial_{x}^{2}P_{n+1}}{\\partial_{x}P_{n+1}}\\right|_{x=x_{n+1,k} }=-\\frac{1}{x_{n+1,k}-z}-\\frac{1}{x_{n+1,k}}+\\frac{1}{x_{n+1,k}-\\beta_{n}}+1- \\frac{\\alpha}{x_{n+1,k}},\\]\n\nwhere \\(\\beta_{n}=-b_{n+1}+(2n+\\alpha+z+3)\\). Observe that the weight function associated with \\(\\boldsymbol{\\ell}\\) is \\(w(x)=x^{\\alpha}e^{-x}=e^{-x+\\alpha\\ln x}.\\) If we define \\(v(x)=x-\\alpha\\ln x,\\) (the external potential [22, Section 3.5]), then\n\n\\[\\left.\\frac{\\partial_{x}^{2}P_{n+1}}{\\partial_{x}P_{n+1}}\\right|_{x=x_{n+1,k} }=-\\frac{1}{x_{n+1,k}-z}-\\frac{1}{x_{n+1,k}}+\\frac{1}{x_{n+1,k}-\\beta_{n}}+v^{ \\prime}(x_"], "nougat": ["\n\n## 9. Zeros\n\nIt is well known that the zeros of orthogonal polynomials with respect to a positive definite linear functional are real, simple, and located in the interior of the convex hull of the support [ 15 , 20 ]. With this in mind, let \\(\\{x_{n,k}(z)\\}_{k=1}^{n}\\) be the the zeros of \\(P_{n}(x)\\) in an increasing order, i. e.,\n\n\\[P_{n}(x_{n,k}(z),z)=0 \\tag{9.1}\\]\n\nwith\n\n\\[x_{n,1}(z)<x_{n,2}(z)<\\cdots<x_{n,n}(z).\\]\n\nNext we will focus our attention on an electrostatic interpretation of the zeros of the polynomial \\(P_{n}(x;z)\\) in terms of the energy associated with a logarithmic potential and next we will study the dynamics of them in terms of the parameter \\(z\\).\n\n### Electrostatic interpretation\n\nEvaluating the operator \\(D_{n+1}\\) defined in ( 5.6 ) at \\(x=x_{n+1,k}\\), we get\n\n\\[\\left.\\frac{\\partial_{x}^{2}P_{n+1}}{\\partial_{x}P_{n+1}}\\right|_ {x=x_{n+1,k}}=-\\frac{(2x_{n+1,k}-z)}{\\phi(x_{n+1,k})}+\\frac{a_{n+1}}{C_{n}(x_{ n+1,k})}\\\\ +\\frac{(x_{n,k}-b_{n})C_{n-1}(x_{n,k};z)}{a_{n}\\phi(x_{n,k};z)}+ \\frac{\\delta_{n}(x_{n,k};z)+\\delta_{n-1}(x_{n,k};z)}{\\phi(x_{n,k};z)},\\]\n\nwhere \\(C_{n}(x;z)\\) and \\(\\delta_{n}(x;z)\\) were defined in Proposition 5.4 . Taking into account ( 5.4 ) the above expression reads\n\n\\[\\left.\\frac{\\partial_{x}^{2}P_{n+1}}{\\partial_{x}P_{n+1}}\\right|_ {x=x_{n+1,k}}=-\\frac{1}{x_{n+1,k}-z}-\\frac{1}{x_{n+1,k}}+\\frac{1}{x_{n+1,k}- \\beta_{n}}+1-\\frac{\\alpha}{x_{n+1,k}},\\]\n\nwhere \\(\\beta_{n}=-b_{n+1}+(2n+\\alpha+z+3)\\). Observe that the weight function associated with \\(\\boldsymbol{\\ell}\\) is \\(w(x)=x^{\\alpha}e^{-x}=e^{-x+\\alpha\\ln x}.\\) If we define \\(v(x)=x-\\alpha\\ln x,\\) (the external potential [ 22 , Section 3.5]), then\n\n\\[\\left.\\frac{\\partial_{x}^{2}P_{n+1}}{\\partial_{x}P_{n+1}}\\right|_ {x=x_{n+1,k}}=-\\frac{1}{x_{n+1,k}-z}-\\frac{1}{x_{n+1,k}}+\\frac{1}{x_{n+1,k}- \\beta_{n}}\n\n"]}, {"edit": ["for all \\(t\\in[0,\\delta)\\) almost surely. Furthermore, for \\(p\\geqslant 4\\), using Ito's formula for \\(\\|u\\|_{1,2}^{p}\\), we have\n\n\\[\\|w^{\\varepsilon}(t)\\|_{1,2}^{p}-\\|w_{0}\\|_{1,2}^{p}=p\\sum_{k}\\lambda _{k}\\int_{0}^{t}\\|w^{\\varepsilon}(s)\\|_{1,2}^{p-2}(\\partial_{x}(\\Psi_{k}w^{ \\varepsilon}(s)),w^{\\varepsilon}(s))_{1,2}\\,d\\beta^{k}(s) \\tag{4.25}\\] \\[+ p\\sum_{k}\\gamma_{k}\\int_{0}^{t}\\|w^{\\varepsilon}(s)\\|_{1,2}^{p-2 }(\\Psi_{k}f(w^{\\varepsilon}(s)),w^{\\varepsilon}(s))_{1,2}d\\beta_{1}^{k}(s)\\] \\[+ \\frac{p}{2}\\int_{0}^{t}\\|w^{\\varepsilon}(s)\\|_{1,2}^{p-2}(2\\langle \\langle A^{\\varepsilon}w^{\\varepsilon}(s),w^{\\varepsilon}(s)\\rangle\\rangle+\\| B(w^{\\varepsilon}(s))\\|_{L_{2}}^{2})\\,ds\\] \\[+ \\frac{p(p-2)}{2}\\sum_{k}\\lambda_{k}^{2}\\int_{0}^{t}\\|w^{ \\varepsilon}(s)\\|_{1,2}^{p-4}(\\partial_{x}(\\Psi_{k}w^{\\varepsilon}(s)),w^{ \\varepsilon}(s))_{1,2}^{2}\\,ds\\] \\[+ \\frac{p(p-2)}{2}\\sum_{k}\\gamma_{k}^{2}\\int_{0}^{t}\\|w^{\\varepsilon }(s)\\|_{1,2}^{p-4}(\\Psi_{k}f(w^{\\varepsilon}(s)),w^{\\varepsilon}(s))_{1,2}^{2} \\,ds\\]\n\nwhere we used the definition of the norm \\(\\|B(u)\\|_{L_{2}}\\). Next, using Burkholder-Davis-Gundy inequality:\n\n\\[\\mathbf{E}\\sup_{t^{\\prime}\\in[0,t)}\\left|\\int_{0}^{t^{\\prime}}\\|w ^{\\varepsilon}(s)\\|_{1,2}^{p-2}(\\partial_{x}(\\Psi_{k}w^{\\varepsilon}(s)),w^{ \\varepsilon})_{1,2}\\,d\\beta^{k}(s)\\right|\\] (4.26) \\[\\leqslant 3\\mathbf{E}\\left(\\int_{0}^{t}\\|w^{\\varepsilon}(s)\\|_{1,2}^{2p-4 }\\partial_{x}(\\Psi_{k}w^{\\varepsilon}(s),w^{\\varepsilon}(s))_{1,2}^{2}\\,ds \\right)^{\\frac{1}{2}}\\] \\[\\leqslant C\\mathbf{E}\\left(\\int_{0}^{t}\\|w^{\\varepsilon}(s)\\|_{1,2}^{2 "], "nougat": ["for all \\(t\\in[0,\\delta)\\) almost surely. Furthermore, for \\(p\\geqslant 4\\), using Ito\u2019s formula for \\(\\|u\\|_{1,2}^{p}\\), we have\n\n\\[\\|w^{\\varepsilon}(t)\\|_{1,2}^{p}-\\|w_{0}\\|_{1,2}^{p}=p\\sum_{k} \\lambda_{k}\\int_{0}^{t}\\|w^{\\varepsilon}(s)\\|_{1,2}^{p-2}(\\partial_{x}(\\Psi_{k} w^{\\varepsilon}(s)),w^{\\varepsilon}(s))_{1,2}\\,d\\beta^{k}(s) \\tag{4.25}\\] \\[+ p\\sum_{k}\\gamma_{k}\\int_{0}^{t}\\|w^{\\varepsilon}(s)\\|_{1,2}^{p-2 }(\\Psi_{k}f(w^{\\varepsilon}(s)),w^{\\varepsilon}(s))_{1,2}d\\beta_{1}^{k}(s)\\] \\[+ \\frac{p}{2}\\int_{0}^{t}\\|w^{\\varepsilon}(s)\\|_{1,2}^{p-2}(2\\langle \\langle A^{\\varepsilon}w^{\\varepsilon}(s),w^{\\varepsilon}(s)\\rangle\\rangle+\\| B(w^{\\varepsilon}(s))\\|_{L_{2}}^{2})\\,ds\\] \\[+ \\frac{p(p-2)}{2}\\sum_{k}\\lambda_{k}^{2}\\int_{0}^{t}\\|w^{ \\varepsilon}(s)\\|_{1,2}^{p-4}(\\partial_{x}(\\Psi_{k}w^{\\varepsilon}(s)),w^{ \\varepsilon}(s))_{1,2}^{2}\\,ds\\] \\[+ \\frac{p(p-2)}{2}\\sum_{k}\\gamma_{k}^{2}\\int_{0}^{t}\\|w^{ \\varepsilon}(s)\\|_{1,2}^{p-4}(\\Psi_{k}f(w^{\\varepsilon}(s)),w^{\\varepsilon}(s) )_{1,2}^{2}\\,ds\\]\n\nwhere we used the definition of the norm \\(\\|B(u)\\|_{L_{2}}\\). Next, using Burkholder-Davis-Gundy inequality:\n\n\\[{\\bf E}\\sup_{t^{\\prime}\\in[0,t)}\\left|\\int_{0}^{t^{\\prime}}\\|w^{ \\varepsilon}(s)\\|_{1,2}^{p-2}(\\partial_{x}(\\Psi_{k}w^{\\varepsilon}(s)),w^{ \\varepsilon})_{1,2}\\,d\\beta^{k}(s)\\right|\\] (4.26) \\[\\leqslant 3{\\bf E}\\left(\\int_{0}^{t}\\|w^{\\varepsilon}(s)\\|_{1,2}^{2p-4} \\partial_{x}(\\Psi_{k}w^{\\varepsilon}(s),w^{\\varepsilon}(s))_{1,2}^{2}\\,ds \\right)^{\\frac{1}{2}}\\] \\[\\leqslant C{\\bf E}\\left(\\int_{0}^{t}\\|w^{\\varepsilon}(s)\\|_{1,2}^{2p "]}, {"edit": ["To simplify the analysis, a narrow frequency band was selected between 24 and 61 Hz, with the fourth and fifth bending modes of the blades dominating the response in this band. Although other modes appear to have a small influence in this band, a 2DOF assumption was imposed. (This assumption results in smoothing of the FRF over the band, and might result in some loss of interpretability, but is acceptable for these preliminary analyses). The real part was modelled as a probabilistic FRF, using the FRF estimate from Eq. (13) as the mean of the likelihood function, as described in Case 1, presented in Section 6 of this paper. The real parts of the averaged FRFs for each blade, at the second accelerometer from the blade root (corresponding to the drive-point location), are shown in Figures 4(a) and 4(b). Figure 4(a) shows the full measured bandwidth, and Figure 4(b) shows the FRF in the bandwidth of interest, between 24 and 61 Hz.\n\nFigures 4(a) and 4(b) show increasing variability with respect to frequency, which is an expected result, given that higher-frequency modes are more sensitive to small physical changes than lower-frequency modes. For modes less than 80 Hz, the maximum frequency difference among the blades was approximately 2.5 Hz; for modes greater than 80 Hz, the maximum frequency difference was approximately 6.3 Hz. Note the grouping visible at several of the peaks, where Blades 1 and 2 appear closely aligned in frequency while Blades 3 and 4 appear closely aligned. These results are quite relevant for PBSHM. All of the helicopter blades are healthy, and represent a normal-condition state of the population. Consider a situation where only FRFs from one of the groupings are available for training a model (or FRFs from the other groups are missing data). The normal condition could be heavily biased towards the training set, and incoming FRFs could be flagged as damaged, even if they are healthy. Further details regarding the data collection and processing for these tests can be found in [27].\n\nFigure 4: Sensor locations on the helicopter blades.\n\nFigure 3: Helicopter blade in a substantiated wall mount.\n\n "], "nougat": ["To simplify the analysis, a narrow frequency band was selected between 24 and 61 Hz, with the fourth and fifth bending modes of the blades dominating the response in this band. Although other modes appear to have a small influence in this band, a 2DOF assumption was imposed. (This assumption results in smoothing of the FRF over the band, and might result in some loss of interpretability, but is acceptable for these preliminary analyses). The real part was modelled as a probabilistic FRF, using the FRF estimate from Eq. (13) as the mean of the likelihood function, as described in Case 1, presented in Section 6 of this paper. The real parts of the averaged FRFs for each blade, at the second accelerometer from the blade root (corresponding to the drive-point location), are shown in Figures 5a and 5b. Figure 5a shows the full measured bandwidth, and Figure 5b shows the FRF in the bandwidth of interest, between 24 and 61 Hz.\n\nFigures 5a and 5b show increasing variability with respect to frequency, which is an expected result, given that higher-frequency modes are more sensitive to small physical changes than lower-frequency modes. For modes less than 80 Hz, the maximum frequency difference among the blades was approximately 2.5 Hz; for modes greater than 80 Hz, the maximum frequency difference was approximately 6.3 Hz. Note the grouping visible at several of the peaks, where Blades 1 and 2 appear closely aligned in frequency while Blades 3 and 4 appear closely aligned. These results are quite relevant for PBSHM. All of the helicopter blades are healthy, and represent a normal-condition state of the population. Consider a situation where only FRFs from one of the groupings are available for training a model (or FRFs from the other groups are missing data). The normal condition could be heavily biased towards the training set, and incoming FRFs could be flagged as damaged, even if they are healthy. Further details regarding the data collection and processing for these tests can be found in [27].\n\nFigure 4: Sensor locations on the helicopter blades.\n\nFigure 3: Helicopter blade in a substantiated wall mount.\n\n "]}, {"edit": ["\n\n# Adaptive multi-stage integration schemes for Hamiltonian Monte Carlo\n\nLorenzo Nagar\n\nlnagar@bcamath.org\n\nMario Fernandez-Pendas\n\nJesus Maria Sanz-Sernac\n\nElena Akhmatskaya\n\nBCAM - Basque Center for Applied Mathematics, Alameda de Mazarredo 14, 48009 Bilbao, Spain DIIPC, Donostia International Physics Center, Manuel Lardizabal Ibilbidea 4, 20018 Donostia, Spain Departamento de Matematicas, Universidad Carlos III de Madrid, Avenida Universidad 30, 28911 Leganes, Spain Ikerbasque - Basque Foundation for Science, Euskadi Plaza 5, 48009 Bilbao, Spain\n\n###### Abstract\n\nHamiltonian Monte Carlo (HMC) is a powerful tool for Bayesian statistical inference due to its potential to rapidly explore high dimensional state space, avoiding the random walk behavior typical of many Markov Chain Monte Carlo samplers. The proper choice of the integrator of the Hamiltonian dynamics is key to the efficiency of HMC. It is becoming increasingly clear that multi-stage splitting integrators are a good alternative to the Verlet method, traditionally used in HMC. Here we propose a principled way of finding optimal, problem-specific integration schemes (in terms of the best conservation of energy for harmonic forces/Gaussian targets) within the families of 2- and 3-stage splitting integrators. The method, which we call Adaptive Integration Approach for statistics, or s-AIA, uses a multivariate Gaussian model and simulation data obtained at the HMC burn-in stage to identify a system-specific dimensional stability interval and assigns the most appropriate 2-/3-stage integrator for any user-chosen simulation step size within that interval. s-AIA has been implemented in the in-house software package HaiCS without introducing computational overheads in the simulations. The efficiency of the s-AIA integrators and their impact on the HMC accuracy, sampling performance and convergence are discussed in compari"], "nougat": ["\n\n# Adaptive multi-stage integration schemes for Hamiltonian Monte Carlo\n\nLorenzo Nagar\n\nlnagar@bcamath.org\n\nMario Fernandez-Pend\u00e1s\n\nJesus Maria Sanz-Serna\n\nElena Akhmatskaya\n\nBCAM - Basque Center for Applied Mathematics, Alameda de Mazarredo 14, 48009 Bilbao, Spain DIIPC, Donostia International Physics Center, Manuel Lardizabal Ibilbidea 4, 20018 Donostia, Spain Departamento de Matematicas, Universidad Carlos III de Madrid, Avenida Universidad 30, 28911 Leganes, Spain Ikerbasque - Basque Foundation for Science, Euskadi Plaza 5, 48009 Bilbao, Spain\n\n###### Abstract\n\nHamiltonian Monte Carlo (HMC) is a powerful tool for Bayesian statistical inference due to its potential to rapidly explore high dimensional state space, avoiding the random walk behavior typical of many Markov Chain Monte Carlo samplers. The proper choice of the integrator of the Hamiltonian dynamics is key to the efficiency of HMC. It is becoming increasingly clear that multi-stage splitting integrators are a good alternative to the Verlet method, traditionally used in HMC. Here we propose a principled way of finding optimal, problem-specific integration schemes (in terms of the best conservation of energy for harmonic forces/Gaussian targets) within the families of 2- and 3-stage splitting integrators. The method, which we call Adaptive Integration Approach for statistics, or s-AIA, uses a multivariate Gaussian model and simulation data obtained at the HMC burn-in stage to identify a system-specific dimensional stability interval and assigns the most appropriate 2-/3-stage integrator for any user-chosen simulation step size within that interval. s-AIA has been implemented in the in-house software package HaiCS without introducing computational overheads in the simulations. The efficiency of the s-AIA integrators and their impact on the HMC accuracy, sampling performance and convergence are discussed in compari"]}, {"edit": ["[5] and the references there for some of the algorithmic developments. A particular feature of numerical approximations of PDE solutions based on DNNs as approximation architectures that was observed in practice was the apparent insensitivity of the DNN approximation quality to the so-called \"curse of dimensionality\" (CoD for short). This is particularly relevant for approximating maps\n\n\\[\\mathcal{G}:\\mathcal{X}\\rightarrow\\mathcal{Y} \\tag{1}\\]\n\nbetween (in general, infinite-dimensional) separable Hilbert spaces1\\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\). Operators \\(\\mathcal{G}\\) as in (1) emerge for example as parameter-to-solution mappings for parametric PDEs within the field of Uncertainty Quantification (see, e.g., [48] and the references there), or in so-called digital twins of complex, physical systems governed by partial differential equations (PDEs) (see [32] and the references there). Owing to the infinite dimension of \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\) in (1), efficient numerical approximations of maps \\(\\mathcal{G}\\) are to overcome the CoD.\n\nFootnote 1: More generally, separably-valued maps \\(\\mathcal{G}\\) into an otherwise nonseparable target space \\(\\mathcal{Y}\\) may be considered. In [35, Section 9, App. B] additional conditions on separable Banach spaces \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\) necessary to extend the present arguments to this more general setting are discussed.\n\nSeveral (intrinsically different) mechanisms for overcoming the CoD in DNN emulations have been identified and mathematically justified recently. This includes the seminal work of A. _Barron_[3], _Monte-Carlo path simulation_ type arguments (e.g. [20, 29] and the references there), and the emulation of sparse (generalized) _polynomial chaos expansions_ (e.g. [2, 17]) by DNNs (e.g. [56, 51, 57]).\n\nSpecifically, in [56, 51, 57], a parametric representation of inputs \\(x\\in\\mathcal{X}\\) of \\(\\mathcal{G}\\) was used to prove DNN emulation rates for approximating \\(\\mathcal{G}\\). The construction used DNNs whose depth scales polylogarithmic in the parameter dimension, and polynomially in the DNN expression accuracy (i.e., emulation fidelity). Key in the proofs of these results is the _holomorphic_ dependence of \\(\\mathcal{G}(x)\\) on the input \\(x\\). The related DNN emulation results were obtained with sparsely connected, deep feedforward NNs with ReLU or smooth (e.g. sigmoidal or \\(\\tanh(\\cdot)\\)) activation. DNN emulation rate results that are free from the CoD for _low regularity maps_\\(\\mathcal{G}\\) between function spaces were obtained e.g. using the so-called Feynman-Kac representation of solutions of Kolmogorov PDEs in (jump-)diffusion models. These results used ReLU DNNs of moderate depth [20, 29], but the error bounds hold in a mean-square sense or only with high probability.\n\nWhile quantified, parametric holomorphy of solution families of parametric PDEs has been verified in many settings (particularly in elliptic and parabolic PDEs, e.g. [27, 66, 31, 12, 23]), there are broad classes of applications where relevant maps are Holder or Lipschitz, but not holomorphic. One purpose of the present paper is to obtain mean-square DNN expression rate bounds for _Operator Network_ (ONet) emulations with architecture (2) below, of Lipschitz (and, more generally, Holder smooth) maps \\(\\mathcal{G}\\) between separable Hilbert spaces.\n\n### Previous work for operator networks\n\nA rather recent line of research uses so-called _Operator Networks_ to emulate the possibly nonlinear input-output map \\(\\mathcal{G}\\), such as for example the coefficient-to-solution map in linear, elliptic divergence form PDEs of second order. A variety of DNN architectures has been put forward recently with the aim of efficient operator emulation, with distinct architectures tailored to the emulation of "], "nougat": ["[5] and the references there for some of the algorithmic developments. A particular feature of numerical approximations of PDE solutions based on DNNs as approximation architectures that was observed in practice was the apparent insensitivity of the DNN approximation quality to the so-called \u201ccurse of dimensionality\u201d (CoD for short). This is particularly relevant for approximating maps\n\n\\[\\mathcal{G}:\\mathcal{X}\\rightarrow\\mathcal{Y} \\tag{1}\\]\n\nbetween (in general, infinite-dimensional) separable Hilbert spaces1\\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\). Operators \\(\\mathcal{G}\\) as in (1) emerge for example as parameter-to-solution mappings for parametric PDEs within the field of Uncertainty Quantification (see, e.g., [48] and the references there), or in so-called digital twins of complex, physical systems governed by partial differential equations (PDEs) (see [32] and the references there). Owing to the infinite dimension of \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\) in (1), efficient numerical approximations of maps \\(\\mathcal{G}\\) are to overcome the CoD.\n\nFootnote 1: More generally, separably-valued maps \\(\\mathcal{G}\\) into an otherwise nonseparable target space \\(\\mathcal{Y}\\) may be considered. In [35, Section 9, App. B] additional conditions on separable Banach spaces \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\) necessary to extend the present arguments to this more general setting are discussed.\n\nSeveral (intrinsically different) mechanisms for overcoming the CoD in DNN emulations have been identified and mathematically justified recently. This includes the seminal work of A. _Barron_[3], _Monte-Carlo path simulation_ type arguments (e.g. [20, 29] and the references there), and the emulation of sparse (generalized) _polynomial chaos expansions_ (e.g. [2, 17]) by DNNs (e.g. [56, 51, 57]).\n\nSpecifically, in [56, 51, 57], a parametric representation of inputs \\(x\\in\\mathcal{X}\\) of \\(\\mathcal{G}\\) was used to prove DNN emulation rates for approximating \\(\\mathcal{G}\\). The construction used DNNs whose depth scales polylogarithmic in the parameter dimension, and polynomially in the DNN expression accuracy (i.e., emulation fidelity). Key in the proofs of these results is the _holomorphic_ dependence of \\(\\mathcal{G}(x)\\) on the input \\(x\\). The related DNN emulation results were obtained with sparsely connected, deep feedforward NNs with ReLU or smooth (e.g. sigmoidal or \\(\\tanh(\\cdot)\\)) activation. DNN emulation rate results that are free from the CoD for _low regularity maps_\\(\\mathcal{G}\\) between function spaces were obtained e.g. using the so-called Feynman-Kac representation of solutions of Kolmogorov PDEs in (jump-)diffusion models. These results used ReLU DNNs of moderate depth [20, 29], but the error bounds hold in a mean-square sense or only with high probability.\n\nWhile quantified, parametric holomorphy of solution families of parametric PDEs has been verified in many settings (particularly in elliptic and parabolic PDEs, e.g. [27, 66, 31, 12, 23]), there are broad classes of applications where relevant maps are Holder or Lipschitz, but not holomorphic. One purpose of the present paper is to obtain mean-square DNN expression rate bounds for _Operator Network_ (ONet) emulations with architecture (2) below, of Lipschitz (and, more generally, Holder smooth) maps \\(\\mathcal{G}\\) between separable Hilbert spaces.\n\n### Previous work for operator networks\n\nA rather recent line of research uses so-called _Operator Networks_ to emulate the possibly nonlinear input-output map \\(\\mathcal{G}\\), such as for example the coefficient-to-solution mappings for parametric PDEs within the field of Uncertainty Quantification (see, e.g., [48] and the references there), or in so-called digital twins of complex "]}, {"edit": ["Let \\(y\\in W_{4}\\). Then \\(y=[b_{k},\\,d_{0},\\,d_{1},\\,d_{2}]\\) for \\(k=0\\) or \\(1\\). The corresponding equation in (23) reads\n\n\\[t_{0}^{b_{k},d_{0},d_{1},d_{2}}a_{[b_{k},\\,d_{0}]}a_{[d_{1},\\,d_{2 }]}+t_{1}^{b_{k},d_{0},d_{1},d_{2}}a_{[b_{k},\\,d_{1}]}a_{[d_{0},\\,d_{2}]}+t_{2}^ {b_{k},d_{0},d_{1},d_{2}}a_{[b_{k},\\,d_{2}]}a_{[d_{0},\\,d_{1}]}=0\\] \\[t_{0}=\\begin{cases}-1&\\text{if}\\quad d_{0}<d_{1}<b_{k}<d_{2}\\\\ +1&\\text{otherwise}\\end{cases}\\] \\[t_{1}=\\begin{cases}-1&\\text{if}\\quad b_{k}<d_{0}<d_{1}<d_{2}\\\\ +1&\\text{otherwise}\\end{cases}\\text{or}\\quad d_{0}<d_{1}<d_{2}<b_{k}\\] \\[t_{2}=\\begin{cases}-1&\\text{if}\\quad d_{0}<b_{k}<d_{1}<d_{2}\\\\ +1&\\text{otherwise}\\end{cases}\\]\n\nEach of \\(a_{[d_{1},\\,d_{2}]},a_{[d_{0},\\,d_{2}]}\\,\\&\\,a_{[d_{0},\\,d_{1}]}\\) are fixed by Equation (24). Substituting in, (multiplying out \\(a_{[b_{0},\\,b_{1}]}\\)), the equation now reads\n\n\\[t_{0}^{b_{k},d_{0},d_{1},d_{2}}a_{[b_{k},\\,d_{0}]}\\big{(}s_{0}^{ b_{0},b_{1},d_{1},d_{2}}a_{[b_{0},\\,d_{1}]}a_{[b_{1},\\,d_{2}]}+s_{1}^{b_{0},b_{1},d_ {1},d_{2}}a_{[b_{0},\\,d_{2}]}a_{[b_{1},\\,d_{1}]}\\big{)}\\\\ \\qquad\\qquad\\qquad+t_{2}^{b_{k},d_{0},d_{1},d_{2}}a_{[b_{k},\\,d_{ 1}]}\\big{(}s_{0}^{b_{0},b_{1},d_{0},d_{2}}a_{[b_{0},\\,d_{0}]}a_{[b_{1},\\,d_{2}] }+s_{1}^{b_{0},b_{1},d_{0},d_{2}}a_{[b_{0}\n\n "], "nougat": ["Let \\(y\\in W_{4}\\). Then \\(y=[b_{k},\\,d_{0},\\,d_{1},\\,d_{2}]\\) for \\(k=0\\) or \\(1\\). The corresponding equation in (23) reads\n\n\\[t_{0}^{b_{k},d_{0},d_{1},d_{2}}a_{[b_{k},\\,d_{0}]}a_{[d_{1},\\,d_{2 }]}+t_{1}^{b_{k},d_{0},d_{1},d_{2}}a_{[b_{k},\\,d_{1}]}a_{[d_{0},\\,d_{2}]}+t_{2}^ {b_{k},d_{0},d_{1},d_{2}}a_{[b_{k},\\,d_{2}]}a_{[d_{0},\\,d_{1}]}=0\\] \\[t_{0}=\\begin{cases}-1&\\text{if}\\quad d_{0}<d_{1}<b_{k}<d_{2}\\\\ +1&\\text{otherwise}\\end{cases}\\] \\[t_{1}=\\begin{cases}-1&\\text{if}\\quad b_{k}<d_{0}<d_{1}<d_{2}\\\\ +1&\\text{otherwise}\\end{cases}\\text{or}\\quad d_{0}<d_{1}<d_{2}<b_{k}\\] \\[t_{2}=\\begin{cases}-1&\\text{if}\\quad d_{0}<b_{k}<d_{1}<d_{2}\\\\ +1&\\text{otherwise}\\end{cases}\\]\n\nEach of \\(a_{[d_{1},\\,d_{2}]},a_{[d_{0},\\,d_{2}]}\\,\\&\\,a_{[d_{0},\\,d_{1}]}\\) are fixed by Equation (24). Substituting in, (multiplying out \\(a_{[b_{0},\\,b_{1}]}\\)), the equation now reads\n\n\\[t_{0}^{b_{k},d_{0},d_{1},d_{2}}a_{[b_{k},\\,d_{0}]}\\big{(}s_{0}^{ b_{0},b_{1},d_{1},d_{2}}a_{[b_{0},\\,d_{1}]}a_{[b_{1},\\,d_{2}]}+s_{1}^{b_{0},b_{1},d _{1},d_{2}}a_{[b_{0},\\,d_{2}]}a_{[b_{1},\\,d_{1}]}\\big{)}\\\\ \\qquad\\qquad\\qquad+t_{2}^{b_{k},d_{0},d_{1},d_{2}}a_{[b_{k},\\,d_{ 1}]}\\big{(}s_{0}^{b_{0},b_{1},d_{0},d_{2}}a_{[b_{0},\\,d_{0}]}a_{[b_{1},\\,d_{2}] }+s_{1}^{b_{0},b_{1},d_{0},d_{2}}a_{[b_{0}\n\n "]}, {"edit": ["one obtains from (3.75)\n\n\\[y_{0}^{(2)}=1-kz_{0}(v^{(2)})=1-\\frac{1}{2}k^{1/2}+\\frac{1}{4}k\\ln k^{-1}+O(k) \\tag{3.94}\\]\n\nand due to (3.79)\n\n\\[y_{1}^{(2)}=-kz_{1}(v^{(2)})=-A(\\zeta_{0})\\frac{k^{1/2}}{2J_{0}}\\cdot\\Big{(}1+O( k^{1/2})\\Big{)}+O(k^{-5/2}). \\tag{3.95}\\]\n\nThen, using the Taylor formula with respect to \\(\\mu\\), we shift initial conditions (3.92) to the point \\(y=y_{0}^{(2)}\\) as follows:\n\n\\[\\begin{split} v_{0}(y_{0}^{(2)})&=v_{0}^{(2)},\\quad v _{1}(y_{0}^{(2)})=-\\frac{dv_{0}}{dy}(y_{0}^{(2)})\\cdot y_{1}^{(2)},\\\\ \\zeta_{1}(y_{0}^{(2)})&=\\zeta_{1}^{(2)},\\quad\\zeta_ {2}(y_{0}^{(2)})=\\zeta_{2}^{(2)}-\\frac{d\\zeta_{1}}{dy}(y_{0}^{(2)})\\cdot y_{1} ^{(2)},\\\\ J_{1}(y_{0}^{(2)})&=J_{1}^{(2)},\\quad J_{2}(y_{0}^{ (2)})=J_{2}^{(2)}-\\frac{dJ_{1}}{dy}(y_{0}^{(2)})\\cdot y_{1}^{(2)}.\\end{split} \\tag{3.96}\\]\n\nSubstituting (3.93) into (3.91) and expanding with respect to \\(\\mu\\), one obtains equations for the \\(i\\)-th approximations \\(\\zeta_{i}\\) and \\(J_{i}\\):\n\n\\[\\frac{d\\zeta_{i}}{dy} = \\mathcal{R}_{i}^{(\\zeta)}(y), \\tag{3.97}\\] \\[\\frac{dJ_{i}}{dy} = \\mathcal{R}_{i}^{(J)}(y).\\]\n\nThe solution of this system is\n\n\\[\\zeta_{i}(y) = \\zeta_{i}(y_{0}^{(2)})+\\int_{y_{0}^{(2)}}^{y}\\mathcal{R}_{i}^{( \\zeta)}(s)\\,\\mathrm{d}s, \\tag{3.98}\\] \\[J_{i}(v) = J_{i}(y_{0}^{(2)})+\\int_{y_{0}^{(2)}}^{y}\\mathcal{R}_{i}^{(J)}(s )\\,\\mathrm{d}s.\\]\n\nFor \\(i=1\\) one has\n\n\\[\\mathcal{R}_{1}^{(\\zeta)}(y) = k^{1/2}\\frac{f_{3}}{y^{2}-1-k(v_{0}(y)-1) "], "nougat": ["one obtains from (3.75)\n\n\\[y_{0}^{(2)}=1-kz_{0}(v^{(2)})=1-\\frac{1}{2}k^{1/2}+\\frac{1}{4}k\\ln k^{-1}+O(k) \\tag{3.79}\\]\n\nand due to (3.79)\n\n\\[y_{1}^{(2)}=-kz_{1}(v^{(2)})=-A(\\zeta_{0})\\frac{k^{1/2}}{2J_{0}}\\cdot\\Big{(}1+O( k^{1/2})\\Big{)}+O(k^{-5/2}). \\tag{3.80}\\]\n\nThen, using the Taylor formula with respect to \\(\\mu\\), we shift initial conditions (3.92) to the point \\(y=y_{0}^{(2)}\\) as follows:\n\n\\[\\begin{split} v_{0}(y_{0}^{(2)})&=v_{0}^{(2)},\\quad v _{1}(y_{0}^{(2)})=-\\frac{dv_{0}}{dy}(y_{0}^{(2)})\\cdot y_{1}^{(2)},\\\\ \\zeta_{1}(y_{0}^{(2)})&=\\zeta_{1}^{(2)},\\quad\\zeta_ {2}(y_{0}^{(2)})=\\zeta_{2}^{(2)}-\\frac{d\\zeta_{1}}{dy}(y_{0}^{(2)})\\cdot y_{1 }^{(2)},\\\\ J_{1}(y_{0}^{(2)})&=J_{1}^{(2)},\\quad J_{2}(y_{0}^{ (2)})=J_{2}^{(2)}-\\frac{dJ_{1}}{dy}(y_{0}^{(2)})\\cdot y_{1}^{(2)}.\\end{split}\\] (3.93) into (3.91) and expanding with respect to \\(\\mu\\), one obtains equations for the \\(i\\)-th approximations \\(\\zeta_{i}\\) and \\(J_{i}\\):\n\n\\[\\frac{d\\zeta_{i}}{dy} = \\mathcal{R}_{i}^{(\\zeta)}(y), \\tag{3.94}\\] \\[\\frac{dJ_{i}}{dy} = \\mathcal{R}_{i}^{(J)}(y).\\]\n\nThe solution of this system is\n\n\\[\\zeta_{i}(y) = \\zeta_{i}(y_{0}^{(2)})+\\int_{y_{0}^{(2)}}^{y}\\mathcal{R}_{i}^{( \\zeta)}(s)\\,\\mathrm{d}s, \\tag{3.95}\\] \\[J_{i}(v) = J_{i}(y_{0}^{(2)})+\\int_{y_{0}^{(2)}}^{y}\\mathcal{R}_{i}^{(J)}(s )\\,\\mathrm{d}s.\\]\n\nFor \\(i=1\\) one has\n\n\\[\\mathcal{R}_{1}^{(\\zeta)}(y) = k^{1/2}\\frac{f_{3}}{y^{2}-1-k(v_{0}(y)-1)},\\] (3.96 "]}, {"edit": ["Notice that at the optimal dual solution \\(\\lambda^{\\rm opt}\\) and \\(\\{\\mu_{k}^{\\rm opt}\\}\\), it must follow that \\(\\lambda^{\\rm opt}>0\\) and \\(\\mathbf{A}(\\lambda^{\\rm opt},\\{\\mu_{k}^{\\rm opt}\\})\\) is of full rank (i.e., \\({\\rm rank}(\\mathbf{A}(\\lambda^{\\rm opt},\\{\\mu_{k}^{\\rm opt}\\}))=N_{t}\\)), since otherwise, the maximum transmit power constraint in (22b) cannot be satisfied. Then, Proposition 1 follows directly from Lemma 3. This completes the proof.\n\n## Appendix B Proof of Lemma 3\n\nFirst, we have \\({\\rm tr}(\\mathbf{A}(\\lambda,\\{\\mu_{k}\\})\\mathbf{S}_{x})={\\rm tr }(\\mathbf{\\Lambda}\\mathbf{U}^{H}\\mathbf{S}_{x}\\mbox {\\boldmath$U$})\\). Let \\(\\tilde{\\mathbf{S}}_{x}=\\mathbf{U}^{H}\\mathbf{S}_{x} \\mathbf{U}\\). It is easy to figure out that \\({\\rm tr}(\\mathbf{S}_{x}^{-1})={\\rm tr}(\\tilde{\\mathbf{S}_{x} }^{-1})\\). Recall that \\(\\mathbf{S}_{x}\\) is positive semi-definite. We denote \\((\\tau_{1},\\ldots,\\tau_{N_{t}})\\) as the diagonal entries of \\(\\tilde{\\mathbf{S}}_{x}\\) to be determined. Note that \\({\\rm tr}(\\mathbf{A}(\\lambda,\\{\\mu_{k}\\})\\mathbf{S}_{x})=\\sum _{i=1}^{N}\\alpha_{i}\\tau_{i}\\). Here, we introduce the following lemma to find the minimum of \\({\\rm tr}(\\tilde{\\mathbf{S}_{x}}^{-1})\\) w.r.t. \\((\\tau_{1},\\ldots,\\tau_{N_{t}})\\), for which the proof can be found in [43, Appendix A] and thus is omitted.\n\n**Lemma 5**.: [43] For a positive semi-definite matrix \\(\\mathbf{B}_{0}\\in\\mathbb{C}^{M\\times M}\\), with \\((m,n)\\)-th entry \\(a(m,n),\\) it holds that\n\n\\[{\\rm tr}(\\mathbf{B}_{0}^{-1})\\geq\\sum_{i=1}^{M}\\frac{1}{a(i,i)}, \\tag{60}\\]\n\nwhere the equality holds if and only if \\(\\mathbf{B}_{0}\\) is diagonal.\n\nHence, \\(\\tilde{\\mathbf{S}}_{x}\\) must be diagonal and we obtain\n\n\\[{\\rm tr}(\\mathbf{A}(\\lambda,\\{\\mu_{k}\\})\\mathbf{S}_{x})+{ \\rm tr}(\\mathbf{S}_{x}^{-1}) = {\\rm tr}(\\mbox{\\boldmath$ "], "nougat": ["Notice that at the optimal dual solution \\(\\lambda^{\\rm opt}\\) and \\(\\{\\mu_{k}^{\\rm opt}\\}\\), it must follow that \\(\\lambda^{\\rm opt}>0\\) and \\(\\mathbf{A}(\\lambda^{\\rm opt},\\{\\mu_{k}^{\\rm opt}\\})\\) is of full rank (i.e., \\({\\rm rank}(\\mathbf{A}(\\lambda^{\\rm opt},\\{\\mu_{k}^{\\rm opt}\\}))=N_{t}\\)), since otherwise, the maximum transmit power constraint in (22b) cannot be satisfied. Then, Proposition 1 follows directly from Lemma 3. This completes the proof. A PPENDIX B P ROOF OF L EMMA 3\n\nFirst, we have \\({\\rm tr}(\\mathbf{A}(\\lambda,\\{\\mu_{k}\\})\\mathbf{S}_{x})={\\rm tr }(\\mathbf{\\Lambda}\\mathbf{U}^{H}\\mathbf{S}_{x} \\mathbf{U})\\). Let \\(\\tilde{\\mathbf{S}}_{x}=\\mathbf{U}^{H}\\mathbf{S}_{x} \\mathbf{U}\\). It is easy to figure out that \\({\\rm tr}(\\mathbf{S}_{x}^{-1})={\\rm tr}(\\tilde{\\mathbf{S}_{x} }^{-1})\\). Recall that \\(\\mathbf{S}_{x}\\) is positive semi-definite. We denote \\((\\tau_{1},\\ldots,\\tau_{N_{t}})\\) as the diagonal entries of \\(\\tilde{\\mathbf{S}}_{x}\\) to be determined. Note that \\({\\rm tr}(\\mathbf{A}(\\lambda,\\{\\mu_{k}\\})\\mathbf{S}_{x})=\\sum _{i=1}^{N}\\alpha_{i}\\tau_{i}\\). Here, we introduce the following lemma to find the minimum of \\({\\rm tr}(\\tilde{\\mathbf{S}_{x}}^{-1})\\) w.r.t. \\((\\tau_{1},\\ldots,\\tau_{N_{t}})\\), for which the proof can be found in [43, Appendix A] and thus is omitted.\n\n**Lemma 5**.: [43] For a positive semi-definite matrix \\(\\mathbf{B}_{0}\\in\\mathbb{C}^{M\\times M}\\), with \\((m,n)\\)-th entry \\(a(m,n),\\) it holds that\n\n\\[{\\rm tr}(\\mathbf{B}_{0}^{-1})\\geq\\sum_{i=1}^{M}\\frac{1}{a(i,i)}, \\tag{60}\\]\n\nwhere the equality holds if and only if \\(\\mathbf{B}_{0}\\) is diagonal.\n\nHence, \\(\\tilde{\\mathbf{S}}_{x}\\) must be diagonal and we obtain\n\n\\[{\\rm tr}(\\mathbf{A}(\\lambda,\\{\\mu_{k}\\})\\mathbf{S}_{x})+{ \\rm tr}(\\mathbf{S}_{x}^{-1}) = {\\rm tr}(\\mbox{ "]}, {"edit": ["Proof.: The proof is an adaptation of the proof of [13, Lemma 4.6]. Since \\(\\omega_{H}^{g}(\\chi(u))=\\tau_{H}^{g}(u)\\), \\(\\lambda_{H}^{g}\\circ\\chi\\) extends to an isometric embedding\n\n\\[L^{2}(Rep(G),\\tau_{H}^{g})\\to l_{g}^{2}(\\hat{H})\\cong L^{2}(\\mathcal{O}(G), \\omega_{H}^{g}).\\]\n\nThe image \\(\\lambda_{H}^{g}\\circ\\chi(\\mathcal{O}(G))\\) is a \\(*\\)-algebra that maps \\(L^{2}(Rep(G),\\tau_{H}^{g})\\) into itself and hence maps \\(L^{2}(Rep(G),\\tau_{H}^{g})^{\\perp}\\) into itself. Therefore \\(\\lambda_{H}^{g}(\\chi(u))\\) has the form\n\n\\[\\begin{bmatrix}\\lambda_{H}^{g}(\\chi(u))|_{L^{2}(Rep(G),\\tau_{H}^{g})}&0\\\\ 0&\\lambda_{H}^{g}(\\chi(u))|_{L^{2}(Rep(G),\\tau_{H}^{g})^{\\perp}}\\end{bmatrix}.\\]\n\nHence\n\n\\[||\\lambda_{H}^{g}(\\chi(u))|| =\\max\\{||\\lambda_{H}^{g}(\\chi(u))|_{L^{2}(Rep(G),\\tau_{H}^{g})}|| ,||\\lambda_{H}^{g}(\\chi(u))|_{L^{2}(Rep(G),\\tau_{H}^{g})^{\\perp}}||\\}\\] \\[\\geq||\\lambda_{H}^{g}(\\chi(u))|_{L^{2}(Rep(G),\\tau_{H}^{g})}||\\] \\[=||\\pi_{\\tau_{H}^{g}}(u)||.\\]\n\nThis proves that the map\n\n\\[\\kappa:\\lambda_{H}^{g}\\circ\\chi(\\mathcal{O}(G))\\rightarrow\\pi_{\\tau_{H}^{g}}( \\mathbb{C}[Rep(G)]),\\lambda_{H}^{g}(\\chi(u)))\\mapsto\\pi_{\\tau_{H}^{g}}(u)\\]\n\nis bounded and therefore extends to a contractive \\(*\\)-homomorphism\n\n\\[\\kappa:\\overline{\\lambda_{H}^{g}\\circ\\chi(\\mathcal{O}(G))}\\to C_{\\tau_{H}^{g}} ^{*}(Rep(G)).\\]\n\nTo finish the proof, we claim \\(\\kappa\\) is injective. This easily follows from the observation\n\n\\[\\omega_{H}^{g}(\\chi(u)^{*}\\chi(v))=\\omega_{H}^{g}(\\chi(\\overline{u}\\cdot v))= \\tau_{H}^{g}(\\overline{u}\\cdot v).\\]\n\nFor \\(\\alpha\\in Irr(H)\\) let \\(P_{\\alpha}\\in l^{\\infty}(\\hat{H})\\) denote the orthogonal projection onto \\(H_{\\alpha}\\) which is nothing more than the identity operator in \\(M_{n_{\\alpha}}\\subset l^{\\infty}(\\hat{H}\n\n "], "nougat": ["Proof.: The proof is an adaptation of the proof of [ Kye08 , Lemma 4.6]. Since \\(\\omega_{H}^{g}(\\chi(u))=\\tau_{H}^{g}(u)\\), \\(\\lambda_{H}^{g}\\circ\\chi\\) extends to an isometric embedding\n\n\\[L^{2}(Rep(G),\\tau_{H}^{g})\\to l_{g}^{2}(\\hat{H})\\cong L^{2}(\\mathcal{O}(G), \\omega_{H}^{g}).\\]\n\nThe image \\(\\lambda_{H}^{g}\\circ\\chi(\\mathcal{O}(G))\\) is a \\(*\\)-algebra that maps \\(L^{2}(Rep(G),\\tau_{H}^{g})\\) into itself and hence maps \\(L^{2}(Rep(G),\\tau_{H}^{g})^{\\perp}\\) into itself. Therefore \\(\\lambda_{H}^{g}(\\chi(u))\\) has the form\n\n\\[\\begin{bmatrix}\\lambda_{H}^{g}(\\chi(u))|_{L^{2}(Rep(G),\\tau_{H}^{g})}&0\\\\ 0&\\lambda_{H}^{g}(\\chi(u))|_{L^{2}(Rep(G),\\tau_{H}^{g})^{\\perp}}\\end{bmatrix}.\\]\n\nHence\n\n\\[||\\lambda_{H}^{g}(\\chi(u))|| =\\max\\{||\\lambda_{H}^{g}(\\chi(u))|_{L^{2}(Rep(G),\\tau_{H}^{g})}|| ,||\\lambda_{H}^{g}(\\chi(u))|_{L^{2}(Rep(G),\\tau_{H}^{g})^{\\perp}}||\\}\\] \\[\\geq||\\lambda_{H}^{g}(\\chi(u))|_{L^{2}(Rep(G),\\tau_{H}^{g})}||\\] \\[=||\\pi_{\\tau_{H}^{g}}(u)||.\\]\n\nThis proves that the map\n\n\\[\\kappa:\\lambda_{H}^{g}\\circ\\chi(\\mathcal{O}(G))\\rightarrow\\pi_{\\tau_{H}^{g}}( \\mathbb{C}[Rep(G)]),\\lambda_{H}^{g}(\\chi(u)))\\mapsto\\pi_{\\tau_{H}^{g}}(u)\\]\n\nis bounded and therefore extends to a contractive \\(*\\)-homomorphism\n\n\\[\\kappa:\\overline{\\lambda_{H}^{g}\\circ\\chi(\\mathcal{O}(G))}\\to C_{\\tau_{H}^{g}}^{ *}(Rep(G)).\\]\n\nTo finish the proof, we claim \\(\\kappa\\) is injective. This easily follows from the observation\n\n\\[\\omega_{H}^{g}(\\chi(u)^{*}\\chi(v))=\\omega_{H}^{g}(\\chi(\\overline{u}\\cdot v))= \\tau_{H}^{g}(\\overline{u}\\cdot v).\\]\n\nFor \\(\\alpha\\in Irr(H)\\) let \\(P_{\\alpha}\\in l^{\\infty}(\\hat{H})\\) denote the orthogonal projection onto \\(H_{\\alpha}\\) which is nothing more than the identity operator in \\(M_{n_{\\alpha}}\\subset l^{\\infty}(\\hat "]}, {"edit": ["\n\n# Quantitative analysis of optimal Sobolev-Lorentz embeddings with \\(\\alpha\\)-homogeneous weights\n\nPetr Gurka, Jan Lang, Zdenek Mihula\n\nMathematics Research Unit, University of Vienna, Oskar-Morgenstern-Platz 1, A-1090 Vienna, Austria petr.gurka@univie.ac.at Jan Lang, Zdenek Mihula, Department of Mathematics, University of Vienna, Oskar-Morgenstern-Platz 1, A-1090 Vienna, Austria j.lang@univie.ac.at Zdenek Mihula, Department of Mathematics, University of Vienna, Oskar-Morgenstern-Platz 1, A-1090 Vienna, Austria zdenek.mihula@univie.ac.at\n\nMay 30, 2024\n\n###### Abstract.\n\nThis paper quantitatively investigates the structure of non-compactness of the optimal weighted Sobolev-Lorentz embedding with homogeneous weights in an open convex cone. We prove the optimal embedding in question and obtain the exact values of all injective strict \\(s\\)-numbers (in particular, the Bernstein numbers) of the embedding. Opposite to the earlier results in this direction, the non-compactness in this case does not occur uniformly over all sub-domains of the underlying domain. Despite that, we find an infinitely dimensional subspace restricted onto which the embedding is isomorphic, proving that the embedding is not strictly singular.\n\nKey words and phrases:Sobolev spaces, Sobolev-Lorentz embeddings, homogeneous weights, compactness, Bernstein numbers, measure of non-compactness 2020 Mathematics Subject Classification: 46E35, 47B06, 46B50 This research was supported by the grant GA23-04720S of the Czech Science Foundation.\n\n"], "nougat": ["\n\n# Quantitative analysis of optimal Sobolev-Lorentz embeddings with \\(\\alpha\\)-homogeneous weights\n\nPetr Gurka, Jan Lang, Zdenek Mihula\n\nMathematics Research Unit, University of Vienna, Oskar-Morgenstern-Platz 1, A-1090 Vienna, Austria petr.gurka@univie.ac.at Jan Lang, Zdenek Mihula, Department of Mathematics, University of Vienna, Oskar-Morgenstern-Platz 1, A-1090 Vienna, Austria j.lang@univie.ac.at Zdenek Mihula, Department of Mathematics, University of Vienna, Oskar-Morgenstern-Platz 1, A-1090 Vienna, Austria zdenek.mihula@univie.ac.at\n\nMay 30, 2024\n\n###### Abstract.\n\nThis paper quantitatively investigates the structure of non-compactness of the optimal weighted Sobolev-Lorentz embedding with homogeneous weights in an open convex cone. We prove the optimal embedding in question and obtain the exact values of all injective strict \\(s\\)-numbers (in particular, the Bernstein numbers) of the embedding. Opposite to the earlier results in this direction, the non-compactness in this case does not occur uniformly over all sub-domains of the underlying domain. Despite that, we find an infinitely dimensional subspace restricted onto which the embedding is isomorphic, proving that the embedding is not strictly singular.\n\nKey words and phrases:Sobolev spaces, Sobolev-Lorentz embeddings, homogeneous weights, compactness, Bernstein numbers, measure of non-compactness 2020 Mathematics Subject Classification: 46E35, 47B06, 46B50 This research was supported by the grant GA23-04720S of the Czech Science Foundation.\n\n"]}, {"edit": ["which we call the limit equations of (1.2). Here unknown functions are the tangential velocity field \\(v\\) and the pressure \\(q\\). Also, \\(f\\) is a given external force. We write \\(P\\), \\(\\nabla_{\\Gamma}\\), \\(\\mathrm{div}_{\\Gamma}\\), \\(D_{\\Gamma}(v)\\), and \\(\\overline{\\nabla}_{v}v\\) for the orthogonal projection onto the tangent plane of \\(\\Gamma\\), the tangential gradient, the surface divergence, the surface strain rate tensor, and the covariant derivative of \\(v\\) along itself, respectively. Also, \\(\\gamma^{0}\\) and \\(\\gamma^{1}\\) are nonnegative constants which stands for the friction coefficients. For details of notations, see Section 2. Note that, when \\(g\\equiv 1\\) on \\(\\Gamma\\) and \\(\\gamma^{0}=\\gamma^{1}=0\\), the limit equations (1.4) reduce to the surface Navier-Stokes equations with Boussinesq-Scriven surface stress tensor (see [4, 52, 2])\n\n\\[\\begin{cases}-2\\nu P\\mathrm{div}_{\\Gamma}[D_{\\Gamma}(v)]+\\overline{\\nabla}_{v }v+\\nabla_{\\Gamma}q=f&\\text{on}\\quad\\Gamma,\\\\ \\mathrm{div}_{\\Gamma}v=0&\\text{on}\\quad\\Gamma.\\end{cases} \\tag{1.5}\\]\n\nMoreover, the equations (1.5) are equivalent to the Navier-Stokes equations on an abstract Riemannian manifold (see [14, 56, 10])\n\n\\[\\begin{cases}-\\nu\\{\\Delta_{B}v+\\mathrm{Ric}(v)\\}+\\overline{\\nabla}_{v}v+ \\nabla_{\\Gamma}q=f&\\text{on}\\quad\\Gamma,\\\\ \\mathrm{div}_{\\Gamma}v=0&\\text{on}\\quad\\Gamma,\\end{cases} \\tag{1.6}\\]\n\nwhere \\(\\Delta_{B}\\) is the Bochner Laplacian on \\(\\Gamma\\) and \\(\\mathrm{Ric}\\) is the Ricci curvature of \\(\\Gamma\\) (see e.g. [34, Lemma C.11] for the equivalence of the above equations).\n\nIn the nonstationary setting, we rigorously derived the limit equations (1.4) from the bulk equations (1.2) by the thin-film limit in our previous work [34]. There we proved under suitable assumptions that, for an \\(L^{2}\\)-strong solution \\(u^{\\varepsilon}\\) to the nonstationary Navier-Stokes equations in \\(\\Omega_{\\varepsilon}\\), its average\n\n\\[Mu^{\\varepsilon}(y)=\\frac{1}{\\varepsilon g(y)}\\int_{\\varepsilon g_{0}(y)}^{ \\varepsilon g_{1}(y)}u^{\\varepsilon}(y+rn(y))\\,dr,\\quad y\\in\\Gamma\\]\n\nconverges weakly to a tangential vector field \\(v\\) on \\(\\Gamma\\) in an appropriate function space as \\(\\varepsilon\\to 0\\), and derived the nonstationary limit equations on \\(\\Gamma\\) by characterizing \\(v\\) as a unique \\(L^{2}\\)-weak solution to the limit equations. We also obtained some estimates for the difference of \\(u^{\\varepsilon}\\) and \\(v\\) which show that \\(v\\) approximates \\(u^{\\varepsilon}\\) in the \\(L^{2}\\) sense when \\(\\varepsilon\\) is small.\n\nAs in the nonstationary case [34], we can derive (1.4) from (1.2) by means of convergence of a solution and characterization of the limit, but the procedure is the "], "nougat": ["which we call the limit equations of ( 1.2 ). Here unknown functions are the tangential velocity field \\(v\\) and the pressure \\(q\\). Also, \\(f\\) is a given external force. We write \\(P\\), \\(\\nabla_{\\Gamma}\\), \\(\\mathrm{div}_{\\Gamma}\\), \\(D_{\\Gamma}(v)\\), and \\(\\overline{\\nabla}_{v}v\\) for the orthogonal projection onto the tangent plane of \\(\\Gamma\\), the tangential gradient, the surface divergence, the surface strain rate tensor, and the covariant derivative of \\(v\\) along itself, respectively. Also, \\(\\gamma^{0}\\) and \\(\\gamma^{1}\\) are nonnegative constants which stands for the friction coefficients. For details of notations, see Section 2 . Note that, when \\(g\\equiv 1\\) on \\(\\Gamma\\) and \\(\\gamma^{0}=\\gamma^{1}=0\\), the limit equations ( 1.4 ) reduce to the surface Navier\u2013Stokes equations with Boussinesq\u2013Scriven surface stress tensor (see [ 4 , 52 , 2 ])\n\n(1.5 ) are equivalent to the Navier\u2013Stokes equations on an abstract Riemannian manifold (see [ 14 , 56 , 10 ])\n\n(1.6 ) \\[\\begin{cases}-\\nu\\{\\Delta_{B}v+\\mathrm{Ric}(v)\\}+\\overline{\\nabla}_{v}v+\\nabla _{\\Gamma}q=f&\\text{on}\\quad\\Gamma,\\\\ \\mathrm{div}_{\\Gamma}v=0&\\text{on}\\quad\\Gamma,\\end{cases}\\]\n\nwhere \\(\\Delta_{B}\\) is the Bochner Laplacian on \\(\\Gamma\\) and \\(\\mathrm{Ric}\\) is the Ricci curvature of \\(\\Gamma\\) (see e.g.  [ 34 , Lemma C.11] for the equivalence of the above equations).\n\nIn the nonstationary setting, we rigorously derived the limit equations ( 1.4 ) from the bulk equations ( 1.2 ) by the thin-film limit in our previous work [ 34 ]. There we proved under suitable assumptions that, for an \\(L^{2}\\)-strong solution \\(u^{\\varepsilon}\\) to the nonstationary Navier\u2013Stokes equations in \\(\\Omega_{\\varepsilon}\\), its average\n\n\\[Mu^{\\varepsilon}(y)=\\frac{1}{\\varepsilon g(y)}\\int_{\\varepsilon g_{0}(y)}^{ \\varepsilon g_{1}(y)}u^{\\varepsilon}(y+rn(y))\\,dr,\\quad y\\in\\Gamma\\]\n\nconverges weakly to a tangential vector field \\(v\\) on \\(\\Gamma\\) in an appropriate function space as \\(\\varepsilon\\to 0\\), and derived the nonstationary limit equations on \\(\\Gamma\\) by characterizing \\(v\\) as a unique \\(L^{2}\\)-weak solution to the limit equations. We also obtained some estimates for the difference of \\(u^{\\varepsilon}\\) and \\(v\\) which show that \\(v\\) approximates \\(u^{\\varepsilon}\\) in the \\(L^{2}\\) sense when \\(\\varepsilon\\) is small.\n\nAs in the nonstationary case [ 34 ], we can derive ( 1.4 ) from ( 1.2 ) by means of convergence of a solution and characterization of the limit, but the procedure is the same so we omit it here. In this paper, we focus on difference estimates for the solutions \\(u^{\\varepsilon}\\) to ( 1.2 ) and \\(v\\) to ( 1.4 ). Let us fix some notations and formally state our main results (see Section 2 for details). Let \\(\\mathbb "]}, {"edit": ["for all \\(\\tau\\in\\mathcal{S}_{0}\\), where \\(\\mathbb{E}[\\cdot]\\) denotes the expectation. In this context, the functions \\(\\tau\\in\\mathcal{S}_{0}\\) are named _weakly convex_. Moreover, [11, Lemma 6] gives a weaker version of Theorem 3: Let \\(\\tau\\in\\mathcal{S}_{0}\\). For \\(a,b\\in[0,\\infty)\\) with \\(a\\geq b\\), it was shown that \\(\\tau(a+b)+\\tau(a-b)\\leq 2\\tau(a)+2\\tau(b)\\).\n\n#### 1.3.5 Statistics\n\nTheorem 1 can be applied to prove rates of convergence for certain kinds of means [10]: We may want to calculate a mean value of some sample points in a metric spaces. One candidate for this is the _Frechet mean_[12], also called _barycenter_. It is the (set of) minimizer(s) of the squared distance to the sample points. If \\(Y\\) is a random variable with values in a metric space \\((\\mathcal{Q},d)\\), the Frechet mean is \\(\\arg\\min_{q\\in\\mathcal{Q}}\\mathbb{E}[\\overline{Y,q^{2}}]\\), where we assume \\(\\mathbb{E}[\\overline{Y,q^{2}}]<\\infty\\) for all \\(q\\in\\mathcal{Q}\\). Similarly, one can define the Frechet median [11] as \\(\\arg\\min_{q\\in\\mathcal{Q}}\\mathbb{E}[\\overline{Y,q}]\\), or a more general \\(\\tau\\)-Frechet mean [10] as \\(\\arg\\min_{q\\in\\mathcal{Q}}\\mathbb{E}[\\tau(\\overline{Y,q})]\\) for functions \\(\\tau\\colon[0,\\infty)\\to\\mathbb{R}\\). Given a sequence of independent random variables \\(Y_{1},Y_{2},\\ldots\\) with the same distribution as \\(Y\\), a standard task in statistics is to bound the distance between the sample statistics and its corresponding population version. In our case, assume the \\(\\tau\\)-Frechet mean is unique and define\n\n\\[m :=\\operatorname*{arg\\,min}_{q\\in\\mathcal{Q}}\\mathbb{E}[\\tau( \\overline{Y,q})]\\,, \\hat{m}_{n} :=\\operatorname*{arg\\,min}_{q\\in\\mathcal{Q}}\\frac{1}{n}\\sum_{i=1} ^{n}\\tau(\\overline{Y_{i},q})\\,.\\]\n\nWe want to bound \\(\\overline{m_{n},m}\\) depending on \\(n\\). One can employ quadruple inequalities such as (3) to obtain a suitable upper bound [10, Theorem 1]. This approach is particularly useful, if we do not want to make the assumption that the diameter of the metric space \\(\\sup_{q,p\\in\\mathcal{Q}}\\overline{q,p}\\) is finite. With Theorem 1, one can obtain such a bound for \\(\\tau\\)-Frechet means with \\(\\tau\\in\\mathcal{S}\\) (under some conditions). We emphasize that this is only possible with (3) and not with (8). Noteworthy examples of \\(\\tau\\in\\mathcal{S}\\) in this context, aside from \\(\\tau=\\tau_{\\alpha}\\), are the Huber loss \\(\\tau_{\\mathsf{H},\\delta}\\)[13] and the Pseudo-Huber loss \\(\\tau_{\\mathsf{PH},\\delta}\\)[12] for \\(\\delta\\in(0, "], "nougat": ["for all \\(\\tau\\in\\mathcal{S}_{0}\\), where \\(\\mathbb{E}[\\cdot]\\) denotes the expectation. In this context, the functions \\(\\tau\\in\\mathcal{S}_{0}\\) are named _weakly convex_. Moreover, [TV97, Lemma 6] gives a weaker version of Theorem 3: Let \\(\\tau\\in\\mathcal{S}_{0}\\). For \\(a,b\\in[0,\\infty)\\) with \\(a\\geq b\\), it was shown that \\(\\tau(a+b)+\\tau(a-b)\\leq 2\\tau(a)+2\\tau(b)\\).\n\n#### 1.3.5 Statistics\n\nTheorem 1 can be applied to prove rates of convergence for certain kinds of means [Sch19]: We may want to calculate a mean value of some sample points in a metric spaces. One candidate for this is the _Frechet mean_[Fre48], also called _barycenter_. It is the (set of) minimizer(s) of the squared distance to the sample points. If \\(Y\\) is a random variable with values in a metric space \\((\\mathcal{Q},d)\\), the Frechet mean is \\(\\arg\\min_{q\\in\\mathcal{Q}}\\mathbb{E}[\\overline{Y,q^{2}}]\\), where we assume \\(\\mathbb{E}[\\overline{Y,q^{2}}]<\\infty\\) for all \\(q\\in\\mathcal{Q}\\). Similarly, one can define the Fr \u0301echet median [FVJ09] as \\(\\arg\\min_{q\\in\\mathcal{Q}}\\mathbb{E}[\\overline{Y,q}]\\), or a more general \\(\\tau\\)-Frechet mean [Sch22] as \\(\\arg\\min_{q\\in\\mathcal{Q}}\\mathbb{E}[\\tau(\\overline{Y,q})]\\) for functions \\(\\tau\\colon[0,\\infty)\\to\\mathbb{R}\\). Given a sequence of independent random variables \\(Y_{1},Y_{2},\\ldots\\) with the same distribution as \\(Y\\), a standard task in statistics is to bound the distance between the sample statistics and its corresponding population version. In our case, assume the \\(\\tau\\)-Frechet mean is unique and define\n\n\\[m :=\\arg\\min_{q\\in\\mathcal{Q}}\\mathbb{E}[\\tau(\\overline{Y,q})]\\,, \\hat{m}_{n} :=\\arg\\min_{q\\in\\mathcal{Q}}\\frac{1}{n}\\sum_{i=1}^{n}\\tau( \\overline{Y_{i},q})\\,.\\]\n\nWe want to bound \\(\\widehat{m}_{n},m\\) depending on \\(n\\). One can employ quadruple inequalities such as (3) to obtain a suitable upper bound [Sch19, Theorem 1]. This approach is particularly useful, if we do not want to make the assumption that the diameter of the metric space \\(\\sup_{q,p\\in\\mathcal{Q}}\\overline{q,p}\\) is finite. With Theorem 1, one can obtain such a bound for \\(\\tau\\)-Frechet means with \\(\\tau\\in\\mathcal{S}\\) (under some conditions). We emphasize that this is only possible with (3) and not with (8). Noteworthy examples of \\(\\tau\\in\\mathcal{S}\\) in this context, aside from \\(\\tau=\\tau_{\\alpha}\\), are the Huber loss \\(\\tau_{\\mathsf{H},\\delta}\\)[Hub64] and the Pseudo-Huber loss \\(\\tau_{\\mathsf{PH},\\delta}\\)[Cha+94] for \\(\\delta\\in(0, "]}, {"edit": ["The above assumptions are fundamental to our approximate calculation, and a further mild technical assumption allows us to avoid tedious consideration of uninteresting cases:\n\n* The time scales of relaxation of flagellar force and orientation are comparable: \\[\\gamma_{F}/\\gamma_{\\Theta}\\sim\\text{ord}(1).\\] (25)\n\nIndeed, we find this condition satisfied by the parameters we inferred from experimental observations in Table 1.\n\nUnder the assumptions described above, we obtain the following approximation for the translational diffusivity:\n\n\\[\\lim_{t\\rightarrow\\infty}\\frac{\\langle\\mathbf{X}^{(\\text{c})}(t) \\odot\\mathbf{X}^{(\\text{c})}(t)\\rangle}{2t} =D_{\\text{t}}^{*}\\mathfrak{l},\\] \\[D_{\\text{t}}^{*} \\equiv D_{\\text{t}}+\\frac{{V^{*}}^{2}}{2}\\frac{D_{\\text{r}}^{*}}{{D_{ \\text{r}}^{*}}^{2}+\\Omega_{\\text{r}}^{*}}^{2}+\\tilde{D_{\\text{t}}} \\tag{26}\\]\n\nwhere \\({V^{*}}^{2}\\) is the mean-square velocity coarse-grained over the flagellar time scale (19), and\n\n\\[\\tilde{D_{\\text{t}}} \\equiv\\frac{a\\Omega_{\\text{r}}^{*}}{2\\gamma_{\\text{r}}^{2}\\gamma _{\\text{r}}({\\Omega_{\\text{r}}^{*}}^{2}+D_{\\text{r}}^{*})}\\left[\\frac{2\\sigma _{F}^{2}}{\\gamma_{F}}\\sum_{j,j^{\\prime}=1}^{N}F_{j}^{(0)}in\\Theta_{j^{\\prime}} ^{(o)}\\cos\\left(\\Theta_{j}^{(o)}-\\Theta_{j^{\\prime}}^{(o)}+\\frac{2\\pi}{N}\\left( j-j^{\\prime}+\\frac{S_{j}-S_{j^{\\prime}}}{l}\\right)\\right)\\right.\\] \\[\\left.+\\frac{\\sigma_{\\Theta}^{2}}{\\gamma_{\\Theta}}\\sum_{j,j^{ \\prime}=1}^{N}F_{j}^{(0)}F_{j^{\\prime}}^{(0)}(F_{j^{\\prime}}^{(0)}\\cos(\\Theta_{ j^{\\prime}}^{(0)})-F_{j}^{(0)}\\cos(\\Theta_{j}^{(0)}))in\\left(\\Theta_{j}^{(o)}- \\Theta_{j^{\\prime}}^{(o)}+\\frac{2\\pi}{N}\\left(j-j^{\\prime}+\\frac{S_{j}-S_{j^{ \\prime}}}{l}\\right)\\right)\\right]\\]\n\nHere \\(\\Omega_{\\text{r}}^{*}\\) is the effective rotational drift coefficient and \\(D_{\\text{r}}^{*}\\) is the effective rotational diffusion coefficient given in (8) and (10) respectively. As shown in Subsection 4.2, the error in our calculation is essentially fourth order in the small parameters \\(\\sigma_{\\Theta}\\) and \\(\\sigma_{F}\\).\n\nThe first two terms of the expression in Eq. (26) are exactly the translational diffusion for a colony with thermal diffusivity "], "nougat": ["The above assumptions are fundamental to our approximate calculation, and a further mild technical assumption allows us to avoid tedious consideration of uninteresting cases:\n\n* The time scales of relaxation of flagellar force and orientation are comparable: \\[\\gamma_{F}/\\gamma_{\\Theta}\\sim\\text{ord}(1).\\] (25)\n\nIndeed, we find this condition satisfied by the parameters we inferred from experimental observations in Table 1.\n\nUnder the assumptions described above, we obtain the following approximation for the translational diffusivity:\n\n\\[\\lim_{t\\rightarrow\\infty}\\frac{\\langle\\mathbf{X}^{(\\text{c})}(t) \\odot\\mathbf{X}^{(\\text{c})}(t)\\rangle}{2t} =D_{\\text{t}}^{*}\\mathfrak{l},\\] \\[D_{\\text{t}}^{*} \\equiv D_{\\text{t}}+\\frac{{V^{*}}^{2}}{2}\\frac{D_{\\text{r}}^{*}}{{D_{ \\text{r}}^{*}}^{2}+\\Omega_{\\text{r}}^{*}}^{2}+\\tilde{D_{\\text{t}}} \\tag{26}\\]\n\nwhere \\({V^{*}}^{2}\\) is the mean-square velocity coarse-grained over the flagellar time scale (19), and\n\n\\[\\tilde{D_{\\text{t}}} \\equiv\\frac{a\\Omega_{\\text{r}}^{*}}{2\\gamma_{\\text{r}}^{2}\\gamma _{\\text{r}}({\\Omega_{\\text{r}}^{*}}^{2}+D_{\\text{r}}^{*})}\\left[\\frac{2\\sigma _{F}^{2}}{\\gamma_{F}}\\sum_{j,j^{\\prime}=1}^{N}F_{j}^{(0)}in\\Theta_{j^{\\prime}} ^{(o)}\\cos\\left(\\Theta_{j}^{(o)}-\\Theta_{j^{\\prime}}^{(o)}+\\frac{2\\pi}{N}\\left( j-j^{\\prime}+\\frac{S_{j}-S_{j^{\\prime}}}{l}\\right)\\right)\\right.\\] \\[\\left.+\\frac{\\sigma_{\\Theta}^{2}}{\\gamma_{\\Theta}}\\sum_{j,j^{ \\prime}=1}^{N}F_{j}^{(0)}F_{j^{\\prime}}^{(0)}(F_{j^{\\prime}}^{(0)}\\cos(\\Theta_ {j^{\\prime}}^{(0)})-F_{j}^{(0)}\\cos(\\Theta_{j}^{(0)}))in\\left(\\Theta_{j}^{(o)}- \\Theta_{j^{\\prime}}^{(o)}+\\frac{2\\pi}{N}\\left(j-j^{\\prime}+\\frac{S_{j}-S_{j^{ \\prime}}}{l}\\right)\\right)\\right]\\]\n\nHere \\(\\Omega_{\\text{r}}^{*}\\) is the effective rotational drift coefficient and \\(D_{\\text{r}}^{*}\\) is the effective rotational diffusion coefficient given in (8) and (10) respectively. As shown in Subsection 4.2, the error in our calculation is essentially fourth order in the small parameters \\(\\sigma_{\\Theta}\\) and \\(\\sigma_{F}\\).\n\nThe first two terms "]}, {"edit": ["\n\n# Quenched decay of correlations for random contracting Lorenz maps\n\nAndrew Larkin\n\nMarks Ruziboev\n\n###### Abstract.\n\nIn this work we consider i.i.d. random perturbations of contracting Lorenz maps sufficiently close to a Rovella parameter. We prove that the quenched correlations of the random dynamical system decays exponentially.\n\n## 1. Introduction\n\nThe Lorenz system was introduced in [30] as a simplified model for atmospheric convection. Numerical simulations have shown that the Lorenz system admits a strange attractor, called the Lorenz attractor, which became one of the most iconic examples in the field.\n\nA rigorous mathematical approach was developed with the introduction of the so called geometric Lorenz flow by [1, 26], which mimicks simulation of the dynamics of the Lorenz flow, and which has a robust strange attractor under \\(C^{1}\\) perturbations. Later in [37, 38] it was shown that the actual Lorenz attractor is indeed a singular hyperbolic attractor, further showing that the geometric Lorenz attractor represents the Lorenz attractor well. Moreover, it admits the so called Sinai-Ruelle-Bowen (SRB) measure, which is ergodic [11]. Its statistical properties, such as mixing rates, limit theorems and their stability under various perturbations, were studied intensively (see for example, [31, 10, 9, 7, 14, 13, 24]).\n\nAnother class of systems with similar properties was introduced in [35] called the _contracting Lorenz flow_. A fundamental difference between these is that the attractor of the system introduced by Rovella is not robust under perturbations, but still abundant in a measure theoretic sense. The set of measures for which the system is chaotic is called Rovella parameters and satisfies strong chaotic properties [8]; moreover, restricted to this set the system is stochastically stable [32, 33]. In [34] the authors addressed thermodynamic formalism for it. Up to now, the contracting Lorenz flow and one dimensional maps with critical points remain a profound example of a truly nonuniformly hyperbolic systems, which is studied via construction of induced schemes. We refer to [3] for a comprehensive account of these constructions.\n\nRecently, there has been increased interest in studying statistical properties of random dynamical systems, especially quenched (path-wise) properties. When the system has good uniformly hyperbolic properties, spectral techniques are still applicable and imply strong statistical properties; we refer to [19, 20, 21, 23, 22] and references therein for results on quenched decay of correlations, limit theorems and stability results in this case.\n\nFor the non-uniformly expanding (or non-uniformly hyperbolic) case, spectral techniques are not applicable directly. In this regards, it is customary to employ inducing techniques, in particular randomised version of Young Tower [39] construction called random Young towers. This was first carried out in [15] for random"], "nougat": ["\n\n# Quenched decay of correlations for random contracting Lorenz maps\n\nAndrew Larkin\n\nMarks Ruziboev\n\n###### Abstract.\n\nIn this work we consider i.i.d. random perturbations of contracting Lorenz maps sufficiently close to a Rovella parameter. We prove that the quenched correlations of the random dynamical system decays exponentially.\n\n## 1. Introduction\n\nThe Lorenz system was introduced in [30] as a simplified model for atmospheric convection. Numerical simulations have shown that the Lorenz system admits a strange attractor, called the Lorenz attractor, which became one of the most iconic examples in the field.\n\nA rigorous mathematical approach was developed with the introduction of the so called geometric Lorenz flow by [1, 26], which mimicks simulation of the dynamics of the Lorenz flow, and which has a robust strange attractor under \\(C^{1}\\) perturbations. Later in [37, 38] it was shown that the actual Lorenz attractor is indeed a singular hyperbolic attractor, further showing that the geometric Lorenz attractor represents the Lorenz attractor well. Moreover, it admits the so called Sinai-Ruelle-Bowen (SRB) measure, which is ergodic [11]. Its statistical properties, such as mixing rates, limit theorems and their stability under various perturbations, were studied intensively (see for example, [31, 10, 9, 7, 14, 13, 24]).\n\nAnother class of systems with similar properties was introduced in [35] called the _contracting Lorenz flow_. A fundamental difference between these is that the attractor of the system introduced by Rovella is not robust under perturbations, but still abundant in a measure theoretic sense. The set of measures for which the system is chaotic is called Rovella parameters and satisfies strong chaotic properties [8]; moreover, restricted to this set the system is stochastically stable [32, 33]. In [34] the authors addressed thermodynamic formalism for it. Up to now, the contracting Lorenz flow and one dimensional maps with critical points remain a profound example of a truly nonuniformly hyperbolic systems, which is studied via construction of induced schemes. We refer to [3] for a comprehensive account of these constructions.\n\nRecently, there has been increased interest in studying statistical properties of random dynamical systems, especially quenched (path-wise) properties. When the system has good uniformly hyperbolic properties, spectral techniques are still applicable and imply strong statistical properties; we refer to [19, 20, 21, 23, 22] and references therein for results on quenched decay of correlations, limit theorems and stability results in this case.\n\nFor the non-uniformly expanding (or non-uniformly hyperbolic) case, spectral techniques are not applicable directly. In this regards, it is customary to employ inducing techniques, in particular randomised version of Young Tower [39] construction called random Young towers. This was first carried out in [15] for random"]}, {"edit": ["though our model exhibits a larger variance in the estimated elasticity values compared to the BLP model.\n\nWe further estimate the average own-elasticity for high-priced, medium-priced, and low-priced cars and construct a confidence interval for each category using our inference procedure. We present our result in Table 14.\n\n## 6 Conclusion\n\nChoice models are fundamental in understanding consumer behavior and informing business decisions. Over the years, various methods, both parametric and non-parametric, have been developed to represent consumer behavior. While parametric methods, such as logit or probit-based models, are favored for their simplicity and interpretability, their restrictive assumptions can limit their ability to fully capture consumer preferences' intricacies. On the other hand, non-parametric methods offer a more flexible approach, but they often suffer from the \"curse of dimensionality\", where the complexity of estimating choice functions escalates exponentially with an increase in the number of products.\n\nIn this paper, we propose a fundamental characterization of choice models that combines the tractability of traditional choice models and the flexibility of non-parametric estimators. This characterization specifically tackles the challenge of high dimensionality in choice systems and facilitates flexible estimation of choice functions. Through extensive simulations, we validate the efficacy of our model, demonstrating its superior ability to capture a range of consumer behaviors that traditional choice models fail to capture. We also show how to\n\nFigure 4: Elasticity Estimation Comparison\n\n"], "nougat": ["though our model exhibits a larger variance in the estimated elasticity values compared to the BLP model.\n\nWe further estimate the average own-elasticity for high-priced, medium-priced, and lowpriced cars and construct a confidence interval for each category using our inference procedure. We present our result in Table 14 .\n\n## 6 Conclusion\n\nChoice models are fundamental in understanding consumer behavior and informing business decisions. Over the years, various methods, both parametric and non-parametric, have been developed to represent consumer behavior. While parametric methods, such as logit or probit-based models, are favored for their simplicity and interpretability, their restrictive assumptions can limit their ability to fully capture consumer preferences\u2019 intricacies. On the other hand, non-parametric methods offer a more flexible approach, but they often suffer from the \u201ccurse of dimensionality\u201d, where the complexity of estimating choice functions escalates exponentially with an increase in the number of products.\n\nIn this paper, we propose a fundamental characterization of choice models that combines the tractability of traditional choice models and the flexibility of non-parametric estimators. This characterization specifically tackles the challenge of high dimensionality in choice systems and facilitates flexible estimation of choice functions. Through extensive simulations, we validate the efficacy of our model, demonstrating its superior ability to capture a range of consumer behaviors that traditional choice models fail to capture. We also show how to\n\nFigure 4: Elasticity Estimation Comparison\n\n"]}, {"edit": ["\n\n## Data Availability\n\nThe original data analysed in this work are part of the Guaranteed Time Observation (GTO) program 1282 (PI: Th. Henning) with number 66 and will become public on 2 August 2023 on the MAST database ([https://mast.stsci.edu](https://mast.stsci.edu)). The portion of the spectrum presented in Fig. 3 is available on Zenodo at [https://zenodo.org/record/7991022](https://zenodo.org/record/7991022). The spectroscopic data for water can be downloaded from the HITRAN database ([https://hitran.org](https://hitran.org)). The Spitzer-IRS spectrum plotted in Fig. 1 is part of the Spitzer-IRS GTO program 40679 (PI: G. Rieke). The spectrum was extracted and calibrated using private codes [56, 64] and is available on Zenodo at [https://zenodo.org/record/7991022](https://zenodo.org/record/7991022). The optical constants of the dust species considered in the fitting procedure for the dust continuum can be downloaded from the HJPDOC database ([https://www2.mpia-hd.mpg.de/HJPDOC](https://www2.mpia-hd.mpg.de/HJPDOC)).\n\n## Code Availability\n\nThe slab model used in this work is a private code developed by B.T. and collaborators. It can be obtained from B.T. upon request. The synthetic spectra presented in this work can be reproduced using the slabspec code, which can be found at [https://doi.org/10.5281/zenodo.4037306](https://doi.org/10.5281/zenodo.4037306). The fitting procedure for the dust continuum uses the publicly available MultiNest Bayesian fitting algorithm ([https://github.com/JohannesBuchner/MultiNest](https://github.com/JohannesBuchner/MultiNest)) and the PyMultiNest package ([https://github.com/JohannesBuchner/PyMultiNest](https://github.com/JohannesBuchner/PyMultiNest)). Figures were made with Matplotlib version 3.5.1. under the Matplotlib license at [https://matplotlib.org/](https://matplotlib.org/).\n\n## Acknowledgements\n\nThe MINDS team would like to thank the entire MIRI European and US instrument team. Support from STScI is also appreciated. The following National and International Funding Agencies funded and supported the MIRI development: NASA; ESA; Belgian Science Policy Office (BELSPO); Centre Nationale d'Etudes Spatiales (CNES); Danish National Space Centre; Deutsches Zentrum fur Luft- und Raumfahrt (DLR); Enterprise Ireland; Ministerio De Economia y Competividad; Netherlands Research School for Astronomy (NOVA); Netherlands Organisation for Scientific Research (NWO); Science and Technology Facilities Council; Swiss Space Office; Swedish National Space Agency; and UK Space Agency. G.P. would like to thank B. Bitsch and E. Gaidos for fruitful discussions and P. Hausschildt for kindly providing the model atmosphere. V.C. and O.A. acknowledge funding from the Belgian F.R.S.-FNRS. Th.H., R.F. and K.S. acknowledge support from the European Research Council under the Horizon 2020 Framework Program via the ERC Advanced Grant Origins 83 24 28. B.T. is a Laureate of the Paris Region fellowship program, which is supported by the Ile-de-France Region and has received funding under the Horizon 2020 innovation framework program and Marie Sklodowska-Curie grant agreement No. 945298. B.T. acknowledges support from the Programme National 'Physique et Chimie du Milieu Interstellaire' (PCMI) of CNRS/INSU with INC/INP cofunded by CNES. D.G. would like to thank the Research Foundation Flanders for co-financing the present research (grant number V435622N). D.G. and I.A. thank the European Space Agency (ESA) and the Belgian Federal Science Policy Office (BELSPO) for their support in the framework of the PRODEX Programme. I.K., A.M.A., and E.v.D. acknowledge support from grant TOP-1614.001.751 from the Dutch Research Council (NWO). I.K. and J.K. acknowledge funding from H2020-MSCA-ITN-2019, grant no. 860470 (CHAMELEON). E.F.v.D. acknowledges support from the ERC grant"], "nougat": ["\n\n## Data Availability\n\nThe original data analysed in this work are part of the Guaranteed Time Observation (GTO) program 1282 (PI: Th. Henning) with number 66 and will become public on 2 August 2023 on the MAST database ( [https://mast.stsci.edu](https://mast.stsci.edu) ). The portion of the spectrum presented in Fig. 3 is available on Zenodo at [https://zenodo.org/record/7991022](https://zenodo.org/record/7991022) . The spectroscopic data for water can be downloaded from the HITRAN database ( [https://hitran.org](https://hitran.org) ). The Spitzer-IRS spectrum plotted in Fig. 1 is part of the Spitzer-IRS GTO program 40679 (PI: G. Rieke). The spectrum was extracted and calibrated using private codes , and is available on Zenodo at [https://zenodo.org/record/7991022](https://zenodo.org/record/7991022) . The optical constants of the dust species considered in the fitting procedure for the dust continuum can be downloaded from the HJPDOC database ( [https://www2.mpia-hd.mpg.de/HJPDOC](https://www2.mpia-hd.mpg.de/HJPDOC) ).\n\n## Code Availability\n\nThe slab model used in this work is a private code developed by B.T. and collaborators. It can be obtained from B.T. upon request. The synthetic spectra presented in this work can be reproduced using the slabspec code, which can be found at [https://doi.org/10.5281/zenodo.4037306](https://doi.org/10.5281/zenodo.4037306) . The fitting procedure for the dust continuum uses the publicly available MultiNest Bayesian fitting algorithm ( [https://github.com/JohannesBuchner/MultiNest](https://github.com/JohannesBuchner/MultiNest) ) and the PyMultiNest package ( [https://github.com/JohannesBuchner/PyMultiNest](https://github.com/JohannesBuchner/PyMultiNest) ). Figures were made with Matplotlib version 3.5.1. under the Matplotlib license at [https://matplotlib.org/](https://matplotlib.org/) .\n\n## Acknowledgements\n\nThe MINDS team"]}, {"edit": ["coupled WGM resonators to achieve multi-band UR of photons through modulation of the intermode backscatterings of resonators[41]. In the reciprocal system that the equivalent transmission in both directions was exhibited. Obviously, these systems previously mentioned only investigated the transmission or reflection characteristics, without considering achieving complete non-reciprocity in both channels, whereas directional transport in both channels is vital to enhance the controllability of photons.\n\nTo this end, we propose a non-reciprocal system consisting of two WGM resonators that are individually embedded with a Zeeman split quantum dot (QD)[42, 43, 44] and indirectly coupled through an optical fiber. By optimizing some system parameters, we demonstrate the simultaneous realization of UR and unidirectional transmissionlessness (UT). Moreover, the conversion between UR and UT can be achieved by adjusting the coupling strength between WGM resonators and optical fiber. Additionally, a one-to-one correspondence is established between the resonant frequencies of QDs energy levels and the positions of UR and UT peaks.\n\n## 1 Model and calculations\n\nThe schematic of system is shown in Fig.1 (a) and energy levels of QD is shown in Fig.1 (b). Assuming that the WGM resonators and QDs have the same loss rates \\(\\gamma\\), then the Hamiltonian of the system can be written as (assuming \\(\\hbar=1\\))\n\n\\[H= \\int dx\\{[-iv_{g}C_{R}^{\\dagger}(x)\\frac{\\partial}{\\partial x}C_{R} (x)+iv_{g}C_{L}^{\\dagger}(x)\\frac{\\partial}{\\partial x}C_{L}(x)]\\] \\[+\\sum_{j=1,2}G_{j}\\delta[x-(j-1)d](C_{R}^{\\dagger}C_{a_{j}}+C_{L} ^{\\dagger}C_{b_{j}}+\\text{H.c.})\\}\\] \\[+\\sum_{j=1,2}\\{[g_{j}(C_{a_{j}}\\sigma_{Rj}^{\\dagger}+C_{b_{j}} \\sigma_{Lj}^{\\dagger})+h_{j}C_{a_{j}}^{\\dagger}C_{b_{j}}+\\text{H.c.}]\\] \\[+(\\omega_{0}-\\omega_{j}-i\\gamma)\\sigma_{Rj}^{\\dagger}\\sigma_{Rj}+ (\\omega_{0}+\\omega_{j}-i\\gamma)\\sigma_{Lj}^{\\dagger}\\sigma_{Lj}\\] \\[+(\\omega_{a_{j}}-i\\gamma)C_{a_{j}}^{\\dagger}C_{a_{j}}+(\\omega_{b_ {j}}-i\\gamma)C_{b_{j}}^{\\dagger}C_{b_{j}}\\}, \\tag{1}\\]\n\nwhere \\(C_{R}^{\\dagger}(x)\\) (\\(C_{R}(x)\\)) and \\(C_{L}^{\\dagger}(x)\\) (\\(C_{L}(x)\\)) are creation (annihilation) operators at \\(x\\) for forward and backward propagating photon along fiber, respectively. \\(C_{b_{j}}^{\\dagger}\\) (\\(C_{b_{j}}\\)) and \\(C_{a_{j}}^{\\dagger}\\) (\\(C_{a_{j}}\\)) are creation (annihilation) operators of CW mode \\(b_{j}\\) and CCW mode \\(a_{j}\\) with resonance frequencies \\(\\omega_{bj}\\) and \\(\\omega_{aj}\\), respectively. \\(\\sigma_{Lj}^{\\dagger}\\) (\\(\\sigma_{Lj}\\)) and \\( "], "nougat": ["coupled WGM resonators to achieve multi-band UR of photons through modulation of the intermode backscatterings of resonators[ 41 ]. In the reciprocal system that the equivalent transmission in both directions was exhibited. Obviously, these systems previously mentioned only investigated the transmission or reflection characteristics, without considering achieving complete nonreciprocity in both channels, whereas directional transport in both channels is vital to enhance the controllability of photons.\n\nTo this end, we propose a non-reciprocal system consisting of two WGM resonators that are individually embedded with a Zeeman split quantum dot (QD)[ 42 \u2013 44 ] and indirectly coupled through an optical fiber. By optimizing some sys-tem parameters, we demonstrate the simultaneous realization of UR and unidirectional transmissionlessness (UT). Moreover, the conversion between UR and UT can be achieved by adjusting the coupling strength between WGM resonators and optical fiber. Additionally, a one-to-one correspondence is established between the resonant frequencies of QDs energy levels and the positions of UR and UT peaks.\n\n## 1 Model and calculations\n\nThe schematic of system is shown in Fig. 1 (a) and energy levels of QD is shown in Fig. 1 (b). Assuming that the WGM resonators and QDs have the same loss rates \\(\\gamma\\), then the Hamiltonian of the system can be written as (assuming \\(\\hbar=1\\))\n\n\\[H= \\int dx\\{[-iv_{g}C_{R}^{\\dagger}(x)\\frac{\\partial}{\\partial x}C_{R} (x)+iv_{g}C_{L}^{\\dagger}(x)\\frac{\\partial}{\\partial x}C_{L}(x)]\\] \\[+\\sum_{j=1,2}G_{j}\\delta[x-(j-1)d](C_{R}^{\\dagger}C_{a_{j}}+C_{L} ^{\\dagger}C_{b_{j}}+\\text{H.c.})\\}\\] \\[+\\sum_{j=1,2}\\{[g_{j}(C_{a_{j}}\\sigma_{Rj}^{\\dagger}+C_{b_{j}} \\sigma_{Lj}^{\\dagger})+h_{j}C_{a_{j}}^{\\dagger}C_{b_{j}}+\\text{H.c.}]\\] \\[+(\\omega_{0}-\\omega_{j}-i\\gamma)\\sigma_{Rj}^{\\dagger}\\sigma_{Rj}+ (\\omega_{0}+\\omega_{j}-i\\gamma)\\sigma_{Lj}^{\\dagger}\\sigma_{Lj}\\] \\[+(\\omega_{a_{j}}-i\\gamma)C_{a_{j}}^{\\dagger}C_{a_{j}}+(\\omega_{b_ {j}}-i\\gamma)C_{b_{j}}^{\\dagger}C_{b_{j}}\\}, \\tag{1}\\]\n\nwhere \\(C_{R}^{\\dagger}(x)\\) (\\(C_{R}(x)\\)) and \\(C_{L}^{\\dagger}(x)\\) (\\(C_{L}(x)\\)) are creation (annihilation) operators at \\(x\\) for forward and backward propagating photon along fiber, respectively. \\(C_{b_{j}}^{\\dagger}\\) (\\(C_{b_{j}}\\)) and \\(C_{a_{j}}^{\\dagger}\\) (\\(C_{a_{j}}\\)) are creation (annihilation) operators of CW mode \\(b_{j}\\) and CCW mode \\(a_{j}\\) with resonance frequencies \\(\\omega_{bj}\\) and \\(\\omega_{aj}\\), respectively. \\(\\sigma_{Lj}^{\\dagger}\\) (\\(\\sigma_{Lj}\\)) and "]}, {"edit": ["with a step size of 0.01. The final weights are used in our OWAF technique. Table 3 illustrates the effects of various fusion strategies. For fare comparison, we use both MRI and DTI data in all cases. The experimental results in the table 3 clearly reveal that the proposed OWAF outperforms other fusion strategies.\n\n### Comparisons with State-of-the-art Approaches\n\nWe compare our method with ten state-of-the-art approaches. There are no results available for a direct 3-class PD classification. So, we compare our results with those papers that have addressed the PD classification on the PPMI database using single or multiple modalities and with three or fewer two-class classifications. The results of comparisons are shown in Table 4. Out of the ten methods we have considered, five are based on machine learning (ML) and the rest five are based on deep learning (DL). Further, in four out of five DL based approaches, only a single modality, namely, MRI is used for classification. Also note that eight of these ten techniques have only addressed a single two-class classification problem between PD and HC and did not consider the challenging SWEDD class at all. The remaining two approaches did consider SWEDD as a third class but have divided the three-class classification problem into multiple binary classes [2, 10]. However, Li at al. [10] did not report the classification results for PD vs. SWEDD in their paper. In order to have fair comparisons, we have also included three binary classifications as obtained from our method in this table. Our direct three-class classification accuracy turns out to be superior than two-class classification accuracy of at least eight out of 10 methods. It is also higher than two out of three binary classification accuracy of [2]. Note that in [2], the authors used a somewhat different experimental protocol by considering two publicly available databases of ADNI and PPMI. In our work, we explicitly consider data with both MRI and DTI for the same individual\n\n\\begin{table}\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|} \\hline \\multirow{2}{*}{Approach} & \\multirow{2}{*}{ML/DL} & \\multirow{2}{*}{MODALITY} & \\multicolumn{3}{c|}{PD vs HC} & \\multicolumn{1}{c|}{PD vs. SWEDD} & HC vs. SWEDD & PD vs. HC vs SWEDD \\\\ \\cline{4-9}  & & & Ac & Pr & Re & Ac & Ac & Ac \\\\ \\hline Adeli 2016 [1] & ML & MRI & 81.9 & - & - & - & - & - \\\\ \\hline Cigdem 2018 [6] & ML & MRI & 93.7 & - & 95 & - & - & - \\\\ \\hline Prashanth 2018 [20] & ML & SPECT & 95 & - & 96.7 & - & - & - \\\\ \\hline Singh 2018 [2] & ML & MRI & 95.37 & - & - & 96.04 & 93.03 & - \\\\ \\hline \\multirow{3}{*}{Gabriel 2021 [21]} & \\multirow{3}{*}{ML} & \\multirow{3}{*}{MRI} & \\multirow{3}{*}{10(M)} & \\multirow{3}{*}{100(M)} & \\multirow{3}{*}{99.3(M)} & \\multirow{3}{*}{-} & \\multirow{3}{*}{-} & \\multirow{3}{*}{-} \\\\  & & & & & & & & \\\\  & & & & 87.10(F "], "nougat": ["with a step size of 0.01. The final weights are used in our OWAF technique. Table 3 illustrates the effects of various fusion strategies. For fare comparison, we use both MRI and DTI data in all cases. The experimental results in the table 3 clearly reveal that the proposed OWAF outperforms other fusion strategies.\n\n### Comparisons with State-of-the-art Approaches\n\nWe compare our method with ten state-of-the-art approaches. There are no results available for a direct 3-class PD classification. So, we compare our results with those papers that have addressed the PD classification on the PPMI database using single or multiple modalities and with three or fewer two-class classifications. The results of comparisons are shown in Table 4. Out of the ten methods we have considered, five are based on machine learning (ML) and the rest five are based on deep learning (DL). Further, in four out of five DL based approaches, only a single modality, namely, MRI is used for classification. Also note that eight of these ten techniques have only addressed a single two-class classification problem between PD and HC and did not consider the challenging SWEDD class at all. The remaining two approaches did consider SWEDD as a third class but have divided the three-class classification problem into multiple binary classes [2, 10]. However, Li at al. [10] did not report the classification results for PD vs. SWEDD in their paper. In order to have fair comparisons, we have also included three binary classifications as obtained from our method in this table. Our direct three-class classification accuracy turns out to be superior than two-class classification accuracy of at least eight out of 10 methods. It is also higher than two out of three binary classification accuracy of [2]. Note that in [2], the authors used a somewhat different experimental protocol by considering two publicly available databases of ADNI and PPMI. In our work, we explicitly consider data with both MRI and DTI for the same individual\n\n\\begin{table}\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|} \\hline \\multirow{2}{*}{Approach} & \\multirow{2}{*}{ML/DL} & \\multirow{2}{*}{MODALITY} & \\multicolumn{3}{c|}{PD vs HC} & \\multicolumn{1}{c|}{PD vs. SWEDD} & HC vs. SWEDD & PD vs. HC vs SWEDD \\\\ \\cline{4-9}  & & & Ac & Pr & Re & Ac & Ac & Ac \\\\ \\hline Adeli 2016 [1] & ML & MRI & 81.9 & - & - & - & - & - \\\\ \\hline Cigdem 2018 [6] & ML & MRI & 93.7 & - & 95 & - & - & - \\\\ \\hline Prashanth 2018 [20] & ML & SPECT & 95 & - & 96.7 & - & - & - \\\\ \\hline Singh 2018 [2] & ML & MRI & 95.37 & - & - & 96.04 & 93.03 & - \\\\ \\hline \\multirow{3}{*}{Gabriel 2021 [21]} & \\multirow{3}{*}{ML} & \\multirow{3}{*}{MRI} & \\multirow{3}{*}{10(M)} & \\multirow{3}{*}{100(M)} & \\multirow{3}{*}{99.3(M)} & \\multirow{3}{*}{-} & \\multirow{3}{*}{-} & \\multirow{3}{*}{-} \\\\  & & & & & & & & \\\\  & & & & 87.10(F "]}, {"edit": ["We then define the damped mode:\n\n\\[\\tilde{W}^{\\varepsilon}:=\\frac{\\nabla\\left(P(\\tilde{\\varrho}^{\\varepsilon})\\right) }{\\tilde{\\varrho}^{\\varepsilon}}+\\tilde{v}^{\\varepsilon}+\\nabla(-\\Delta)^{-1}( \\tilde{\\varrho}^{\\varepsilon}-\\overline{\\varrho}). \\tag{1.4}\\]\n\nAs the first equation of (1.3) can be rewritten as\n\n\\[\\partial_{t}\\tilde{\\varrho}^{\\varepsilon}-\\Delta\\left(P(\\tilde{\\varrho}^{ \\varepsilon})\\right)-\\operatorname{div}\\left(\\tilde{\\varrho}^{\\varepsilon} \\nabla(-\\Delta)^{-1}(\\tilde{\\varrho}^{\\varepsilon}-\\overline{\\varrho})\\right)= \\operatorname{div}(\\tilde{\\varrho}^{\\varepsilon}\\tilde{W}^{\\varepsilon}),\\]\n\nwe expect the limit density \\(N\\) to satisfy the following _parabolic-elliptic Keller-Segel system_ :\n\n\\[\\left\\{\\begin{array}{l}\\partial_{t}N-\\Delta\\left(P(N)\\right)=\\operatorname{ div}\\left(N\\ \\nabla V\\right)\\\\ -\\Delta V=N-\\overline{\\varrho}\\end{array}\\right. \\tag{1.5}\\]\n\nsupplemented with the initial data \\(\\lim\\limits_{\\varepsilon\\to 0}\\tilde{\\varrho}_{0}^{\\varepsilon}\\).\n\nOur second aim is to justify the passage to the limit when \\(\\varepsilon\\to 0\\) of the Euler-Poisson system towards the parabolic-elliptic Keller-Segel system.\n\nRecall that (1.5) is a model for describing the evolution of density \\(N=N(t,x)\\in\\mathsf{R}_{+}\\) of a biological population under the influence of a chemical agent with concentration \\(V=V(t,x)\\in\\mathsf{R}^{d}\\). Chemotaxis are an important means of cell communication. How cells are arranged and organized is determined by communication by chemical signals. Studying such a biological process is important because it has repercussions in many branches of medicine such as cancer [0], [0], embryonic development [0] or vascular networks [0], [0]. The previous system is famous in biology and comes from E.F Keller and L.A Segel in [0]. This basic model was used to describe the collective movement of bacteria possibly leading to cell aggregation by chemotactic effect. We refer to the articles [0] and [0] for more details and information about the different Keller-Segel models studied since the 1970s.\n\nOur aim here is to demonstrate that (1.5) may be obtained from the Euler-Poisson system with damping when the parameter \\(\\varepsilon\\) tends to \\(0.\\) This question has been addressed in [0] on the torus case and Sobolev spaces in a situation where the potential satisfies a less singular equation : the author justifies the passage to the limit for regular periodic solutions. A lot of articles justify another limit: the passage from the parabolic-parabolic Keller-Segel system to the parabolic-elliptic Keller-Segel system (see e.g. the paper [0] by P-G. Lemarie-Rieusset for the case of Morrey spaces).\n\nIn the same spirit as this article, T. Crin-Barat, Q. He and L. Shou in [0] justified the high relaxation asymptotics for the (less singular) parabolic-parabolic Keller-Segel system (the potential satisfies the equation \\(-\\Delta V+bV=aN\\) with \\(a,b>0\\)) : this other system comes from the system (HPC) (hyperbolic-parabolic-chemotaxis) which is a damped isentropic compressible Euler system with a potential satisfying an elliptical equation. In comparison with what is done here, T. Crin-Barat _et al_ used a parabolic approach to justify their passage to the limit. Here, we "], "nougat": ["We then define the damped mode:\n\n(1.3) can be rewritten as\n\n\\[\\partial_{t}\\tilde{\\varrho}^{\\varepsilon}-\\Delta\\left(P(\\tilde{\\varrho}^{ \\varepsilon})\\right)-\\operatorname{div}\\left(\\tilde{\\varrho}^{\\varepsilon}\\nabla (-\\Delta)^{-1}(\\tilde{\\varrho}^{\\varepsilon}-\\overline{\\varrho})\\right)= \\operatorname{div}(\\tilde{\\varrho}^{\\varepsilon}\\tilde{W}^{\\varepsilon}), \\tag{1.4}\\]\n\nwe expect the limit density \\(N\\) to satisfy the following _parabolic-elliptic Keller-Segel system_ :\n\n\\[\\left\\{\\begin{array}{l}\\partial_{t}N-\\Delta\\left(P(N)\\right)=\\operatorname{ div}\\left(N\\ \\nabla V\\right)\\\\ -\\Delta V=N-\\overline{\\varrho}\\end{array}\\right. \\tag{1.5}\\]\n\nsupplemented with the initial data \\(\\lim\\limits_{\\varepsilon\\to 0}\\tilde{\\varrho}_{0}^{\\varepsilon}\\).\n\nOur second aim is to justify the passage to the limit when \\(\\varepsilon\\to 0\\) of the Euler-Poisson system towards the parabolic-elliptic Keller-Segel system.\n\nRecall that (1.5) is a model for describing the evolution of density \\(N=N(t,x)\\in\\mathsf{R}_{+}\\) of a biological population under the influence of a chemical agent with concentration \\(V=V(t,x)\\in\\mathsf{R}^{d}\\). Chemotaxis are an important means of cell communication. How cells are arranged and organized is determined by communication by chemical signals. Studying such a biological process is important because it has repercussions in many branches of medicine such as cancer [0], [0], embryonic development [0] or vascular networks [0], [0]. The previous system is famous in biology and comes from E.F Keller and L.A Segel in [0]. This basic model was used to describe the collective movement of bacteria possibly leading to cell aggregation by chemotactic effect. We refer to the articles [0] and [0] for more details and information about the different Keller-Segel models studied since the 1970s.\n\nOur aim here is to demonstrate that (1.5) may be obtained from the Euler-Poisson system with damping when the parameter \\(\\varepsilon\\) tends to \\(0.\\) This question has been addressed in [0] on the torus case and Sobolev spaces in a situation where the potential satisfies a less singular equation : the author justifies the passage to the limit for regular periodic solutions. A lot of articles justify another limit: the passage from the parabolic-parabolic Keller-Segel system to the parabolic-elliptic Keller-Segel system (see e.g. the paper [0] by P-G. Lemari\u00e9-Rieusset for the case of Morrey spaces).\n\nIn the same spirit as this article, T. Crin-Barat, Q. He and L. Shou in [0] justified the high relaxation asymptotics for the (less singular) parabolic-parabolic Keller-Segel system (the potential satisfies the equation \\(-\\Delta V+bV=aN\\) with \\(a,b>0\\)) : this other system comes from the system (HPC) (hyperbolic-parabolicchemotaxis) which is a damped isentropic compressible Euler system with a potential satisfying an elliptical equation. In comparison with what is done here, T. Crin-Barat _et al_ used a parabolic approach to justify their passage to the limit. Here, we have to handle the more singular case where the limit system is parabolicelliptic.\n\n## 2. Main results and sketch of the proof\n\nIn this section, we will first present and motivate the functional spaces used. Secondly we will state the results and the sketch of the proofs about the well-posedness behavior of Euler-Poisson system and the justification of the passage to the limit to parabolic-elliptic Keller-Segel system.\n\n### Functional spaces\n\nBefore describing the main "]}, {"edit": ["\n\n**4.5**.: **Corollary.** _Let \\(P\\) be a polyhedron of dimension \\(n\\) with abelian \\(\\pi_{1}(P)\\) and finitely generated \\(H_{i}(\\tilde{P})\\), for \\(i\\geq 2\\). Then \\(D(P)\\leq\\sum_{i=1}^{n}n_{i}\\), where \\(n_{1}\\) and \\(n_{i}\\)\\((i\\geq 2)\\) are the number of nonzero direct summands in the canonical form of \\(\\pi_{1}(P)\\) and \\(H_{i}(\\tilde{P})\\), respectively._\n\n**4.6**.: **Corollary.** _Let \\(P\\) be a polyhedron of dimension \\(n\\) with free \\(\\pi_{1}(P)\\) and finitely generated \\(H_{i}(\\tilde{P})\\), for \\(i\\geq 2\\). Then \\(D(P)\\leq rank(\\pi_{1}(P))+\\sum_{i=2}^{n}n_{i}\\), where \\(n_{i}\\) is the number of nonzero direct summands in the canonical form of \\(H_{i}(\\tilde{P})\\)._\n\n**4.7**.: **Corollary.** _Let \\(P\\) be a polyhedron of dimension \\(n\\) with elementary amenable \\(\\pi_{1}(P)\\) of finite cohomological dimension and finitely generated \\(H_{i}(\\tilde{P})\\), for \\(i\\geq 2\\). Then \\(D(P)\\leq h(\\pi_{1}(P))+\\sum_{i=2}^{n}n_{i}\\), where \\(h(\\pi_{1}(P))\\) is the Hirsch length of \\(\\pi_{1}(P)\\) and \\(n_{i}\\) is the number of nonzero direct summands in the canonical form of \\(H_{i}(\\tilde{P})\\)._\n\nRecently, in [14], Kolodziejczyk proved that 2-dimensional polyhedra whose fundamental groups are elementary amenable with finite cohomological dimension have finite depth. In the sequel, we are going to present upper bounds for such polyhedra.\n\nRecall that if \\(P\\) is a polyhedron of dimension \\(n\\), then \\(H_{n}(P)\\) is free abelian (see, for example, [18, Theorem 7.24]). Now we state our second main result.\n\n**4.8**.: **Theorem.** _If \\(P\\) is a 2-dimensional polyhedron and \\(sl(\\pi_{1}(P))<\\infty\\), then \\(D(P)\\leq sl\\big{(}\\pi_{1}(P)\\big{)}+rank\\big{(}H_{2}(P)\\big{)}\\)._\n\nProof.: Consider the following chain of CW-complexes:\n\n\\[\\cdots<X_{i+1}<X_{i}<\\cdots<X_{3}<X_{2}<X_{1}<X_{0}=P.\\]\n\nLet \\(d_{X_{i+1}}:X_{i}\\to X_{i+1}\\) and \\(u_{X_{i+1}}:X_{i+1}\\to X_{i}\\) be the domination of \\(X_{i}\\) over \\(X_{i+1}\\) and the converse map, i.e., \\(d_{X_{i+1}}u_{X_{i+1}}\\simeq id_{X_{i+1}}\\). Then \\(\\pi_{1}(d_"], "nougat": ["\n\n**4.5. Corollary.**_Let \\(P\\) be a polyhedron of dimension \\(n\\) with abelian \\(\\pi_{1}(P)\\) and finitely generated \\(H_{i}(\\tilde{P})\\), for \\(i\\geq 2\\). Then \\(D(P)\\leq\\sum_{i=1}^{n}n_{i}\\), where \\(n_{1}\\) and \\(n_{i}\\)\\((i\\geq 2)\\) are the number of nonzero direct summands in the canonical form of \\(\\pi_{1}(P)\\) and \\(H_{i}(\\tilde{P})\\), respectively._\n\n**4.6. Corollary.**_Let \\(P\\) be a polyhedron of dimension \\(n\\) with free \\(\\pi_{1}(P)\\) and finitely generated \\(H_{i}(\\tilde{P})\\), for \\(i\\geq 2\\). Then \\(D(P)\\leq rank(\\pi_{1}(P))+\\sum_{i=2}^{n}n_{i}\\), where \\(n_{i}\\) is the number of nonzero direct summands in the canonical form of \\(H_{i}(\\tilde{P})\\)._\n\n**4.7. Corollary.**_Let \\(P\\) be a polyhedron of dimension \\(n\\) with elementary amenable \\(\\pi_{1}(P)\\) of finite cohomological dimension and finitely generated \\(H_{i}(\\tilde{P})\\), for \\(i\\geq 2\\). Then \\(D(P)\\leq h(\\pi_{1}(P))+\\sum_{i=2}^{n}n_{i}\\), where \\(h(\\pi_{1}(P))\\) is the Hirsch length of \\(\\pi_{1}(P)\\) and \\(n_{i}\\) is the number of nonzero direct summands in the canonical form of \\(H_{i}(\\tilde{P})\\)._\n\nRecently, in [14], Ko lodziejczyk proved that 2-dimensional polyhedra whose fundamental groups are elementary amenable with finite cohomological dimension have finite depth. In the sequel, we are going to present upper bounds for such polyhedra.\n\nRecall that if \\(P\\) is a polyhedron of dimension \\(n\\), then \\(H_{n}(P)\\) is free abelian (see, for example, [18, Theorem 7.24]). Now we state our second main result.\n\n**4.8. Theorem.**_If \\(P\\) is a 2-dimensional polyhedron and \\(sl(\\pi_{1}(P))<\\infty\\), then \\(D(P)\\leq sl\\big{(}\\pi_{1}(P)\\big{)}+rank\\big{(}H_{2}(P)\\big{)}\\)._\n\nProof.: Consider the following chain of CW-complexes:\n\n\\[\\cdots<X_{i+1}<X_{i}<\\cdots<X_{3}<X_{2}<X_{1}<X_{0}=P.\\]\n\nLet \\(d_{X_{i+1}}:X_{i}\\to X_{i+1}\\) and \\(u_{X_{i+1}}:X_{i+1}\\to X_{i}\\) be the domination of \\(X_{i}\\) over \\(X_{i+1}\\) and the converse map, i.e., \\(d_{X_{i+1}}u_{X_{i+1}}\\simeq id_{X_{i+1}}\\). Then \\(\\pi_{1}(d_{X_{i+1}})\\pi_{1}(u_"]}, {"edit": ["Outline.We use Fenichel's reduction to regularize the singular perturbation in existence and eigenvalue problem in Section 2. In Section 4, we study the resulting regularized traveling wave problem using functional-analytic methods, using methods developed in [2, 4, 3] to find pulled and pushed front profiles as well as the transition curve. Section 5 establishes marginal spectral stability of these fronts thus justifying the pushed and pulled terminology. In Section 6, we briefly compare the expansions obtained in Theorem 1.2 to those obtained using numerical continuation. The appendix contains the construction and properties of traveling fronts at \\(\\delta=0\\).\n\n## 2 Regularization via geometric singular perturbation theory\n\n### Reduction of existence problem\n\nWe express (1.5) as a dynamical system in the variable \\(x\\) by choosing coordinates \\(U,W=U^{\\prime},H=\\frac{V-U}{\\delta^{2}},Z=\\delta H^{\\prime}\\), obtaining\n\n\\[U^{\\prime} =W\\] \\[W^{\\prime} =-\\frac{1}{d_{1}}\\Gamma(U,W,H,Z)\\] \\[\\delta H^{\\prime} =Z\\] \\[\\delta Z^{\\prime} =H+\\frac{1}{d_{1}}\\Gamma(U,W,H,Z), \\tag{2.1}\\]\n\nwhere\n\n\\[\\Gamma(U,W,H,Z)=cW+W^{2}+\\delta WZ+UH+U(1-U). \\tag{2.2}\\]\n\nWhen \\(\\delta=0\\) the system (2.1) reduces to two algebraic equations coupled to two differential equations. One identifies the following reduced slow manifold comprised of solutions of the algebraic of equations in the singular limit \\(\\delta=0\\),\n\n\\[\\mathcal{M}_{0}=\\left\\{(U,W,H,Z)\\;\\left|\\;Z=0,\\;H=-\\frac{cW+W^{2}+ U-U^{2}}{d_{1}+U}\\right.\\right\\}. \\tag{2.3}\\]\n\nThe linearization of (2.1) at any such fixed point has two zero eigenvalues and two hyperbolic eigenvalues \\(\\pm\\sqrt{1+\\frac{U}{d_{1}}}\\) for \\(U\\geq 0\\). The eigenspaces of the non-zero eigenvalues are traverse to \\(\\mathcal{M}_{0}\\) and therefore the reduced manifold is normally hyperbolic. Fenichel's Persistence Theorem [5] implies that \\(\\mathcal{M}_{0}\\) persists as an invariant manifold \\(\\mathcal{M}_{\\delta}\\) with the following properties.\n\n**Proposition 2.1** (Reduction for existence problem).: _Fix \\(0<\\underline{d}<\\overline{d}\\), \\(M>1\\), and an integer \\(k\\geq 2\\). There exists a \\(\\overline{\\delta}>0\\) such that all trajectories of (2.1) with \\(|\\delta|<\\overline{\\delta}\\), \\(\\underline{d}<d_{1}<\\overline{d}\\) satisfying \\(-\\frac{d_{1}}{2}<U<M\\) and \\(|W|\\leq M\\) lie in a slow manifold \\(\\mathcal{M}_{\\delta}\\), which is normally hyperbolic and invariant under the flow of (2.1), and may be written as a graph \\(\\mathcal{M}_{\\delta}=\\{(U,W,H,Z):H=\\psi_{H}(U,W;\\delta),Z=\\psi_{Z}(U,W;\\delta)\\}\\), where_\n\n\\[H =\\psi_{H}(U,W;\\delta)=\\psi_{H}^{0}(U,W)+\\delta\\psi_{H}^{1}(U,W)+ "], "nougat": ["Outline.We use Fenichel\u2019s reduction to regularize the singular perturbation in existence and eigenvalue problem in Section 2. In Section 4, we study the resulting regularized traveling wave problem using functional-analytic methods, using methods developed in [2, 4, 3] to find pulled and pushed front profiles as well as the transition curve. Section 5 establishes marginal spectral stability of these fronts thus justifying the pushed and pulled terminology. In Section 6, we briefly compare the expansions obtained in Theorem 1.2 to those obtained using numerical continuation. The appendix contains the construction and properties of traveling fronts at \\(\\delta=0\\).\n\n## 2 Regularization via geometric singular perturbation theory\n\n### Reduction of existence problem\n\nWe express (1.5) as a dynamical system in the variable \\(x\\) by choosing coordinates \\(U,W=U^{\\prime},H=\\frac{V-U}{\\delta^{2}},Z=\\delta H^{\\prime}\\), obtaining\n\n\\[U^{\\prime} =W\\] \\[W^{\\prime} =-\\frac{1}{d_{1}}\\Gamma(U,W,H,Z)\\] \\[\\delta H^{\\prime} =Z\\] \\[\\delta Z^{\\prime} =H+\\frac{1}{d_{1}}\\Gamma(U,W,H,Z),\\] (2.1) reduces to two algebraic equations coupled to two differential equations. One identifies the following reduced slow manifold comprised of solutions of the algebraic of equations in the singular limit \\(\\delta=0\\), \\[\\mathcal{M}_{0}=\\left\\{(U,W,H,Z)\\,\\mid\\,Z=0,\\,\\,H=-\\frac{cW+W^{2}+ U-U^{2}}{d_{1}+U}\\right\\}. \\tag{2.2}\\]\n\nThe linearization of ( 2.1) at any such fixed point has two zero eigenvalues and two hyperbolic eigenvalues \\(\\pm\\sqrt{1+\\frac{U}{d_{1}}}\\) for \\(U\\geq 0\\). The eigenspaces of the non-zero eigenvalues are traverse to \\(\\mathcal{M}_{0}\\) and therefore the reduced manifold is normally hyperbolic. Fenichel\u2019s Persistence Theorem [5] implies that \\(\\mathcal{M}_{0}\\) persists as an invariant manifold \\(\\mathcal{M}_{\\delta}\\) with the following properties.\n\n**Proposition 2.1** (Reduction for existence problem).: _Fix \\(0<\\underline{d}<\\overline{d}\\), \\(M>1\\), and an integer \\(k\\geq 2\\). There exists a \\(\\overline{\\delta}>0\\) such that all trajectories of (2.1) with \\(|\\delta|<\\overline{\\delta}\\), \\(\\underline{d}<d_{1}<\\overline{d}\\) satisfying \\(-\\frac{d_{1}}{2}<U<M\\) and \\(|W|\\leq M\\) lie in a slow manifold \\(\\mathcal{M}_{\\delta}\\), which is normally hyperbolic and invariant under the flow of (2.1), and may be written as a graph \\(\\mathcal{M}_{\\delta}=\\{(U,W,H,Z):H=\\psi_{H}(U,W;\\delta),Z=\\psi_{Z}(U,W;\\delta)\\}\\), where_\n\n\\[H =\\psi_{H}(U,W;\\delta)=\\psi_{H}^{0}(U,W)+\\delta\\psi_{H}^{1}(U,W)+ \\delta^{2}\\psi_{H}^{2}(U,W)+\\mathrm{O}(\\delta^{4}),\\] \\[Z =\\psi_{Z}(U,W;\\delta)=\\delta\\psi_{Z}^{1}( "]}, {"edit": ["the input data dimension. Figure 3(a) shows the memory size (in MB) of the 4 shallow classifiers considered in our experiments, based on the input size in terms of the percentage of PCA explained variance. As expected, the size of most of the classifiers increases according to the input dimension, except for Random Forest (RF), whose size remains nearly constant at around 1 MB (between 0.98 and 1.11). Logistic Regression (LR), the simplest classifier, is also the one with the lowest memory footprint in all the experiments, starting from less than 1 KB (i.e., 922 Bytes), up to 3.64 KB with 99% of PCA explained variance. On the other hand, AdaBoost (AB) results to be the most demanding model in terms of memory, with an overall size that ranges from just 5.74 MB with 60% of PCA, up to 185.05 MB with the full dimension of the input. Finally, SVM has an intermediate memory footprint among the other classifiers, ranging from 42.6 KB up to 1.88 MB.\n\nOn the other hand, when the deep audio models are fine-tuned, the size of the additional fully-connected layers should be considered to estimate the overall memory footprint. Figure 3(b) shows the average size of the fine-tuned models, highlighting both the size of the original pre-trained models, and the size of the additional layers for classification. We can note that, in general,\n\nFigure 4: Memory footprint of (a) the considered shallow classifiers based on different input sizes (i.e., percentage of PCA explained variance), and (b) the fine-tuned deep learning models. Please, consider that in (a) the overall memory footprint is given by taking into account also the size of the deep embedding models to extract the input features from the raw audio sample.\n\n "], "nougat": ["the input data dimension. Figure 4a shows the memory size (in MB) of the 4 shallow classifiers considered in our experiments, based on the input size in terms of the percentage of PCA explained variance. As expected, the size of most of the classifiers increases according to the input dimension, except for Random Forest (RF), whose size remains nearly constant at around 1 MB (between 0.98 and 1.11). Logistic Regression (LR), the simplest classifier, is also the one with the lowest memory footprint in all the experiments, starting from less than 1 KB (i.e., 922 Bytes), up to 3.64 KB with 99% of PCA explained variance. On the other hand, AdaBoost (AB) results to be the most demanding model in terms of memory, with an overall size that ranges from just 5.74 MB with 60% of PCA, up to 185.05 MB with the full dimension of the input. Finally, SVM has an intermediate memory footprint among the other classifiers, ranging from 42.6 KB up to 1.88 MB.\n\nOn the other hand, when the deep audio models are fine-tuned, the size of the additional fully-connected layers should be considered to estimate the overall memory footprint. Figure 4b shows the average size of the fine-tuned models, highlighting both the size of the original pre-trained models, and the size of the additional layers for classification. We can note that, in general,\n\nFigure 4: Memory footprint of (a) the considered shallow classifiers based on different input sizes (i.e., percentage of PCA explained variance), and (b) the fine-tuned deep learning models. Please, consider that in (a) the overall memory footprint is given by taking into account also the size of the deep embedding models to extract the input features from the raw audio sample.\n\n "]}, {"edit": ["be effective for support vector machines (SVM) [79] and large language models [25]. Another line of work called transductive learning uses test data to add constraints to the margin of SVMs [31, 11, 66]. The principle of transduction, as stated by Vapnik, also emphasizes locality [18, 67]. \"Try to get the answer that you really need but not a more general one.\"\n\nIn computer vision, the idea of training at test time has been well explored for specific applications [30, 57, 46, 73], especially depth estimation [62, 63, 82, 84, 43]. Our paper extends TTT-MAE [19], detailed in Section 3. TTT-MAE, in turn, is inspired by the work Sun et al. [61], which proposed the general framework for test-time training with self-supervision, regardless of application. The particular self-supervised task used in [61] is rotation prediction [21]. Many other papers have followed this framework since then [24, 60, 40, 77], including [69] on videos discussed in Section 1, and [5] which we discuss next.\n\nIn [5], each video is treated as a dataset of unordered frames instead of a stream. In particular, there is no concept of past vs. future frames. The same model is used on the entire video. In contrast, our paper emphasizes locality. We have access to only the current and past frames, and our model keeps learning over time. In addition, all of our results are on real world videos, while [5] experiment on videos with artificial corruptions. These corruptions are also i.i.d. across frames.\n\nOur paper is very much inspired by [45]. To make video segmentation more efficient, [45] makes predictions frame-by-frame using a small student model. If the student is not confident, it queries an expensive teacher model, and then trains the student to fit the prediction from the teacher online. Thanks to temporal smoothness, the student can generalize confidently across many frames without querying the teacher, so learning and predicting combined is still faster than naively using the teacher at every frame. Our method only consists of one model, which learns from a self-supervised task instead of a teacher model. Rather than focusing on computational efficiency as in [45], the main goal of our paper is to improve inference quality. Behind their particular algorithm, however, we see the shared idea of locality, regardless of the form of supervision.\n\n## 3 Background: TTT-MAE\n\nOur paper extends the work of _Test-Time Training with Masked Autoencoders_ (TTT-MAE) [19], and uses TTT-MAE as the inner loop when updating the model for each frame. This section briefly describes TTT-MAE, as background for our extension. Figure 3 illustrates the process of TTT-MAE.\n\nThe architecture for TTT with self-supervision [61] is Y-shaped with a stem and two heads: a prediction head \\(g\\) for the self-supervised task, a prediction head \\(h\\) for the main task, and a feature extractor \\(f\\) as the stem. The output features of \\(f\\) are shared between \\(g\\) and \\(h\\) as input. For TTT-MAE, the self-supervised task is masked image reconstruction [27]. Following standard terminology for autoencoders, \\(f\\) is also called the encoder, and \\(g\\) the decoder.\n\nEach input image \\(x\\) is first split into many non-overlapping patches. To produce the autoencoder input \\(\\tilde{x}\\), we mask out majority, e.g. 80%, of the patches in \\(x\\) at random. The self-supervised objective \\(\\ell_{s}(g\\circ f(\\tilde{x}),x)\\) compares the reconstructed patches from \\(g\\circ f(\\tilde{x})\\) to the masked patches in \\(x\\), and computes the pixel-wise mean squared error. For the main task, e.g. segmentation, all patches in the original \\(x\\) are given as input to \\(h\\circ f\\), during both training and testing.\n "], "nougat": ["be effective for support vector machines (SVM) [ 79 ] and large language models [ 25 ]. Another line of work called transductive learning uses test data to add constraints to the margin of SVMs [ 31 , 11 , 66 ]. The principle of transduction, as stated by Vapnik, also emphasizes locality [ 18 , 67 ]: \"Try to get the answer that you really need but not a more general one.\"\n\nIn computer vision, the idea of training at test time has been well explored for specific applications [ 30 , 57 , 46 , 73 ], especially depth estimation [ 62 , 63 , 82 , 84 , 43 ]. Our paper extends TTT-MAE [ 19 ], detailed in Section 3 . TTT-MAE, in turn, is inspired by the work Sun et al. [ 61 ], which proposed the general framework for test-time training with self-supervision, regardless of application. The particular self-supervised task used in [ 61 ] is rotation prediction [ 21 ]. Many other papers have followed this framework since then [ 24 , 60 , 40 , 77 ], including [ 69 ] on videos discussed in Section 1 , and [ 5 ] which we discuss next.\n\nIn [5 ] experiment on videos with artificial corruptions. These corruptions are also i.i.d. across frames.\n\nOur paper is very much inspired by [ 45 ]. To make video segmentation more e ffi cient, [ 45 ] makes predictions frame-by-frame using a small student model. If the student is not confident, it queries an expensive teacher model, and then trains the student to fit the prediction from the teacher online. Thanks to temporal smoothness, the student can generalize confidently across many frames without querying the teacher, so learning and predicting combined is still faster than naively using the teacher at every frame. Our method only consists of one model, which learns from a self-supervised task instead of a teacher model. Rather than focusing on computational e ffi ciency as in [ 45 ], the main goal of our paper is to improve inference quality. Behind their particular algorithm, however, we see the shared idea of locality, regardless of the form of supervision.\n\n## 3 Background: TTT-MAE\n\nOur paper extends the work of _Test-Time Training with Masked Autoencoders_ (TTT-MAE) [ 19 ], and uses TTT-MAE as the inner loop when updating the model for each frame. This section briefly describes TTT-MAE, as background for our extension. Figure 3 illustrates the process of TTT-MAE.\n\nThe architecture for TTT with self-supervision [ 61 ] is Y-shaped with a stem and two heads: a prediction head \\(g\\) for the self-supervised task, a prediction head \\(h\\) for the main task, and a feature extractor \\(f\\) as the stem. The output features of \\(f\\) are shared between \\(g\\) and \\(h\\) as input. For TTT-MAE, the self-supervised task is masked image reconstruction [ 27 ]. Following standard terminology for autoencoders, \\(f\\) is also called the encoder, and \\(g\\) the decoder.\n\nEach input image \\(x\\) is first split into many non-overlapping patches. To produce the autoencoder input \\(\\tilde{x}\\), we mask out majority, e.g. 80%, of the patches in \\(x\\) at random. The self-supervised objective \\(\\ell_{s}(g\\circ f(\\tilde{x}),x)\\) compares the reconstructed patches from \\(g\\circ f(\\tilde{x})\\) to the masked patches in \\(x\\), and computes the pixel-wise mean squared error. For the main task, e.g. segmentation, all patches in the original \\(x\\) are given as input to \\(h\\circ f\\), during both training and testing.\n\n### Training-Time Training\n\nThere are three widely accepted ways to optimize the model components (\\(f,g,h\\)) at training "]}, {"edit": ["the temperature feature to evaluate its impact on the clustering algorithm's efficiency in identifying locations of interest.\n\n#### 3.3.1. Obtaining Historical Temperature Data\n\nHowever, such temperature analysis is only feasible when temperature data is available. Certain studies concerning elephant movement (Tsalyuk et al. [23]; Wall et al. [24]) lack a temperature feature. Therefore, we explored methods to approximate temperature data from other data sources. Using the meteostat python package and API, we identified weather stations proximate to the study site. The historical data was queried and appended to the study data, enabling calculation of Temperature-influenced centroids that would have been impossible to calculate otherwise.\n\nThe procedure entailed three key steps: (1) Identifying a nearby weather station, (2) Matching timestamps with the queried data, and (3) Evaluating the capability of the appended historical temperature data in calculating temperature-influenced centroids.\n\nFor the first step, the median latitude and longitude of the given elephant's movement data was computed, which was then used to query a nearby station. The second step involved normalizing and interpolating the time series data from the station, provided by meteostat, to ensure a higher temporal granularity that matches the given data. In the third step, the correlation between the historical station data and Kruger temperature data was evaluated using the coefficient of determination, R-squared.\n\nOur results indicate a moderate correlation between the study data and the station data. This correlation, combined with the performance of the Temperature-influenced centroids with the weather data, gives us confidence to extend this technique to datasets that lack temperature data. Based on our experiment with elephant AM306 (See Figure 1) from the Kruger dataset, we found that the Temperature-influenced feature space aided in revealing more nuanced locations of interest within the larger clusters identified by the Without Temperature influence feature space.\n\n#### 3.3.2. Fuzzy Timestamp Matching\n\nFuzzy timestamp matching is an advanced data processing technique that matches timestamps not based on exact equality but within a certain tolerance level. This tolerance level, or fuzzy threshold, is usually calculated by taking half of the median of the difference of timestamps in the dataset. The mathematical representation of the fuzzy timestamp matching process could be described as follows:\n\nGiven two timestamps, \\(t1\\) and \\(t2\\), and a tolerance level \\(\\delta\\), the timestamps \\(t1\\) and \\(t2\\) are said to match if:\n\n\\[|t1-t2|\\leqslant\\delta \\tag{2}\\]\n\nwhere \\(|t1-t2|\\) denotes the absolute difference between the timestamps \\(t1\\) and \\(t2\\). In this case, \\(\\delta\\) is calculated as:\n\n\\[\\delta=0.5*median(|t[i+1]-t[i]|),\\forall i~{}1~{}to~{}N-1 \\tag{3}\\]\n\nwhere N is the total number of timestamps, and t[i] represents the ith timestamp in the ordered sequence. This fuzzy matching approach increases the likelihood of matches and can help to mitigate data loss when aligning data from different sources or with different temporal resolutions. However, it is important to note that this technique may also introduce some uncertainty into the analysis due to the mismatched timestamps. Hence, an appropriate balance between data retention and accuracy should be maintained while deciding the value of \\(\\delta\\).\n\nThe integration of weather station temperature data with animal movement datasets presented a significant challenge due to the relatively low percentage of matching timestamps. For instance, in the case of AM189 from Etosha, a mere 19.662% of timestamps corresponded. This limited overlap signifies a considerable loss of data, which undermines the analysis. To address this issue, we utilized \"fuzzy\" timestamp matching. This method extends the criteria of a match beyond exact timestamp equality, incorporating a pre-defined threshold for the discrepancy between two timestamps that still qualifies them as a match. The mathematical formulation of this concept is as follows: Given two timestamps t1 and t2, and a tolerance level (or fuzzy threshold) \\(\\delta\\), the timestamps t1 and t2 are said to match if the absolute difference between them, denoted as [t1 - t2], does not exceed \\(\\delta\\). The fuzzy threshold \\(\\delta\\) is calculated as half the median of the differences between all sequential pairs of timestamps in the dataset.\n\nBy employing fuzzy timestamp matching, the "], "nougat": ["the temperature feature to evaluate its impact on the clustering algorithm\u2019s efficiency in identifying locations of interest.\n\n#### 3.3.1. Obtaining Historical Temperature Data\n\nHowever, such temperature analysis is only feasible when temperature data is available. Certain studies concerning elephant movement (Tsalyuk et al. [ 23 ]; Wall et al. [ 24 ]) lack a temperature feature. Therefore, we explored methods to approximate temperature data from other data sources. Using the meteostat python package and API, we identified weather stations proximate to the study site. The historical data was queried and appended to the study data, enabling calculation of Temperature-influenced centroids that would have been impossible to calculate otherwise.\n\nThe procedure entailed three key steps: (1) Identifying a nearby weather station, (2) Matching timestamps with the queried data, and (3) Evaluating the capability of the appended historical temperature data in calculating temperature-influenced centroids.\n\nFor the first step, the median latitude and longitude of the given elephant\u2019s movement data was computed, which was then used to query a nearby station. The second step involved normalizing and interpolating the time series data from the station, provided by meteostat, to ensure a higher temporal granularity that matches the given data. In the third step, the correlation between the historical station data and Kruger temperature data was evaluated using the coefficient of determination, R-squared.\n\nOur results indicate a moderate correlation between the study data and the station data. This correlation, combined with the performance of the Temperature-influenced centroids with the weather data, gives us confidence to extend this technique to datasets that lack temperature data. Based on our experiment with elephant AM306 (See Figure 1) from the Kruger dataset, we found that the Temperature-influenced feature space aided in revealing more nuanced locations of interest within the larger clusters identified by the Without Temperature influence feature space.\n\n#### 3.3.2. Fuzzy Timestamp Matching\n\nFuzzy timestamp matching is an advanced data processing technique that matches timestamps not based on exact equality but within a certain tolerance level. This tolerance level, or fuzzy threshold, is usually calculated by taking half of the median of the difference of timestamps in the dataset. The mathematical representation of the fuzzy timestamp matching process could be described as follows: Given two timestamps, \\(t1\\) and \\(t2\\), and a tolerance level \\(\\delta\\), the timestamps \\(t1\\) and \\(t2\\) are said to match if:\n\n\\[|t1-t2|\\leqslant\\delta \\tag{2}\\]\n\nwhere \\(|t1-t2|\\) denotes the absolute difference between the timestamps \\(t1\\) and \\(t2\\). In this case, \\(\\delta\\) is calculated as:\n\n\\[\\delta=0.5*median(|t[i+1]-t[i] represents the ith timestamp in the ordered sequence. This fuzzy matching approach increases the likelihood of matches and can help to mitigate data loss when aligning data from different sources or with different temporal resolutions. However, it is important to note that this technique may also introduce some uncertainty into the analysis due to the mismatched timestamps. Hence, an appropriate balance between data retention and accuracy should be maintained while deciding the value of \\(\\delta\\).\n\nThe integration of weather station temperature data with animal movement datasets presented a significant challenge due to the relatively low percentage of matching timestamps. For instance, in the case of AM189 from Etosha, a mere 19.662% of timestamps corresponded. This limited overlap signifies a considerable loss of data, which undermines the analysis. To address this issue, we utilized \"fuzzy\" timestamp matching. This method extends the criteria of a match beyond exact timestamp equality, incorporating a pre-defined threshold for the discrepancy between two timestamps that still qualifies them as a match. The mathematical formulation of this concept is as follows: Given two timestamps t1 and t2, and a tolerance level (or fuzzy threshold) \\(\\delta\\), the timestamps t1 and t2 are said to match if the absolute difference between them, denoted as [t1 - t2], does not exceed \\(\\delta\\). The fuzzy threshold \\(\\delta\\) is calculated as half the median of the differences between all sequential pairs of timestamps in the dataset.\n\nBy employing fuzzy timestamp matching, the percentage of matched data can be substantially increased. For example, in the case of AG191 from Etosha, conventional timestamp matching resulted in a match percentage of 41.85% "]}, {"edit": ["marking a quantity evaluated at the stationary expansion point \\((U^{\\circ},\\overrightarrow{r}^{\\otimes})\\) as \\(X^{\\otimes}\\). We will use this notation throughout this work.6 Equation 7 yields an alternative formulation of Equation 4 using the exchanged second mixed derivative (Eq. 1).\n\nFootnote 6: As a further clarification on the difference between \\(X^{*}\\) and \\(X^{\\otimes}\\): \\(X^{*}(U)\\) essentially is a shortcut for writing \\(X(U,\\overrightarrow{r}^{*}(U))\\), i.e. for denoting quantities evaluated at the stationary point at the desired potential. The additional circle in \\(X^{\\otimes}\\) indicates a property evaluated at a stationary expansion point. Essentially, we derive quantities \\(Y^{*}\\) at a desired potential \\(U\\) based on quantities \\(X^{\\otimes}\\) evaluated at a stationary expansion point \\((U^{\\circ},\\overrightarrow{r}^{*}(U^{\\circ}))\\), i.e. at a different potential \\(U^{\\circ}\\).\n\nGrand canonical energy of a stationary pointWe can now insert the just derived potential-dependent geometric shift of a stationary point \\(\\overrightarrow{\\Delta r}^{*}(\\Delta U)\\) (Eq. 7) into the second-order expansion of the gcPES (Eq. 6) around a known stationary point \\((U^{\\circ},\\overrightarrow{r}^{\\otimes})\\), eliminating the spatial dependence and returning the potential-dependent grand canonical energy \\(\\mathscr{E}^{*}(U)\\) of a stationary point accurate to second order:\n\n\\[\\mathscr{E}^{*}(U)= \\mathscr{E}(U^{\\circ}+\\Delta U,\\overrightarrow{r}^{\\otimes}+ \\overrightarrow{\\Delta r}^{*}(\\Delta U))\\] \\[= \\mathscr{E}(U^{\\circ},\\overrightarrow{r}^{\\otimes})-q^{\\otimes}\\Delta U\\] \\[+ \\frac{1}{2}\\left(\\sum_{i,j,k,l}(\\mathscr{H}^{\\otimes-1}_{j,k}( \\tfrac{\\partial q}{\\partial r_{k}})^{\\otimes}\\Delta U)\\mathscr{H}^{\\otimes}_{ i,j}(\\mathscr{H}^{\\otimes-1}_{i,l}(\\tfrac{\\partial q}{\\partial r_{l}})^{\\otimes} \\Delta U)\\right.\\] \\[\\left.-2\\sum_{i,j}(\\tfrac{\\partial q}{\\partial r_{l}})^{\\otimes}( \\mathscr{H}^{\\otimes-1}_{i,j}(\\tfrac{\\partial q}{\\partial r_{j}})^{\\otimes} \\Delta U)\\Delta U-C^{\\otimes}_{\\mathrm{el}}\\Delta U^{2}\\right)+\\mathscr{O}( \\Delta U^{3})\\quad,\\]\n\nwhere we dropped the force-contribution, since \\(\\mathscr{F}^{\\otimes}_{i}=0\\). Rearranging and using \\(\\sum_{i}\\mathscr{H}^{\\otimes-1}_{k,i}\\mathscr{H}^{\\otimes}_{i,j}=\\delta_{k,j}\\) then yields the energy \\(\\mathscr{E}^{*}\\) of the stationary point at potential \\(U=U^{\\circ}+\\Delta U\\):\n\n\\[\\mathscr{E}^{*}(U)= \\mathscr{E}^{\\otimes}-q^{\\otimes}\\Delta U-\\frac{1}{2}\\underbrace{ (C^{\\otimes}_{\\mathrm{ "], "nougat": ["marking a quantity evaluated at the stationary expansion point \\((U^{\\circ},\\overrightarrow{r}^{\\otimes})\\) as \\(X^{\\otimes}\\). We will use this notation throughout this work.6 Equation 7 yields an alternative formulation of Equation 4 using the exchanged second mixed derivative (Eq. 1).\n\nFootnote 6: As a further clarification on the difference between \\(X^{*}\\) and \\(X^{\\otimes}\\): \\(X^{*}(U)\\) essentially is a shortcut for writing \\(X(U,\\overrightarrow{r}^{*}(U))\\), i.e. for denoting quantities evaluated at the stationary point at the desired potential. The additional circle in \\(X^{\\otimes}\\) indicates a property evaluated at a stationary expansion point. Essentially, we derive quantities \\(Y^{*}\\) at a desired potential \\(U\\) based on quantities \\(X^{\\otimes}\\) evaluated at a stationary expansion point \\((U^{\\circ},\\overrightarrow{r}^{*}(U^{\\circ}))\\), i.e. at a different potential \\(U^{\\circ}\\).\n\nGrand canonical energy of a stationary pointWe can now insert the just derived potential-dependent geometric shift of a stationary point \\(\\overrightarrow{\\Delta r}^{*}(\\Delta U)\\) (Eq. 7) into the second-order expansion of the gcPES (Eq. 6) around a known stationary point \\((U^{\\circ},\\overrightarrow{r}^{\\otimes})\\), eliminating the spatial dependence and returning the potential-dependent grand canonical energy \\(\\mathscr{E}^{*}(U)\\) of a stationary point accurate to second order:\n\n\\[\\mathscr{E}^{*}(U)= \\mathscr{E}(U^{\\circ}+\\Delta U,\\overrightarrow{r}^{\\otimes}+ \\overrightarrow{\\Delta r}^{*}(\\Delta U))\\] \\[= \\mathscr{E}(U^{\\circ},\\overrightarrow{r}^{\\otimes})-q^{\\otimes}\\Delta U\\] \\[+ \\frac{1}{2}\\left(\\sum_{i,j,k,l}(\\mathscr{H}^{\\otimes-1}_{j,k}( \\tfrac{\\partial q}{\\partial r_{k}})^{\\otimes}\\Delta U)\\mathscr{H}^{\\otimes}_{ i,j}(\\mathscr{H}^{\\otimes-1}_{i,l}(\\tfrac{\\partial q}{\\partial r_{l}})^{\\otimes} \\Delta U)\\right.\\] \\[\\left.-2\\sum_{i,j}(\\tfrac{\\partial q}{\\partial r_{l}})^{\\otimes}( \\mathscr{H}^{\\otimes-1}_{i,j}(\\tfrac{\\partial q}{\\partial r_{j}})^{\\otimes} \\Delta U)\\Delta U-C^{\\otimes}_{\\mathrm{el}}\\Delta U^{2}\\right)+\\mathscr{O}( \\Delta U^{3})\\quad,\\]\n\nwhere we dropped the force-contribution, since \\(\\mathscr{F}^{\\otimes}_{i}=0\\). Rearranging and using \\(\\sum_{i}\\mathscr{H}^{\\otimes-1}_{k,i}\\mathscr{H}^{\\otimes}_{i,j}=\\delta_{k,j}\\) then yields the energy \\(\\mathscr{E}^{*}\\) of the stationary point at potential \\(U=U^{\\circ}+\\Delta U\\):\n\n\\[\\mathscr{E}^{*}(U)= \\mathscr{E}^{\\otimes}-q^{\\otimes}\\Delta U-\\frac{1}{2}\\underbrace{ (C^{\\otimes}_{\\mathrm{ "]}, {"edit": ["and scales like \\(m_{\\nu}^{4}\\) such that the dynamical friction effect is dominated by the most massive neutrino eigenstate. We may then assume a single neutrino species for simplicity, or explicitly write the total dynamical friction force as a sum of individual contribution from different eigenstates.\n\nIn order to connect Eq. (24) with the results of future sections, it is convenient to rewrite it in terms of a quantity with dimension of inverse time. Since \\(F=Mdv_{\\rm H}/dt\\), we can define:\n\n\\[\\tau^{-1}=-\\frac{\\vec{F}\\cdot\\vec{v}_{\\rm H}}{Mv_{\\rm H}^{2}}=\\frac{2}{3\\pi} \\log\\Lambda G^{2}Mm_{\\nu}^{4}=3.4\\times 10^{-5}\\frac{\\log\\Lambda}{\\log 100} \\frac{M}{10^{13}M_{\\bigodot}}\\left(\\frac{m_{\\nu}}{0.1\\rm eV}\\right)^{4}H_{0}\\,, \\tag{25}\\]\n\nwhich is the characteristic time scale for an order one fractional decrease in the halo velocity due to the dynamical friction effect. Note that \\(1/\\tau H_{0}=\\Delta v/v\\) is the overall relative decrease in the halo velocity over the age of the Universe \\(t\\sim 1/H_{0}\\). We obtain a numerical value of \\(\\Delta v/v=3.4\\times 10^{-5}\\) for a halo mass \\(M=10^{13}M_{\\bigodot}\\) and individual neutrino mass \\(m_{\\nu}=0.1\\rm eV\\), when also assuming \\(\\Lambda=100\\). This already suggests that the dynamical friction effect is quite small, although it can pick up some significant contributions from the clustering of nearby halos as we will see in Sec. 5.\n\n### Limitations to the 1-halo approach\n\nThus far we have determined the anisotropic clustering of massive neutrinos behind moving point mass halos and the corresponding dynamical friction force. A more realistic calculation would have to account for both the finite extent of the halo and the presence of large-scale structure. Indeed, the Eq. (25) involves an unknown Coulomb logarithm, \\(\\log\\Lambda\\), where in typical applications of the dynamical friction formula the cutoff \\(\\Lambda\\) can be estimated as the ratio of maximum and minimum impact parameters, \\(\\Lambda\\sim b_{\\rm max}/b_{\\rm min}\\)[51]. Here \\(b_{\\rm min}\\sim R_{\\rm halo}\\) is the halo radius, and \\(b_{\\rm max}\\sim\\lambda_{\\rm coh}\\sim 0.1\\) Mpc\\({}^{-1}\\) is the CDM velocity coherence scale. The CDM bulk flow is only coherent over sufficiently small scales and hence our analysis based on a single moving halo is expected to break down at scales \\(\\lambda\\gtrsim\\lambda_{\\rm coh}\\).3 This point will be made more clear in the next section, where we also provide a precise definition for the velocity coherence scale.\n\nFigure 2: Suppression of Chandrasekhar\u2019s formula for the dynamical friction force \\(F/F_{\\Lambda\\to\\infty}\\) for finite values of the cutoff \\(\\Lambda\\) in the simple case of a halo subject to Hubble drag. The solid black curve corresponds to a choice of halo formation redshift of \\(z_{\\rm i}=1\\), and the dashed blue curve to \\(z_{\\rm i}=2\\). In realistic applications we might expect \\(\\Lambda\\approx\\lambda_{\\rm coh}/R_{\\rm halo}\\approx 10-1000\\) as we will see in Sec. 5.\n\n "], "nougat": ["and scales like \\(m_{\\nu}^{4}\\) such that the dynamical friction effect is dominated by the most massive neutrino eigenstate. We may then assume a single neutrino species for simplicity, or explicitly write the total dynamical friction force as a sum of individual contribution from different eigenstates.\n\nIn order to connect Eq. ( 3.19 ) with the results of future sections, it is convenient to rewrite it in terms of a quantity with dimension of inverse time. Since \\(F=Mdv_{\\rm H}/dt\\), we can define:\n\n\\[\\tau^{-1}=-\\frac{\\vec{F}\\cdot\\vec{v}_{\\rm H}}{Mv_{\\rm H}^{2}}=\\frac{2}{3\\pi} \\log\\Lambda G^{2}Mm_{\\nu}^{4}=3.4\\times 10^{-5}\\frac{\\log\\Lambda}{\\log 100} \\frac{M}{10^{13}M_{\\bigodot}}\\left(\\frac{m_{\\nu}}{0.1\\rm eV}\\right)^{4}H_{0}\\,, \\tag{3.20}\\]\n\nwhich is the characteristic time scale for an order one fractional decrease in the halo velocity due to the dynamical friction effect. Note that \\(1/\\tau H_{0}=\\Delta v/v\\) is the overall relative decrease in the halo velocity over the age of the Universe \\(t\\sim 1/H_{0}\\). We obtain a numerical value of \\(\\Delta v/v=3.4\\times 10^{-5}\\) for a halo mass \\(M=10^{13}M_{\\bigodot}\\) and individual neutrino mass \\(m_{\\nu}=0.1\\rm eV\\), when also assuming \\(\\Lambda=100\\). This already suggests that the dynamical friction effect is quite small, although it can pick up some significant contributions from the clustering of nearby halos as we will see in Sec. 5 .\n\n### Limitations to the 1-halo approach\n\nThus far we have determined the anisotropic clustering of massive neutrinos behind moving point mass halos and the corresponding dynamical friction force. A more realistic calculation would have to account for both the finite extent of the halo and the presence of large-scale structure. Indeed, the Eq. ( 3.20 ) involves an unknown Coulomb logarithm, \\(\\log\\Lambda\\), where in typical applications of the dynamical friction formula the cutoff \\(\\Lambda\\) can be estimated as the ratio of maximum and minimum impact parameters, \\(\\Lambda\\sim b_{\\rm max}/b_{\\rm min}\\)[51]. Here \\(b_{\\rm min}\\sim R_{\\rm halo}\\) is the halo radius, and \\(b_{\\rm max}\\sim\\lambda_{\\rm coh}\\sim 0.1\\) Mpc\\({}^{-1}\\) is the CDM velocity coherence scale. The CDM bulk flow is only coherent over sufficiently small scales and hence our analysis based on a single moving halo is expected to break down at scales \\(\\lambda\\gtrsim\\lambda_{\\rm coh}\\).3 This point will be made more clear in the next section, where we also provide a precise definition for the velocity coherence scale.\n\nFigure 2: Suppression of Chandrasekhar\u2019s formula for the dynamical friction force \\(F/F_{\\Lambda\\rightarrow\\infty}\\) for finite values of the cutoff \\(\\Lambda\\) in the simple case of a halo subject to Hubble drag. The solid black curve corresponds to a choice of halo formation redshift of \\(z_{\\rm i}=1\\), and the dashed blue curve to \\(z_{\\rm i}=2\\). In realistic applications we might expect \\(\\Lambda\\approx\\lambda_{\\rm coh}/R_{\\rm halo}\\approx 10-1000\\) as we will see in Sec. 5.\n\n "]}, {"edit": ["\n\n# Branches Mutual Promotion for End-to-End Weakly Supervised Semantic Segmentation\n\nLei Zhu, Hangzhou He, Xinliang Zhang, Qian Chen, Shuang Zeng, Qiushi Ren, Yanye Lu*\n\n###### Abstract\n\nEnd-to-end weakly supervised semantic segmentation aims at optimizing a segmentation model in a single-stage training process based on only image annotations. Existing methods adopt an online-trained classification branch to provide pseudo annotations for supervising the segmentation branch. However, this strategy makes the classification branch dominate the whole concurrent training process, hindering these two branches from assisting each other. In our work, we treat these two branches equally by viewing them as diverse ways to generate the segmentation map, and add interactions on both their supervision and operation to achieve mutual promotion. For this purpose, a bidirectional supervision mechanism is elaborated to force the consistency between the outputs of these two branches. Thus, the segmentation branch can also give feedback to the classification branch to enhance the quality of localization seeds. Moreover, our method also designs interaction operations between these two branches to exchange their knowledge to assist each other. Experiments indicate our work outperforms existing end-to-end weakly supervised segmentation methods.\n\n Weakly Supervised Learning, Image Segmentation, Object Localization\n\n## I Introduction\n\nSemantic segmentation is a primary vision task, aiming to annotate pixels in an image as target objects or backgrounds. However, training a segmentation model in a fully-supervised manner requires annotating all pixels in training images, costing extensive human resources. To solve this problem, weakly supervised semantic segmentation (WSSS) appears and attracts extensive attention, which adopts only image-level annotation for the training process. However, as shown in Fig. 1 **A**, WSSS methods usually require multiple training stages, _e.g._, tuning a classification network with image annotations to produce localization seeds [1, 2, 3], deriving pseudo annotations after refining the seeds [4, 5, 6], and finally training the segmentation network with the pseudo annotations [7, 8].\n\nRecently, some end-to-end weakly supervised semantic segmentation (E2E-WSSS) methods arose to simplify the heavy multi-stage training process into a single stage [9, 10, 11, 12]. As shown in Fig. 1 **B**, these methods train a two-branch network in only a single stage, where the classification branch supervised by image-level annotation can online provide pseudo annotations for the segmentation branch. Compared with multi-stage WSSS, the concurrently-trained classification branch cannot stably provide seed to derive accurate pseudo annotations for supervising the segmentation branch. So, existing E2E-WSSS methods focus on improving the classification branch to provide better supervision by refining the localization seed with online spatial propagation [11, 12, 13] or determining reliable regions on the pseudo annotations [9, 10].\n\nIn our work, we argue that current E2E-WSSS methods may fall into a trap, following the multi-stage WSSS to unidirectionally supervise the segmentation branch based on the prediction of the classification branch, without considering the feedback of the segmentation branch. In this way, the classification branch will dominate the whole training process, even if it may perform worse than the segmentation branch, as visualized in Fig. 2. Thus, the classification branch will converge to a similar optimum as the offline trained classification network but cannot stably provide pseudo annotations for the segmentation branch, which causes the large performance gap between current E2E-WSSS and multi-stage WSSS methods.\n\nActually, in the E2E-WSSS setting, these two branches are basically at equal status because they are concurrently optimized during training. From another perspective, the segmentation branch can also assist the concurrently-trained classification branch in generating better localization seeds, which is a crucial trait of E2E-WSSS and yet to be explored by existing methods. Based on this perspective, our work treats these two branches equally by viewing them as diverse ways to achieve the same goal, generating the segmentation map of input images. Thus, as shown in Fig. 1 **C**, interactions are\n\nFig. 1: Comparison of WSSS strategies: **A**. Multi-stage WSSS contains multiple training stages. **B**. Existing E2E-WSSS unidirectionally supervise the segmentation branch with pseudo annotations online provided by the classification branch. **C**. Our proposed E2E-WSSS strategy interacts both the supervision and operation between these two branches to achieve mutual promotion.\n\n"], "nougat": ["\n\n# Branches Mutual Promotion for End-to-End Weakly Supervised Semantic Segmentation\n\nLei Zhu, Hangzhou He, Xinliang Zhang, Qian Chen, Shuang Zeng, Qiushi Ren, Yanye Lu*\n\n###### Abstract\n\nEnd-to-end weakly supervised semantic segmen-tation aims at optimizing a segmentation model in a single-stage training process based on only image annotations. Existing methods adopt an online-trained classification branch to provide pseudo annotations for supervising the segmentation branch. However, this strategy makes the classification branch dominate the whole concurrent training process, hindering these two branches from assisting each other. In our work, we treat these two branches equally by viewing them as diverse ways to generate the segmentation map, and add interactions on both their supervision and operation to achieve mutual promotion. For this purpose, a bidirectional supervision mechanism is elaborated to force the consistency between the outputs of these two branches. Thus, the segmentation branch can also give feedback to the classification branch to enhance the quality of localization seeds. Moreover, our method also designs interaction operations between these two branches to exchange their knowledge to assist each other. Experiments indicate our work outperforms existing end-to-end weakly supervised segmentation methods.\n\n Weakly Supervised Learning, Image Segmenta-tion, Object Localization\n\n## I Introduction\n\nSemantic segmentation is a primary vision task, aiming to annotate pixels in an image as target objects or backgrounds. However, training a segmentation model in a fullysupervised manner requires annotating all pixels in training images, costing extensive human resources. To solve this problem, weakly supervised semantic segmentation (WSSS) appears and attracts extensive attention, which adopts only image-level annotation for the training process. However, as shown in Fig. 1 **A**, WSSS methods usually require multiple training stages, _e.g._, tuning a classification network with image annotations to produce localization seeds [1]\u2013[3], deriving pseudo annotations after refining the seeds [4]\u2013[6], and finally training the segmentation network with the pseudo annotations [7], [8].\n\nRecently, some end-to-end weakly supervised semantic segmentation (E2E-WSSS) methods arose to simplify the heavy multi-stage training process into a single stage [9]-[12]. As shown in Fig. 1 **B**, these methods train a two-branch network in only a single stage, where the classification branch supervised by image-level annotation can online provide pseudo annotations for the segmentation branch. Compared with multi-stage WSSS strategies: the concurrently-trained classification branch cannot stably provide seed to derive accurate pseudo annotations for supervising the segmentation branch. So, existing E2E-WSSS methods focus on improving the classification branch to provide better supervision by refining the localization seed with online spatial propagation [11]-[13] or determining reliable regions on the pseudo annotations [9], [10].\n\nIn our work, we argue that current E2E-WSSS methods may fall into a trap, following the multi-stage WSSS contains multiple training stages. In this paper, we propose a multi-stage WSSS methods: the segmentation branch based on the prediction of the classification branch, without considering the feedback of the segmentation branch. In this way, the classification branch will dominate the whole training process, even if it may perform worse than the segmentation branch, as visualized in Fig. 2. Thus, the classification branch will converge to a similar optimum as the offline trained classification network but cannot stably provide pseudo annotations for the segmentation branch, which causes the large performance gap between current E2E-WSSS unidirectionally supervises methods.\n\nActually, in the E2E-WSSS) methods arose to simplify the heavy multi-stage training process into a single stage [9]\u2013 [12]. As shown in Fig. 1 **B**, these methods train a twobranch network in only a single stage, where the classification branch supervised by image-level annotation can online provide pseudo annotations for the segmentation branch. Compared with multi-stage WSSS, the concurrently-trained classification branch cannot stably provide seed to derive accurate pseudo annotations for supervising the segmentation branch. So, existing E2E-WSSS methods focus on improving the classification branch to provide better supervision by refining the localization seed with online spatial propagation [11]\u2013[13].\n\nFig. 1: Comparison of WSSS"]}, {"edit": ["shown that the dependence of mining rewards on propagation latency is more intricate than this [35]. Specifically, an honest miner that is well connected with other miners inadvertently creates efficient, low latency paths for other miners by acting as a centrally located bridge between the miners. However, to maximize the marginal gains in reward due to the network, it is important for a miner to have paths to other miners that are, on average, of a lower delay _relative_ to the delays of paths between other miners. For example, if miners are arranged as a star topology with links of unit delay and uniform compute power across nodes, the central node receives a higher reward compared to the leaf nodes by including more blocks on the blockchain. On the other hand, on a complete graph topology with unit delay links and uniform compute power as before, all nodes receive the same reward. A node identically connected to other nodes in the two cases (i.e., the central node in the star topology and any arbitrary node in the complete graph topology: both have direct links to all other nodes) receives different rewards, as rewards depend not only on the node's own connections but also on how other nodes' connections. Thus, there is an inherent tension for a miner in increasing her own connectivity to the rest of the network while simultaneously ensuring that the connectivity between other miners do not significantly increase. A systematic research of this tension, and efficient connection policies to maximize marginal mining reward gain due to the network, have not been done to our best knowledge.\n\nIn this work, we formalize the p2p topology construction problem as a game between miners and present Cobalt, a decentralized policy for optimizing reward. We consider a simplified setting where only a single node chooses its connections, while the rest of the network's topology is fixed. We assume that the global topology of the p2p network is unknown to miners. We thus model the problem of optimizing rewards by the connections-deciding miner node as a Markov decision process (MDP) with no state and an action set with a combinatorial number of actions.\n\nWe derive the optimal neighbor selection policy using a combinatorial multi-armed bandit (MAB) approach [14]. In the MAB algorithm, the agent (miner) explores various candidate connection configurations, and gradually adapts its connections based on past experience to gain the most mining rewards. A key contribution of our work is a network coordinates based model for efficiently learning the MAB environment [19]. In this model, miners are assigned real-valued vectors from an Euclidean space, which capture the relative location of miners with respect to each other in the network. The coodinates are continuously updated based upon the reward feedback the agent receives from the environment. Thus, despite not having global knowledge of the network initially, we show that it is possible for an agent to learn about the network by just using the observed reward information.\n\nTo enable the deployment of MAB algorithm, we have built a simulator. To simplify the reward computation in the simulator, rather than simulating the actual mining process at each step of the MDP, we consider a computationally easier function that only depends on the pairwise shortest path lengths between miners. Importantly, our MDP reward function captures the property that a miner's mining gains depends on how small the shortest path lengths between the agent and other miners are relative to the shortest path lengths between other miners. Experimentally we show Cobalt outperforms or matches heuristics on diverse network settings.\n\n## II Related Work\n\nP2P network design for optimizing mining rewards has remained a relatively under-explored topic in the community. The work that is closest to our is Perigee [34] which proposes an adaptive peer-selection algorithm for minimizing block propagation latency in the network. However, Perigee does not model the game-theoretic competition between miners. Subsequent works [43, 11] consider optimizing the network to maximize extractable value (MEV) from transactions. A number of prior works have exposed the impact of the network on mining [26, 28, 37, 40, 47, 48, 12]. While these works generally suggest that better network connectivity translates to higher mining rewards earned, the competitive effects of network connectivity and methods to optimize them have not been discussed. Other related works include KadCast [38] which proposes a Kadmila-based structured overlay for efficient block broadcast, and relay networks such as BloXroute [29] for transports blocks quickly across vast geographic distances.\n\nThe idea of network coordinates for p2p networks has been prominently explored in the network systems literature since the turn of the millenium, including distributed approaches to learn them [19, 32, 36]. More recently, a number of theoretical works have studied using low-distortion embeddings in finite metrics (i.e.\n\n "], "nougat": ["shown that the dependence of mining rewards on propagation latency is more intricate than this [35]. Specifically, an honest miner that is well connected with other miners inadvertently creates efficient, low latency paths for other miners by acting as a centrally located bridge between the miners. However, to maximize the marginal gains in reward due to the network, it is important for a miner to have paths to other miners that are, on average, of a lower delay _relative_ to the delays of paths between other miners. For example, if miners are arranged as a star topology with links of unit delay and uniform compute power across nodes, the central node receives a higher reward compared to the leaf nodes by including more blocks on the blockchain. On the other hand, on a complete graph topology with unit delay links and uniform compute power as before, all nodes receive the same reward. A node identically connected to other nodes in the two cases (i.e., the central node in the star topology and any arbitrary node in the complete graph topology: both have direct links to all other nodes) receives different rewards, as rewards depend not only on the node\u2019s own connections but also on how other nodes\u2019 connections. Thus, there is an inherent tension for a miner in increasing her own connectivity to the rest of the network while simultaneously ensuring that the connectivity between other miners do not significantly increase. A systematic research of this tension, and efficient connection policies to maximize marginal mining reward gain due to the network, have not been done to our best knowledge.\n\nIn this work, we formalize the p2p topology construction problem as a game between miners and present Cobalt, a decentralized policy for optimizing reward. We consider a simplified setting where only a single node chooses its connections, while the rest of the network\u2019s topology is fixed. We assume that the global topology of the p2p network is unknown to miners. We thus model the problem of optimizing rewards by the connections-deciding miner node as a Markov decision process (MDP) with no state and an action set with a combinatorial number of actions.\n\nWe derive the optimal neighbor selection policy using a combinatorial multi-armed bandit (MAB) approach [14]. In the MAB algorithm, the agent (miner) explores various candidate connection configurations, and gradually adapts its connections based on past experience to gain the most mining rewards. A key contribution of our work is a network coordinates based model for efficiently learning the MAB environment [19]. In this model, miners are assigned realvalued vectors from an Euclidean space, which capture the relative location of miners with respect to each other in the network. The coodinates are continuously updated based upon the reward feedback the agent receives from the environment. Thus, despite not having global knowledge of the network initially, we show that it is possible for an agent to learn about the network by just using the observed reward information.\n\nTo enable the deployment of MAB algorithm, we have built a simulator. To simplify the reward computation in the simulator, rather than simulating the actual mining process at each step of the MDP, we consider a computationally easier function that only depends on the pairwise shortest path lengths between miners. Importantly, our MDP reward function captures the property that a miner\u2019s mining gains depends on how small the shortest path lengths between the agent and other miners are relative to the shortest path lengths between other miners. Experimentally we show Cobalt outperforms or matches heuristics on diverse network settings.\n\n## II Related Work\n\nP2P network design for optimizing mining rewards has remained a relatively under-explored topic in the community. The work that is closest to our is Perigee [34] which proposes an adaptive peer-selection algorithm for minimizing block propagation latency in the network. However, Perigee does not model the game-theoretic competition between miners. Subsequent works [11], [43] consider optimizing the network to maximize extractable value (MEV) from transactions. A number of prior works have exposed the impact of the network on mining [12], [26], [28], [37], [40], [47], [48]. While these works generally suggest that better network connectivity translates to higher mining rewards earned, the competitive effects of network connectivity and methods to optimize them have not been discussed. Other related works include KadCast [38] which proposes a Kadmila-based structured overlay for efficient block broadcast, and relay networks such as BloXroute [29] for transports blocks quickly across vast geographic distances.\n\nThe idea of network coordinates for p2p networks has been prominently explored in the network systems literature since the turn of the millenium, including distributed approaches to learn them [19], [32], [36]. More recently, a number of theoretical works "]}, {"edit": ["COMO-ViT.Given the input feature \\(\\mathbf{F}_{l-1}\\in\\mathbb{R}^{H\\times W\\times c}\\) of COMO-ViT, we conduct two branches of operations. In the first branch, we uniformly split it into \\(n\\) non-overlapping windows \\(\\mathcal{P}=[\\mathbf{P}^{1},\\mathbf{P}^{2},\\cdots,\\mathbf{P}^{n}]\\in\\mathbb{R}^{n \\times w\\times c}\\), where \\((w,w)\\) is the window resolution. SNR [43] and STAR [52] downsample images, losing local structures and some important pixel-level information. Instead, the proposed COMO-ViT completely models the dependencies among all pixels of an image via a local-to-global hierarchical self-attention. Locally, each pixel in a window \\(\\mathbf{P}^{i}\\) is regarded as an individual, we thus reshape \\(\\mathbf{P}^{i}\\) as follows:\n\n\\[\\mathbf{P}^{i}\\rightarrow[p^{i,1},p^{i,2},\\cdots,p^{i,m}], \\tag{10}\\]\n\nwhere \\(p^{i,j}\\in\\mathbb{R}^{1\\times 1\\times c}\\), \\(m=w^{2}\\) is the number of pixels in \\(\\mathbf{P}^{i}\\). With a linear projection, we then transform the pixels into a sequence of pixel embeddings \\(\\mathbf{X}^{i}=[x^{i,1},x^{i,2},\\cdots,x^{i,m}]\\), where \\(x^{i,j}\\in\\mathbb{R}^{c1}\\) is the \\(j\\)-th pixel embedding, \\(c1\\) is the embedding dimension. For \\(\\mathbf{X}^{i}\\), we utilize a local Transformer module to extract deep features as follows:\n\n\\[\\mathbf{Y}^{{}^{\\prime}i} =\\mathbf{X}^{i}+\\mathrm{MSA}(\\mathrm{LN}(\\mathbf{X}^{i})), \\tag{11}\\] \\[\\mathbf{Y}^{i} =\\mathbf{Y}^{{}^{\\prime}i}+\\mathrm{MLP}(\\mathrm{LN}(\\mathbf{Y}^{ {}^{\\prime}i})),\\]\n\nwhere \\(\\mathbf{Y}^{i}\\) is the feature learned by the local Transformer module, \\(\\mathrm{MSA}(\\cdot)\\) is the Multi-head Self-Attention [35], \\(\\mathrm{LN}(\\cdot)\\) is layer normalization [1] for stable training and faster convergence, \\(\\mathrm{MLP}(\\cdot)\\) is multi-layer perceptron for feature transformation at channel dimension and non-linearity. In such a process, we adopt 1D learnable location embedding to encode the spatial information of pixels.\n\nTo complement the non-overlapping window attention, in the second branch which is parallel with local attention, we use a CNN module to model local pixel dependencies in \\(\\mathbf{F}_{l-1}\\) via an overlapped sliding kernel to recover image details, in which a SE block [11] is used to explore channel relationship to boost representative power:\n\n\\[\\mathbf{F}^{{}^{\\prime}}=\\mathrm{Conv}(\\mathrm{LN}(\\mathbf{F}_{l-1})),\\quad \\mathbf{F}_{conv}=\\mathbf{F}^{{}^{\\prime}}\\odot\\mathrm{SE}(\\mathbf{F}^{{}^{ \\prime}}). \\tag{12}\\]\n\n\\(\\mathbf{F}_{conv}\\) is then split into \\(n\\) non-overlapping windows \\(\\mathcal{Q}=[\\mathbf{Q}^{1},\\ "], "nougat": ["COMO-ViT.Given the input feature \\(\\mathbf{F}_{l-1}\\in\\mathbb{R}^{H\\times W\\times c}\\) of COMO-ViT, we conduct two branches of operations. In the first branch, we uniformly split it into \\(n\\) non-overlapping windows \\(\\mathcal{P}=[\\mathbf{P}^{1},\\mathbf{P}^{2},\\cdots,\\mathbf{P}^{n}]\\in\\mathbb{R}^{n \\times w\\times c}\\), where \\((w,w)\\) is the window resolution. SNR [43] and STAR [ 52 ] downsample images, losing local structures and some important pixel-level information. Instead, the proposed COMO-ViT completely models the dependencies among all pixels of an image via a local-to-global hierarchical selfattention. Locally, each pixel in a window \\(\\mathbf{P}^{i}\\) is regarded as an individual, we thus reshape \\(\\mathbf{P}^{i}\\) as follows:\n\n\\[\\mathbf{P}^{i}\\rightarrow[p^{i,1},p^{i,2},\\cdots,p^{i,m}], \\tag{10}\\]\n\nwhere \\(p^{i,j}\\in\\mathbb{R}^{1\\times 1\\times c}\\), \\(m=w^{2}\\) is the number of pixels in \\(\\mathbf{P}^{i}\\). With a linear projection, we then transform the pixels into a sequence of pixel embeddings \\(\\mathbf{X}^{i}=[x^{i,1},x^{i,2},\\cdots,x^{i,m}]\\), where \\(x^{i,j}\\in\\mathbb{R}^{c1}\\) is the \\(j\\)-th pixel embedding, \\(c1\\) is the embedding dimension. For \\(\\mathbf{X}^{i}\\), we utilize a local Transformer module to extract deep features as follows:\n\n\\[\\mathbf{Y}^{{}^{\\prime}i} =\\mathbf{X}^{i}+\\mathrm{MSA}(\\mathrm{LN}(\\mathbf{X}^{i})), \\tag{11}\\] \\[\\mathbf{Y}^{i} =\\mathbf{Y}^{{}^{\\prime}i}+\\mathrm{MLP}(\\mathrm{LN}(\\mathbf{Y}^{ {}^{\\prime}i})),\\]\n\nwhere \\(\\mathbf{Y}^{i}\\) is the feature learned by the local Transformer module, \\(\\mathrm{MSA}(\\cdot)\\) is the Multi-head Self-Attention [ 35 ], \\(\\mathrm{LN}(\\cdot)\\) is layer normalization [ 1 ] for stable training and faster convergence, \\(\\mathrm{MLP}(\\cdot)\\) is multi-layer perceptron for feature transformation at channel dimension and nonlinearity. In such a process, we adopt 1D learnable location embedding to encode the spatial information of pixels.\n\nTo complement the non-overlapping window attention, in the second branch which is parallel with local attention, we use a CNN module to model local pixel dependencies in \\(\\mathbf{F}_{l-1}\\) via an overlapped sliding kernel to recover image details, in which a SE block [ 11 ] is used to explore channel relationship to boost representative power:\n\n\\[\\mathbf{F}^{{}^{\\prime}}=\\mathrm{Conv}(\\mathrm{LN}(\\mathbf{F}_{l-1})),\\quad \\mathbf{F}_{conv}=\\mathbf{F}^{{}^{\\prime}}\\odot\\mathrm{SE}(\\mathbf{F}^{{}^{ \\prime}}). \\tag{12}\\]\n\n\\(\\mathbf{F}_{conv}\\) is then split into \\(n\\) non-overlapping windows \\(\\mathcal{Q}=[\\mathbf{Q}^ "]}, {"edit": ["of \\(10^{3}\\) G (Reiners & Christensen 2010), we get a magnetic field strength of 4.5\\(\\times\\)10\\({}^{1}\\) G at the eclipse edge (\\(\\phi_{b}=0.31\\)). A magnetosphere field strength of 10 G is sufficient to trap and dominate plasma (assuming protons and electrons) of number density \\(<3\\times 10^{13}\\) cm\\({}^{-3}\\) and velocity \\(\\sim V_{\\rm orb}\\).\n\nThompson et al. (1994) suggests that, at the edge of the magnetosphere of the brown dwarf, the magnetic pressure should balance the pulsar wind pressure, while the pulsar wind energy density is \\(U_{E}=\\frac{\\dot{E}}{4\\pi\\epsilon_{0}a^{2}}\\) and the magnetic pressure is \\(\\frac{B_{E}^{2}}{8\\pi}\\). The \\(\\dot{E}\\) is the spin-down luminosity, c is the speed of light, \\(a\\) is the orbital separation, and \\(B_{E}\\) is the magnetic field of the eclipse medium. From this, the magnetic field strength of \\(B_{E}\\) should be \\(\\approx 8\\) G (Wang et al. 2021).\n\nInterestingly, the derived theoretical magnetic field strength (45 G) is more than sufficient for the required field strength (8 G) at the eclipsing edge. However, these field strengths are more than three orders of magnitude higher than the value observed in our egress (10 mG).\n\n#### Pulsar wind\n\nThe third scenario (Fig. 7 c) supplements the second one with pulsar wind and a shock boundary, and fixes the inconsistency mentioned above. Such a picture was proposed by Phinney et al. (1988) as one of the early models. In this picture, a shock boundary exists between the magnetosphere and the pulsar wind. Outside of the shock boundary are high-speed, low-density pulsar wind particles traveling with a low magnetic field, and inside, the slow-moving, high-density plasma trapped by the companion's magnetic fields. This is similar to the boundary shock observed from the Solar wind and the Earth magnetosphere (Sckopke et al. 1983) where both the electron density and magnetic field rose suddenly as the ISEE-18 probe traveled downstream of the Solar wind into the Earth magnetosphere.\n\nFootnote 8: International Sun-Earth Explorer 1\n\nThe majority of energy in the pulsar wind is carried by relativistic particles. The magnetic fields in the pulsar wind could be much smaller than the magnetic field of the companion at the orbital distance. After all, the pulsar's magnetic field is only 1.6\\(\\times\\)10\\({}^{8}\\) G at its 10 km radius surface (Tab. 1). The pulsar wind is almost transparent to the pulsar emission. This is because of the low density and the high Lorenz factor of the wind particles. The wind particles have motion masses far exceeding their rest masses, causing their Faraday rotation effect to be negligible (Quataert & Gruzinov 2000; Wang et al. 2011). When a moderate amount of slow-moving ionized materials from the companion's magnetosphere flow out of the boundary and come to the pulsar wind side, the combination of the extra slow electrons and a reasonably low magnetic field (10 mG) environment leads to the incomplete depolarization and the Faraday rotation. As we mentioned in the previous section, such a condition is rarely met (only be observed in MJD 59214). In most of the ingresses and egresses of this pulsar, the out-flowing electrons are either too dense or too variable and often completely depolarize the pulsar signal.\n\nThompson et al. (1994) predicted that the pulsar wind could contain an oscillating part around the eclipsing edge with an oscillation length of \\(cP/2\\simeq 500\\) km, where \\(c\\) is the speed of light and \\(P\\) is the spin period of the pulsar. It should be noted that such reciprocating magnetic fields in the pulsar wind was already illustrated in the model of Phinney et al. ( "], "nougat": ["of \\(10^{3}\\) G (Reiners & Christensen 2010), we get a magnetic field strength of 4.5\\(\\times\\)10\\({}^{1}\\) G at the eclipse edge (\\(\\phi_{b}=0.31\\)). A magnetosphere field strength of 10 G is sufficient to trap and dominate plasma (assuming protons and electrons) of number density \\(<3\\times 10^{13}\\) cm\\({}^{-3}\\) and velocity \\(\\sim V_{\\rm orb}\\).\n\nThompson et al. (1994) suggests that, at the edge of the magnetosphere of the brown dwarf, the magnetic pressure should balance the pulsar wind pressure, while the pulsar wind energy density is \\(U_{E}=\\frac{\\dot{E}}{4\\pi\\epsilon_{0}a^{2}}\\) and the magnetic pressure is \\(\\frac{B_{E}^{2}}{8\\pi}\\). The \\(\\dot{E}\\) is the spin-down luminosity, c is the speed of light, \\(a\\) is the orbital separation, and \\(B_{E}\\) is the magnetic field of the eclipse medium. From this, the magnetic field strength of \\(B_{E}\\) should be \\(\\approx 8\\) G (Wang et al. 2021) .\n\nInterestingly, the derived theoretical magnetic field strength (45 G) is more than sufficient for the required field strength (8 G) at the eclipsing edge. However, these field strengths are more than three orders of magnitude higher than the value observed in our egress (10 mG).\n\n#### Pulsar wind\n\nThe third scenario (Fig. 7 c) supplements the second one with pulsar wind and a shock boundary, and fixes the inconsistency mentioned above. Such a picture was proposed by Phinney et al. (1988) as one of the early models. In this picture, a shock boundary exists between the magnetosphere and the pulsar wind. Outside of the shock boundary are high-speed, low-density pulsar wind particles traveling with a low magnetic field, and inside, the slow-moving, high-density plasma trapped by the companion\u2019s magnetic fields. This is similar to the boundary shock observed from the Solar wind and the Earth magnetosphere (Sckopke et al. 1983) where both the electron density and magnetic field rose suddenly as the ISEE-1 8 probe traveled downstream of the Solar wind into the Earth magnetosphere.\n\nFootnote 8: International Sun-Earth Explorer 1\n\nThe majority of energy in the pulsar wind is carried by relativistic particles. The magnetic fields in the pulsar wind could be much smaller than the magnetic field of the companion at the orbital distance. After all, the pulsar\u2019s magnetic field is only 1.6\\(\\times\\)10\\({}^{8}\\) G at its 10 km radius surface (Tab. 1). The pulsar wind is almost transparent to the pulsar emission. This is because of the low density and the high Lorenz factor of the wind particles. The wind particles have motion masses far exceeding their rest masses, causing their Faraday rotation effect to be negligible (Quataert & Gruzinov 2000; Wang et al. 2011). When a moderate amount of slow-moving ionized materials from the companion\u2019s magnetosphere flow out of the boundary and come to the pulsar wind side, the combination of the extra slow electrons and a reasonably low magnetic field (10 mG) environment leads to the incomplete depolarization and the Faraday rotation. As we mentioned in the previous section, such a condition is rarely met (only be observed in MJD 59214). In most of the ingresses and egresses of this pulsar, the out-flowing electrons are either too dense or too variable and often completely depolarize the pulsar signal.\n\nThompson et al. (1994) predicted that the pulsar wind could contain an oscillating part around the eclipsing edge with an oscillation length of \\(cP/2\\simeq 500\\) km, where \\(c\\) is the speed of light and \\(P\\) is the spin "]}, {"edit": ["between these works and ours is that they assume the buyer is _fully strategic_ and processes fully how their actions today affect the seller's decisions tomorrow (whereas we instead model buyers as no-regret learners).\n\nThe most related work to ours is in the [1] model itself. Here we provide a brief summary of the main results in [1] and their connection to our main results. [1] studies the one seller one buyer scenario, where the buyer employs a mean-based no-regret algorithm. The authors present three results, each obtained under different assumptions regarding the behavior of the buyers. Firstly (as we have already mentioned earlier in the introduction), [1] shows that for vanilla mean-based no-regret buyers, [1] can extract revenue that is an arbitrarily large fraction of the bidder's expected value. Our Theorem 4 extends this result to the multiple buyer setting, overcoming novel technical and conceptual challenges. Second, [1] designs a novel (not mean-based) learning algorithm against which the optimal mechanism for the seller is simply Myerson's auction in each round. Their proof of this result naturally accommodates multiple buyers. Finally, [1] shows that if the buyer is clever and mean-based no regret (where they do not overbid their value), then the optimal auction has a clean tractable format (pay-your-bid with declining reserve over time). As we have discussed in the \"No Overbidding\" section of the introduction, our work shows several formal barriers in extending these results to multiple buyers. In summary, our main result extends their first main result to multiple bidders. Their second result already holds for multiple bidders (so there is nothing for us to extend). Our secondary results establish formal barriers to extending their final main result to multiple bidders.\n\nTwo recent follow-ups have extended the setting in [1] in a different direction. First, [2] considers the problem of playing a two-player game against a no-regret learner. While technically not an auctions problem, there is thematic overlap with our main result. [2] extends the single-buyer results in [1] to be _prior-free_. Specifically, they show how to design auctions achieving the same guarantees as those in [1] but where the buyer's values are chosen adversarially. In comparison to these works, ours is the first to extend the model to consider multiple buyers.\n\nFinally, recent work of [10] considers interaction between a learning buyer and a _learning_ seller. Their seller does not have a prior against which to optimize, and instead itself targets a no-regret guarantee. In comparison, our seller (like the seller in all previously cited works) optimizes expected revenue with respect to a prior.\n\n## 2 Preliminaries\n\nWe consider the same setting as [1], extended to multiple buyers. Specifically, there are \\(n\\) buyers and \\(T\\) rounds. In each round, there is a single item for sale. Each buyer \\(i\\) has value \\(v_{i,t}\\) for the item during round \\(t\\), and each \\(v_{i,t}\\) is drawn from \\(\\mathcal{D}\\) independently (that is, the buyers are i.i.d., and the rounds are i.i.d. as well). For simplicity of exposition (and to match prior work), we assume \\(\\mathcal{D}\\) has finite support \\(0\\leq w_{1}<w_{2}<\\ldots<w_{m}\\leq 1\\) and we define \\(q_{j}\\) to be the probability \\(w_{j}\\) is drawn from \\(\\mathcal{D}\\).\n\nEach round, the seller presents \\(K\\) arms for the buyers. Each arm is labeled with a bid, and we assume that one of the arms is labeled with \\(0\\) (to represent a bid of \"don't participate\"). Note that the same set of arms is presented to all buyers, and the same set of arms is presented in each round.\n\nIn each round \\(t\\), the seller defines an anonymous auction. Specifically, for all \\(i,t\\), the seller defines "], "nougat": ["between these works and ours is that they assume the buyer is _fully strategic_ and processes fully how their actions today affect the seller\u2019s decisions tomorrow (whereas we instead model buyers as no-regret learners).\n\nThe most related work to ours is in the [BMSW18] model itself. Here we provide a brief summary of the main results in [BMSW18] and their connection to our main results. [BMSW18] studies the one seller one buyer scenario, where the buyer employs a mean-based no-regret algorithm. The authors present three results, each obtained under different assumptions regarding the behavior of the buyers. Firstly (as we have already mentioned earlier in the introduction), [BMSW18] shows that for vanilla mean-based no-regret buyers, [BMSW18] can extract revenue that is an arbitrarily large fraction of the bidder\u2019s expected value. Our Theorem 4 extends this result to the multiple buyer setting, overcoming novel technical and conceptual challenges. Second, [BMSW18] designs a novel (not mean-based) learning algorithm against which the optimal mechanism for the seller is simply Myerson\u2019s auction in each round. Their proof of this result naturally accommodates multiple buyers. Finally, [BMSW18] shows that if the buyer is clever and mean-based no regret (where they do not overbid their value), then the optimal auction has a clean tractable format (pay-your-bid with declining reserve over time). As we have discussed in the \u201cNo Overbidding\u201d section of the introduction, our work shows several formal barriers in extending these results to multiple buyers. In summary, our main result extends their first main result to multiple bidders. Their second result already holds for multiple bidders (so there is nothing for us to extend). Our secondary results establish formal barriers to extending their final main result to multiple bidders.\n\nTwo recent follow-ups have extended the setting in [BMSW18] in a different direction. First, [DSS19b] considers the problem of playing a two-player game against a no-regret learner. While technically not an auctions problem, there is thematic overlap with our main result. [DSS19a] extends the single-buyer results in [BMSW18] to be _prior-free_. Specifically, they show how to design auctions achieving the same guarantees as those in [BMSW18] but where the buyer\u2019s values are chosen adversarially. In comparison to these works, ours is the first to extend the model to consider multiple buyers.\n\nFinally, recent work of [CHJ20] considers interaction between a learning buyer and a _learning_ seller. Their seller does not have a prior against which to optimize, and instead itself targets a noregret guarantee. In comparison, our seller (like the seller in all previously cited works) optimizes expected revenue with respect to a prior.\n\n## 2 Preliminaries\n\nWe consider the same setting as [BMSW18], extended to multiple buyers. Specifically, there are \\(n\\) buyers and \\(T\\) rounds. In each round, there is a single item for sale. Each buyer \\(i\\) has value \\(v_{i,t}\\) for the item during round \\(t\\), and each \\(v_{i,t}\\) is drawn from \\(\\mathcal{D}\\) independently (that is, the buyers are i.i.d., and the rounds are i.i.d. as well). For simplicity of exposition (and to match prior work), we assume \\(\\mathcal{D}\\) has finite support \\(0\\leq w_{1}<w_{2}<\\ldots<w_{m}\\leq 1\\) and we define \\(q_{j}\\) to be the probability \\(w_{j}\\) is drawn from \\(\\mathcal{D}\\).\n\nEach round, the seller presents \\(K\\) arms for the buyers. Each arm is labeled with a bid, and we assume that one of the arms is labeled with \\(0\\) (to represent a bid of \u201cdon\u2019t participate\u201d). Note that the same set of arms is presented to all buyers, and the same set of arms is presented in each round.\n\nIn each round \\(t\\), the seller defines an anonymous auction. Specifically, for all \\(i,t "]}, {"edit": ["but consistent with a model where only the gravitational potential of the gas is considered.\n\nTo investigate the impact of the SMBH on the kinematics of J0109-3047 further, we construct a simple \"dispersion-dominated + SMBH\" model in _QUBEFit_. In this model the velocity dispersion is the sum of the SMBH component of Eq. 1 and a constant dispersion value throughout the quasar host (\\(\\sigma_{\\rm CII,tot}^{2}=\\sigma_{\\rm CII,SMBH}(r)^{2}+\\sigma_{\\rm const}^{2}\\)). The intensity profile is assumed to be exponentially declining as in the constant dispersion model (see Section 3). We note that any additional contribution (besides the central SMBH) to the kinematics as a constant is in agreement with the inferred gas mass profile (see Fig. 4).\n\nIn Fig. 5 we show the best-fit dispersion fields for different SMBH masses from \\(10^{8}\\ M_{\\odot}\\) to \\(10^{9}\\ M_{\\odot}\\). We find that the [C ii] kinematics of J0109-3047 are clearly incompatible with a \\(\\sim 10^{9}\\ M_{\\odot}\\) SMBH. A full MCMC fit of the model to the data yields an upper limit of \\(M_{\\rm SMBH}<6.5\\times 10^{8}\\ M_{\\odot}(2\\sigma)\\) (see Appendix A for the full posterior distribution of the model parameters). However, even for the maximum-likelihood model (\\(M_{\\rm BH}=2.4\\times 10^{8}\\ M_{\\odot}\\)), the BIC is slightly higher than for a model without a black hole (\\(\\Delta{\\rm BIC}=5.2\\)), showing that any SMBH contribution to the dispersion velocity field is disfavored by the observations.\n\nOne way to alleviate the tension with the rest-frame UV mass measurement could be to change the radial [C ii]-emitting gas profile. By definition, the observed [C ii] kinematics are a luminosity-weighted, beam-convolved realisation of the intrinsic kinematics. Following Eq. 1, the velocity dispersion increases exponentially close to the black hole, and due to the exponential intensity profile, these inner regions will contribute more to the beam-convolved velocity dispersion measurement in the center. As a result, the observed velocity dispersion could be reduced, if the [C ii] intensity profile is not increasing close to the black hole (for example due to feedback).\n\nTo address this further, we have used a toy model where the gas density profile follows an exponentially declining profile with a central gap where the [C ii] emission is null. As in the fiducial model, the velocity dispersion is composed of the SMBH component and a constant. We use this simple model to calculate the size of the central gap necessary to \"hide\" the SMBH impact on the [C ii] kinematics tracer. We find that, for a SMBH with a fixed mass \\(M_{\\rm BH}=1.1\\times 10^{9}M_{\\odot}\\), the best-fit central gap is constrained to be \\(r<22\\) pc (\\(2\\sigma\\)) to reproduce the [C ii] profile and kinematics. The best-fit model has \\(r_{\\rm pc}=0.015^{+0.015}_{-0.010}\\) pc (see Appendix A), and is formally ruled out with an increased \\(\\Delta{\\rm BIC}=10.42\\) compared to the model without a gap. Moreover, a central gap in the gas distribution would be at odds with simulations and observations where the central \\(\\sim 400-500\\) pc region contains up to \\(\\sim 10\\) times the mass of the BH in gas (e.g., Lupi et al., 2022; Walter et al., 2022).\n\nIn summary, the flat velocity dispersion profile implies a flat radial mass density profile. The constant dispersion implies that the underlying mass distribution is not centrally peaked, consistent with the expectations of the gas mass distribution derived from the far-infrared continuum emission under standard assumptions. This leaves only "], "nougat": ["but consistent with a model where only the gravitational potential of the gas is considered.\n\nTo investigate the impact of the SMBH on the kinematics of J0109\u20133047 further, we construct a simple \u201cdispersion\u2013dominated + SMBH\u201d model in QUBEFit . In this model the velocity dispersion is the sum of the SMBH component of Eq. 1 and a constant dispersion value throughout the quasar host (\\(\\sigma_{\\rm{CII,tot}}^{2}=\\sigma_{\\rm{CII,SMBH}}(r)^{2}+\\sigma_{\\rm{const}}^{2}\\)). The intensity profile is assumed to be exponentially declining as in the constant dispersion model (see Section 3 ). We note that any additional contribution (besides the central SMBH) to the kinematics as a constant is in agreement with the inferred gas mass profile (see Fig. 4 ).\n\nIn Fig. 5 we show the best-fit dispersion fields for different SMBH masses from \\(10^{8}\\)\\(M_{\\odot}\\) to \\(10^{9}\\)\\(M_{\\odot}\\). We find that the [C II ] kinematics of J0109\u20133047 are clearly incompatible with a \\(\\sim 10^{9}\\)\\(M_{\\odot}\\) SMBH. A full MCMC fit of the model to the data yields an upper limit of \\(M_{\\rm{SMBH}}<6.5\\times 10^{8}\\)\\(M_{\\odot}(2\\sigma)\\) (see Appendix A for the full posterior distribution of the model parameters). However, even for the maximum-likelihood model (\\(M_{\\rm{BH}}=2.4\\times 10^{8}\\)\\(M_{\\odot}\\)), the BIC is slightly higher than for a model without a black hole (\\(\\Delta{\\rm BIC}=5.2\\)), showing that any SMBH contribution to the dispersion velocity field is disfavored by the observations.\n\nOne way to alleviate the tension with the rest-frame UV mass measurement could be to change the radial [C II ]\u2013emitting gas profile. By definition, the observed [C II ] kinematics are a luminosity\u2013weighted, beam\u2013 convolved realisation of the intrinsic kinematics. Following Eq. 1 , the velocity dispersion increases exponentially close to the black hole, and due to the exponential intensity profile, these inner regions will contribute more to the beam\u2013convolved velocity dispersion measurement in the center. As a result, the observed velocity dispersion could be reduced, if the [C II ] intensity profile is not increasing close to the black hole (for example due to feedback).\n\nTo address this further, we have used a toy model where the gas density profile follows an exponentially declining profile with a central gap where the [C II ] emission is null. As in the fiducial model, the velocity dispersion is composed of the SMBH component and a constant. We use this simple model to calculate the size of the central gap necessary to \u201chide\u201d the SMBH impact on the [C II ] kinematics tracer. We find that, for a SMBH with a fixed mass \\(M_{\\rm{BH}}=1.1\\times 10^{9}M_{\\odot}\\), the best-fit central gap is constrained to be \\(r<22\\) pc (\\(2\\sigma\\)) to reproduce the [C II ] profile and kinematics. The best-fit model has \\(r_{\\rm{pc}}=0.015^{+0.015}_{-0.010}\\) pc (see Appendix A ), and is formally ruled out with an increased \\(\\Delta{\\rm BIC}=10.42\\) compared to the model without a gap. Moreover, a central gap in the gas distribution would be at odds with simulations and observations where the central region contains up to \\(\\sim 10\\) times the mass of the BH in gas (e.g., Lupi et al. 2022 ; Walter et al. 2022 ).\n\nIn summary, the "]}, {"edit": ["\n\n**The particular case of a Lie group. In what follows, we will show the previous reduction process in the particular case when the initial manifold \\(Q\\) is a Lie group \\(G.\\) In such a case, one may use the left trivialization of the cotangent bundle \\(T^{*}G\\) in order to identify \\(T^{*}G\\) with the product manifold \\(G\\times\\mathfrak{g}^{*},\\) where \\((\\mathfrak{g},[\\cdot,\\cdot]_{\\mathfrak{g}})\\) is the Lie algebra of \\(G,\\) in such a way that the canonical projection \\(\\tau^{*}_{G}:T^{*}G\\to G\\) is just the first projection \\(p_{1}:G\\times\\mathfrak{g}^{*}\\to G.\\) The left action \\(\\Phi:G\\times G\\to G\\) on \\(G\\) is the one defined by the group operation of \\(G\\). We take the left invariant vector field \\(Y=\\overset{\\leftarrow}{\\xi}\\) on \\(G\\) induced by an element \\(\\xi\\) of \\(\\mathfrak{g}.\\) In the first reduction with the cotangent lift of \\(\\Phi,\\) the reduced space is \\((T^{*}G-0_{G})/G\\cong\\mathfrak{g}^{*}-\\{0\\}\\) and the reduced function induced by \\(Y\\) is the restriction to \\(\\mathfrak{g}^{*}-\\{0\\}\\) of the linear map \\(\\xi^{\\ell}\\) associated with \\(\\xi\\in\\mathfrak{g},\\) i.e.**\n\n\\[\\xi^{\\ell}:\\mathfrak{g}^{*}-\\{0\\}\\rightarrow\\mathbb{R},\\quad\\xi^{\\ell}(\\alpha )=\\alpha(\\xi).\\]\n\n**On the other hand, the Lie-Poisson bracket \\(\\{\\cdot,\\cdot\\}_{\\mathfrak{g}^{*}}\\) on \\((T^{*}G-0_{G})/G\\cong\\mathfrak{g}^{*}-\\{0\\}\\) is characterized by**\n\n\\[\\{\\xi_{1}{}^{\\ell},\\xi_{2}{}^{\\ell}\\}_{\\mathfrak{g}^{*}}=-[\\xi_{1},\\xi_{2}]^{ \\ell}_{\\mathfrak{g}^{*}},\\quad\\quad\\text{for all }\\xi_{1},\\xi_{2}\\in\\mathfrak{g}.\\]\n\n**The scaling symmetry on \\(\\mathfrak{g}^{*}-\\{0\\}\\) is just**\n\n\\[\\phi^{G}:(\\mathbb{R}-\\{0\\})\\times(\\mathfrak{g}^{*}-\\{0\\})\\rightarrow( \\mathfrak{g}^{*}-\\{0\\}),\\quad\\quad(s,\\alpha)\\to s\\alpha. \\tag{40}\\]\n\n**Now, we apply the second reduction step to the (Lie)-Poisson Hamiltonian system \\((\\mathfrak{g}^{*}-\\{0\\},\\{\\cdot,\\cdot\\}_{\\mathfrak{g}^{*}},\\xi^{\\ell}),\\) with respect to the scaling symmetry \\(\\phi^{G}.\\) In this case, the reduced space is the projective space \\(\\mathbb{P}\\mathfrak{g}^{*}.\\) The corresponding line bundle \\(\\pi_{L}:L:=(\\mathfrak{g}^{*}-\\{0\\}\\times\\mathbb{R})/(\\mathbb{R}-\\{0\\}) \\rightarrow\\mathbb{P}\\mathfrak{g}^{*}\\) is defined by the action**"], "nougat": ["\n\n**The particular case of a Lie group. In what follows, we will show the previous reduction process in the particular case when the initial manifold \\(Q\\) is a Lie group \\(G.\\) In such a case, one may use the left trivialization of the cotangent bundle \\(T^{*}G\\) in order to identify \\(T^{*}G\\) with the product manifold \\(G\\times\\mathfrak{g}^{*},\\) where \\((\\mathfrak{g},[\\cdot,\\cdot]_{\\mathfrak{g}})\\) is the Lie algebra of \\(G,\\) in such a way that the canonical projection \\(\\tau^{*}_{G}:T^{*}G\\to G\\) is just the first projection \\(p_{1}:G\\times\\mathfrak{g}^{*}\\to G.\\) The left action \\(\\Phi:G\\times G\\to G\\) on \\(G\\) is the one defined by the group operation of \\(G\\). We take the left invariant vector field \\(Y=\\overset{\\leftarrow}{\\xi}\\) on \\(G\\) induced by an element \\(\\xi\\) of \\(\\mathfrak{g}.\\) In the first reduction with the cotangent lift of \\(\\Phi,\\) the reduced space is \\((T^{*}G-0_{G})/G\\cong\\mathfrak{g}^{*}-\\{0\\}\\) and the reduced function induced by \\(Y\\) is the restriction to \\(\\mathfrak{g}^{*}-\\{0\\}\\) of the linear map \\(\\xi^{\\ell}\\) associated with \\(\\xi\\in\\mathfrak{g},\\) i.e.**\n\n\\[\\xi^{\\ell}:\\mathfrak{g}^{*}-\\{0\\}\\rightarrow\\mathbb{R},\\quad\\xi^{\\ell}(\\alpha )=\\alpha(\\xi).\\]\n\n**On the other hand, the Lie-Poisson bracket \\(\\{\\cdot,\\cdot\\}_{\\mathfrak{g}^{*}}\\) on \\((T^{*}G-0_{G})/G\\cong\\mathfrak{g}^{*}-\\{0\\}\\) is characterized by**\n\n\\[\\{\\xi_{1}{}^{\\ell},\\xi_{2}{}^{\\ell}\\}_{\\mathfrak{g}^{*}}=-[\\xi_{1},\\xi_{2}]_{ \\mathfrak{g}^{\\ell}}^{\\ell},\\quad\\quad\\text{for all }\\xi_{1},\\xi_{2}\\in \\mathfrak{g}.\\]\n\n**The scaling symmetry on \\(\\mathfrak{g}^{*}-\\{0\\}\\) is just**\n\n\\[\\phi^{G}:(\\mathbb{R}-\\{0\\})\\times(\\mathfrak{g}^{*}-\\{0\\})\\rightarrow( \\mathfrak{g}^{*}-\\{0\\}),\\quad\\quad(s,\\alpha)\\to s\\alpha. \\tag{40}\\]\n\n**Now, we apply the second reduction step to the (Lie)-Poisson Hamiltonian system \\((\\mathfrak{g}^{*}-\\{0\\},\\{\\cdot,\\cdot\\}_{\\mathfrak{g}^{*}},\\xi^{\\ell}),\\) with respect to the scaling symmetry \\(\\phi^{G}.\\) In this case, the reduced space is the projective space \\(\\mathbb{P}\\mathfrak{g}^{*}.\\) The corresponding line bundle \\(\\pi_{L}:L:=(\\mathfrak{g}^{*}-\\{0\\}\\times\\mathbb{R})/(\\mathbb{R}-\\{0\\}) \\rightarrow\\mathbb{P}\\mathfrak{g}^{*}\\) is defined by the action**"]}, {"edit": ["community filtered by Tutorial tag on social media website?_), and acts via planning, summarizing by HTML-T5, and then programming by Flan-U-PaLM. See Appendix C for the example workflow. We finetune HTML-T5 with traces that are collected using scripted agents by procedurally generating instructions from human curated templates. This results in 260 episodes on real estate website and 230 episodes on social media website (about 20/10 steps per episode respectively).\n\nWe prepare 20 different natural language instructions, and measure the success rate and score for the evaluation. The score represents the percentage of required attributes covered during the episode [81]; for instance (1) _apartments_ for (2) _corporate housing_ with (3) _studio bedroom_ and (4) _1+ bathroom_ located in (5) _oroville, ca._ When the agents could search the housing satisfying (1), (2), (5) and not (3), (4), the score would be 60 (\\(=100\\times 3/5\\)). When the agents could achieve 100 score, that episode would mark as success.\n\n**Results** For comparison, we prepare three baselines, consisting of partial plug-in language models and a single LLM prompting different examplers per role: WebAgent replacing closed-loop planning from HTML-T5 with few-shot open-loop planning from Flan-U-PaLM (**Plan**: \\(\\mathcal{X}\\)), replacing HTML summarization from HTML-T5 with regular-expression-based retrieval (**Sum**: \\(\\mathcal{X}\\)), and both of them (**Plan**: \\(\\mathcal{X}\\), **Sum**: \\(\\mathcal{X}\\)). Table 2 shows that WebAgent with HTML-T5 for planning and summarization (**Plan**: \\(\\boldsymbol{\\nu}\\), **Sum**: \\(\\boldsymbol{\\nu}\\)) achieves best 65% success and 87.6 score on real-estate and 70% success and 85.8 score on social-media, significantly outperforming single LLM (**Plan**: \\(\\mathcal{X}\\), **Sum**: \\(\\mathcal{X}\\)), that with open-loop planning (**Plan**: \\(\\mathcal{X}\\)), and that with regular-expression retrieval (**Sum**: \\(\\mathcal{X}\\)) (most of those roughly achieve only 10 - 20% success). This result suggests that closed-loop planning grounded on HTML observations via finetuning of domain language models is much more suitable for open-ended web navigation than open-loop planning with few-shot LLMs, which is remarkable in real-estate (even **Sum**: \\(\\mathcal{X}\\) achieves 50% success), where the longer planning horizon is needed to fulfill instructions. We guess enhancing the planning ability to decompose the given instructions adaptively and robustly can help further improve WebAgent.\n\n\\begin{table}\n\\begin{tabular}{l c c c} \\hline \\hline\n**Models** & **Data** & **Success** & **Diff.** \\\\ \\hline CC-Net [28] & 2.4M & 32.0\\% & \u2013 \\\\ WebN-T5-XL [24] & 12K & 48.4\\% & \u2013 \\\\ \\hline LongT5-Base & & 53.8\\% & 0.0 \\\\ LongT5-Large & 12K & 56.3\\% & 0.0 \\\\ LongT5-XL & & 60.4\\% & 0.0 \\\\ \\hline Flan-LongT5-Base & & 54.1\\% & +0.3 \\\\ Flan-LongT5-Large & 12K & 56.1\\% & -0.2 \\\\ Flan-LongT5-XL & & 61.1\\% "], "nougat": ["community filtered by Tutorial tag on social media web-site?_), and acts via planning, summarizing by HTML-T5, and then programming by Flan-U-PaLM. See Appendix C for the example workflow. We finetune HTML-T5 with traces that are collected using scripted agents by procedurally generating instructions from human curated templates. This results in 260 episodes on real estate website and 230 episodes on social media website (about 20/10 steps per episode respectively). We prepare 20 different natural language instructions, and measure the success rate and score for the evaluation. The score represents the percentage of required attributes covered during the episode [ 81 ]; for instance (1) _apartments_ for (2), (5) and not (3), (4), the score would be 60 (\\(=100 score, that episode would mark as success. See Appendix H for the detailed results.\n\n**Results** For comparison, we prepare three baselines, consisting of partial plug-in language models and a single LLM prompting different examplers per role: WebAgent replacing closed-loop planning from HTML-T5 with fewshot open-loop planning from Flan-U-PaLM ( Plan: \\(\\mathcal{X}\\)), replacing HTML summarization from HTML-T5 with regular-expression-based retrieval (Sum: \\(\\mathcal{X}\\)), and both of them (Plan: \\(\\mathcal{X}\\), Sum: \\(\\mathcal{X}\\)). Table 2 shows that WebAgent with HTML-T5 for planning and summarization ( Plan: \\(\\boldsymbol{\\nu}\\), Sum: \\(\\boldsymbol{\\nu}\\)) achieves best 65% success and 87.6 score on real-estate and 70% success and 85.8 score on social-media, significantly outperforming single LLM ( Plan: \\(\\mathcal{X}\\), Sum: \\(\\mathcal{X}\\)), that with open-loop planning (Plan: \\(\\mathcal{X}\\)), and that with regular-expression retrieval (Sum: \\(\\mathcal{X}\\)) (most of those roughly achieve only 10 20% success). This result suggests that closed-loop planning grounded on HTML observations via finetuning of domain language models is much more suitable for open-ended web navigation than open-loop planning with few-shot LLMs, which is remarkable in real-estate (even Sum: \\(\\mathcal{X}\\) achieves 50% success), where the longer planning horizon is needed to fulfill instructions. We guess enhancing the planning ability to decompose the given instructions adaptively and robustly can help further improve WebAgent.\n\n\\begin{table}\n\\begin{tabular}{l r r r} \\hline \\hline\n**Models** & **Data** & **Success** & **Diff.** \\\\ \\hline CC-Net [28] & 2.4M & 32.0\\% & \u2013 \\\\ WebN-T5-XL [24] & 12K & 48.4\\% & \u2013 \\\\ \\hline LongT5-Base & & 53.8\\% & 0.0 \\\\ LongT5-Large & 12K & 56.3\\% & 0.0 \\\\ LongT5-XL & & 60.4\\% & 0.0 \\\\ \\hline Flan-LongT5-Base & & 54.1\\% & +0.3 \\\\ Flan-LongT5-Large & 12K & 56.1\\% & -0.2 \\\\ Flan-LongT5-XL & & 61.1\\% & +0.7 \\\\ \\hline HTML-T5-Base (ours) & & 57.0\\% & +3.2 \\\\ HTML-T5-Large (ours) & 12K & 60.8\\% & +4.5 \\\\ HTML-T5-XL (ours) & & **63.3**\\% & "]}, {"edit": ["* [JX23b] Zhuchao Ji and Junyi Xie. Homoclinic orbits, multiplier spectrum and rigidity theorems in complex dynamics. _Forum Math. Pi_, 11:Paper No. e11, 37, 2023.\n* [Lev14] A. Levy. Aim workshop postcritically finite maps in complex and arithmetic dynamics, 2014.\n* [McM87] Curt McMullen. Families of rational maps and iterative root-finding algorithms. _Ann. of Math. (2)_, 125(3):467-493, 1987.\n* [Mil06] John Milnor. On Lattes maps. _Dynamics on the Riemann Sphere: A Bodil Branner Festschrift_, page 9, 2006.\n* [MS14] Alice Medvedev and Thomas Scanlon. Invariant varieties for polynomial dynamical systems. _Ann. of Math. (2)_, 179(1):81-177, 2014.\n* [Nar04] Wladysl aw Narkiewicz. _Elementary and analytic theory of algebraic numbers_. Springer Monographs in Mathematics. Springer-Verlag, Berlin, third edition, 2004.\n* [Pak23] Fedor Pakovich. Invariant curves for endomorphisms of \\(\\mathbb{P}^{1}\\times\\mathbb{P}^{1}\\). _Math. Ann._, 385(1-2):259-307, 2023.\n* [Poo17] Bjorn Poonen. _Rational points on varieties_, volume 186 of _Graduate Studies in Mathematics_. American Mathematical Society, Providence, RI, 2017.\n* [Sil98] Joseph H. Silverman. The space of rational maps on \\(\\mathbf{P}^{1}\\). _Duke Math. J._, 94(1):41-77, 1998.\n* [Sil07] Joseph H. Silverman. _The arithmetic of dynamical systems_, volume 241 of _Graduate Texts in Mathematics_. Springer-Verlag, New York, 2007.\n* [Sil12] Joseph H. Silverman. _Moduli spaces and arithmetic dynamics_, volume 30 of _CRM Monograph Series_. American Mathematical Society, Providence, RI, 2012.\n* [Tuc14] T. Tucker. Problem 6 in the problem list of the aim workshop postcritically finite maps in complex and arithmetic dynamics, 2014.\n* [Xie17] Junyi Xie. The existence of Zariski dense orbits for polynomial endomorphisms of the affine plane. _Compos. Math._, 153(8):1658-1672, 2017.\n* [Xie22] Junyi Xie. The existence of Zariski dense orbits for endomorphisms of projective surfaces (with an appendix in collaboration with T. Tucker). _J. Amer. Math. Soc._, 2022. published online.\n* [Xie23] Junyi Xie. Remarks on algebraic dynamics in positive characteristic. _J. Reine Angew. Math._, 797:117-153, 2023.\n* [XY23] Junyi Xie and Xinyi Yuan. Partial heights and the geometric Bombieri-Lang conjecture. arXiv:2305.14789, 2023.\n* [Yua08] Xinyi Yuan. Big line bundles over arithmetic varieties. _Invent. Math._, 173(3):603-649, 2008.\n* [Zdu14] Anna Zdunik. Characteristic exponents of rational functions. _Bulletin of the Polish Academy of Sciences. Mathematics_, 62(3), 2014.\n* [Zha95] Shou-Wu Zhang. Small points and adelic metrics. _J. Algebraic Geom._, 4(2):281-3 "], "nougat": ["* [JX23b] Zhuchao Ji and Junyi Xie. Homoclinic orbits, multiplier spectrum and rigidity theorems in complex dynamics. _Forum Math. Pi_, 11:Paper No. e11, 37, 2023.\n* [Lev14] A. Levy. Aim workshop postcritically finite maps in complex and arithmetic dynamics, 2014.\n* [McM87] Curt McMullen. Families of rational maps and iterative root-finding algorithms. _Ann. of Math. (2)_, 125(3):467-493, 1987.\n* [Mil06] John Milnor. On Lattes maps. _Dynamics on the Riemann Sphere: A Bodil Branner Festschrift_, page 9, 2006.\n* [MS14] Alice Medvedev and Thomas Scanlon. Invariant varieties for polynomial dynamical systems. _Ann. of Math. (2)_, 179(1):81-177, 2014.\n* [Nar04] Wladysl aw Narkiewicz. _Elementary and analytic theory of algebraic numbers_. Springer Monographs in Mathematics. Springer-Verlag, Berlin, third edition, 2004.\n* [Pak23] Fedor Pakovich. Invariant curves for endomorphisms of \\(\\mathbb{P}^{1}\\times\\mathbb{P}^{1}\\). _Math. Ann._, 385(1-2):259-307, 2023.\n* [Poo17] Bjorn Poonen. _Rational points on varieties_, volume 186 of _Graduate Studies in Mathematics_. American Mathematical Society, Providence, RI, 2017.\n* [Sil98] Joseph H. Silverman. The space of rational maps on \\(\\mathbf{P}^{1}\\). _Duke Math. J._, 94(1):41-77, 1998.\n* [Sil07] Joseph H. Silverman. _The arithmetic of dynamical systems_, volume 241 of _Graduate Texts in Mathematics_. Springer-Verlag, New York, 2007.\n* [Sil12] Joseph H. Silverman. _Moduli spaces and arithmetic dynamics_, volume 30 of _CRM Monograph Series_. American Mathematical Society, Providence, RI, 2012.\n* [Tuc14] T. Tucker. Problem 6 in the problem list of the aim workshop postcritically finite maps in complex and arithmetic dynamics, 2014.\n* [Xie17] Junyi Xie. The existence of Zariski dense orbits for polynomial endomorphisms of the affine plane. _Compos. Math._, 153(8):1658-1672, 2017.\n* [Xie22] Junyi Xie. The existence of Zariski dense orbits for endomorphisms of projective surfaces (with an appendix in collaboration with T. Tucker). _J. Amer. Math. Soc._, 2022. published online.\n* [Xie23] Junyi Xie. Remarks on algebraic dynamics in positive characteristic. _J. Reine Angew. Math._, 797:117-153, 2023.\n* [XY23] Junyi Xie and Xinyi Yuan. Partial heights and the geometric Bombieri-Lang conjecture. arXiv:2305.14789, 2023.\n* [Yua08] Xinyi Yuan. Big line bundles over arithmetic varieties. _Invent. Math._, 173(3):603-649, 2008.\n* [Zdu14] Anna Zdunik. Characteristic exponents of rational functions. _Bulletin of the Polish Academy of Sciences. Mathematics_, 62(3), 2014.\n* [Zha95] Shou-Wu Zhang. Small points and adelic metrics. _J. Algebraic Geom._, 4(2):281-3 "]}, {"edit": ["some additional components are added to the matter distribution due to which the number of unknowns grow that make more challenging to solve the Einstein field equations analytically. In this regard, such analytical solutions developed via gravitational decoupling (GD) with minimal geometric deformation (MGD) method in both cosmology and astrophysics [21; 22].\n\nMultiple methods studied to investigate important properties of self-gravitating objects, including the phenomenon of stability and hydrodynamic equilibrium, the upper limit of the mass-to-radius ratio, the upper limit of superficial redshift, and dynamics of matter content under energy conditions, etc. [23]. One of these techniques is MGD approach, which was initially intended as an optional means of deforming Schwarzschild space-time in framework of the Randall-Sundrum braneworld [24; 25]. Recently, there is a lot of interest in developing novel analytic and an anisotropic solutions for Einstein field equations, which is a difficult task as Einstein field equations are non-linear and difficult to handle. In this way, the method of MGD to gain new models representing relativistic objects with well-determined characteristics have been proposed [26]. For a compact spherical distribution, the analytical solution of an anisotropic fluid as well as the braneworld model of Tolman IV solution have been found [27]. Two essential components are required in which first one is dimensionless coupling constant \"\\(\\alpha\\)\" to incorporate an extra source into the stress-energy tensor of seed solution. Second one is MGD method on the metric potentials (often on the radial component of metric) in the context of braneworld model. If the seed solution is assumed to be anisotropic, the inclusion of this additional component combined with a static and spherically-symmetric system gives rise to a complex simultaneous equations. The MGD technique separates Einstein field equations into two systems, namely the \"Einstein system\" and \"quasi-Einstein system\", which in comparison to the original system are easy to solve. At this point, a few observations are appropriate, firstly, the decoupled systems satisfy Bianchi Identities and secondly, the extra source may be a scalar, vector, or tensor field [28; 29; 30; 31; 32]. Moreover, a number of interesting results on the solutions of black hole with 2+1 and 3+1 decomposition obtained in [33; 34; 35; 36]. Additionally, the solutions of new hairy black hole have just been explored [37], and a mechanism is created as well to turn any non-rotational black hole into a rotational one [38; 39].\n\nWhen weak gravitational forces are at work, the hypothesis in GR has effectively aligned with many tests carried out within the solar system, demonstrating its success in cosmology. To get more accurate and dependable results, this theory may need to be modified when dealing with high gravitational fields or while being observed on a big scale. These changes may be very important in explaining the phenomena of accelerated expansion. These modifications are termed as modified theories (see, for instance [40; 41; 42; 43; 44; 45; 46; 47])\n\nMany modified theories of gravity [48; 49; 50; 51; 52] are taken into account by changing the Einstein-Hilbert action that is frequently used to study both the existence of dark energy and dark matter as well as the mystery of universe rapid expansion. Geometrical representation and scalar tensor representation of \\(f(G,T)\\) gravity has been presented to establish novel junction condition [53].\n\nSeveral researchers are interested to explore the gravitational collapse phenomena because it is a prominent case in a strong-field regime [54; 55; 56]. Jordan [57] developed a full gravitational theory which gave the title of a gravitational scalar field to gravitational constant. Brans and Dicke [58] developed a scalar-tensor field theory named as the Brans-Dicke (BD) theory obtained by substituting a time modifying constant \\(G(t)\\) and with the help of a scalar field (\\(\\Phi\\)) having interaction along with the geometry. Additionally, the well-known scalar field coupling constant or parameter (\\(\\omega_{BD}\\)) of the BD theory is a constant that can be adjusted to get the desired outcomes in Jordan frame. It is assumed that the \\(\\Phi\\) is reciprocal of the dynamical gravitational constant, i.e., \\(G(t)=\\frac{1}{\\Phi(t)}\\). The test particles travel along geodesics according to "], "nougat": ["some additional components are added to the matter distribution due to which the number of unknowns grow that make more challenging to solve the Einstein field equations analytically. In this regard, such analytical solutions developed via gravitational decoupling (GD) with minimal geometric deformation (MGD) method in both cosmology and astrophysics [21, 22].\n\nMultiple methods studied to investigate important properties of self-gravitating objects, including the phenomenon of stability and hydrodynamic equilibrium, the upper limit of the mass-to-radius ratio, the upper limit of superficial redshift, and dynamics of matter content under energy conditions, etc. [23]. One of these techniques is MGD approach, which was initially intended as an optional means of deforming Schwarzschild space-time in framework of the Randall-Sundrum braneworld [24, 25]. Recently, there is a lot of interest in developing novel analytic and an anisotropic solutions for Einstein field equations, which is a difficult task as Einstein field equations are non-linear and difficult to handle. In this way, the method of MGD to gain new models representing relativistic objects with well-determined characteristics have been proposed [26]. For a compact spherical distribution, the analytical solution of an anisotropic fluid as well as the braneworld model of Tolman IV solution have been found [27]. Two essential components are required in which first one is dimensionless coupling constant \"\\(\\alpha\\)\" to incorporate an extra source into the stress-energy tensor of seed solution. Second one is MGD method on the metric potentials (often on the radial component of metric) in the context of braneworld model. If the seed solution is assumed to be anisotropic, the inclusion of this additional component combined with a static and spherically-symmetric system gives rise to a complex simultaneous equations. The MGD technique separates Einstein field equations into two systems, namely the \u201cEinstein system\u201d and \u201cquasi-Einstein system\u201d, which in comparison to the original system are easy to solve. At this point, a few observations are appropriate, firstly, the decoupled systems satisfy Bianchi Identities and secondly, the extra source may be a scalar, vector, or tensor field [28\u201332]. Moreover, a number of interesting results on the solutions of black hole with 2+1 and 3+1 decomposition obtained in [33\u201336]. Additionally, the solutions of new hairy black hole have just been explored [37], and a mechanism is created as well to turn any non-rotational black hole into a rotational one [38, 39].\n\nWhen weak gravitational forces are at work, the hypothesis in GR has effectively aligned with many tests carried out within the solar system, demonstrating its success in cosmology. To get more accurate and dependable results, this theory may need to be modified when dealing with high gravitational fields or while being observed on a big scale. These changes may be very important in explaining the phenomena of accelerated expansion. These modifications are termed as modified theories (see, for instance [40\u201347])\n\nMany modified theories of gravity [48\u201352] are taken into account by changing the Einstein-Hilbert action that is frequently used to study both the existence of dark energy and dark matter as well as the mystery of universe rapid expansion. Geometrical representation and scalar tensor representation of \\(f(G,T)\\) gravity has been presented to establish novel junction condition [53].\n\nSeveral researchers are interested to explore the gravitational collapse phenomena because it is a prominent case in a strong-field regime [54\u201356]. Jordan [57] developed a full gravitational theory which gave the title of a gravitational scalar field to gravitational constant. Brans and Dicke [58] developed a scalar-tensor field theory named as the BransDicke (BD) theory obtained by substituting a time modifying constant \\(G(t)\\) and with the help of a scalar field (\\(\\Phi\\)) having interaction along with the geometry. Additionally, the well-known scalar field coupling constant or parameter (\\(\\omega_{BD}\\)) of the BD theory is a constant that can be adjusted to get the desired outcomes in Jordan frame. It is assumed that the \\(\\Phi\\) is reciprocal of the dynamical gravitational constant, i.e., \\(G(t)=\\frac{1}{\\Phi(t)}\\). The test particles travel along geodesics according to the BD theory, they consequently obey the weak equivalence principle, which states that the gravitational mass and inertial mass are equivalent. Mach principle, agreement with the weak equivalence principle, and Dirac's large number hypothesis are the main ingredients of the BD theory. This theory includes a metric tensor and a scalar field that describes gravity.\n\nThe large "]}, {"edit": ["\n\n# Adaptive Low Rank Adaptation of Segment Anything to Salient Object Detection\n\nRuikai Cui\n\nAustralian National University\n\n1\n\nSiyuan He\n\nAustralian National University\n\n1\n\nShi Qiu\n\nAustralian National University\n\n1\n\nFootnote 1: email: {ruikai.cui, siyuan.he, shi.qiu}@anu.edu.au\n\n###### Abstract\n\nFoundation models, such as OpenAI's GPT-3 and GPT-4, Meta's LLaMA, and Google's PaLM2, have revolutionized the field of artificial intelligence. A notable paradigm shift has been the advent of the Segment Anything Model (SAM), which has exhibited a remarkable capability to segment real-world objects, trained on 1 billion masks and 11 million images. Although SAM excels in general object segmentation, it lacks the intrinsic ability to detect salient objects, resulting in suboptimal performance in this domain. To address this challenge, we present the Segment Salient Object Model (SSOM), an innovative approach that adaptively fine-tunes SAM for salient object detection by harnessing the low-rank structure inherent in deep learning. Comprehensive qualitative and quantitative evaluations across five challenging RGB benchmark datasets demonstrate the superior performance of our approach, surpassing state-of-the-art methods.\n\nKeywords:salient object detection large-scale pre-trained models parameter-efficient fine-tuning.\n\n## 1 Introduction\n\nFoundation models [14, 23, 3] have received significant interests in recent years, owing to their exceptional performance across a multitude of diverse tasks These models typically consume billions of parameters, trained on expansive web-scaled datasets for fundamental tasks such as next token prediction [6] or masked region completion [7]. A particularly compelling instance of these models is the Segment-Anything Model (SAM) [14], which has been trained on an unprecedentedly vast dataset comprising 11 million images and 1 billion masks.\n\nDespite the Segment-Anything Model's (SAM) noteworthy proficiency in generating masks to segment real-world objects, it is deficient in the detection of salient objects. This shortcoming leads to suboptimal performance in isolating a single salient object from a given RGB image, a crucial aspect of computer vision that emphasizes the identification of the most visually striking or attention-demanding object within an image.\n\nTraditional approaches for harnessing the capabilities of foundation models for downstream tasks generally include fine-tuning the entire model [11] or integrating additional adapter layers [9]. However, most foundation models possess"], "nougat": ["\n\n# Adaptive Low Rank Adaptation of Segment Anything to Salient Object Detection\n\nRuikai Cui, Siyuan He, and Shi Qiu\n\nAustralian National University\n\n1\n\nFootnote 1: email: {ruikai.cui, siyuan.he, shi.qiu}@anu.edu.au\n\n###### Abstract\n\nFoundation models, such as OpenAI's GPT-3 and GPT-4, Meta's LLaMA, and Google's PaLM2, have revolutionized the field of artificial intelligence. A notable paradigm shift has been the advent of the Segment Anything Model (SAM), which has exhibited a remarkable capability to segment real-world objects, trained on 1 billion masks and 11 million images. Although SAM excels in general object segmentation, it lacks the intrinsic ability to detect salient objects, resulting in suboptimal performance in this domain. To address this challenge, we present the Segment Salient Object Model (SSOM), an innovative approach that adaptively fine-tunes SAM for salient object detection \u00b7 large-scale pre-trained models \u00b7 large-scale pre-"]}, {"edit": ["ten publications were applied to more than one language: six publications considered two programming languages, one publication considered three languages, and three publications considered four languages. This results in an average of 1.33 programming languages considered per publication.\n\nIn addition to programming languages considered, we collect training details, such as hardware used and training time for each publication. However, those are not always provided. There are 22 out of 52 publications without hardware details (42%) and 26 out of 52 without training time (50%), 33% shared neither information (17 out of 52 publications). The training time of 26 publications with such details ranges from two hours or less [41, 55, 78, 79, 85] to hundreds of hours [47, 53]. While it is common to perform training on GPUs, there are four publications that did not use any GPU for their training procedure, published from 2015-2019 [41, 53, 77, 86]. Commonly, publications used a single GPU for training [39, 40, 43, 44, 49, 55, 63, 68, 75, 78, 79, 80, 87, 88], sometimes in combination with CPUs. The highest amount of GPUs have been used by Svyatkovskiy et al. [47]. They utilized 5 Lambda V100 boxes, with 16 V100 GPUs each, resulting in 80 GPUs.\n\nWhile we focus on the training procedure and the energy associated with creating and sharing an ML model, we note the application of such models can vary highly for different SE tasks. Usually, the reported tested times are lower than the required training time (e.g., more than 100 times quicker than training [40, 75, 76]), but in particular, program repair experiments can require long testing times. For example, Chen et al. [55] applied Sequencer for 130 hours to find patches for 75 bugs. White et al. [56] applied their program repair tool DeepRepair for 2,616 days. Data extraction and preparation steps can also require considerable amounts of time and compute resources, ranging from 5-12 days [73, 78, 81].\n\nThe majority of task-specific publications provided access to the full trained models, some of which one needs to request access to [51, 76]. Moreover, there are approaches shared as online tools [44, 49, 86] or IDE extensions [47, 48, 83, 85]. There are also 12 out of 52 publications that did not share the full model, but trained embedding files, which are used by the model. These are marked in Table II with the \\({}^{\\dagger}\\) symbol.\n\n## V Task-Agnostic Code Models\n\nThis section presents task-agnostic code models which share means of representing source code as embeddings, for a variety of downstream tasks. These models are able to transform code snippets to embeddings, which can be fine-tuned to SE tasks. For example, Lu et al. [108] provided fine-tuning details for the CodeXGLUE benchmark, with information for task-specific training and inference time for each task.3 The fine-tuning time ranges from 2 GPU hours (defect detection) to 60 hours (text-to-code generation, documentation translation).\n\nFootnote 3: [https://microsoft.github.io/CodeXGLUE/](https://microsoft.github.io/CodeXGLUE/)\n\nIn total, we collected 27 task-agnostic models, as shown in Table III. For each publication, we list the model name and the programming languages it was trained on. If available, we list details on hardware configuration and training times. Among the 27 publications, 52% did not provide training time details (14 out of 27) and 26% did not provide their hardware configurations (7 out of 27). For publications without hardware details, training time is not reported as well.\n\nAmong the publications that shared training time details, the shortest duration is found for code2vec [101], which was "], "nougat": ["ten publications were applied to more than one language: six publications considered two programming languages, one publication considered three languages, and three publications considered four languages. This results in an average of 1.33 programming languages considered per publication.\n\nIn addition to programming languages considered, we collect training details, such as hardware used and training time for each publication. However, those are not always provided. There are 22 out of 52 publications without hardware details (42%) and 26 out of 52 without training time (50%), 33% shared neither information (17 out of 52 publications). The training time of 26 publications with such details ranges from two hours or less [41, 55, 78, 79, 85] to hundreds of hours [47, 53]. While it is common to perform training on GPUs, there are four publications that did not use any GPU for their training procedure, published from 2015\u20132019 [41, 53, 77, 86]. Commonly, publications used a single GPU for training [39, 40, 43, 44, 49, 55, 63, 68, 75, 78\u201380, 87, 88], sometimes in combination with CPUs. The highest amount of GPUs have been used by Svyatkovskiy et al. [47]. They utilized 5 Lambda V100 boxes, with 16 V100 GPUs each, resulting in 80 GPUs.\n\nWhile we focus on the training procedure and the energy associated with creating and sharing an ML model, we note the application of such models can vary highly for different SE tasks. Usually, the reported tested times are lower than the required training time (e.g., more than 100 times quicker than training [40, 75, 76]), but in particular, program repair experiments can require long testing times. For example, Chen et al. [55] applied Sequencer for 130 hours to find patches for 75 bugs. White et al. [56] applied their program repair tool DeepRepair for 2,616 days. Data extraction and preparation steps can also require considerable amounts of time and compute resources, ranging from 5-12 days [73, 78, 81].\n\nThe majority of task-specific publications provided access to the full trained models, some of which one needs to request access to [51, 76]. Moreover, there are approaches shared as online tools [44, 49, 86] or IDE extensions [47, 48, 83, 85]. There are also 12 out of 52 publications that did not share the full model, but trained embedding files, which are used by the model. These are marked in Table II with the \\({}^{\\dagger}\\) symbol.\n\n## V Task-Agnostic Code Models\n\nThis section presents task-agnostic code models which share means of representing source code as embeddings, for a variety of downstream tasks. These models are able to transform code snippets to embeddings, which can be fine-tuned to SE tasks. For example, Lu et al. [108] provided fine-tuning details for the CodeXGLUE benchmark, with information for task-specific training and inference time for each task.3 The fine-tuning time ranges from 2 GPU hours (defect detection) to 60 hours (text-to-code generation, documentation translation).\n\nFootnote 3: [https://microsoft.github.io/CodeXGLUE/](https://microsoft.github.io/CodeXGLUE/)\n\nIn total, we collected 27 task-agnostic models, as shown in Table III. For each publication, we list the model name and the programming languages it was trained on. If available, we list details on hardware configuration and training times. Among the 27 publications, 52% did not provide training time details (14 out of 27) and 26% did not provide their hardware configurations (7 out of 27). For publications without hardware details, training time is not reported as well.\n\nAmong the publications that shared training time details, the shortest duration is found for code2vec [101], which was "]}, {"edit": ["For the fourth property we need to find a value for \\(\\beta\\) such that \\(|Opt(G)-|I_{F}||\\leq\\beta|Opt(T_{G},T^{\\prime}_{G})-|F||\\). Let \\(\\ell=|I_{F}|\\) be the number of \\(A_{v}\\) in \\(F^{\\prime\\prime}\\) that are covered by 6 components. We know that \\(|F|\\geq|F^{\\prime}|\\geq|F^{\\prime\\prime}|=12n-\\ell\\). Observe:\n\n\\[|Opt(T_{G},T^{\\prime}_{G})-|F|| =|F|-Opt(T_{G},T^{\\prime}_{G})\\] \\[\\geq|F^{\\prime\\prime}|-Opt(T_{G},T^{\\prime}_{G})\\] \\[=12n-\\ell-Opt(T_{G},T^{\\prime}_{G})\\] \\[=12n-\\ell-(12n-k)\\] \\[=k-\\ell\\] \\[=|Opt(G)-|I_{F}||\\]\n\nSo we pick \\(\\beta=1\\) and we are done.\n\n## 5 A tight 7k kernel\n\nRecall the definitions of common subtrees and common chains from the preliminaries. It is well-known that the following two polynomial-time reduction rules do not alter the size of the uMAF [2]:\n\n**Subtree reduction.** If \\(T\\) and \\(T^{\\prime}\\) have a maximal common pendant subtree \\(S\\) with at least two leaves, then reduce \\(T\\) and \\(T^{\\prime}\\) to \\(T_{r}\\) and \\(T^{\\prime}_{r}\\), respectively, by replacing \\(S\\) with a single leaf with a new label.\n\n**Chain reduction.** If \\(T\\) and \\(T^{\\prime}\\) have a maximal common \\(n\\)-chain \\(C=(\\ell_{1},\\ell_{2},\\ldots,\\ell_{n})\\) with \\(n\\geq 4\\), then reduce \\(T\\) and \\(T^{\\prime}\\) to \\(T_{r}=T|X\\setminus\\{\\ell_{4},\\ell_{5},\\ldots,\\ell_{n}\\}\\) and \\(T^{\\prime}_{r}=T^{\\prime}|X\\setminus\\{\\ell_{4},\\ell_{5},\\ldots,\\ell_{n}\\}\\), respectively.\n\nWhen applied to exhaustion on two unrooted binary trees, at which point we say the trees are _fully reduced_, these rules yield an instance with (ignoring additive terms) at most \\(15k\\) taxa [10], where \\(k\\) is the size of the uMAF3, and the analysis is tight.\n\nFootnote 3: The kernel bound given in [10] is in terms of TBR distance, rather than uMAF, but as noted earlier these quantities only differ by 1, so only additive terms are affected.\n\nNote that applying the subtree or chain reduction to a caterpillar produces a new caterpillar. In this section we will show that, when applied to exhaustion on two caterpillars, a much smaller kernel is obtained than on general unrooted binary trees.\n\n**Theorem 4**.: _There is a 7k kernel for uMAF on caterpillars using only the common chain and subtree reductions, and this is tight up to a constant additive term._ "], "nougat": ["For the fourth property we need to find a value for \\(\\beta\\) such that \\(|Opt(G)-|I_{F}||\\leq\\beta|Opt(T_{G},T^{\\prime}_{G})-|F||\\). Let \\(\\ell=|I_{F}|\\) be the number of \\(A_{v}\\) in \\(F^{\\prime\\prime}\\) that are covered by 6 components. We know that \\(|F|\\geq|F^{\\prime}|\\geq|F^{\\prime\\prime}|=12n-\\ell\\). Observe:\n\n\\[|Opt(T_{G},T^{\\prime}_{G})-|F|| =|F|-Opt(T_{G},T^{\\prime}_{G})\\] \\[\\geq|F^{\\prime\\prime}|-Opt(T_{G},T^{\\prime}_{G})\\] \\[=12n-\\ell-Opt(T_{G},T^{\\prime}_{G})\\] \\[=12n-\\ell-(12n-k)\\] \\[=k-\\ell\\] \\[=|Opt(G)-|I_{F}||\\]\n\nSo we pick \\(\\beta=1\\) and we are done.\n\n## 5 A tight 7k kernel\n\nRecall the definitions of common subtrees and common chains from the preliminaries. It is well-known that the following two polynomial-time reduction rules do not alter the size of the uMAF [2]:\n\n**Subtree reduction.** If \\(T\\) and \\(T^{\\prime}\\) have a maximal common pendant subtree \\(S\\) with at least two leaves, then reduce \\(T\\) and \\(T^{\\prime}\\) to \\(T_{r}\\) and \\(T^{\\prime}_{r}\\), respectively, by replacing \\(S\\) with a single leaf with a new label.\n\n**Chain reduction.** If \\(T\\) and \\(T^{\\prime}\\) have a maximal common \\(n\\)-chain \\(C=(\\ell_{1},\\ell_{2},\\ldots,\\ell_{n})\\) with \\(n\\geq 4\\), then reduce \\(T\\) and \\(T^{\\prime}\\) to \\(T_{r}=T|X\\setminus\\{\\ell_{4},\\ell_{5},\\ldots,\\ell_{n}\\}\\) and \\(T^{\\prime}_{r}=T^{\\prime}|X\\setminus\\{\\ell_{4},\\ell_{5},\\ldots,\\ell_{n}\\}\\), respectively.\n\nWhen applied to exhaustion on two unrooted binary trees, at which point we say the trees are _fully reduced_, these rules yield an instance with (ignoring additive terms) at most \\(15k\\) taxa [10], where \\(k\\) is the size of the uMAF 3, and the analysis is tight.\n\nFootnote 3: The kernel bound given in [10] is in terms of TBR distance, rather than uMAF, but as noted earlier these quantities only differ by 1, so only additive terms are affected.\n\nNote that applying the subtree or chain reduction to a caterpillar produces a new caterpillar. In this section we will show that, when applied to exhaustion on two caterpillars, a much smaller kernel is obtained than on general unrooted binary trees.\n\n**Theorem 4**.: _There is a 7k kernel for uMAF on caterpillars using only the common chain and subtree reductions, and this is tight up to a constant additive term._ "]}, {"edit": ["all \\(p_{i}\\)'s are integers and \\(B=\\frac{4ac^{*}}{\\varepsilon}\\). We can compute \\(\\min(h_{\\mathcal{I}_{1}^{0}}(x),B)\\) exactly using standard dynamic programming in \\(O(|\\mathcal{I}_{1}^{0}|\\cdot B)=\\widetilde{O}(\\frac{1}{\\varepsilon^{2}})\\) time.\n\nThe complexity of the resulting function is \\(O(|\\mathcal{I}_{1}^{0}|)=\\widetilde{O}(\\frac{1}{\\varepsilon})\\).\n\nIn \\(\\widetilde{O}(n+\\frac{1}{\\varepsilon^{2}})\\) time, we can compute a function \\(\\widetilde{f}_{\\mathcal{I}_{3}^{0}}\\) that approximates \\(\\min(f_{\\mathcal{I}_{3}^{0}},\\frac{4c^{*}}{\\varepsilon^{3/2}})\\) with a factor of \\(1+\\widetilde{O}(\\varepsilon)\\) via Lemma 7. The additive error caused by \\(\\widetilde{f}_{\\mathcal{I}_{3}^{0}}\\) is at most \\(\\widetilde{O}(\\varepsilon\\cdot\\frac{4c^{*}}{\\varepsilon^{3/2}})\\leqslant \\widetilde{O}(\\frac{1}{a\\varepsilon})\\) as \\(a\\leqslant\\frac{1}{\\varepsilon^{1/2}}\\), and the complexity of \\(\\widetilde{f}_{\\mathcal{I}_{3}^{0}}\\) is \\(\\widetilde{O}(\\frac{1}{\\varepsilon})\\).\n\n**Lemma 13**.: _Let \\(\\widetilde{f}_{\\mathcal{I}_{1}^{0}}\\) and \\(\\widetilde{f}_{\\mathcal{I}_{3}^{0}}\\) be two functions that approximate \\(\\max(f_{\\mathcal{I}_{1}^{0}},p(\\mathcal{I}_{1}^{0})-\\frac{4c^{*}}{\\varepsilon ^{3/2}})\\) and \\(\\min(f_{\\mathcal{I}_{3}^{0}},\\frac{4c^{*}}{\\varepsilon^{3/2}})\\) with additive error \\(\\widetilde{O}(\\frac{1}{a\\varepsilon})\\) and complexity \\(\\widetilde{O}(\\frac{1}{\\varepsilon})\\), respectively. We can approximate \\(\\widetilde{f}_{\\mathcal{I}_{1}^{0}}\\oplus\\widetilde{f}_{\\mathcal{I}_{3}^{0}}\\) with additive error \\(\\widetilde{O}(\\frac{1}{a\\varepsilon})\\) and complexity \\(\\widetilde{O}(\\frac{1}{\\varepsilon})\\) in \\(\\widetilde{O}(\\frac{a^{2}}{\\varepsilon})\\) time._\n\nProof.: Let \\(u=\\max(0,p(\\mathcal{I}_{1}^{0})-\\frac{4c^{*}}{\\varepsilon^{3/2}})\\). Note that \\(\\widetilde{f}_{\\mathcal{I}_{1}^{0}}\\) have a range contained in \\([u,p(\\mathcal{I}_{1}^{0})]\\), so \\(\\widetilde{f}_{\\mathcal{I}_{1}^{0}}-u+1\\) have a range contained in \\([1,\\frac{4c^{*}}{\\varepsilon^{3/ "], "nougat": ["all \\(p_{i}\\)'s are integers and \\(B=\\frac{4ac^{*}}{\\varepsilon}\\). We can compute \\(\\min(h_{\\mathcal{I}_{1}^{0}}(x),B)\\) exactly using standard dynamic programming in \\(O(|\\mathcal{I}_{1}^{0}|\\cdot B)=\\widetilde{O}(\\frac{1}{\\varepsilon^{2}})\\) time.\n\nThe complexity of the resulting function is \\(O(|\\mathcal{I}_{1}^{0}|)=\\widetilde{O}(\\frac{1}{\\varepsilon})\\).\n\nIn \\(\\widetilde{O}(n+\\frac{1}{\\varepsilon^{2}})\\) time, we can compute a function \\(\\widetilde{f}_{\\mathcal{I}_{3}^{0}}\\) that approximates \\(\\min(f_{\\mathcal{I}_{3}^{0}},\\frac{4c^{*}}{\\varepsilon^{3/2}})\\) with a factor of \\(1+\\widetilde{O}(\\varepsilon)\\) via Lemma 7. The additive error caused by \\(\\widetilde{f}_{\\mathcal{I}_{3}^{0}}\\) is at most \\(\\widetilde{O}(\\varepsilon\\cdot\\frac{4c^{*}}{\\varepsilon^{3/2}})\\leqslant \\widetilde{O}(\\frac{1}{a\\varepsilon})\\) as \\(a\\leqslant\\frac{1}{\\varepsilon^{1/2}}\\), and the complexity of \\(\\widetilde{f}_{\\mathcal{I}_{3}^{0}}\\) is \\(\\widetilde{O}(\\frac{1}{\\varepsilon})\\).\n\n**Lemma 13**.: _Let \\(\\widetilde{f}_{\\mathcal{I}_{1}^{0}}\\) and \\(\\widetilde{f}_{\\mathcal{I}_{3}^{0}}\\) be two functions that approximate \\(\\max(f_{\\mathcal{I}_{1}^{0}},p(\\mathcal{I}_{1}^{0})-\\frac{4c^{*}}{\\varepsilon ^{3/2}})\\) and \\(\\min(f_{\\mathcal{I}_{3}^{0}},\\frac{4c^{*}}{\\varepsilon^{3/2}})\\) with additive error \\(\\widetilde{O}(\\frac{1}{a\\varepsilon})\\) and complexity \\(\\widetilde{O}(\\frac{1}{\\varepsilon})\\), respectively. We can approximate \\(\\widetilde{f}_{\\mathcal{I}_{1}^{0}}\\oplus\\widetilde{f}_{\\mathcal{I}_{3}^{0}}\\) with additive error \\(\\widetilde{O}(\\frac{1}{a\\varepsilon})\\) and complexity \\(\\widetilde{O}(\\frac{1}{\\varepsilon})\\) in \\(\\widetilde{O}(\\frac{a^{2}}{\\varepsilon})\\) time._\n\nProof.: Let \\(u=\\max(0,p(\\mathcal{I}_{1}^{0})-\\frac{4c^{*}}{\\varepsilon^{3/2}})\\). Note that \\(\\widetilde{f}_{\\mathcal{I}_{1}^{0}}\\) have a range contained in \\([u,p(\\mathcal{I}_{1}^{0})]\\), so \\(\\widetilde{f}_{\\mathcal{I}_{1}^{0}}-u+1\\) have a range contained in \\([1,\\frac{4c^{*}}{\\varepsilon^{3/ "]}, {"edit": ["Many of the earlier mathematical works on rcsps focused on determining their satisfiability thresholds and verifying the sharpness of sat-unsat transitions. For models that are known not to exhibit rsb, such goals were established. These models include random 2-sat[12, 13], random 1-in-\\(k\\)-sat[1], \\(k\\)-xor-sat[1, 10, 14], and random linear equations [1]. On the other hand, for the models which are predicted to belong to 1rsb class, intensive studies have been conducted to estimate their satisfiability threshold, as shown in [15, 16, 17] (random \\(k\\)-sat), [1, 18, 19] (random \\(k\\)-sate), and [1, 18, 19] (random graph coloring).\n\nMore recently, the satisfiability thresholds for rcsps that exhibits rsb have been rigorously determined for several models, namely the random regular \\(k\\)-nae-sat[12], maximum independent set on \\(d\\)-regular graphs [12], random regular \\(k\\)-sat[19] and random \\(k\\)-sat[12] for large \\(k\\) and \\(d\\). Although determining the location of \\(q\\)-colorability threshold for the sparse Erdos Renyi graph is left open, the _condensation threshold_\\(\\alpha_{\\sf cond}\\) for random graph coloring, where the _free energy_ becomes non-analytic, was settled in [1]. They carried out a technically challenging analysis based on a clever \"planting\" technique, where the results were further generalized to other models in [1]. Similarly, [1] identified the condensation threshold for random regular \\(k\\)-sat, where each variable appears \\(d/2\\)-times positive and \\(d/2\\)-times negative. Further, in the condensation regime \\(\\alpha\\in(\\alpha_{\\sf cond},\\alpha_{\\sf sat})\\), many quantities of interest was established for random regular \\(k\\)-nae-sat with large enough \\(k\\), matching the statistical physics prediction. Namely, the number of solutions at exponential scale (free energy) [12], the concentration of the _overlap_[12, 13], and the local weak limit [12] were established. Establishing the same quantities for other models in the condensation regime is left open.\n\nThe closest result to ours in the literature is by Ayre, Coja-Oghlan, and Greenhill [1], where they lower bound the chromatic number (or equivalently, upper bound the colorability threshold) of the random regular graph of any degree, which is conjectured to be tight. [1] also considers the sparse Erdos Renyi graph, which is more complicated since the conjectured chromatic number is defined in terms of a distributional (rather than real-valued) optimization due to the randomness of the local neighborhoods. In this work, we do not consider Erdos Renyi type problems, but we additionally address the question of the uniqueness of the bp fixed point for any \\(k\\geq 3\\) (unique solution to the equation (1.1)). As in [1], we use an interpolation bound, which gives an upper bound of the satisfiability threshold also for the (non-regular) random \\(k\\)-nae-sat model. It would be interesting to address the uniqueness of the bp fixed point for random \\(k\\)-nae-sat and random \\(k\\)-sat for small \\(k\\geq 3\\). We refer to [12, 13, 14, 15] which addresses the uniqueness of bp fixed point for various models.\n\n### Proof methods\n\nWe aim to rigorously establish the upper bound the satisfiability threshold predicted by the so-called '1rsb cavity method' from statistical physics [10]. To do so, instead of using moment methods, we use a technique called 'interpolation method' from the theory of spin glasses developed by [13, 14, 15]. The interpolation method has been successful in upperbounding the satisfiability threshold for random \\(k\\)-sat[ "], "nougat": ["Many of the earlier mathematical works on rcsps focused on determining their satisfiability thresholds and verifying the sharpness of sat-unsat transitions. For models that are known not to exhibit rsb, such goals were established. These models include random 2sat [ CR92 , BBC + 01 ], random 1in -\\(k\\)-sat [ ACIM01 ], \\(k\\)-xor-sat [ DM02 , DGM + 10 , PS16 ], and random linear equations [ ACOGM20 ]. On the other hand, for the models which are predicted to belong to 1 rsb class, intensive studies have been conducted to estimate their satisfiability threshold, as shown in [ KKKS98 , AP04 , COP16 ] (random \\(k\\)-sat), [ AM06 , CO13 , COV13 , COEH16 ] (random graph coloring).\n\nMore recently, the satisfiability thresholds for r csps that exhibits rsb have been rigorously determined for several models, namely the random regular \\(k\\)-nae-sat [ DSS16b ], maximum independent set on \\(d\\)-regular graphs [ DSS16a ], random regular \\(k\\)-sat [ COP16 ] and random \\(k\\)-sat [ DSS22 ] for large \\(k\\) and \\(d\\). Although determining the location of \\(q\\)-colorability threshold for the sparse Erdos Renyi graph is left open, the _condensation threshold_\\(\\alpha_{\\mathsf{cond}}\\) for random graph coloring, where the _free energy_ becomes non-analytic, was settled in [ BCOH + 16 ]. They carried out a technically challenging analysis based on a clever \u201cplanting\u201d technique, where the results were further generalized to other models in [ COKPZ18 ]. Similarly, [ BCO16 ] identified the condensation threshold for random regular \\(k\\)-sat, where each variable appears \\(d/2\\)-times positive and \\(d/2\\)-times negative. Further, in the condensation regime \\(\\alpha\\in(\\alpha_{\\mathsf{cond}},\\alpha_{\\mathsf{sat}})\\), many quantities of interest was established for random regular \\(k\\)-nae-sat with large enough \\(k\\), matching the statistical physics prediction. Namely, the number of solutions at exponential scale (free energy) [ SSZ22 ], the concentration of the overlap [ NSS20 , NSS21 ], and the local weak limit [ SS23 ] were established. Establishing the same quantities for other models in the condensation regime is left open.\n\nThe closest result to ours in the literature is by Ayre, Coja-Oghlan, and Greenhill [ ACOG22 ], where they lower bound the chromatic number (or equivalently, upper bound the colorability threshold) of the random regular graph of any degree, which is conjectured to be tight. [ ACOG22 ] also considers the sparse Erdos Renyi graph, which is more complicated since the conjectured chromatic number is defined in terms of a distributional (rather than real-valued) optimization due to the randomness of the local neighborhoods. In this work, we do not consider Erdos Renyi type problems, but we additionally address the question of the uniqueness of the bp fixed point for any \\(k\\geq 3\\) (unique solution to the equation ( 1.1 )). As in [ ACOG22 ], we use an interpolation bound, which gives an upper bound of the satisfiability threshold also for the (nonregular) random \\(k\\)-nae-sat model. It would be interesting to address the uniqueness of the bp fixed point for random \\(k\\)-nae-sat and random \\(k\\)-sat for small \\(k\\geq 3\\). We refer to [ ST03 , MRSY19 , YP22 , GP23 ] which addresses the uniqueness of bp fixed point for various models.\n\n### Proof methods\n\nWe aim to rigorously establish the upper bound the satisfiability threshold predicted by the so-called \u20181rsb cavity "]}, {"edit": ["expanding box like model valid for both radially and temporally varying spherical flows. The generalisation of the expanding box model to radially varying flows was done in Tenerani & Velli (2017). The generalisation to background flows which are also time dependant, motivated by the stellar formation problem, results in a model close to the accelerated expanding box (although our treatment of pressure will be closer to the distorted shearing box models of Ogilvie & Latter (2013); Ogilvie & Barker (2014)). We shall focus, in this paper, on the hydrodynamic case as it posses a number of important features that are worth understanding before generalising to MHD.\n\nIn Section 2 we present the derivation of our local model. Sections 2.4 and 2.5 derives symmetries and conservation laws of the local model. Section 3 presents some nonlinear solutions to the local model - and discuss how these relate to the global problem. In Section 4 we derive the linear theory of our local model. We discuss possible extension of our model in Section 5. We present our conclusions in Section 6 and additional mathematical details (including alternative formulations which maybe more convenient for implementation in hydrocodes) are presented in the appendices.\n\n## 2 Derivation\n\n### Global geometry\n\nTo derive a local model for spherical collapse/expansion consider a local neighbourhood of a point, \\(o\\), located on the equator of a sphere of radius \\(R\\). The line element of the usual spherical polar coordinate system is\n\n\\[\\mathrm{d}s^{2}=\\mathrm{d}R^{2}+R^{2}(\\mathrm{d}\\theta^{2}+\\sin^{2}\\theta\\, \\mathrm{d}\\phi^{2}). \\tag{1}\\]\n\nWe are interested in describing the local dynamics near to \\(p\\) occurring on a horizontal lengthscale \\(L_{\\mathrm{H}}\\ll R\\) (See Figure 1, which show the relationship between the global and local geometries). Without loss of generality, we can locate our local model on the equator of the sphere (\\(\\theta=\\pi/2\\)) meaning we can approximate the line element by\n\n\\[\\mathrm{d}s^{2}=\\mathrm{d}R^{2}+R^{2}(\\mathrm{d}\\theta^{2}+\\mathrm{d}\\phi^{2} )+\\mathcal{O}((L_{\\mathrm{H}}/R)^{2}d\\phi^{2}), \\tag{2}\\]\n\nwhich results in metric tensor components,\n\n\\[g_{RR}=1,\\quad g_{\\theta\\theta}=g_{\\phi\\phi}=R^{2}, \\tag{3}\\]\n\nand inverse metric tensor components\n\n\\[g^{RR}=1,\\quad g^{\\theta\\theta}=g^{\\phi\\phi}=R^{-2}, \\tag{4}\\]\n\nwith all other components zero. The Christoffel Symbols components, for this coordinate system, are\n\n\\[\\Gamma^{R}_{\\theta\\theta}=\\Gamma^{R}_{\\phi\\phi}=-R, \\tag{5}\\] \\[\\Gamma^{\\theta}_{\\theta R}=\\Gamma^{\\theta}_{R\\theta}=\\Gamma^{\\phi }_{\\phi R}=\\Gamma^{\\phi}_{R\\phi}=R^{-1},\\]\n\nwith all others vanishing. The fluid equations in this coordinate system are\n\n\\[Du^{\\theta}+\\frac{2}{R}u^{R}u^{\\theta} =-R^{-2}\\left(\\partial_{\\theta}\\Phi+\\frac{1}{\\rho}\\partial_{ \\theta}p\\right),\\] (6) \\[Du^{\\phi}+\\frac{2}{R}u^{R}u^{\\phi} =-R^{-2}\\left(\\partial_{\\phi}\\Phi+\\frac{1}{\\rho}\\partial_{\\theta }p\\right),\\] (7) \\[Du^{R}-Ru "], "nougat": ["expanding box like model valid for both radially and temporally varying spherical flows. The generalisation of the expanding box model to radially varying flows was done in Tenerani & Velli ( 2017 ). The generalisation to background flows which are also time dependant, motivated by the stellar formation problem, results in a model close to the accelerated expanding box (although our treatment of pressure will be closer to the distorted shearing box models of Ogilvie & Latter ( 2013 ); Ogilvie & Barker ( 2014 )). We shall focus, in this paper, on the hydrodynamic case as it posses a number of important features that are worth understanding before generalising to MHD.\n\nIn Section 2 we present the derivation of our local model. Sections 2.4 and 2.5 derives symmetries and conservation laws of the local model. Section 3 presents some nonlinear solutions to the local model and discuss how these relate to the global problem. In Section 4 we derive the linear theory of our local model. We discuss possible extension of our model in Section 5 . We present our conclusions in Section 6 and additional mathematical details (including alternative formulations which maybe more convenient for implementation in hydrocodes) are presented in the appendices.\n\n## 2 Derivation\n\n### Global geometry\n\nTo derive a local model for spherical collapse/expansion consider a local neighbourhood of a point, \\(o\\), located on the equator of a sphere of radius \\(R\\). The line element of the usual spherical polar coordinate system is\n\n\\[\\mathrm{d}s^{2}=\\mathrm{d}R^{2}+R^{2}(\\mathrm{d}\\theta^{2}+\\sin^{2}\\theta\\, \\mathrm{d}\\phi^{2}). \\tag{1}\\]\n\nWe are interested in describing the local dynamics near to \\(p\\) occurring on a horizontal lengthscale \\(L_{\\mathrm{H}}\\ll R\\) (See Figure 1 , which show the relationship between the global and local geometries). Without loss of generality, we can locate our local model on the equator of the sphere (\\(\\theta=\\pi/2\\)) meaning we can approximate the line element by\n\n\\[\\mathrm{d}s^{2}=\\mathrm{d}R^{2}+R^{2}(\\mathrm{d}\\theta^{2}+\\mathrm{d}\\phi^{2} )+\\mathcal{O}((L_{\\mathrm{H}}/R)^{2}d\\phi^{2}), \\tag{2}\\]\n\nwhich results in metric tensor components,\n\n\\[g_{RR}=1,\\quad g_{\\theta\\theta}=g_{\\phi\\phi}=R^{2}, \\tag{3}\\]\n\nand inverse metric tensor components\n\n\\[g^{RR}=1,\\quad g^{\\theta\\theta}=g^{\\phi\\phi}=R^{-2}, \\tag{4}\\]\n\nwith all other components zero. The Christoffel Symbols components, for this coordinate system, are\n\n\\[\\begin{split}\\Gamma^{R}_{\\theta\\theta}=\\Gamma^{R}_{\\phi\\phi}=-R, \\\\ \\Gamma^{\\theta}_{\\theta R}=\\Gamma^{\\theta}_{R\\theta}=\\Gamma^{\\phi}_ {\\phi R}=\\Gamma^{\\phi}_{R\\phi}=R^{-1},\\end{split} \\tag{5}\\]\n\nwith all others vanishing. The fluid equations in this coordinate system are\n\n\\[Du^{\\theta}+\\frac{2}{R}u^{R}u^{\\theta}=-R^{-2}\\left(\\partial_{ \\theta}\\Phi+\\frac{1}{\\rho}\\partial_{\\theta}p\\right),\\] (6) \\[Du^{\\phi}+\\frac{2}{R}u^{R}u^{\\phi}=-R^{-2}\\left(\\partial_{\\phi} \\Phi+\\frac{1}{\\rho}\\partial_{\\theta}p\\right),\\] ( "]}, {"edit": ["We design a unified pipeline implemented by Target-Aware Attention Framework (TAF), which is different from the separate pipeline used by existing methods. In order to evaluate the effectiveness of the unified pipeline, we built a separate pipeline model for comparison. The TAF-separate model adopts the same architecture as TAF. It implements self-attention for both two heads of all Target-Aware Attention modules in the front, while the last Target-Aware Attention module only implements cross-attention for both two heads. We use the Synthetic dataset to conduct ablation study, the results are shown in Table 4. By comparing the TAF-separate model and the TAF model, we can see that the TAF with unified pipeline performs better than the separate pipeline. After introducing the multi-scale projection mechanism to achieve multi-scale attention, the MSTAF-separate and MSTAF models both showed improvements in localization performance. The MSTAF with unified pipeline design and multi-scale attention mechanism presents the best localization performance on all three subsets.\n\nIn (Wang et al., 2019), they use the Scale set to analyze the model's robustness against scale transformation. To further verify the effectiveness of the multi-scale attention mechanism. We also use Scale set to evaluate our models and the result as shown in Tab 5. In the Scale set, the spliced region is only processed with scale transformation of different degrees for ablation in (Wang et al., 2019). It contains 9000 testing pairs and is equally divided into Difficult, Normal, Easy subsets. In the Difficult subset and Normal subset, there are more samples with larger scale degrees. On the contrary, in the Easy subset, the size of the spliced region between the two images tends to be more consistent. We can see that with the help of multi-scale attention mechanism, MSTAF achieves much better localization performance than TAF on the Normal and difficult subset. It demonstrates that MSTAF has an advantage in dealing with various scale transformation samples. After introducing the multi-scale attention mechanism, MSTAF is more robust against scale transformation. The visual comparison can refer to Fig. 6 and Fig. 7.\n\n## 5. Conclusion\n\nIn this work, we propose a Multi-scale Target-Aware Framework to simplify the pipeline of existing methods. It adopts self-attention for feature extraction and cross-attention for correlation matching simultaneously. This unified design enables feature extraction and correlation matching to mutually promote each other, thereby enhancing the matching performance of the model. We further design a multi-scale attention mechanism to model the matching between image patches of different scales, which further improves the robustness against scale transformation. Experiment results demonstrate that our model is robust against scaling and outperforms state-of-the-art methods.\n\n## Acknowledgments\n\nThis work was supported in part by the Natural Science Foundation of China under Grant 62001304; in part by the Guangdong Basic and Applied Basic Research Foundation under Grant 2022A1515010645; in part by the Foundation for Science and Technology Innovation of Shenzhen under Grant RCBS20210609103708014 and the Key\n\n\\begin{table}\n\\begin{tabular}{c|c c c|c c c|c c c} \\hline \\multirow{2}{*}{Model} & \\multicolumn{3}{c|}{Difficult} & \\multicolumn{3}{c|}{Normal} & \\multicolumn{3}{c}{Easy} \\\\ \\cline{2-11}  & IoU & MCC & NMM & IoU & MCC & NMM & IoU & MCC & NMM \\\\ \\hline TAF-separate & 0.6239 & 0.7167 & 0.2963 & 0.8738 & 0.9143 & 0.7744 & 0.9490 & 0.9604 & 0.9164 \\\\ MSTAF-separate & 0.7712 & 0.8432 & 0.5670 & 0.9210 & 0.9486 & 0.8583 & 0.9693 & 0.9764 & 0.9488 \\\\ \\hline TAF & 0.8001 "], "nougat": ["We design a unified pipeline implemented by Target-Aware Attention Framework (TAF), which is different from the separate pipeline used by existing methods. In order to evaluate the effectiveness of the unified pipeline, we built a separate pipeline model for comparison. The TAF-separate model adopts the same architecture as TAF. It implements self-attention for both two heads of all Target-Aware Attention modules in the front, while the last Target-Aware Attention module only implements cross-attention for both two heads. We use the Synthetic dataset to conduct ablation study, the results are shown in Table 4. By comparing the TAF-separate model and the TAF model, we can see that the TAF with unified pipeline performs better than the separate pipeline. After introducing the multi-scale projection mechanism to achieve multi-scale attention, the MSTAF-separate and MSTAF models both showed improvements in localization performance. The MSTAF with unified pipeline design and multi-scale attention mechanism presents the best localization performance on all three subsets.\n\nIn (Wang et al., 2019), they use the Scale set to analyze the model\u2019s robustness against scale transformation. To further verify the effectiveness of the multi-scale attention mechanism. We also use Scale set to evaluate our models and the result as shown in Tab 5. In the Scale set, the spliced region is only processed with scale transformation of different degrees for ablation in [ 16 ]. It contains 9000 testing pairs and is equally divided into Difficult, Normal, Easy subsets. In the Difficult subset and Normal subset, there are more samples with larger scale degrees. On the contrary, in the Easy subset, the size of the spliced region between the two images tends to be more consistent. We can see that with the help of multi-scale attention mechanism, MSTAF achieves much better localization performance than TAF on the Normal and difficult subset. It demonstrates that MSTAF has an advantage in dealing with various scale transformation samples. After introducing the multi-scale attention mechanism, MSTAF is more robust against scale transformation. The visual comparison can refer to Fig. 6 and Fig. 7.\n\n## 5. Conclusion\n\nIn this work, we propose a Multi-scale Target-Aware Framework to simplify the pipeline of existing methods. It adopts self-attention for feature extraction and cross-attention for correlation matching simultaneously. This unified design enables feature extraction and correlation matching to mutually promote each other, thereby enhancing the matching performance of the model. We further design a multi-scale attention mechanism to model the matching between image patches of different scales, which further improves the robustness against scale transformation. Experiment results demonstrate that our model is robust against scaling and outperforms state-of-the-art methods.\n\n## Acknowledgments\n\nThis work was supported in part by the Natural Science Foundation of China under Grant 62001304; in part by the Guangdong Basic and Applied Basic Research Foundation under Grant 2022A1515010645; in part by the Foundation for Science and Technology Innovation of Shenzhen under Grant RCBS20210609103708014 and the Key\n\n\\begin{table}\n\\begin{tabular}{c|c c c|c c c|c c c} \\hline \\multirow{2}{*}{Model} & \\multicolumn{3}{c|}{Difficult} & \\multicolumn{3}{c|}{Normal} & \\multicolumn{3}{c}{Easy} \\\\ \\cline{2-11}  & IoU & MCC & NMM & IoU & MCC & NMM & IoU & MCC & NMM \\\\ \\hline TAF-separate & 0.6239 & 0.7167 & 0.2963 & 0.8738 & 0.9143 & 0.7744 & 0.9490 & 0.9604 & 0.9164 \\\\ MSTAF-separate & 0.7712 & 0.8432 & 0.5670 & 0.9210 & 0.9486 & 0.8583 & 0.9693 & 0.9764 & 0.9488 \\\\ \\hline TAF & 0.8001 & 0.8586 & 0.\n\n "]}, {"edit": ["the geometry of these regions is identical to the Einstein-Rosen bridge in a two-sided BTZ geometry with the same mass as the corresponding operator. We will take the interpretation that the details of what appears outside the domain of dependence of \\(\\mathcal{W}\\) is part of the definition of the operator associated to that region. So in sum, the action of the Lorentzian cap is\n\n\\[I_{\\text{cap}}=\\frac{1}{4\\pi G_{\\text{\\tiny N}}}V_{\\mathcal{W}}+\\sum_{i=\\text{ BH}}I_{i}=\\frac{\\pi}{4G_{\\text{\\tiny N}}}\\sum_{i}\\left(\\text{Re}(\\eta_{i})-\\frac{1}{3 }\\right)+\\sum_{i=\\text{BH}}I_{i}\\,. \\tag{6.8}\\]\n\nHence we see that it is the sum of three contributions that each depend only on one of the three operators. Thus, we can absorb the corresponding phase generated by the action of the Lorentzian geometry into the definition of the operators, and we will arrive at a real result for the three-point function.\n\nHence we arrive at the total action for the single-sided geometry,\n\n\\[I_{\\text{single-sided}}=\\frac{1}{2}I_{\\text{wormhole}}+i\\sum_{i}f(i)\\,. \\tag{6.9}\\]\n\nAgain, it is a nontrivial result that the imaginary term is given by the sum shown above since this allows us to absorb the corresponding phases into the definition of the operators. After eliminating these phases, we recover the three-point function from the single-sided bulk geometry, _i.e.,_\n\n\\[e^{-I}\\approx G_{L}(z_{1},z_{2},z_{3})\\,, \\tag{6.10}\\]\n\nwhere \\(G_{L}(z_{1},z_{2},z_{3})\\) is the semiclassical Liouville three-point correlator in [17].\n\n## 7 Discussion\n\nIn this paper we discussed three-dimensional asymptotically AdS\\({}_{3}\\) geometries that are sourced by the insertion of boundary operators whose scaling dimensions is heavy as the central charge of the holographic CFT\\({}_{2}\\). The presence of any such operators deforms the AdS geometry by inducing a non vanishing expectation value for the holographic stress tensor, close to the boundary. This is true perturbatively in general dimensions, but in three-dimensions there is an exact solution, due to Banados [4], that describes such deformation. However, this metric does not describe the full bulk spacetime. When only two black hole operators are inserted, we showed that the full geometry is simply an infinite covering of the Euclidean BTZ black hole [1], but when three or more operators are inserted, we found that the completion of the Banados metric into the bulk is a wormhole geometry involving multiple asymptotic boundaries. To understand this rather non trivial fact we rephrased the construction of the bulk geometry as a quotient of AdS\\({}_{3}\\) realized by domes and doors. The dome construction is a well know characterization of hyperbolic geometries with an asymptotically AdS\\({}_{3}\\) metric, and more familiar from the study of black hole thermodynamics, see _e.g.,_[38], but the addition of the doors is new as far as we can tell.\n\nAs in the description of a Euclidean two-point function geometry in section 2, _i.e.,_ as empty AdS\\({}_{3}\\) with identifications, the doors are needed to describe the insertion of boundary "], "nougat": ["the geometry of these regions is identical to the Einstein-Rosen bridge in a two-sided BTZ geometry with the same mass as the corresponding operator. We will take the interpretation that the details of what appears outside the domain of dependence of \\(\\mathcal{W}\\) is part of the definition of the operator associated to that region. So in sum, the action of the Lorentzian cap is\n\n\\[I_{\\text{cap}}=\\frac{1}{4\\pi G_{\\text{\\tiny N}}}V_{\\mathcal{W}}+\\sum_{i=\\text{ BH}}I_{i}=\\frac{\\pi}{4G_{\\text{\\tiny N}}}\\sum_{i}\\left(\\text{Re}(\\eta_{i})-\\frac{1}{3 }\\right)+\\sum_{i=\\text{BH}}I_{i}\\,. \\tag{6.8}\\]\n\nHence we see that it is the sum of three contributions that each depend only on one of the three operators. Thus, we can absorb the corresponding phase generated by the action of the Lorentzian geometry into the definition of the operators, and we will arrive at a real result for the three-point function.\n\nHence we arrive at the total action for the single-sided geometry,\n\n\\[I_{\\text{single-sided}}=\\frac{1}{2}I_{\\text{wormhole}}+i\\sum_{i}f(i)\\,. \\tag{6.9}\\]\n\nAgain, it is a nontrivial result that the imaginary term is given by the sum shown above since this allows us to absorb the corresponding phases into the definition of the operators. After eliminating these phases, we recover the three-point function from the single-sided bulk geometry, _i.e.,_\n\n\\[e^{-I}\\approx G_{L}(z_{1},z_{2},z_{3})\\,, \\tag{6.10}\\]\n\nwhere \\(G_{L}(z_{1},z_{2},z_{3})\\) is the semiclassical Liouville three-point correlator in [17].\n\n## 7 Discussion\n\nIn this paper we discussed three-dimensional asymptotically AdS\\({}_{3}\\) geometries that are sourced by the insertion of boundary operators whose scaling dimensions is heavy as the central charge of the holographic CFT 2 . The presence of any such operators deforms the AdS geometry by inducing a non vanishing expectation value for the holographic stress tensor, close to the boundary. This is true perturbatively in general dimensions, but in three-dimensions there is an exact solution, due to Ba \u0303nados [ 4 ], that describes such deformation. However, this metric does not describe the full bulk spacetime. When only two black hole operators are inserted, we showed that the full geometry is simply an infinite covering of the Euclidean BTZ black hole [ 1 ], but when three or more operators are inserted, we found that the completion of the Ba \u0303nados metric into the bulk is a wormhole geometry involving multiple asymptotic boundaries. To understand this rather non trivial fact we rephrased the construction of the bulk geometry as a quotient of AdS\\({}_{3}\\) realized by domes and doors. The dome construction is a well know characterization of hyperbolic geometries with an asymptotically AdS\\({}_{3}\\) metric, and more familiar from the study of black hole thermodynamics, see _e.g.,_[38], but the addition of the doors is new as far as we can tell.\n\nAs in the description of a Euclidean two-point function geometry in section 2 , i.e., as empty AdS\\({}_{3}\\) with identifications, the doors are needed to describe the insertion of boundary "]}, {"edit": ["\n\n#### 3.1.3. The case of a general periodic force\n\nFinally, we remark that in the general case of a \\(\\theta\\)-periodic force of the form\n\n\\[\\mathcal{F}(t/\\theta)=\\sum_{\\ell=1}^{+\\infty}F_{\\ell}\\cos(\\omega(\\ell)t),\\quad \\text{where }\\omega(\\ell):=\\frac{2\\pi\\ell}{\\theta} \\tag{3.20}\\]\n\nwhose real valued Fourier coefficients satisfy \\(\\sum_{\\ell=1}^{+\\infty}(\\ell F_{\\ell})^{2}<+\\infty\\), the work performed by the force can be determined from the formula:\n\n\\[W(n)=\\sum_{\\ell=1}^{+\\infty}\\big{(}\\omega(\\ell)F_{\\ell}\\big{)}^{2}\\frac{N( \\omega(\\ell),n)}{D(\\omega(\\ell),n)}. \\tag{3.21}\\]\n\nTherefore its behavior, as \\(n\\) gets large, can be determined from the term by term analysis of the series appearing on the right hand side of (3.21).\n\n### Energy\n\nAs in Section 3.1 we assume that the periodic force \\(\\mathcal{F}(t)\\) is given by (3.3). The time average of the expectation of the total energy energy of the chain \\(E(\\omega,n)\\) breaks up into the sum of thermal component \\(E_{\\text{th}}(\\omega,n)=\\sum_{x\\in\\mathbb{I}_{n}}\\langle\\langle e_{x}^{\\text{ th}}\\rangle\\rangle\\) and the mechanical one \\(E_{\\text{mech}}(\\omega,n)=\\sum_{x\\in\\mathbb{I}_{n}}\\langle\\langle e_{x}^{\\text {mech}}\\rangle\\rangle\\), with \\(e_{x}^{\\text{th}}\\) and \\(e_{x}^{\\text{mech}}\\) defined in (2.17) and (2.16), respectively.\n\nConsidering the behavior of the thermal energy functional, defined in (2.15), it has been shown in [0], that in the case \\(\\omega_{0}=0\\) and \\(\\gamma_{-}=\\gamma_{+}\\) we have \\(\\langle\\langle e_{x}^{\\text{th}}\\rangle\\rangle=\\frac{1}{2}(T_{-}+T_{+})\\) for all \\(x=1,\\ldots,n-1\\). If \\(\\omega_{0}>0\\) and \\(\\gamma_{-}=\\gamma_{+}\\), then [0, formulas (38) and (42)] give\n\n\\[\\langle\\langle e_{x}^{\\text{th}}\\rangle\\rangle=\\frac{1}{2}(T_{-}+T_{+})(1+o_{x }),\\quad\\text{where }|o_{x}|\\leqslant\\frac{C}{g^{x\\wedge(n+1-x)}}\\]\n\nfor some constants \\(C>0\\), \\(g>1\\) independent of \\(n\\). As a result we have \\(E^{\\text{th}}(\\omega,n)\\sim n\\), as \\(n\\to+\\infty\\).\n\n#### 3.2.1. Formula for the total mechanical energy functional for a single mode oscillating force\n\nIn what follows we consider the behavior of the mechanical component of the energy. Again, assume that the force is given by (3.3). It turns out, see Section C of the Appendix, that the time average over the period of the microscopic mechanical energy density equals"], "nougat": ["\n\n#### 3.1.3. The case of a general periodic force\n\nFinally, we remark that in the general case of a \\(\\theta\\)-periodic force of the form\n\n\\[\\mathcal{F}(t/\\theta)=\\sum_{\\ell=1}^{+\\infty}F_{\\ell}\\cos(\\omega(\\ell)t),\\quad \\text{where }\\omega(\\ell):=\\frac{2\\pi\\ell}{\\theta} \\tag{3.20}\\]\n\nwhose real valued Fourier coefficients satisfy \\(\\sum_{\\ell=1}^{+\\infty}(\\ell F_{\\ell})^{2}<+\\infty\\), the work performed by the force can be determined from the formula:\n\n\\[W(n)=\\sum_{\\ell=1}^{+\\infty}\\big{(}\\omega(\\ell)F_{\\ell}\\big{)}^{2}\\frac{N( \\omega(\\ell),n)}{D(\\omega(\\ell),n)}.\\] (3.21 ).\n\nTherefore its behavior, as \\(n\\) gets large, can be determined from the term by term analysis of the series appearing on the right hand side of (3.21).\n\n### Energy\n\nAs in Section 3.1 we assume that the periodic force \\(\\mathcal{F}(t)\\) is given by ( 3.3 ). The time average of the expectation of the total energy energy of the chain \\(E(\\omega,n)\\) breaks up into the sum of thermal component \\(E_{\\rm th}(\\omega,n)=\\sum_{x\\in\\mathbb{I}_{n}}\\langle\\langle e_{x}^{\\rm th}\\rangle\\rangle\\) and the mechanical one \\(E_{\\rm mech}(\\omega,n)=\\sum_{x\\in\\mathbb{I}_{n}}\\langle\\langle e_{x}^{\\rm mech }\\rangle\\rangle\\), with \\(e_{x}^{\\rm th}\\) and ( 2.16 ), respectively.\n\nConsidering the behavior of the thermal energy functional, defined in ( 2.15 ), it has been shown in [ 0 ], that in the case \\(\\omega_{0}=0\\) and (42)] give\n\n\\[\\langle\\langle e_{x}^{\\rm th}\\rangle\\rangle=\\frac{1}{2}(T_{-}+T_{+})(1+o_{x}) ,\\quad\\text{where }|o_{x}|\\leqslant\\frac{C}{g^{x\\wedge(n+1-x)}}\\]\n\nfor some constants \\(C>0\\), \\(g>1\\) independent of \\(n\\). As a result we have \\(E^{\\rm th}(\\omega,n)\\sim n\\), as \\(n\\to+\\infty\\).\n\n2.1. e. In what follows we consider the behavior of the mechanical component of the energy. Again, assume that the force is given by ( 3.3 ). It turns out, see Section C of the Appendix, that the time average over the period of the microscopic mechanical energy density equals\n\n\\[\\langle\\langle e_{x}^{\\rm mech}\\rangle\\rangle=\\frac{F^{2}}{2}\\cdot\\frac{M_{x}( \\omega,n)}{D(\\omega,n)}, \\tag{3.22}\\]\n\nwhere \\(D(\\omega,n)\\) is given by ( 3.5 ) and\n\n\\[M_{x}(\\omega,n)=G_{x}^{1}(\\omega,n)^{2}(\\omega^{2}+\\omega_{0}^{2})+(\\nabla^{*} G_{x}^{1})(\\omega,"]}, {"edit": ["\n\n### Study 1\n\nBased on the pilot, we decided to proceed by fixing the reported bugs and replacing the usability scale with potential mediators to better assess which users would prefer the different bots. We conducted this study with n=71 participants (see Section 3).\n\n#### 4.2.1 Measures\n\nIn Study 1, we added 7-level bipolar rating scales for direct comparison (See Section 3.2). We were interested in how much control participants experienced; how natural they felt the chats to be; how well their intent was fulfilled in the chats; and how satisfied they were with them. See Table 2 for items and reliabilities of the constructs.\n\nTo account for potential mediation, we assessed interindividual difference variables, namely the most prominent personality measure, big 5, using a 15-item scale (BFI-2-XS) [39]. As insert expansions exist to smooth interaction by shaping expectations, we also assessed our participants need for cognitive closure (NFC-15) [40].\n\n#### 4.2.2 Procedure\n\nAfter a short demographic questionnaire, n=71 participants were given 3 scenarios (see Figure 2), rating each before turning to the next (see Figure 3.2). Having completed all scenarios and evaluations, feedback was elicited and interindividual variables were assessed.\n\n#### 4.2.3 Feedback\n\nTo quantify the feedback, we repeated the procedure described for the pilot. Feedback on the study was 69.01% positive (neutral opinions tend to be rated as negative as well, e.g. \"I have no opinion about the study.\"), on the interface only 52.11% (negatives include no feedback at all, neutral statements such as \"It was fast and responsive, just feel like the loading is too big and the lettering also\", but also some on bugs like \"It was fine, although sometimes my prompt would trigger a loading animation that the bot would never reply to, so I had to prompt again, which left the loading anim on the screen for one of the bots but not the other, not a big deal.\" or \"It was frusting when it could not listen or answer all\n\n\\begin{table}\n\\begin{tabular}{|l|l|l|} \\hline\n**Scale** & **Items** & **Reliability** \\\\ \\hline \\multicolumn{3}{|c|}{Which of the two chats...} \\\\ \\hline \\multirow{3}{*}{Control} & enabled more personal direction? & _Study 1: \\(\\alpha=.77\\), \\(\\lambda=.79\\) (overall), for scenarios_ \\\\  & offered you more autonomy? & _.77.79, .84/.85, .91/.91 each; Study 2: \\(\\alpha=.81\\), \\(\\lambda=.84\\)_ \\\\ \\hline \\multirow{3}{*}{Naturalness} & seemed more authentic? & _Study 1: \\(\\alpha=.93\\), \\(\\lambda=.93\\) (overall), for scenarios_ \\\\  & had a more genuine feel? & _.93/.93, .92/.91, .97/.97 each; Study 2: \\(\\alpha=.93\\), \\(\\lambda=.95\\)_ \\\\ \\hline \\multirow{2}{*}{Intent-Effectiveness} & had more suitable responses? & _Study 1: \\(\\alpha=.93\\), \\(\\lambda=.93\\) (overall), for scenarios_ \\\\  & lived up to your expectations better? & _.93/.91, .95/.93, .96/.95 each; Study 2: \\(\\alpha=.95\\), \\(\\lambda=.96\\)_ \\\\ \\hline \\multirow{2}{*}{Satisfaction} & was more to your liking? & _Study 1: \\(\\alpha=.95\\),"], "nougat": ["\n\n### Study 1\n\nBased on the pilot, we decided to proceed by fixing the reported bugs and replacing the usability scale with potential mediators to better assess which users would prefer the different bots. We conducted this study with n=71 participants (see Section 3).\n\n#### 4.2.1 Measures\n\nIn Study 1, we added 7-level bipolar rating scales for direct comparison (See Section 3.2). We were interested in how much control participants experienced; how natural they felt the chats to be; how well their intent was fulfilled in the chats; and how satisfied they were with them. See Table 2 for items and reliabilities of the constructs.\n\nTo account for potential mediation, we assessed interindividual difference variables, namely the most prominent personality measure, big 5, using a 15-item scale (BFI-2-XS) [ 39 ]. As insert expansions exist to smooth interaction by shaping expectations, we also assessed our participants need for cognitive closure (NFC-15) [40].\n\n#### 4.2.2 Procedure\n\nAfter a short demographic questionnaire, n=71 participants were given 3 scenarios (see Figure 2), rating each before turning to the next (see Figure 3.2). Having completed all scenarios and evaluations, feedback was elicited and interindividual variables were assessed.\n\n#### 4.2.3 Feedback\n\nTo quantify the feedback, we repeated the procedure described for the pilot. Feedback on the study was 69.01% positive (neutral opinions tend to be rated as negative as well, e.g. \"I have no opinion about the study.\"), on the interface only 52.11% (negatives include no feedback at all, neutral statements such as \"It was fast and responsive, just feel like the loading is too big and the lettering also\", but also some on bugs like \"It was fine, although sometimes my prompt would trigger a loading animation that the bot would never reply to, so I had to prompt again, which left the loading anim on the screen for one of the bots but not the other, not a big deal.\" or \"It was frusting when it could not listen or answer all\n\n\\begin{table}\n\\begin{tabular}{|l|l|l|} \\hline\n**Scale** & **Items** & **Reliability** \\\\ \\hline \\multicolumn{3}{|c|}{Which of the two chats...} \\\\ \\hline \\multirow{3}{*}{Control} & enabled more personal direction? & _Study 1: \\(\\alpha=.77\\), \\(\\lambda=.79\\) (overall), for scenarios_ \\\\  & offered you more autonomy? & _.77.79, .84/.85, .91/.91 each; Study 2: \\(\\alpha=.81\\), \\(\\lambda=.84\\)_ \\\\ \\hline \\multirow{3}{*}{Naturalness} & seemed more authentic? & _Study 1: \\(\\alpha=.93\\), \\(\\lambda=.93\\) (overall), for scenarios_ \\\\  & had a more genuine feel? & _.93/.93, .92/.91, .97/.97 each; Study 2: \\(\\alpha=.93\\), \\(\\lambda=.95\\)_ \\\\ \\hline \\multirow{2}{*}{Intent-Effectiveness} & had more suitable responses? & _Study 1: \\(\\alpha=.93\\), \\(\\lambda=.93\\) (overall), for scenarios_ \\\\  & lived up to your expectations better? & _.93/.91, .95/.93, .96/.95 each; Study 2: \\(\\alpha=.95\\), \\(\\lambda=.96\\)_ \\\\ \\hline \\multirow{2}{*}{Satisfaction} & was more to your liking? & _Study 1: \\(\\alpha=.95\\"]}, {"edit": ["\n\n## 1. Introduction\n\nThe (logarithmic) Mahler measure of a non-zero rational function \\(P\\in\\mathbb{C}\\left(x_{1},\\ldots,x_{n}\\right)^{*}\\) is defined by\n\n\\[\\mathrm{m}\\left(P\\right)=\\mathrm{m}(P(x_{1},\\ldots,x_{n})):=\\frac{1}{\\left(2\\pi i \\right)^{n}}\\int_{\\mathbb{T}^{n}}\\log|P\\left(x_{1},\\ldots,x_{n}\\right)|\\frac{dx _{1}}{x_{1}}\\cdots\\frac{dx_{n}}{x_{n}}, \\tag{1}\\]\n\nwhere \\(\\mathbb{T}^{n}=\\{(x_{1},\\ldots,x_{n})\\in\\mathbb{C}^{*}\\times\\mathbb{C}^{*} \\times\\cdots\\times\\mathbb{C}^{*}:|x_{1}|=\\cdots=|x_{n}|=1\\}.\\)\n\nThe first appearance of this quantity (for one variable polynomials) can be traced back to Lehmer's work [1] on Mersenne numbers, and its several variable form first appeared in the work of Mahler [2] regarding a simpler proof of the Gel'fond-Mahler inequality, and it was later named after him.\n\nIn the early 80's, Smyth [3] discovered the following remarkable identities:\n\n\\[\\mathrm{m}(x+y+1)= \\frac{3\\sqrt{3}}{4\\pi}L(\\chi_{-3},2),\\] \\[\\mathrm{m}(1+x+y+z)= \\frac{7}{2\\pi^{2}}\\zeta(3),\\]\n\nwhere \\(L(\\chi_{-3},2)\\) is the Dirichlet \\(L\\)-function of the quadratic character \\(\\chi_{-3}\\) of conductor \\(3\\), and \\(\\zeta(s)\\) is the Riemann zeta function (for more details see [4]). These are two of the initial formulas for several variable cases.\n\nLater the work of Boyd [5], Deninger [6], Rodriguez-Villeags [7] and others provided us with interesting connections among Mahler measure, higher regulators, and Beilinson's conjectures. The conjectural formulas to support their work, such as\n\n\\[\\mathrm{m}(P_{k}(x,y))\\overset{?}{=}r_{k}L^{\\prime}(E_{N(k)},0),\\qquad r_{k}\\in \\mathbb{Q},\\]\n\nwere eventually proved for certain polynomials, due to Rodriguez-Villegas [7], Rogers and Zudilin [8, 9] et al. Here \\(E_{N(k)}\\) is an elliptic curve of conductor \\(N(k)\\) associated to \\(P_{k}\\), and the question mark stands for a numerical formula that is true for at least 20 decimal places. (See the book of Brunault and Zudilin [10] for more details.)\n\nIn a different direction, Cassaigne and Maillot [11] generalized the formula found by Smyth to \\(\\mathrm{m}(a+bx+cy)\\) for arbitrary complex constants \\(a,b,\\text{ and }c:\\)\n\n(2) \\[\\pi\\mathrm{m}(ax+by+c)=\\left\\{\\begin{array}{ll}\\alpha\\log|a|+\\beta\\log|b|+ \\gamma\\log|c|+D\\left(\\frac{|a|}{|b|}e^{i\\gamma}\\right)&\\text{if $\\Delta$ holds,}\\\\ \\log\\max\\{|a|,|"], "nougat": ["\n\n## 1. Introduction\n\nThe (logarithmic) Mahler measure of a non-zero rational function \\(P\\in\\mathbb{C}\\left(x_{1},\\ldots,x_{n}\\right)^{*}\\) is defined by\n\n\\[\\mathrm{m}\\left(P\\right)=\\mathrm{m}(P(x_{1},\\ldots,x_{n})):=\\frac{1}{\\left(2\\pi i \\right)^{n}}\\int_{\\mathbb{T}^{n}}\\log|P\\left(x_{1},\\ldots,x_{n}\\right)|\\frac{dx _{1}}{x_{1}}\\cdots\\frac{dx_{n}}{x_{n}}, \\tag{1}\\]\n\nwhere \\(\\mathbb{T}^{n}=\\{(x_{1},\\ldots,x_{n})\\in\\mathbb{C}^{*}\\times\\mathbb{C}^{*} \\times\\cdots\\times\\mathbb{C}^{*}:|x_{1}|=\\cdots=|x_{n}|=1\\}.\\)\n\nThe first appearance of this quantity (for one variable polynomials) can be traced back to Lehmer\u2019s work [1] on Mersenne numbers, and its several variable form first appeared in the work of Mahler [2] regarding a simpler proof of the Gel\u2019fond-Mahler inequality, and it was later named after him.\n\nIn the early 80 \u2019s, Smyth [3] discovered the following remarkable identities:\n\n\\[\\mathrm{m}(x+y+1)= \\frac{3\\sqrt{3}}{4\\pi}L(\\chi_{-3},2),\\] \\[\\mathrm{m}(1+x+y+z)= \\frac{7}{2\\pi^{2}}\\zeta(3),\\]\n\nwhere \\(L(\\chi_{-3},2)\\) is the Dirichlet \\(L\\)-function of the quadratic character \\(\\chi_{-3}\\) of conductor 3, and \\(\\zeta(s)\\) is the Riemann zeta function (for more details see [4]). These are two of the initial formulas for several variable cases.\n\nLater the work of Boyd [5], Deninger [6], Rodriguez-Villeags [7] and others provided us with interesting connections among Mahler measure, higher regulators, and Be \u0306\u0131linson\u2019s conjectures. The conjectural formulas to support their work, such as\n\n\\[\\mathrm{m}(P_{k}(x,y))\\overset{?}{=}r_{k}L^{\\prime}(E_{N(k)},0),\\qquad r_{k} \\in\\mathbb{Q},\\]\n\nwere eventually proved for certain polynomials, due to Rodriguez-Villegas [7], Rogers and Zudilin [8, 9] et al. Here \\(E_{N(k)}\\) is an elliptic curve of conductor \\(N(k)\\) associated to \\(P_{k},\\) and the question mark stands for a numerical formula that is true for at least 20 decimal places. (See the book of Brunault and Zudilin [10] for more details.)\n\nIn a different direction, Cassaigne and Maillot [11] generalized the formula found by Smyth to \\(\\mathrm{m}(a+bx+cy)\\) for arbitrary complex constants \\(a,b,\\text{ and }c:\\)\n\n(2) \\[\\pi\\mathrm{m}(ax+by+c)=\\left\\{\\begin{array}{ll}\\alpha\\log|a|+\\beta\\log|b|+ \\gamma\\log|c|+D\\left(\\frac{|a|}{|b|}e^{i\\gamma}\\right)&\\text{if $\\Delta$ holds,}\\\\ \\log\\max\\{|a|,|b|"]}, {"edit": ["where the initial \\(\\mathbf{u}^{(0)}_{AD}\\) is set as \\(\\overline{\\mathbf{u}}\\). By running the iteration (16) \\(N\\) times, we obtain \\(\\mathbf{u}_{AD}\\doteq\\mathbf{u}^{(N)}_{AD}\\) as the \\(N^{\\text{th}}\\) order van Cittert operator applied to \\(\\overline{\\mathbf{u}}\\). However, to make this iteration practical, we need to recall that each application of the filter \\(G\\) requires the inversion of a differential operator. Therefore, denoting by \\(\\widetilde{\\mathbf{u}}^{(n+1)}\\) the quantity \\(\\widetilde{\\mathbf{u}}^{(n+1)}\\doteq G\\mathbf{u}^{(n)}_{AD}\\), and substituting in our filter from (12), we observe that \\(\\widetilde{\\mathbf{u}}^{(n+1)}\\) is found by solving\n\n\\[(I-\\delta^{2}\\Delta)^{-1}\\mathbf{u}^{(n)}_{AD}=\\widetilde{\\mathbf{u}}^{(n+1)} \\Longleftrightarrow(I-\\delta^{2}\\Delta)\\widetilde{\\mathbf{u}}^{(n+1)}=\\mathbf{u}^{(n )}_{AD}. \\tag{17}\\]\n\nEquation (17) requires to solve a linear system at each iteration \\(n=0,\\ldots,N-1\\). As discussed above, in order to employ the van Cittert AD in a ROM setting, by multiplying (17) by each test function in our ROM space and expanding our prospective solution as a linear combination of ROM basis functions, we obtain the linear system\n\n\\[\\left(\\mathbf{M}+\\delta^{2}\\mathbf{S}\\right)\\widetilde{\\mathbf{c}}^{(n+1)}=\\mathbf{M}\\mathbf{c}^{ n}_{AD}. \\tag{18}\\]\n\nThus, the iterative process (16) amounts to setting \\(\\mathbf{c}^{(0)}_{AD}=\\overline{\\mathbf{c}}\\), updating the coefficients of the ROM AD velocity as\n\n\\[\\mathbf{c}^{(n+1)}_{AD}=\\mathbf{c}^{(n)}_{AD}+\\{\\overline{\\mathbf{c}}-\\widetilde{\\mathbf{c}}^{ (n+1)}\\}\\qquad n=0,\\ldots,N-1,\\]\n\nand finally defining \\(\\mathbf{c}_{AD}\\doteq\\mathbf{c}^{(N)}_{AD}\\).\n\n### The Tikhonov AD\n\nThe Tikhonov method of approximate deconvolution is defined as\n\n\\[\\mathbf{u}_{AD}=D^{T}_{\\mu}\\overline{\\mathbf{u}}=(G^{*}G+\\mu I)^{-1}G^{*}\\overline{\\bm {u}}, \\tag{19}\\]\n\nwhere \\(G^{*}\\) denotes the adjoint of the operator \\(G\\), and \\(\\mu\\in\\mathbb{R}^{+}\\) is a positive constant; we refer, e.g., to (39, section 3.3.1) for further details on the Tikhonov AD. When plugging in the specific filter from (12) and proceeding formally, we can write\n\n\\[[G^{*}G+\\mu I]\\mathbf{u}_{AD} =G^{*}\\overline{\\mathbf{u}},\\] \\[[(I-\\delta^{2}\\Delta)^{-*}(I-\\delta^{2}\\Delta)^{-1}+\\mu I]\\mathbf{u} _{AD} =(I-\\delta^{2}\\ "], "nougat": ["where the initial \\(\\mathbf{u}^{(0)}_{AD}\\) is set as \\(\\overline{\\mathbf{u}}\\). By running the iteration (16) \\(N\\) times, we obtain \\(\\mathbf{u}_{AD}\\doteq\\mathbf{u}^{(N)}_{AD}\\) as the \\(N^{\\text{th}}\\) order van Cittert operator applied to \\(\\overline{\\mathbf{u}}\\). However, to make this iteration practical, we need to recall that each application of the filter \\(G\\) requires the inversion of a differential operator. Therefore, denoting by \\(\\widetilde{\\mathbf{u}}^{(n+1)}\\) the quantity \\(\\widetilde{\\mathbf{u}}^{(n+1)}\\doteq G\\mathbf{u}^{(n)}_{AD}\\), and substituting in our filter from (12), we observe that \\(\\widetilde{\\mathbf{u}}^{(n+1)}\\) is found by solving\n\n\\[(I-\\delta^{2}\\Delta)^{-1}\\mathbf{u}^{(n)}_{AD}=\\widetilde{\\mathbf{u}}^{(n+1)} \\Longleftrightarrow(I-\\delta^{2}\\Delta)\\widetilde{\\mathbf{u}}^{(n+1)}=\\mathbf{u}^{(n )}_{AD}.\\] (17) requires to solve a linear system at each iteration \\(n=0,\\ldots,N-1\\). As discussed above, in order to employ the van Cittert AD in a ROM setting, by multiplying (17) by each test function in our ROM space and expanding our prospective solution as a linear combination of ROM basis functions, we obtain the linear system\n\n\\[\\left(\\mathbf{M}+\\delta^{2}\\mathbf{S}\\right)\\widetilde{\\mathbf{c}}^{(n+1)}=\\mathbf{M}\\mathbf{c}^{ n}_{AD}.\\] (16) amounts to setting \\[\\mathbf{c}^{(0)}_{AD}=\\overline{\\mathbf{c}}\\] , updating the coefficients of the ROM AD velocity as \\[\\mathbf{c}^{(n+1)}_{AD}=\\mathbf{c}^{(n)}_{AD}+\\{\\overline{\\mathbf{c}}-\\widetilde{\\mathbf{c}}^ {(n+1)}\\}\\qquad n=0,\\ldots,N-1,\\] and finally defining \\[\\mathbf{c}_{AD}\\doteq\\mathbf{c}^{(N)}_{AD}.\\]\n\n### The Tikhonov AD\n\nThe Tikhonov method of approximate deconvolution is defined as\n\n\\[\\mathbf{u}_{AD}=D^{T}_{\\mu}\\overline{\\mathbf{u}}=(G^{*}G+\\mu I)^{-1}G^{*}\\overline{\\bm {u}}, \\tag{18}\\]\n\nwhere \\(G^{*}\\) denotes the adjoint of the operator \\(G\\), and \\(\\mu\\in\\mathbb{R}^{+}\\) is a positive constant; we refer, e.g., to [39, section 3.3.1] for further details on the Tikhonov AD. When plugging in the specific filter from (12) and proceeding formally, we can write\n\n\\[[G^{*}G+\\mu I]\\mathbf{u}_{AD} =G^{*}\\overline{\\mathbf{u}},\\] \\[[(I-\\delta^{2}\\Delta)^{-*}(I-\\delta^{2}\\Delta)^{-1}+\\mu I]\\mathbf{u} _{AD} =(I-\\delta^{2}\\Delta)^{-*}\\overline{\\mathbf{u}}],\\] \\ "]}, {"edit": ["to accept a trigger of \\(40\\,\\mathrm{ns}\\) and an integration time of \\(325\\,\\mathrm{ns}\\) to collect photons on the sensor tile. The overvoltage of the silicon photomultipliers (SiPMs) is set to \\(3\\,\\mathrm{V}\\). As this is a digital tile it is possible to disable the SPADs which produce a high number of dark counts. This inhibit fraction is set to \\(10\\,\\mathrm{\\char 37}\\). The surface of each sensor tile is covered with a glass plate of \\(1.1\\,\\mathrm{mm}\\). The sensor tiles are connected to a singles processing unit (SPU) which manages their voltage supply and feeds their data to the data acquisition and processing server (DAPS). During the measurement, the tiles are cooled by a \\(15\\,\\mathrm{\\SIUnitSymbolCelsius}\\) liquid cooling system.\n\n#### 2.1.3 Masks\n\nWe perform measurements in one and two dimensions, i.e., we reconstruct an image along one axis or on a plane. For these two tasks, we use a one- and a two-dimensional versions of a MURA mask of rank \\(476\\), clipped to \\(31\\times 31\\) central pixels (see Fig. 2). The mask rank as well as the setup geometry have been optimised via Monte Carlo simulations before the experiment. To construct the physical masks we use tungsten rods of \\((2.26\\times 2.26\\times 20)\\,\\mathrm{mm}^{3}\\) which are inserted into 3D printed rasters made from Pro Grey Resin. The rod manufacturing reaches a precision of \\(0.1\\,\\mathrm{mm}\\). The resulting masks have a dimension of \\((73.6\\times 73.6)\\,\\mathrm{mm}^{2}\\). The rasters have a total thickness of \\(13\\,\\mathrm{mm}\\) and the holes to insert the rods are \\(10\\,\\mathrm{mm}\\) deep. To prevent the rods from falling out, the assembled masks are wrapped in cling film.\n\n### Radioactive sources\n\nFor image reconstruction, the experimental data were obtained with a radioactive \\({}^{22}\\)Na source with an activity of \\(2.89\\,\\mathrm{MBq}\\). The active material in that source covers an area of \\(1\\,\\mathrm{mm}\\times 1\\,\\mathrm{mm}\\). As a \\(\\beta^{+}\\)-emitter, \\({}^{22}\\)Na provides two photons of \\(511\\,\\mathrm{keV}\\) emitted back-to-back, which can be used for electronic collimation. For calibration of the detectors we additionally used the \\(1275\\,\\mathrm{keV}\\) gamma line of \\({}^{22}\\)Na and two more radioactive sources: a \\({}^{137}\\)Cs source with a gamma line at \\(662\\,\\mathrm{keV}\\) with an activity of \\(1.73\\,\\mathrm{MBq}\\) and a \\({}^{133}\\)Ba source with\n\nFigure 2: Coded masks for 1D measurement with the small-scale prototype (a) and for 2D measurement with the three-layered PET array (b).\n\n "], "nougat": ["to accept a trigger of \\(40\\,\\mathrm{ns}\\) and an integration time of \\(325\\,\\mathrm{ns}\\) to collect photons on the sensor tile. The overvoltage of the silicon photomultipliers (SiPMs) is set to 3 V . As this is a digital tile it is possible to disable the SPADs which produce a high number of dark counts. This inhibit fraction is set to 10 %. The surface of each sensor tile is covered with a glass plate of \\(1.1\\,\\mathrm{mm}\\). The sensor tiles are connected to a singles processing unit (SPU) which manages their voltage supply and feeds their data to the data acquisition and processing server (DAPS). During the measurement, the tiles are cooled by a \\(15\\,\\mathrm{\\SIUnitSymbolCelsius}\\) liquid cooling system.\n\n#### 2.1.3. Masks\n\nWe perform measurements in one and two dimensions, i.e., we reconstruct an image along one axis or on a plane. For these two tasks, we use a oneand a two-dimensional versions of a MURA mask of rank 476, clipped to \\(31\\times 31\\) central pixels (see Fig. 2). The mask rank as well as the setup geometry have been optimised via Monte Carlo simulations before the experiment. To construct the physical masks we use tungsten rods of \\((2.26\\times 2.26\\times 20)\\,\\mathrm{mm}^{3}\\) which are inserted into 3D printed rasters made from Pro Grey Resin. The rod manufacturing reaches a precision of \\(0.1\\,\\mathrm{mm}\\). The resulting masks have a dimension of \\((73.6\\times 73.6)\\,\\mathrm{mm}^{2}\\). The rasters have a total thickness of \\(13\\,\\mathrm{mm}\\) and the holes to insert the rods are \\(10\\,\\mathrm{mm}\\) deep. To prevent the rods from falling out, the assembled masks are wrapped in cling film.\n\n### Radioactive sources\n\nFor image reconstruction, the experimental data were obtained with a radioactive 22 Na source with an activity of \\(2.89\\,\\mathrm{MBq}\\). The active material in that source covers an area of 1 mm \\(\\times\\) 1 mm. As a \\(\\beta^{+}\\)-emitter, Na provides two photons of \\(511\\,\\mathrm{keV}\\) emitted back-to-back, which can be used for electronic collimation. For calibration of the detectors we additionally used the 1275 keV gamma line of Na and two more radioactive sources: a \\({}^{137}\\)Cs source with a gamma line at \\(662\\,\\mathrm{keV}\\) with an activity of \\(1.73\\,\\mathrm{MBq}\\) and a \\({}^{133}\\)Ba source with\n\nFigure 2: Coded masks for 1D measurement with the small-scale prototype (a) and for 2D measurement with the three-layered PET array (b).\n\n "]}, {"edit": ["zz0 = cell(1, N);  for i = 1 : N-1  zz0(i) = {[initial_position(:, i); initial_position(:, i + 1)]};  end  zz0(N) = {[initial_position(:, N); initial_position(:, 1)]}; end\n\nSuch that the overall problem can be set up with the following function:\n\nfunction [sProb ] = setupSolver(N, sigma) n = 4;  d = 2;  y = sym('y%d%d', [N n], 'real');  y = y';  [eta, eta_bar] = getEta(N, d, sigma);  F = getObjective(N, y, eta, eta_bar, sigma);  H = getInequalityConstr(N, y, eta_bar);  AA = getCouplingMatrix(N, n);  zz0 = getStartValue(N, sigma);  sProb.llbx = cell(1, N);  sProb.uubx = cell(1, N);  for i = 1 : N  sProb.llbx(i) = mat2cell([-inf; -inf; -inf; -inf], 4, 1);  sProb.uubx(i) = mat2cell([ inf; inf; inf; inf], 4, 1); end  sProb.locFuns.ffi = cell(1, N);  sProb.locFuns.hhi = cell(1, N);  for i = 1 : N  sProb.locFuns.ffi(i) = {matlabFunction(F(i), 'Vars', {y(:, i)})} ;  sProb.locFuns.hhi(i) = {matlabFunction(H(i), 'Vars', {y(:, i)})} ;  end  sProb.AA = AA;  sProb.zz0 = zz0;\n\n#### 12.2.4 Runtime Analysis\n\nFor the runtime analysis, the idea is to run the sensor network localization problem with varying number of sensors both with a decentral and a central optimization step. To do so, firstly a vector with a number of sensors is needed and secondly a vector with variances. Then, the time needed for the decentral and the central optimization is measured and can be plotted.\n\nN = [5, 10, 15 , 20, 25, 30, 35, 40, 50, 60, 70, 80, 90, 100];  sigma = [0.5, 1, 1.5, 2, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5];  time = zeros(2, length(N));  for i = 1 : length(N)  sProb = setupSolver(N(i), sigma(i)); "], "nougat": ["zz0 = cell(1, N);  for i = 1 : N-1  zz0(i) = {[initial_position(:, i); initial_position(:, i + 1)]};  end  zz0(N) = {[initial_position(:, N); initial_position(:, 1)]}; end\n\nSuch that the overall problem can be set up with the following function:\n\nfunction [sProb ] = setupSolver(N, sigma) n = 4;  d = 2;  y = sym('y%d%d', [N n], 'real');  y = y';  [eta, eta_bar] = getEta(N, d, sigma);  F = getObjective(N, y, eta, eta_bar, sigma);  H = getInequalityConstr(N, y, eta_bar);  AA = getCouplingMatrix(N, n);  zz0 = getStartValue(N, sigma);  sProb.llbx = cell(1, N);  sProb.uubx = cell(1, N);  for i = 1 : N  sProb.llbx(i) = mat2cell([-inf; -inf; -inf; -inf], 4, 1);  sProb.uubx(i) = mat2cell([ inf; inf; inf; inf], 4, 1); end  sProb.locFuns.ffi = cell(1, N);  sProb.locFuns.hhi = cell(1, N);  for i = 1 : N  sProb.locFuns.ffi(i) = {matlabFunction(F(i), 'Vars', {y(:, i)})} ;  sProb.locFuns.hhi(i) = {matlabFunction(H(i), 'Vars', {y(:, i)})} ;  end  sProb.AA = AA;  sProb.zz0 = zz0;\n\n#### 12.2.4 Runtime Analysis\n\nFor the runtime analysis, the idea is to  \u0301run the sensor network localization problem with varying number of sensors both with a decentral and a central optimization step. To do so, firstly a vector with a number of sensors is needed and secondly a vector with variances. Then, the time needed for the decentral and the central optimization is measured and can be plotted.\n\nN = [5, 10, 15 , 20, 25, 30, 35, 40, 50, 60, 70, 80, 90, 100];  sigma = [0.5, 1, 1.5, 2, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5];  time = zeros(2, length(N));  for i = 1 : length(N)  sProb = setupSolver(N(i), sigma(i)); "]}, {"edit": ["\n\n**Definition 4.11**.: We say a spin system \\(\\mu\\) on \\(\\mathbb{Z}^{d}\\) satisfies the _strong spatial mixing (SSM)_ condition if there exist constants \\(\\alpha,\\gamma,L>0\\) such that for every \\(d\\)-dimensional rectangle \\(\\Lambda\\subset\\mathbb{Z}^{d}\\) of side length between \\(L\\) and \\(2L\\) and every subset \\(B\\subset\\Lambda\\), with any pair \\((\\tau,\\tau^{\\prime})\\) of boundary configurations on \\(\\partial\\Lambda\\) that only differ at a vertex \\(u\\), we have\n\n\\[\\|\\mu_{B}^{\\tau}(\\cdot)-\\mu_{B}^{\\tau^{\\prime}}(\\cdot)\\|_{TV}\\leq\\gamma\\cdot \\exp(-\\alpha\\cdot dist(u,B)),\\]\n\nwhere \\(dist(\\cdot,\\cdot)\\) denotes graph distance.\n\nThe definition above differs from other variants of SSM in the literature (e.g., [13, 14]) in that \\(\\Lambda\\) has been restricted to \"regular enough\" rectangles. In particular, our variant of SSM is easier to satisfy than those in [13, 14] but more restricting than the one in [14] (that only considers squares). Nevertheless, it follows from [12, 14, 15, 16] that for the ferromagnetic Ising model, this form of SSM holds up to a critical threshold temperature \\(\\beta<\\beta_{c}(2)=\\ln(1+\\sqrt{2})\\) on \\(\\mathbb{Z}^{2}\\).\n\nCorollary 1.9 from the introduction states that for \\(b\\)-marginally bounded monotone spin system on \\(d\\)-dimensional cubes \\(V\\subseteq\\mathbb{Z}^{d}\\), SSM implies that the mixing time of any systematic scan \\(P_{\\phi}\\) is \\(O(\\log n)\\). As mentioned there, this result in turn implies that any systematic scan dynamics for the ferromagnetic Ising model is mixing in \\(O(\\log n)\\) steps on boxes of \\(\\mathbb{Z}^{2}\\) when \\(\\beta<\\beta_{c}(2)\\). Another interesting consequence of Corollary 1.9 is that we obtain \\(O(\\log n)\\) mixing time for any systematic scan dynamics \\(P_{\\phi}\\) for the hardcore model on \\(\\mathbb{Z}^{2}\\) when \\(\\lambda<2.538\\), which is the best known condition for ensuring SSM [13, 12].\n\nOur proof of Corollary 1.9 relies on Lemma 4.12 that is restated below. Remarkably, Lemma 4.12 generalizes beyond monotone systems and may be of independent interests.\n\n**Lemma 4.12**.: _For a spin system on a \\(d\\)-dimensional cube \\(V\\subseteq\\mathbb{Z}^{d}\\), SSM implies \\(\\eta\\)-spectral independence, where \\(\\eta=O(1)\\)._\n\n*\n\nProof of Corollary 1.9.: Assume a monotone spin system satisfies SSM condition. Then the spin system satisfies \\(\\eta\\)-spectral independence, where \\(\\eta=O(1)\\) by Lemma 4.12. By noting that \\(\\Delta=2^{d}\\) the corollary follows from Theorem 4.3. \n\nLastly, we give a proof of Lemma 4.12. For this, we recall the notion of a \\(\\kappa\\)-contractive coupling which is known to imply spectral independence. We say a distribution \\(\\mu\\) is \\(\\kappa\\)_-contractive_ with respect to a Markov chain \\(P\\) if for all \\(X_{0}\\), \\(Y_{0}\\in\\Omega\\), there exists"], "nougat": ["\n\n**Definition 4.12.** We say a spin system on a \\(\\mathbb{Z}^{d}\\) satisfies the _strong spatial mixing (SSM)_ condition if there exist constants \\(\\alpha,\\gamma,L>0\\) such that for every \\(d\\)-dimensional rectangle \\(\\Lambda\\subset\\mathbb{Z}^{d}\\) of side length between \\(L\\) and \\(2L\\) and every subset \\(B\\subset\\Lambda\\), with any pair \\((\\tau,\\tau^{\\prime})\\) of boundary configurations on \\(\\partial\\Lambda\\) that only differ at a vertex \\(u\\), we have\n\n\\[\\|\\mu_{B}^{\\tau}(\\cdot)-\\mu_{B}^{\\tau^{\\prime}}(\\cdot)\\|_{TV}\\leq\\gamma\\cdot \\exp(-\\alpha\\cdot dist(u,B)),\\]\n\nwhere \\(dist(\\cdot,\\cdot)\\) denotes graph distance.\n\nThe definition above differs from other variants of SSM in the literature (e.g., [13, 14]) in that \\(\\Lambda\\) has been restricted to \"regular enough\" rectangles. In particular, our variant of SSM is easier to satisfy than those in [13, 14] but more restricting than the one in [14] (that only considers squares). Nevertheless, it follows from [12, 14, 15, 16] that for the ferromagnetic Ising model, this form of SSM holds up to a critical threshold temperature \\(\\beta<\\beta_{c}(2)=\\ln(1+\\sqrt{2})\\) on \\(\\mathbb{Z}^{2}\\).\n\nCorollary 1.11.1 from the introduction states that for \\(b\\)-marginally bounded monotone spin system on \\(d\\)-dimensional cubes \\(V\\subseteq\\mathbb{Z}^{d}\\), SSM implies that the mixing time of any systematic scan \\(P_{\\phi}\\) is \\(O(\\log n)\\). As mentioned there, this result in turn implies that any systematic scan dynamics for the ferromagnetic Ising model is mixing in \\(O(\\log n)\\) steps on boxes of \\(\\mathbb{Z}^{2}\\) when \\(\\beta<\\beta_{c}(2)\\). Another interesting consequence of Corollary 1.11.1.1 is that we obtain \\(O(\\log n)\\) mixing time for any systematic scan dynamics \\(P_{\\phi}\\) for the hardcore model on \\(\\mathbb{Z}^{2}\\) when \\(\\lambda<2.538\\), which is the best known condition for ensuring SSM [13, 12].\n\nOur proof of Corollary 1.11.1.1 relies on Lemma 4.12 that is restated below. Remarkably, Lemma 4.12 generalizes beyond monotone systems and may be of independent interests.\n\n**Lemma 4.13.**_For a spin system on a \\(d\\)-dimensional cube \\(V\\subseteq\\mathbb{Z}^{d}\\), SSM implies \\(\\eta\\)-spectral independence, where \\(\\eta=O(1)\\)._\n\n*\n\nProof of Corollary 1.11.1.: Assume a monotone spin system satisfies SSM condition. Then the spin system satisfies \\(\\eta\\)-spectral independence, where \\(\\eta=O(1)\\) by Lemma 4.13. By noting that \\(\\Delta=2^{d}\\) the corollary follows from Theorem 4.12. \n\nLastly, we give a proof of Lemma 4.13. For this, we recall the notion of a \\(\\kappa\\)-contractive coupling which is known to imply spectral independence. We say a distribution \\(\\mu\\) is \\(\\kappa\\)_-contractive_ with respect to a Markov chain \\(P\\) if for all \\(X_{0}\\), \\(Y_{0}\\"]}, {"edit": ["The initial conditions for the Taylor-Green vortex are\n\n\\[\\rho(x,y,z) =1\\] \\[u(x,y,z) =\\sin(x)\\cos(y)\\cos(z)\\] \\[v(x,y,z) =-\\cos(x)\\sin(y)\\cos(z)\\] \\[w(x,y,z) =0\\] \\[p(x,y,z) =10+\\frac{(\\cos(2x)+\\cos(2y))(\\cos(2x)+2)-2}{16}\\]\n\nwith a pressure value corresponding to a Mach number \\(M\\approx 0.26\\). The triperiodic domain has side length \\(2\\pi\\) in all directions and is discretized using \\(32\\times 32\\times 32\\) nodes. The chosen CFL value is sufficiently small that linear invariants are exactly conserved to machine precision for all schemes. The time evolution of the entropy integral for this test is shown in Fig. 2 and it is in agreement with the previous results. In this test, since the pressure is not constant, \\(A\\rho\\)-\\(He\\) is no longer equivalent to \\(A\\rho\\)-\\(Ap\\); in this case we have better performances from \\(A\\rho\\)-\\(He\\) and \\(G\\rho\\)-\\(Ge\\) when compared to \\(A\\rho\\)-\\(Ap\\) and \\(A\\rho\\)-\\(Ae\\) and this result is found for both fourth-order and six-order accurate fluxes. An improvement can be obtained using an additional term in the expansions and KEEP\\({}^{(1)}\\) and AEP\\({}^{(1)}\\) are the schemes which more closely achieve a constant value for the entropy integral. Information about the reliability of the scheme can be obtained thorough the study of the evolution of thermodynamic fluctuations in time. We checked that for all the schemes tested, the density and temperature fluctuations do not have an unbound growth (not shown). This is the desired behaviour, since for inviscid isotropic homogeneous turbulence they are reported to level off to a constant value [18, 2].\n\n## 6 Conclusions\n\nWe proposed a new class of asymptotically entropy-preserving fluxes for the discretization of the convective terms in the compressible Euler equations with interesting properties. It provides a consistent asymptotic approximation of an existing entropy-preserving scheme based on the logarithmic mean, and it consists of economical algebraic fluxes based on the harmonic mean. Moreover, at all orders of approximation, the numerical fluxes have the pressure-equilibrium preservation property. The theoretical predictions are confirmed on two test cases, verifying that the new schemes are able to numerically maintain pressure equilibrium and demonstrating good entropy-conservation property. It was also shown that the error on entropy can be reduced by using additional terms in the expansion of the AEC fluxes.\n\nThese results suggest that AEC fluxes could be good candidate for the discretization of compressible flow equations in high performance solvers. Due to the their algebraic form, they are less computationally expensive than the fluxes based on the logarithmic mean, while retaining many important properties. In fact, they guarantee the KEP and PEP properties, combined with arbitrarily small error on entropy preservation.\n\n## Appendix A High-order extension\n\nThe second-order accurate two-point fluxes presented in this article can be extended to higher-order formulations by using the approach proposed by Ranocha [7] in the context of Discontinuous Galerkin discretization of the Euler equations. The main result of interest for us is that contained in Theorem 3.1 of [7], which can be reformulated in FD terms as follows. We consider a numerical flux \\(\\mathcal{F}(\\mathbf{w_{i}},\\mathbf{w_{i+k}})\\) for a generic quantity \\(\\rho\\varphi\\), which depends on the values of the variables vector \\(\\mathbf{w}\\) in the nodal points \\(i\\) and \\(i+k\\). In our context \\(\\mathcal{F}\\) can be any of the numerical fluxes specified in Eqs. (1)-(4),(9)-(10) or (13)-(15) and \\(\\mathbf{w}\\) is the set of variables \\((\\rho,u,e)\\). We will assume that the numerical flux is "], "nougat": ["The initial conditions for the Taylor-Green vortex are\n\n\\[\\rho(x,y,z) =1\\] \\[u(x,y,z) =\\sin(x)\\cos(y)\\cos(z)\\] \\[v(x,y,z) =-\\cos(x)\\sin(y)\\cos(z)\\] \\[w(x,y,z) =0\\] \\[p(x,y,z) =10+\\frac{(\\cos(2x)+\\cos(2y))(\\cos(2x)+2)-2}{16}\\]\n\nwith a pressure value corresponding to a Mach number \\(M\\approx 0.26\\). The triperiodic domain has side length \\(2\\pi\\) in all directions and is discretized using \\(32\\times 32\\times 32\\) nodes. The chosen CFL value is sufficiently small that linear invariants are exactly conserved to machine precision for all schemes. The time evolution of the entropy integral for this test is shown in Fig. 2 and it is in agreement with the previous results. In this test, since the pressure is not constant, \\(A\\rho\\)-\\(He\\) is no longer equivalent to \\(A\\rho\\)-\\(Ap\\); in this case we have better performances from \\(A\\rho\\)-\\(He\\) and \\(G\\rho\\)-\\(Ge\\) when compared to \\(A\\rho\\)-\\(Ap\\) and \\(A\\rho\\)-\\(Ae\\) and this result is found for both fourth-order and six-order accurate fluxes. An improvement can be obtained using an additional term in the expansions and KEEP\\({}^{(1)}\\) and AEP are the schemes which more closely achieve a constant value for the entropy integral. Information about the reliability of the scheme can be obtained thorough the study of the evolution of thermodynamic fluctuations in time. We checked that for all the schemes tested, the density and temperature fluctuations do not have an unbound growth (not shown). This is the desired behaviour, since for inviscid isotropic homogeneous turbulence they are reported to level off to a constant value [18, 2].\n\n## 6 Conclusions\n\nWe proposed a new class of asymptotically entropy-preserving fluxes for the discretization of the convective terms in the compressible Euler equations with interesting properties. It provides a consistent asymptotic approximation of an existing entropy-preserving scheme based on the logarithmic mean, and it consists of economical algebraic fluxes based on the harmonic mean. Moreover, at all orders of approximation, the numerical fluxes have the pressure-equilibrium preservation property. The theoretical predictions are confirmed on two test cases, verifying that the new schemes are able to numerically maintain pressure equilibrium and demonstrating good entropy-conservation property. It was also shown that the error on entropy can be reduced by using additional terms in the expansion of the AEC fluxes.\n\nThese results suggest that AEC fluxes could be good candidate for the discretization of compressible flow equations in high performance solvers. Due to the their algebraic form, they are less computationally expensive than the fluxes based on the logarithmic mean, while retaining many important properties. In fact, they guarantee the KEP and PEP properties, combined with arbitrarily small error on entropy preservation.\n\n## Appendix A High-order extension\n\nThe second-order accurate two-point fluxes presented in this article can be extended to higher-order formulations by using the approach proposed by Ranocha [ 7 ] in the context of Discontinuous Galerkin discretization of the Euler equations. The main result of interest for us is that contained in Theorem 3.1 of [ 7 ], which can be reformulated in FD terms as follows. We consider a numerical flux \\(\\mathcal{F}(\\mathbf{w_{i}},\\mathbf{w_{i+k}})\\) for a generic quantity \\(\\rho\\varphi\\), which depends on the values of the variables vector \\(\\mathbf{w}\\) in the nodal points \\(i\\) and \\(i+k\\). In our context \\(\\mathcal{F}\\) can be any of the numerical fluxes specified in Eqs. (1)-(4),(9)-(10) or (13)-(15) and \\(\\mathbf{w}\\) is the set of variables \\((\\rho,u,e)\\). We will "]}, {"edit": ["Empirical data\n\nWe observe the changes of activity over time of the empirical networks for the four data sets (Figure 11). The _US school_ presents periodic patterns, varying from low contact periods when the students are in class to high contact periods when there is a recreational time. The _US flight_ and _Conference_ networks have circadian patterns as there are respectively less flights and less contacts at night. Finally, the _Resistance game_ does not present any periodic change in its activity as every player of the game is looking at someone else at each time step.\n\n## References\n\n* [1]P. Holme and J. Saramaki (2012) Temporal networks. Physics reports519 (3), pp. 97-125. Cited by: SS1.\n* [2]P. Holme (2015) Modern temporal network theory: a colloquium. The European Physical Journal B88, pp. 1-30. Cited by: SS1.\n* [3]N. Masuda and R. Lambiotte (2016) A guide to temporal networks. World Scientific. Cited by: SS1.\n* [4]A. Barrat and C. Cattuto (2013) Temporal networks of face-to-face human interactions. Temporal networks, pp. 191-216. Cited by: SS1.\n* [5]S. Lehmann (2019) Fundamental structures in temporal communication networks. Temporal Network Theory, pp. 25-48. Cited by: SS1.\n* [6]M. Saqr and S. Lopez-Pernas (2022) The why, the what and the how to model a dynamic relational learning process with temporal networks. In Proceedings of the NetSciLA22 workshop, Cited by: SS1.\n\nFigure 11: Number of events as a function of time for the four data sets: the _US school_ (panel a), the _US flight_ (panel b), the _Conference_ (panel c) and the _Resistance game_ (panel d). The _US school_ network contains high activity periods during recreational moments of the students\u2019 day, while the _US flight_ and the _Conference_ networks present circadian patterns. The Resistance game network does not have particular periodic activity changes.\n\n "], "nougat": ["Empirical data\n\nWe observe the changes of activity over time of the empirical networks for the four data sets (Figure 11). The _US school_ presents periodic patterns, varying from low contact periods when the students are in class to high contact periods when there is a recreational time. The _US flight_ and _Conference_ networks have circadian patterns as there are respectively less flights and less contacts at night. Finally, the _Resistance game_ does not present any periodic change in its activity as every player of the game is looking at someone else at each time step.\n\n## References\n\n* [1] Petter Holme and Jari Saram \u0308aki. Temporal networks. _Physics reports_, 519(3):97\u2013125, 2012. [2] Petter Holme. Modern temporal network theory: a colloquium. _The European Physical Journal B_, 88:1\u201330, 2015. [3] Naoki Masuda and Renaud Lambiotte. _A guide to temporal networks_. World Scientific, 2016. [4] Alain Barrat and Ciro Cattuto. Temporal networks of face-to-face human interactions. _Temporal networks_, pages 191\u2013216, 2013. [5] Sune Lehmann. Fundamental structures in temporal communication networks. _Temporal Network Theory_, pages 25\u201348, 2019. [6] Mohammed Saqr and Sonsoles L \u0301opez-Pernas. The why, the what and the how to model a dynamic relational learning process with temporal networks. In _Proceedings of the NetSciLA22 workshop_, 2022.\n\nFigure 11: Number of events as a function of time for the four data sets: the _US school_ (panel a), the _US flight_ (panel b), the _Conference_ (panel c) and the _Resistance game_ (panel d). The _US school_ network contains high activity periods during recreational moments of the students\u2019 day, while the _US flight_ and the _Conference_ networks present circadian patterns. The Resistance game network does not have particular periodic activity changes.\n\n "]}, {"edit": ["\\[=\\sum_{|x|\\leq(n-1)N_{k_{j-1}}}\\beta_{k_{j}}^{*q}(x)\\theta(-x)\\] \\[\\leq\\sum_{|x|\\leq(n-1)N_{k_{j-1}}}\\beta_{k_{j}}^{*q}(x)\\] \\[=\\beta_{k_{j}}^{*q}\\left(\\left[-(n-1)N_{k_{j}-1},(n-1)N_{k_{j}-1} \\right]\\right)\\] \\[\\leq\\varepsilon_{n},\\]\n\nwhere we used the fact that \\(\\beta_{k_{j}}\\) satisfies Equation (2), with \\(k_{j}\\geq p_{n}\\).\n\nWe conclude that \\(\\nu^{*n}(0)=\\gamma_{1}(0)+\\gamma_{2}(0)\\leq 2\\varepsilon_{n}\\), which finishes the proof. \n\nIf we weaken the hypothesis \\(\\mathbb{E}\\left(|\\mathrm{supp}(\\sigma_{1})|\\right)<\\infty\\) from Lemma 4.10 it is possible that the permutation coordinate never stabilizes. Indeed, with ideas similar to an example of [11], we obtain the following.\n\n**Proposition 4.14**.: _The group \\(\\mathsf{Shuffler}(\\mathbb{Z})\\) admits probability measures \\(\\mu\\) with an infinite first moment and a finite \\((1-\\varepsilon)\\)-moment, for every \\(0<\\varepsilon<1\\), that induce a transient random walk on \\(\\mathbb{Z}\\) and for which the permutation coordinate of the \\(\\mu\\)-random walk does not stabilize. Such measures can be chosen to satisfy \\(\\mathbb{E}(|\\mathrm{supp}(\\sigma_{1})|)=\\infty\\) and \\(\\mathbb{E}(|\\mathrm{supp}(\\sigma_{1})|^{1-\\varepsilon})<\\infty\\) for every \\(0<\\varepsilon<1\\)._\n\nProof.: For each \\(n\\geq 1\\), denote by \\(r_{n}:\\mathbb{Z}\\to\\mathbb{Z}\\) the permutation\n\n\\[r_{n}(x)=\\left\\{\\begin{aligned} & x+1,\\,\\text{if}\\,\\,0\\leq x<n-1,\\\\ & 0,\\,\\text{if}\\,\\,x=n-1,\\,\\,\\text{and}\\\\ & x,\\,\\text{otherwise}.\\end{aligned}\\right.\\]\n\nWe define the measure \\(\\mu\\) on \\(\\mathsf{Shuffler}(\\mathbb{Z})\\) as follows. Let\n\n\\[\\mu((\\mathsf{id},1))=1/8,\\ \\mu((\\mathsf{id},-1))=3/8,\\]\n\nand\n\n\\[\\mu((r_{n},0))=\\frac{1}{2n(n+1)},\\ \\ \\text{for}\\ n\\geq 1.\\]\n\nNote that \\(\\sum_{n\\geq 1}\\frac{1}{n(n+1)}=1\\), so that \\(\\mu\\) is indeed a probability measure. Also note that \\(|\\mathrm{supp}(r_{n})|=n\\). From this, the fact that the harmonic series \\(\\sum_{n\\geq 1}\\frac{1}{n}\\) diverges implies that \\(\\mathbb{E}(|\\mathrm{supp}(\\sigma_{1})|)\\) is infinite. Moreover, since \\(\\|(r_{n},0) "], "nougat": ["\\[=\\sum_{|x|\\leq(n-1)N_{k_{j-1}}}\\beta_{k_{j}}^{*q}(x)\\theta(-x)\\] \\[\\leq\\sum_{|x|\\leq(n-1)N_{k_{j-1}}}\\beta_{k_{j}}^{*q}(x)\\] \\[=\\beta_{k_{j}}^{*q}\\left(\\left[-(n-1)N_{k_{j}-1},(n-1)N_{k_{j}-1} \\right]\\right)\\] \\[\\leq\\varepsilon_{n},\\]\n\nwhere we used the fact that \\(\\beta_{k_{j}}\\) satisfies Equation (2), with \\(k_{j}\\geq p_{n}\\).\n\nWe conclude that \\(\\nu^{*n}(0)=\\gamma_{1}(0)+\\gamma_{2}(0)\\leq 2\\varepsilon_{n}\\), which finishes the proof. \n\nIf we weaken the hypothesis \\(\\mathbb{E}\\left(|\\mathrm{supp}(\\sigma_{1})|\\right)<\\infty\\) from Lemma 4.10 it is possible that the permutation coordinate never stabilizes. Indeed, with ideas similar to an example of [11], we obtain the following.\n\n**Proposition 4.14**.: _The group \\(\\mathsf{Shuffler}(\\mathbb{Z})\\) admits probability measures \\(\\mu\\) with an infinite first moment and a finite \\((1-\\varepsilon)\\)-moment, for every \\(0<\\varepsilon<1\\), that induce a transient random walk on \\(\\mathbb{Z}\\) and for which the permutation coordinate of the \\(\\mu\\)-random walk does not stabilize. Such measures can be chosen to satisfy \\(\\mathbb{E}(|\\mathrm{supp}(\\sigma_{1})|)=\\infty\\) and \\(\\mathbb{E}(|\\mathrm{supp}(\\sigma_{1})|^{1-\\varepsilon})<\\infty\\) for every \\(0<\\varepsilon<1\\)._\n\nProof.: For each \\(n\\geq 1\\), denote by \\(r_{n}:\\mathbb{Z}\\to\\mathbb{Z}\\) the permutation\n\n\\[r_{n}(x)=\\left\\{\\begin{aligned} & x+1,\\,\\text{if}\\,\\,0\\leq x<n-1,\\\\ & 0,\\,\\text{if}\\,\\,x=n-1,\\,\\,\\text{and}\\\\ & x,\\,\\text{otherwise}.\\end{aligned}\\right.\\]\n\nWe define the measure \\(\\mu\\) on \\(\\mathsf{Shuffler}(\\mathbb{Z})\\) as follows. Let\n\n\\[\\mu((\\mathsf{id},1))=1/8,\\ \\mu((\\mathsf{id},-1))=3/8,\\]\n\nand\n\n\\[\\mu((r_{n},0))=\\frac{1}{2n(n+1)},\\ \\ \\text{for}\\ n\\geq 1.\\]\n\nNote that \\(\\sum_{n\\geq 1}\\frac{1}{n(n+1)}=1\\), so that \\(\\mu\\) is indeed a probability measure. Also note that \\(|\\mathrm{supp}(r_{n})|=n\\). From this, the fact that the harmonic series \\(\\sum_{n\\geq 1}\\frac{1}{n}\\) diverges implies that \\(\\mathbb{E}(|\\mathrm{supp}(\\sigma_{1})|)\\) is infinite. Moreover, since \\(\\|(r_{n},0) "]}, {"edit": ["\n\n### Metrics\n\nThree metrics were widely used across the papers: accuracy, specificity, and sensitivity. The equations for these four metrics can be seen below.\n\n\\[\\text{Accuracy}=\\frac{\\text{Correctly classified speech}}{\\text{Total speech samples}} \\tag{1}\\]\n\n\\[\\text{Specificity}=\\frac{\\text{Correctly classified healthy speech}}{\\text{Total healthy speech}} \\tag{2}\\]\n\n\\[\\text{Sensitivity}=\\frac{\\text{Correctly classified pathological speech}}{\\text{Total pathological speech}} \\tag{3}\\]\n\nAnother metric was often used in the papers using multi-class classification - unweighted average recall (UAR). This metric is calculated by averaging the recall value for each of the specific pathologies included in the dataset. Equation 4 shows how it is calculated, where N is the number of pathologies in the dataset and \\(R_{i}\\) is the recall of the ith pathology in the dataset.\n\n\\[\\text{UAR}=\\frac{\\sum_{i=1}^{N}R_{i}}{N} \\tag{4}\\]\n\n### Binary Classification Literature\n\n[2020] investigate the classification of cancer patients from healthy controls using six machine learning algorithms. The dataset used includes recordings of the prolonged vowel /ah/ from 50 male laryngeal\n\nFigure 6: MFCCs generated from a short audio signal.\n\n"], "nougat": ["\n\n### Metrics\n\nThree metrics were widely used across the papers: accuracy, specificity, and sensitivity. The equations for these four metrics can be seen below.\n\n\\[\\text{Accuracy}=\\frac{\\text{Correctly classified speech}}{\\text{Total speech samples}} \\tag{1}\\]\n\n\\[\\text{Specificity}=\\frac{\\text{Correctly classified healthy speech}}{\\text{Total healthy speech}} \\tag{2}\\]\n\n\\[\\text{Sensitivity}=\\frac{\\text{Correctly classified pathological speech}}{\\text{Total pathological speech}} \\tag{3}\\]\n\nAnother metric was often used in the papers using multi-class classification unweighted average recall (UAR). This metric is calculated by averaging the recall value for each of the specific pathologies included in the dataset. Equation 4 shows how it is calculated, where N is the number of pathologies in the dataset and \\(R_{i}\\) is the recall of the ith pathology in the dataset.\n\n\\[\\text{UAR}=\\frac{\\sum_{i=1}^{N}R_{i}}{N} \\tag{4}\\]\n\n### Binary Classification Literature\n\n[2020) investigate the classification of cancer patients from healthy controls using six machine learning algorithms. The dataset used includes recordings of the prolonged vowel /ah/ from 50 male laryngeal\n\nFigure 6: MFCCs generated from a short audio signal.\n\n"]}, {"edit": ["for this paper, since a majority of features in keystroke sounds are within the lower frequencies [15, 3, 4] and would therefore be less distinguishable on a linear scale. Meanwhile, MFCC involves performing the discrete cosine transform on a mel-spectrogram, producing a compressed representation that prioritises the frequencies used in human speech. Since, for this paper, human speech is not the target, and the removal of frequencies could risk the loss of relevant data, MFCC was decided to be less suitable than mel-spectrograms.\n\n**Data augmentation:** Prior to feature extraction, signals were time-shifted randomly by up to 40% in either direction. This time shifting is an instance of data augmentation, in which the amount of data input to a DL model is artificially increased by slightly adjusting existing inputs [28]. The mel-spectrograms were then generated using 64 mel bands, a window length of 1024 samples and hop length of 500 (255 for the MacBook keystrokes, given their shorter length), resulting in 64x64 images. Using the spectrograms, a second method of data augmentation was implemented called masking. This method involves taking a random 10% of both the time and frequency axis and setting all values within those ranges to the mean of the spectrogram, essentially 'blocking out' a portion of the image. Using time warping and spectrogram masking combined is called SpecAugment and was found to encourage the model to generalise and avoid overfitting the training data [25, 10].\n\nHaving converted keystrokes from each data set into a more visual medium, more direct comparisons could be made. MacBook keystrokes (similar to the keystrokes examined in the literature [4, 39, 6]) have only 2 visible peaks: the 'push' and 'release' peaks respectively. The 2 peak structures shown in Fig. 2 are similar to each other, implying that such a structure is native to the MacBook keyboard regardless of recording method, a noticeable difference however is the large range of frequencies present in the zoom recording. The Zoom peaks extend much higher than that of the phone-based recordings, indicating significant data in multiple frequencies that were not present when recorded via phone.\n\nThe overall data preparation procedure for our data was inspired by the structure presented in [10] and is shown in Fig. 3.\n\n### Model Selection and Implementation\n\nFigure 2: Waveform and corresponding mel-spectrogram of Left: Phone recording, and Right: Zoom recording.\n\n "], "nougat": ["for this paper, since a majority of features in keystroke sounds are within the lower frequencies [15, 3, 4] and would therefore be less distinguishable on a linear scale. Meanwhile, MFCC involves performing the discrete cosine transform on a mel-spectrogram, producing a compressed representation that prioritises the frequencies used in human speech. Since, for this paper, human speech is not the target, and the removal of frequencies could risk the loss of relevant data, MFCC was decided to be less suitable than mel-spectrograms.\n\n**Data augmentation:** Prior to feature extraction, signals were time-shifted randomly by up to 40% in either direction. This time shifting is an instance of data augmentation, in which the amount of data input to a DL model is artificially increased by slightly adjusting existing inputs [28]. The mel-spectrograms were then generated using 64 mel bands, a window length of 1024 samples and hop length of 500 (255 for the MacBook keystrokes, given their shorter length), resulting in 64x64 images. Using the spectrograms, a second method of data augmentation was implemented called masking. This method involves taking a random 10].0% of both the time and frequency axis and setting all values within those ranges to the mean of the spectrogram, essentially 'blocking out' a portion of the image. Using time warping and spectrogram masking combined is called SpecAugment and was found to encourage the model to generalise and avoid overfitting the training data [25, 10].\n\nHaving converted keystrokes from each data set into a more visual medium, more direct comparisons could be made. MacBook keystrokes (similar to the keystrokes examined in the literature [4, 39, 6]) have only 2 visible peaks: the 'push' and 'release' peaks respectively. The 2 peak structures shown in Fig. 2 are similar to each other, implying that such a structure is native to the MacBook keyboard regardless of recording method, a noticeable difference however is the large range of frequencies present in the zoom recording. The Zoom peaks extend much higher than that of the phone-based recordings, indicating significant data in multiple frequencies that were not present when recorded via phone.\n\nThe overall data preparation procedure for our data was inspired by the structure presented in [10] and is shown in Fig. 3.\n\n### Model Selection and Implementation\n\nFigure 2: Waveform and corresponding mel-spectrogram of Left: Phone recording, and Right: Zoom recording.\n\n "]}, {"edit": ["CNPq through grant 308900/2019-7. R. Clemente acknowledges partial support from CNPq through grant 304454/2022-2.\n\n99\n\nAbreu, E., O, J. & Medeiros, E. Properties of positive harmonic functions on the half-space with a nonlinear boundary condition. _J. Differential Equations_. **248**, 617-637 (2010), [https://doi.org/10.1016/j.jde.2009.07.006](https://doi.org/10.1016/j.jde.2009.07.006)\n\nAdams, R. & Fournier, J. Sobolev spaces. (Elsevier/Academic Press, Amsterdam,2003)\n\nAleksandrov, A. Uniqueness theorems for surfaces in the large. I. _Amer. Math. Soc. Transl. (2)._**21** pp. 341-354 (1962), [https://doi.org/10.1090/trans2/021/09](https://doi.org/10.1090/trans2/021/09)\n\nAlexandrov, A. A characteristic property of spheres. _Ann. Mat. Pura Appl. (4)._**58** pp. 303-315 (1962), [https://doi.org/10.1007/BF02413056](https://doi.org/10.1007/BF02413056)\n\nAllegretto, W. & Huang, Y. A Picone's identity for the p-Laplacian and applications. _Nonlinear Anal._. **32**, 819-830 (1998), [https://doi.org/10.1016/S0362-546X](https://doi.org/10.1016/S0362-546X)(97)00530-0\n\nBonder, J. & Rossi, J. Existence results for the p-Laplacian with nonlinear boundary conditions. _J. Math. Anal. Appl._. **263**, 195-223 (2001), [https://doi.org/10.1006/jmaa.2001.7609](https://doi.org/10.1006/jmaa.2001.7609)\n\nChipot, M., Chlebik, M., Fila, M. & Shafrir, I. Existence of positive solutions of a semilinear elliptic equation in \\(\\mathbb{R}^{n}_{+}\\) with a nonlinear boundary condition. _J. Math. Anal. Appl._. **223**, 429-471 (1998), [https://doi.org/10.1006/jmaa.1998.5958](https://doi.org/10.1006/jmaa.1998.5958)\n\nCuesta, M. & Takac, P. A strong comparison principle for positive solutions of degenerate elliptic equations. _Differential Integral Equations_. **13**, 721-746 (2000)\n\nDamascelli, L. & Pacella, F. Monotonicity and symmetry of solutions of p-Laplace equations, \\(1<p<2\\), via the moving plane method. _Ann. Scuola Norm. Sup. Pisa Cl. Sci. (4)._**26**, 689-707 (1998), [http://www.numdam.org/item?id=ASNSP_1998](http://www.numdam.org/item?id=ASNSP_1998)\\(4\\)26\\(4\\)689_0\n\nDamascelli, L. & Sciunzi, B. Regularity, monotonicity and symmetry of positive solutions of m-Laplace equations. _J. Differential Equations_. **206**, 483-515 (2004), [https://doi.org/10.1016/j.jde.2004.05.012](https://doi.org/10.1016/j.jde.2004.05.012)\n\nDegiovanni, M., Musesti, A. & Squassina, M. On the regularity of solutions in the\n\nPucci-Serrin identity. _Calc. Var. Partial Differential Equations_. **18**, 317-334 (2003), [https://doi.org/10.1007/s00526-](https://doi.org/10.1007/s00526-) "], "nougat": ["CNPq through grant 308900/2019-7. R. Clemente acknowledges partial support from CNPq through grant 304454/2022-2.\n\n99\n\nAbreu, E.,  \u0301 O, J. & Medeiros, E. Properties of positive harmonic functions on the half-space with a nonlinear boundary condition. _J. Differential Equations_. **248**, 617-637 (2010), Adams, R. & Fournier, J. Sobolev spaces. (Elsevier/Academic Press, Amsterdam,2003) Aleksandrov, A. Uniqueness theorems for surfaces in the large. I. _Amer. Math. Soc. Transl. (2)._**21** pp. 341-354 (1962), [https://doi.org/10.1016/j.jde.2009.07.006](https://doi.org/10.1016/j.jde.2009.07.006) [https://doi.org/10.1090/trans2/021/09](https://doi.org/10.1090/trans2/021/09) Alexandrov, A. A characteristic property of spheres. _Ann. Mat. Pura Appl. (4)._**58** pp. 303-315 (1962), [https://doi.org/10.1007/BF02413056](https://doi.org/10.1007/BF02413056) Allegretto, W. & Huang, Y. A Picone\u2019s identity for the p-Laplacian and applications. _Nonlinear Anal._. **32**, 819-830 (1998), [https://doi.org/10.1016/S0362-546X](https://doi.org/10.1016/S0362-546X)(97)00530-0 Bonder, J. & Rossi, J. Existence results for the p-Laplacian with nonlinear boundary conditions. _J. Math. Anal. Appl._. **263**, 195-223 (2001), Chipot, M., Chleb\u0131 \u0301\u0131k, M., Fila, M. & Shafrir, I. Existence of positive solutions of a semilinear elliptic equation in \\(\\mathbb{R}^{n}_{+}\\) with a nonlinear boundary condition. _J. Math. Anal. Appl._. **223**, 429-471 (1998), [https://doi.org/10.1006/jmaa.2001.7609](https://doi.org/10.1006/jmaa.2001.7609) [https://doi.org/10.1006/jmaa.1998.5958](https://doi.org/10.1006/jmaa.1998.5958) Cuesta, M. & Tak \u0301a\u02c7c, P. A strong comparison principle for positive solutions of degenerate elliptic equations. _Differential Integral Equations_. **13**, 721-746 (2000) Damascelli, L. & Pacella, F. Monotonicity and symmetry of solutions of p-Laplace equations, 1 \\(<p<2\\), via the moving plane method. _Ann. Scuola Norm. Sup. Pisa Cl. Sci. (4)._**26**, 689-707 (1998), [http://www.numdam.org/item?id=ASNSP](http://www.numdam.org/item?id=ASNSP) 1998 4 26 4 689 0 Damascelli, L. & Sciunzi, B. Regularity, monotonicity and symmetry of positive solutions of m-Laplace equations. _J. Differential Equations_. **206**, 483-515 (2004), Degiovanni, M., Musesti, A. & Squassina, M. On the regularity of solutions in the Pucci-Serrin identity. _Calc. Var. Partial Differential Equations_. **18**, 317-334 (2003),  \u0301O, J. & Medeiros, E. Remarks on least energy solutions for quasilinear elliptic problems in \\(\\mathbb{R}^{N}\\). _Electron. J. Differential Equations_. pp.\n\n "]}, {"edit": ["to \\(Ax=b\\) has full support. By Proposition 4, columns of \\(A\\) are integrally independent. It follows from Theorem 6 that inequality (5) holds. \n\n**Remark 1**.: _We demonstrate how to modify the proof of Theorem 6 to obtain the bound (7) given in [1]. We use the same notation as in the proof of Theorem 6. For any \\(m\\times m\\) submatrix of \\(A\\) whose columns are indexed by \\(J\\) where \\(|J|=m\\), we have_\n\n\\[|\\det(A_{[m]\\times J})| =|\\det(D)|\\cdot|\\det((U^{-1})_{[m]\\times J})|\\] \\[=\\gcd(A)\\cdot|\\det((U^{-1})_{[m]\\times J})|\\] \\[=\\gcd(A)\\cdot|\\det(U_{[n]\\vee\\times[m+1:n]})|.\\]\n\n_We also know that_\n\n\\[\\prod_{i\\in[n]\\setminus J}q_{i}\\ \\Big{|}\\ |\\det(U_{[n]\\vee\\times[m+1:n]})|,\\]\n\n_and thus_\n\n\\[\\prod_{i\\in[n]\\setminus J}q_{i}\\ \\Big{|}\\ \\frac{|\\det(A_{[m]\\times J})|}{\\gcd(A)},\\]\n\n_where \\(q_{i},i\\in[n]\\setminus J\\) are primes numbers and with the same prime repeating at most \\(m\\) times in \\(\\{q_{i}\\ |\\ i\\in[n]\\setminus J\\}\\). Recall notation \\(\\Omega_{m}(z)=\\sum_{i=1}^{k}\\min\\{s_{i},m\\}\\) for the prime factorization of \\(z=r_{1}^{s_{1}}\\cdots r_{k}^{s_{k}}\\) with multiplicities \\(s_{1},...,s_{k}\\in\\mathbb{Z}_{>0}\\). Clearly, when \\(x\\ |\\ y,\\Omega_{m}(x)\\leqslant\\Omega_{m}(y)\\). Thus, \\(\\Omega_{m}(\\prod_{i\\in[n]\\setminus J}q_{i})\\leqslant\\Omega_{m}(\\frac{|\\det(A_{ [m]\\times J})|}{\\gcd(A)})\\). Moreover, since the multiplicity of each \\(q_{i}\\) in \\(\\prod_{i\\in[n]\\setminus J}q_{i}\\) is at most \\(m\\), we have \\(\\Omega_{m}(\\prod_{i\\in[n]\\setminus J}q_{i})=|[n]\\setminus J|=n-m\\). Therefore, \\(n-m\\leqslant\\Omega_{m}(\\frac{|\\det(A_{[m]\\times J})|}{\\gcd(A)})\\). Since \\(J\\) is an arbitrary subset of \\([n]\\) with cardinality \\(m\\), we obtain \\(n\\leqslant m+\\min_{r\\in\\binom{[n]}{m},\\det(A_{r})\\neq 0}\\Omega_{m}\\Big{(}\\frac{| \\det(A_{r})|}{\\gcd(A)}\\Big{)}\\). Applying the same argument as in the proof of Theorem 2 above, we obtain \\(f(A)\\leqslant m+\\min_{r\\in\\binom{[n]}{m},\\det(A_{r})\\neq 0}\\ "], "nougat": ["to \\(Ax=b\\) has full support. By Proposition 4 , columns of \\(A\\) are integrally independent. It follows from Theorem 6 to obtain the bound ( 7 ) given in [ AADLO22 ]. We use the same notation as in the proof of Theorem 6 . For any \\(m\\times m\\) submatrix of \\(A\\) whose columns are indexed by \\(J\\) where \\(|J|=m\\), we have_\n\n\\[|\\det(A_{[m]\\times J})| =|\\det(D)|\\cdot|\\det((U^{-1})_{[m]\\times J})|\\] \\[=\\gcd(A)\\cdot|\\det((U^{-1})_{[m]\\times J})|\\] \\[=\\gcd(A)\\cdot|\\det(U_{[n]\\vee\\times[m+1:n]})|.\\]\n\n_We also know that_\n\n\\[\\prod_{i\\in[n]\\setminus J}q_{i}\\ \\Big{|}\\ |\\det(U_{[n]\\vee\\times[m+1:n]})|,\\]\n\n_and thus_\n\n\\[\\prod_{i\\in[n]\\setminus J}q_{i}\\ \\Big{|}\\ \\frac{|\\det(A_{[m]\\times J})|}{\\gcd(A)},\\]\n\n_where \\(q_{i},i\\in[n]\\setminus J\\) are primes numbers and with the same prime repeating at most \\(m\\) times in \\(\\{q_{i}\\ |\\ i\\in[n]\\setminus J\\}\\). Recall notation \\(\\Omega_{m}(z)=\\sum_{i=1}^{k}\\min\\{s_{i},m\\}\\) for the prime factorization of \\(z=r_{1}^{s_{1}}\\cdots r_{k}^{s_{k}}\\) with multiplicities \\(s_{1},...,s_{k}\\in\\mathbb{Z}_{>0}\\). Clearly, when \\(x\\ |\\ y,\\Omega_{m}(x)\\leqslant\\Omega_{m}(y)\\). Thus, \\(\\Omega_{m}(\\prod_{i\\in[n]\\setminus J}q_{i})\\leqslant\\Omega_{m}(\\frac{|\\det(A_ {[m]\\times J})|}{\\gcd(A)})\\). Moreover, since the multiplicity of each \\(q_{i}\\) in \\(\\prod_{i\\in[n]\\setminus J}q_{i}\\) is at most \\(m\\), we have \\(\\Omega_{m}(\\prod_{i\\in[n]\\setminus J}q_{i})=|[n]\\setminus J|=n-m\\). Therefore, \\(n-m\\leqslant\\Omega_{m}(\\frac{|\\det(A_{[m]\\times J})|}{\\gcd(A)})\\). Since \\(J\\) is an arbitrary subset of \\([n]\\) with cardinality \\(m\\), we obtain \\(n\\leqslant m+\\min_{r\\in\\binom{[n]}{m},\\det(A_{r})\\neq 0}\\Omega_{m}\\Big{(}\\frac{| \\det(A_{r})|}{\\gcd(A)}\\Big{)}\\). Applying the same argument as in the proof of Theorem 2 above, we obtain \\(f(A)\\leqslant m+\\min_{r\\in\\binom{[n]}{m},\\det(A_{r})\\neq 0}\\Omega_{m}\\Big{(}\\frac{| \\det(A_{r "]}, {"edit": ["Comparative Analysis of Smaller Dataset (65 Images) and Larger Dataset (2700 Images) using Principal Component Analysis\n\nWe conducted an experiment using Principal Component Analysis (PCA) for image reconstruction with both smaller (65 images) and larger (2700 images) datasets, utilizing 50 principal components for representation. The explained variance for the smaller dataset was approximately \\(98.19\\%\\), while for the larger one it was roughly \\(98.35\\%\\) (Figure 18).\n\nHowever, visual inspections revealed a marked degradation in quality between the two datasets(Figure 18). In the 65-image dataset, the reconstructions showed anomalies like purplish lines in the background and bluish color distortions. For the 2700-image dataset, the performance was even worse, with greenish horizontal lines appearing in the foreground and failure in capturing essential details. Several factors may contribute to this unexpected result. The increased complexity and variability within the larger dataset might have overwhelmed PCA's ability to represent finer details. Since PCA relies on linear assumptions, it might have failed to handle the nonlinear structures and dependencies that become more pronounced with the increase in data complexity. The \\(1.5\\%-2\\%\\) unexplained variance might contain critical information affecting the visual quality, especially in a larger, more intricate dataset. Moreover, the choice of 50 components might have been insufficient for capturing nuanced variations in the larger dataset, despite sufficing for the smaller one. These observations highlight PCA's limitations in handling highly complex image data and emphasize that capturing a high percentage of variance does not guarantee accurate or visually pleasing reconstruction.\n\nFigure 16: **(a) Column 1**: codebook sizes = [1024, 8192, 1024], Latent dimensions = 256, image sizes \\(=[2700,185,2700]\\), with, with and without positional encoding respectively. (refer Figure 12 & Figure 8) **(b) Column 2**: **Smaller network**, codebook size = 8192, latent dimension size = 256, image size = 65, without and with positional encoding. (refer Figure 15)\n\nFigure 15: **Smaller Network**, Original Images (First Four), Reconstructed Images (Last Four) **(a) Row 1**: codebook size=8192, latent dimension size = 256, 65 images, without positional encoding. **(b) Row 2**: codebook size=8192, latent dimension size = 256, 65 images, with positional encoding. (Figure 16(b))\n\n"], "nougat": ["Comparative Analysis of Smaller Dataset (65 Images) and Larger Dataset (2700 Images) using Principal Component Analysis (PCA) for image reconstruction with both smaller (65 images) and larger (2700 images) datasets, utilizing 50 principal components for representation. The explained variance for the smaller dataset was approximately \\(98.19\\%\\), while for the larger one it was roughly \\(98.35\\%\\) (Figure 18). However, visual inspections revealed a marked degradation in quality between the two datasets(Figure 18). In the 65image dataset, the reconstructions showed anomalies like purplish lines in the background and bluish color distortions. For the 2700-image dataset, the performance was even worse, with greenish horizontal lines appearing in the foreground and failure in capturing essential details. Several factors may contribute to this unexpected result. The increased complexity and variability within the larger dataset might have overwhelmed PCA\u2019s ability to represent finer details. Since PCA relies on linear assumptions, it might have failed to handle the nonlinear structures and dependencies that become more pronounced with the increase in data complexity. The \\(1.5\\%-2\\%\\) unexplained variance might contain critical information affecting the visual quality, especially in a larger, more intricate dataset. Moreover, the choice of 50 components might have been insufficient for capturing nuanced variations in the larger dataset, despite sufficing for the smaller one. These observations highlight PCA\u2019s limitations in handling highly complex image data and emphasize that capturing a high percentage of variance does not guarantee accurate or visually pleasing reconstruction.\n\nFigure 16: **(a) Column 1**: codebook sizes = [1024, 8192, 1024], Latent dimensions = 256, image sizes \\(=[2700,185,2700]\\), with, with and without positional encoding respectively. (refer Figure 12 & Figure 8) **(b) Column 2**: **Smaller network**, codebook size = 8192, latent dimension size = 256, image size = 65, without and with positional encoding. (refer Figure 15)\n\nFigure 15: **Smaller Network**, Original Images (First Four), Reconstructed Images (Last Four) **(a) Row 1**: codebook size=8192, latent dimension size = 256, 65 images, without positional encoding. **(b) Row 2**: codebook size=8192, latent dimension size = 256, 65 images, with positional encoding. (Figure 16(b))\n\n"]}, {"edit": ["beam alignment case, an excessive number of antennas is adverse to the network coverage probability. As for the perfect beam alignment case, Fig. 6(b) shows that, as the number of antennas becomes large, the outage probability decreases, and the trend of decreasing gradually slows down. This figure implies that from the perspective such as hardware cost, it is not necessary to equip the UAV with too many antennas since the performance gain is very small.\n\n### _Effect of UAV Deployment Height_\n\nWith regards to the effect of UAV deployment height, Fig. 6(a) shows that the higher deployment of UAVs will lead to a larger outage probability under the imperfect alignment case. The reason is as follows. The higher height implies the larger coverage area of the main-lobe beam on certain tiers, which introduces more interference from more UAVs to the typical UE with the main-lobe pointed. When the beam is mispointed, this kind of effect on SINR becomes worse. Besides that, for the same projection point, the higher height indicates that the transmission link is more likely to be LoS, which means the signal strength from the interfering UAV becomes stronger. Hence, when the misalignment for the beamforming is non-negligible, the lower deployment of UAVs is preferred for both schemes. Under the case of perfect beam alignment, from Fig. 6(b), it can be seen that the outage probability of MAPAS is smaller for the lower height of UAVs. The explanation is the same as before. However, in terms of the performance of CDAS, it seems that the outage probability for the higher altitude of UAVs can be better. This is mainly because of the fact the UE is associated with the closest UAV and their link can be either LoS or NLoS. As mentioned before, for the same projection point, the link from UAV at the lower height is much more likely to be NLoS, which reduces the desired signal strength at the UE whereby degrading the outage performance.\n\n### _Effect of UAV Density_\n\nFig. 7 plots the outage probability versus the number of antennas for UAV under different UAV densities for both imperfect and perfect alignment cases. Under the imperfect beam alignment scenario, as expected, the outage probability drops at first and then rises with the increase in the number of antennas. Moreover, since more interfering UAVs are involved in the system, the higher UAV density leads to worse outage probability performance. Besides that, Fig. 7(a) shows that the optimal number of UAV antennas increases as the UAV density rises, e.g., for MAPAS, the optimal number antennas is 4 for \\(\\lambda_{k}=0.5\\times 10^{-5}\\)\\(\\mathrm{m}^{-2}\\) while it goes to 16 for \\(\\lambda_{k}=5\\times 10^{-5}\\)\\(\\mathrm{m}^{-2}\\). The reason is as follows. When the UAV density is very sparse, the number of interfering UAVs falling into the region covered by the main-lobe beam of the UE is very small. In other words, the interference is not that severe. Then the serving UAV needs to ensure that the typical UE is covered by its main-lobe beam; hence, a larger main-lobe beamwidth (equivalently, a smaller number of antennas) is preferred. However, when the interfering UAVs are very dense, the interference becomes very severe. One way to reduce the interference is to reduce the density of interfering UAVs with main-lobe beam pointed to the typical UE. From our analysis, this can be achieved by narrowing the main-lobe beamwidth. Note that it cannot be too narrow, because this can degrade the signal strength from the serving UAV due to the beam misalignment. Hence, a relatively larger number of antennas is preferred for the case of denser UAVs. Under the perfect beam scenario, for the MAPAS, sparse UAVs lead to a lower outage probability as expected. However, this is not the case for the CDAS. Fig. 7(b) shows that the denser UAVs can even result in a better outage probability, especially when the number of antennas is large. The reason is as follows. When the UAV density is very sparse, the closest UAV (i.e., the serving UAV) can be very far away, which consequently leads to a very weak signal strength from the serving UAV. Increasing the density of UAVs somehow improves the signal strength from the serving\n\nFig. 6: Outage probability versus the number of antennas for UAV \\(N_{v}\\) under different UAV heights for both imperfect and perfect alignment cases.\n\n "], "nougat": ["beam alignment case, an excessive number of antennas is adverse to the network coverage probability. As for the perfect beam alignment case, Fig. 6(b) shows that, as the number of antennas becomes large, the outage probability decreases, and the trend of decreasing gradually slows down. This figure implies that from the perspective such as hardware cost, it is not necessary to equip the UAV with too many antennas since the performance gain is very small.\n\n### _Effect of UAV Deployment Height_\n\nWith regards to the effect of UAV deployment height, Fig. 6(a) shows that the higher deployment of UAVs will lead to a larger outage probability under the imperfect alignment case. The reason is as follows. The higher height implies the larger coverage area of the main-lobe beam on certain tiers, which introduces more interference from more UAVs to the typical UE with the main-lobe pointed. When the beam is mispointed, this kind of effect on SINR becomes worse. Besides that, for the same projection point, the higher height indicates that the transmission link is more likely to be LoS, which means the signal strength from the interfering UAV becomes stronger. Hence, when the misalignment for the beamforming is non-negligible, the lower deployment of UAVs is preferred for both schemes. Under the case of perfect beam alignment, from Fig. 6(b), it can be seen that the outage probability of MAPAS is smaller for the lower height of UAVs. The explanation is the same as before. However, in terms of the performance of CDAS, it seems that the outage probability for the higher altitude of UAVs can be better. This is mainly because of the fact the UE is associated with the closest UAV and their link can be either LoS or NLoS. As mentioned before, for the same projection point, the link from UAV at the lower height is much more likely to be NLoS, which reduces the desired signal strength at the UE whereby degrading the outage performance.\n\n### _Effect of UAV Density_\n\nFig. 7 plots the outage probability versus the number of antennas for UAV under different UAV densities for both imperfect and perfect alignment cases. Under the imperfect beam alignment scenario, as expected, the outage probability drops at first and then rises with the increase in the number of antennas. Moreover, since more interfering UAVs are involved in the system, the higher UAV density leads to worse outage probability performance. Besides that, Fig. 7(a) shows that the optimal number of UAV antennas increases as the UAV density rises, e.g., for MAPAS, the optimal number antennas is 4 for \\(\\lambda_{k}=0.5\\times 10^{-5}\\)\\(\\mathrm{m}^{-2}\\) while it goes to 16 for \\(\\lambda_{k}=5\\times 10^{-5}\\)\\(\\mathrm{m}^{-2}\\). The reason is as follows. When the UAV density is very sparse, the number of interfering UAVs falling into the region covered by the main-lobe beam of the UE is very small. In other words, the interference is not that severe. Then the serving UAV needs to ensure that the typical UE is covered by its main-lobe beam; hence, a larger main-lobe beamwidth (equivalently, a smaller number of antennas) is preferred. However, when the interfering UAVs are very dense, the interference becomes very severe. One way to reduce the interference is to reduce the density of interfering UAVs with main-lobe beam pointed to the typical UE. From our analysis, this can be achieved by narrowing the main-lobe beamwidth. Note that it cannot be too narrow, because this can degrade the signal strength from the serving UAV due to the beam misalignment. Hence, a relatively larger number of antennas is preferred for the case of denser UAVs. Under the perfect beam scenario, for the MAPAS, sparse UAVs lead to a lower outage probability as expected. However, this is not the case for the CDAS. Fig. 7(b) shows that the denser UAVs can even result in a better outage probability, especially when the number of antennas is large. The reason is as follows. When the UAV density is very sparse, the closest UAV (i.e., the serving UAV) can be very far away, which consequently leads to a very weak signal strength from the serving UAV. Increasing the density of UAVs somehow improves the signal strength from the serving\n\nFig. 6: Outage probability versus the number of antennas for UAV \\(N_{v}\\) under different UAV heights for both imperfect and perfect alignment cases.\n\n "]}, {"edit": ["This iteration was introduced by Nadel in [31] where he also proved that periodic points of order two or three must be Kahler-Einstein metrics. This was generalized [20, 33] to periodic points \\(\\omega\\) of any order, that is, those satisfying \\(\\rho_{\\omega}^{k}=\\lambda\\omega\\) for any \\(k\\in\\mathbb{N}\\). The iteration (3) can be regarded as a discretization of the Kahler-Ricci flow. Observe that, if the manifold \\(M\\) is compact, by Calabi's conjecture one does not need positivity assumptions to reverse the construction above and define the _inverse Ricci-Kahler iterations_\\(\\rho_{\\omega}^{-k}\\). Both of this discrete dynamical systems have been studied in the literature, see for instance [5, 13].\n\nHere we focus on the following generalized Monge-Ampere equation on a complex manifold \\(M\\)\n\n\\[\\rho_{\\omega}^{k}=\\lambda\\Omega \\tag{4}\\]\n\nwhere \\(\\Omega\\) is again a Kahler form. In particular we study equation (4) for a special class of well-behaved Kahler metrics. Our second result is the following theorem dealing with Kahler metrics induced by the flat metric (which are well-behaved by Example 4 below).\n\n**Theorem 2**.: _Let \\(M\\) be a complex manifold with two metrics \\(g\\) and \\(G\\) induced by the flat metric. If the corresponding Kahler forms \\(\\omega,\\Omega\\) satisfy \\(\\rho_{\\omega}^{k}=\\lambda\\Omega\\) for some \\(k\\geqslant 1\\) and \\(\\lambda\\in\\mathbb{R}\\), then \\((M,g)\\) is a totally geodesic submanifold of the flat ambient space._\n\nObserve that this result can be seen as a generalization of [40, Theorem 2.1]. Namely, when \\(k=1\\) and \\(g=G\\), Theorem 2 shows that a Kahler-Einstein submanifolds of \\(\\mathbb{C}^{n}\\) with the flat metric is necessarily Ricci-flat, hence a totally geodesic submanifold. It is worth pointing out that the metric \\(g\\) is assumed to be induced by a flat one for simplicity, but the hypothesis on \\(g\\) can be sensibly relaxed, cf. Remark 11 below.\n\nOur third and last result deals with Kahler-Ricci solitons (KRS). Recall that a Kahler metric \\(G\\) is a KRS if there exists a holomorphic vector field \\(X\\) (the solitonic vector field) such that \\(\\operatorname{Ric}(G)=\\mu G+L_{X}G\\) where \\(\\operatorname{Ric}(G)\\) denotes the Ricci tensor of \\(G\\) and \\(L_{X}\\) the Lie derivative in the direction of \\(X\\). Kahler-Ricci solitons are important generalizations of Kahler-Einstein metrics which arise in the study of the Kahler-Ricci flow. Our next result show that KRS cannot arise as Kahler-Ricci iterations of metrics induced by complex space forms.\n\n**Theorem 3**.: _Let \\(M\\) be a complex manifold with two real analytic Kahler metrics \\(g\\) and \\(G\\). Assume that \\(g\\) is induced by a complex space form and \\(G\\) is a KRS. If the corresponding Kahler forms \\(\\omega,\\Omega\\) satisfy \\(\\rho_{\\omega}^{k}=\\lambda\\Omega\\) for some \\(\\lambda\\neq 0\\) and \\(k\\geqslant 0\\), then \\(G\\) is trivial, i.e. Kahler-Einstein._\n\nObserve that, when \\(\\lambda=0\\), one cannot draw any conclusion. Take for instance \\(g\\) to be the flat metric on \\(\\mathbb{C}\\) and \\(G\\) to be Hamilton's cigar KRS [17]. It is worth mentioning that we do not know of any Kah "], "nougat": ["This iteration was introduced by Nadel in [31] where he also proved that periodic points of order two or three must be K \u0308ahler-Einstein metrics. This was generalized [20, 33] to periodic points \\(\\omega\\) of any order, that is, those satisfying \\(\\rho_{\\omega}^{k}=\\lambda\\omega\\) for any \\(k\\in\\mathbb{N}\\). The iteration (3) can be regarded as a discretization of the K \u0308ahler\u2013Ricci flow. Observe that, if the manifold \\(M\\) is compact, by Calabi\u2019s conjecture one does not need positivity assumptions to reverse the construction above and define the _inverse Ricci-K\u00e4hler iterations_\\(\\rho_{\\omega}^{-k}\\). Both of this discrete dynamical systems have been studied in the literature, see for instance [5, 13].\n\nHere we focus on the following generalized Monge-Ampere equation on a complex manifold \\(M\\)\n\n\\[\\rho_{\\omega}^{k}=\\lambda\\Omega \\tag{4}\\]\n\nwhere \\(\\Omega\\) is again a Kahler form. In particular we study equation (4) for a special class of well-behaved Kahler metrics. Our second result is the following theorem dealing with Kahler metrics induced by the flat metric (which are well-behaved by Example 4 below).\n\n**Theorem 2**.: _Let \\(M\\) be a complex manifold with two metrics \\(g\\) and \\(G\\) induced by the flat metric. If the corresponding Kahler forms \\(\\omega,\\Omega\\) satisfy \\(\\rho_{\\omega}^{k}=\\lambda\\Omega\\) for some \\(k\\geqslant 1\\) and \\(\\lambda\\in\\mathbb{R}\\), then \\((M,g)\\) is a totally geodesic submanifold of the flat ambient space._\n\nObserve that this result can be seen as a generalization of [40, Theorem 2.1]. Namely, when \\(k=1\\) and \\(g=G\\), Theorem 2 shows that a Kahler-Einstein submanifolds of \\(\\mathbb{C}^{n}\\) with the flat metric is necessarily Ricci-flat, hence a totally geodesic submanifold. It is worth pointing out that the metric \\(g\\) is assumed to be induced by a flat one for simplicity, but the hypothesis on \\(g\\) can be sensibly relaxed, cf. Remark 11 below.\n\nOur third and last result deals with Kahler-Ricci solitons (KRS). Recall that a Kahler metric \\(G\\) is a KRS if there exists a holomorphic vector field \\(X\\) (the solitonic vector field) such that \\(\\operatorname{Ric}(G)=\\mu G+L_{X}G\\) where \\(\\operatorname{Ric}(G)\\) denotes the Ricci tensor of \\(G\\) and \\(L_{X}\\) the Lie derivative in the direction of \\(X\\). Kahler-Ricci solitons are important generalizations of Kahler-Einstein metrics which arise in the study of the Kahler-Ricci flow. Our next result show that KRS cannot arise as Kahler-Ricci iterations of metrics induced by complex space forms.\n\n**Theorem 3**.: _Let \\(M\\) be a complex manifold with two real analytic Kahler metrics \\(g\\) and \\(G\\). Assume that \\(g\\) is induced by a complex space form and \\(G\\) is a KRS. If the corresponding Kahler forms \\(\\omega,\\Omega\\) satisfy \\(\\rho_{\\omega}^{k}=\\lambda\\Omega\\) for some \\(\\lambda\\neq 0\\) and \\(k\\geqslant 0\\), then \\(G\\) is trivial, i.e. Kahler-Einstein._\n\nObserve that, when \\(\\lambda=0\\), one cannot draw any conclusion. Take for instance \\(g\\) to be the flat metric on \\(\\mathbb{C}\\) and \\(G\\) to be Hamilton's cigar KRS [17]. It is worth mentioning that we do not know "]}, {"edit": ["Finally, let us simplify Term 1:\n\n\\[\\text{Term1} =\\exp\\left(\\{\\}-i\\alpha_{1}(s)E\\right\\}\\exp\\left(\\{\\}-i\\alpha_{2}(s )P\\right\\}\\exp\\left(\\{\\}-i\\alpha_{3}(s)Q\\right\\}\\bigg{(}\\exp\\left(\\{\\}-i\\alpha_{ 4}(s)H\\right)\\alpha_{4}^{\\prime}(s)H\\bigg{)}\\] \\[=\\alpha_{4}^{\\prime}(s)\\exp\\left(\\{\\}-i\\alpha_{1}(s)E\\right\\}\\exp \\left(\\{\\}-i\\alpha_{2}(s)P\\right\\}\\bigg{(}\\exp\\left(\\{\\}-i\\alpha_{3}(s)Q\\right) H\\bigg{)}\\exp\\left(\\{\\}-i\\alpha_{4}(s)H\\right)\\] \\[=\\alpha_{4}^{\\prime}(s)e^{-i\\alpha_{1}(s)E}e^{-i\\alpha_{2}(s)P} \\bigg{(}\\exp\\left(\\{\\}-i\\alpha_{3}(s)Q\\right)H\\bigg{)}\\underbrace{e^{i\\alpha_{ 3}(s)Q}e^{-i\\alpha_{3}(s)Q}}_{\\text{Identity operator inserted}}\\] \\[\\quad\\exp\\left(\\{\\}-i\\alpha_{4}(s)H\\right\\}\\] \\[=\\alpha_{4}^{\\prime}(s)\\exp\\left(\\{\\}-i\\alpha_{1}(s)E\\right\\}\\exp \\left(\\{\\}-i\\alpha_{2}(s)P\\right)\\bigg{(}\\underbrace{\\exp\\left(\\{\\}-i\\alpha_{ 3}(s)Q\\right)H\\exp\\left(\\{\\}\\,i\\alpha_{3}(s)Q\\right\\}\\bigg{)}_{\\text{Apply Baker-Campbell Hausdorff formula}}\\bigg{)}\\] \\[\\quad\\exp\\left(\\{\\}-i\\alpha_{3}(s)Q\\right)\\exp\\left(\\{\\}-i\\alpha_{ 4}(s)H\\right\\}\\] \\[=\\alpha_{4}^{\\prime}(s)\\exp\\left(\\{\\}-i\\alpha_{1}(s)E\\right\\}\\exp \\left(\\{\\}-i\\alpha_{2}(s)P\\right)\\bigg{(}H+\\alpha_{3}(s)P+\\frac{\\alpha_{3}(s) ^{2}}{2}E\\bigg{)}\\] \\[\\quad\\quad\\exp\\left(\\{\\}-i\\alpha_{3}(s)Q\\right\\}\\exp\\left(\\{\\}-i \\alpha_{4}(s)H\\right\\}\\] \\[=\\alpha_{4}^{\\prime}(s)\\exp\\left(\\{\\}-i\\alpha_{1}(s)E\\right\\} \\exp\\left(\\{\\}-i\\alpha_{2}(s)P\\right)H\\exp\\left(\\{\\}-i\\alpha_{3}(s)Q\\right)\\exp \\left(\\{\\}-i\\alpha_{4}(s)H\\right\\}\\] \\[\\quad\\quad+\\alpha_{4}^{\\prime}(s)\\exp\\left(\\{\\}-i\\alpha_{1}(s)E \\right\\}\\exp\\left(\\{\\}-i\\alpha_{2}(s)P "], "nougat": ["Finally, let us simplify Term 1:\n\n\\[\\text{Term1} =\\exp\\left(\\{\\}-i\\alpha_{1}(s)E\\right\\}\\exp\\left(\\{\\}-i\\alpha_{2}(s )P\\right\\}\\exp\\left(\\{\\}-i\\alpha_{3}(s)Q\\right\\}\\bigg{(}\\exp\\left(\\{\\}-i\\alpha_{ 4}(s)H\\right)\\alpha_{4}^{\\prime}(s)H\\bigg{)}\\] \\[=\\alpha_{4}^{\\prime}(s)\\exp\\left(\\{\\}-i\\alpha_{1}(s)E\\right\\}\\exp \\left(\\{\\}-i\\alpha_{2}(s)P\\right\\}\\bigg{(}\\exp\\left(\\{\\}-i\\alpha_{3}(s)Q\\right) H\\bigg{)}\\exp\\left(\\{\\}-i\\alpha_{4}(s)H\\right)\\] \\[=\\alpha_{4}^{\\prime}(s)e^{-i\\alpha_{1}(s)E}e^{-i\\alpha_{2}(s)P} \\bigg{(}\\exp\\left(\\{\\}-i\\alpha_{3}(s)Q\\right)H\\bigg{)}\\underbrace{e^{i\\alpha_{ 3}(s)Q}e^{-i\\alpha_{3}(s)Q}}_{\\text{Identity operator inserted}}\\] \\[\\quad\\exp\\left(\\{\\}-i\\alpha_{4}(s)H\\right\\}\\] \\[=\\alpha_{4}^{\\prime}(s)\\exp\\left(\\{\\}-i\\alpha_{1}(s)E\\right\\}\\exp \\left(\\{\\}-i\\alpha_{2}(s)P\\right)\\bigg{(}\\underbrace{\\exp\\left(\\{\\}-i\\alpha_{ 3}(s)Q\\right)H\\exp\\left(\\{\\}\\,i\\alpha_{3}(s)Q\\right\\}\\bigg{)}_{\\text{Apply Baker-Campbell Hausdorff formula}}\\bigg{)}\\] \\[\\quad\\exp\\left(\\{\\}-i\\alpha_{3}(s)Q\\right)\\exp\\left(\\{\\}-i\\alpha_{ 4}(s)H\\right\\}\\] \\[=\\alpha_{4}^{\\prime}(s)\\exp\\left(\\{\\}-i\\alpha_{1}(s)E\\right\\}\\exp \\left(\\{\\}-i\\alpha_{2}(s)P\\right)\\bigg{(}H+\\alpha_{3}(s)P+\\frac{\\alpha_{3}(s) ^{2}}{2}E\\bigg{)}\\] \\[\\quad\\quad\\exp\\left(\\{\\}-i\\alpha_{3}(s)Q\\right\\}\\exp\\left(\\{\\}-i \\alpha_{4}(s)H\\right\\}\\] \\[=\\alpha_{4}^{\\prime}(s)\\exp\\left(\\{\\}-i\\alpha_{1}(s)E\\right\\} \\exp\\left(\\{\\}-i\\alpha_{2}(s)P\\right)H\\exp\\left(\\{\\}-i\\alpha_{3}(s)Q\\right)\\exp \\left(\\{\\}-i\\alpha_{4}(s)H\\right\\}\\] \\[\\quad\\quad+\\alpha_{4}^{\\prime}(s)\\exp\\left(\\{\\}-i\\alpha_{1}(s)E \\right\\}\\exp\\left(\\{\\}-i\\alpha_{2}(s)P "]}, {"edit": ["The first integral of (5.21) can be estimated as follows: for any \\(\\epsilon>0\\),\n\n\\[\\int_{\\frac{1}{4}}^{\\frac{1}{2}}\\int_{|y^{\\prime}|\\leq 1} \\frac{1}{(t-s)^{\\frac{1}{2}}(|x-y^{\\prime}|^{2}+(t-s))^{\\frac{n-1 }{2}}}\\left|\\partial_{s}g_{k}^{\\mathcal{T}}(s)\\right|dy^{\\prime}ds\\] \\[\\lesssim\\int_{\\frac{1}{4}}^{\\frac{1}{2}}\\frac{1}{(t-s)^{\\frac{1}{ 2}}}\\frac{\\log\\left(2+\\frac{1}{\\sqrt{t-s}}\\right)}{(|x|+1+\\sqrt{t-s})^{n-1}}ds\\] \\[\\lesssim\\frac{1}{\\langle x\\rangle^{n-1}}\\int_{\\sqrt{t-\\frac{1}{2 }}}^{\\sqrt{t-\\frac{1}{2}}}\\log\\left(2+\\frac{1}{u}\\right)du\\lesssim\\frac{1}{ \\langle x\\rangle^{n-1}}\\int_{\\sqrt{t-\\frac{1}{2}}}^{\\sqrt{t-\\frac{1}{2}}}\\frac {1}{v^{\\epsilon}}dv\\] \\[\\lesssim\\frac{(t-\\frac{1}{2})^{\\frac{1-\\epsilon}{2}}}{\\langle x \\rangle^{n-1}}.\\]\n\nThe second integral of (5.21) can be estimated as follows:\n\n\\[\\int_{\\frac{1}{2}}^{1}\\int_{|y^{\\prime}|\\leq 1} \\frac{1}{(t-s)^{\\frac{1}{2}}(|x-y^{\\prime}|^{2}+(t-s))^{\\frac{n-1 }{2}}}\\left|\\partial_{s}g_{k}^{\\mathcal{T}}(s)\\right|dy^{\\prime}ds\\] \\[\\lesssim\\int_{\\frac{1}{2}}^{1}\\int_{|y^{\\prime}|\\leq 1}\\frac{1}{(t-s)^{ \\frac{1}{2}}(|x-y^{\\prime}|^{2}+(t-s))^{\\frac{n-1}{2}}(1-s)^{1-a}}dy^{\\prime}ds\\] \\[\\lesssim\\int_{\\frac{1}{2}}^{1}\\frac{1}{(t-s)^{\\frac{1}{2}}}\\frac {\\log\\left(2+\\frac{1}{\\sqrt{t-s}}\\right)}{(|x|+\\sqrt{t-s}+1)^{n-1}}\\frac{1}{(1 -s)^{1-a}}ds\\mathbbm{1}_{|x|<2}+\\int_{\\frac{1}{2}}^{1}\\frac{1}{(t-s)^{\\frac{1}{ 2}}(1-s)^{1-a}}ds\\frac{1}{|x|^{n-1}}\\mathbbm{1}_{|x|>2}\\] \\[\\lesssim\\int_{\\sqrt{t-1}}^{\\sqrt{t-\\frac{1}{2}}}\\frac{\\log\\left(2 +\\frac{1}{u}\n\n "], "nougat": ["The first integral of (5.21) can be estimated as follows: for any \\(\\epsilon>0\\),\n\n\\[\\int_{\\frac{1}{4}}^{\\frac{1}{2}}\\int_{|y^{\\prime}|\\leq 1} \\frac{1}{(t-s)^{\\frac{1}{2}}(|x-y^{\\prime}|^{2}+(t-s))^{\\frac{n-1 }{2}}}\\left|\\partial_{s}g_{k}^{\\mathcal{T}}(s)\\right|dy^{\\prime}ds\\] \\[\\lesssim\\int_{\\frac{1}{4}}^{\\frac{1}{2}}\\frac{1}{(t-s)^{\\frac{1}{ 2}}}\\frac{\\log\\left(2+\\frac{1}{\\sqrt{t-s}}\\right)}{(|x|+1+\\sqrt{t-s})^{n-1}}ds\\] \\[\\lesssim\\frac{1}{\\langle x\\rangle^{n-1}}\\int_{\\sqrt{t-\\frac{1}{2 }}}^{\\sqrt{t-\\frac{1}{2}}}\\log\\left(2+\\frac{1}{u}\\right)du\\lesssim\\frac{1}{ \\langle x\\rangle^{n-1}}\\int_{\\sqrt{t-\\frac{1}{2}}}^{\\sqrt{t-\\frac{1}{2}}}\\frac {1}{v^{\\epsilon}}dv\\] \\[\\lesssim\\frac{(t-\\frac{1}{2})^{\\frac{1-\\epsilon}{2}}}{\\langle x \\rangle^{n-1}}.\\]\n\nThe second integral of (5.21) can be estimated as follows:\n\n\\[\\int_{\\frac{1}{2}}^{1}\\int_{|y^{\\prime}|\\leq 1} \\frac{1}{(t-s)^{\\frac{1}{2}}(|x-y^{\\prime}|^{2}+(t-s))^{\\frac{n-1 }{2}}}\\left|\\partial_{s}g_{k}^{\\mathcal{T}}(s)\\right|dy^{\\prime}ds\\] \\[\\lesssim\\int_{\\frac{1}{2}}^{1}\\int_{|y^{\\prime}|\\leq 1}\\frac{1}{(t-s)^{ \\frac{1}{2}}(|x-y^{\\prime}|^{2}+(t-s))^{\\frac{n-1}{2}}(1-s)^{1-a}}dy^{\\prime}ds\\] \\[\\lesssim\\int_{\\frac{1}{2}}^{1}\\frac{1}{(t-s)^{\\frac{1}{2}}}\\frac {\\log\\left(2+\\frac{1}{\\sqrt{t-s}}\\right)}{(|x|+\\sqrt{t-s}+1)^{n-1}}\\frac{1}{(1 -s)^{1-a}}ds\\mathbbm{1}_{|x|<2}+\\int_{\\frac{1}{2}}^{1}\\frac{1}{(t-s)^{\\frac{1}{ 2}}(1-s)^{1-a}}ds\\frac{1}{|x|^{n-1}}\\mathbbm{1}_{|x|>2}\\] \\[\\lesssim\\int_{\\sqrt{t-1}}^{\\sqrt{t-\\frac{1}{2}}}\\frac{\\log\\left(2 +\\frac{1}{u}\n\n "]}, {"edit": ["corresponding statistical estimators. In this case, we also use the LP/B03/ExD model from the spec-\\(z\\) as reference to check the impact of the GaZNet redshift in terms of accuracy and scatter. Basically, the results show that, for the same correlations seen in Fig. 3, the relative bias of the different configurations is not worsened, meaning that the accuracy of the mass estimates is not affected by the use of the morphoto-\\(z\\). This is eventually a consequence of the good accuracy of these latter as seen in Fig. 2. On the other hand, we register an evident increase of the NMAD as a consequence of the morphoto-\\(z\\) intrinsic statistical errors and outlier fractions, which is also mirrored by the scatter of the residual, at the bottom of the 1-to-1 relations, which is now of the order of 0.23 dex, for \\(\\log M_{*}/M_{\\odot}>9\\), and 0.49 dex for \\(\\log M_{*}/M_{\\odot}<9\\), on average. These large scatter at low stellar masses are mainly caused by the trend we see that below \\(\\log M_{*}/M_{\\odot}=8.5\\), where stellar masses are systematically overestimated compared to those obtained with the spec-\\(z\\). This is not an effect that comes from the particular set-up of the fitting procedure, as shown by the comparison of the LP/B03/ExD/morphoto-\\(z\\) against the same set-up with spec-\\(z\\) (bottom/left plot in Fig. 4). Even in this latter case, we see that below \\(\\log M_{*}/M_{\\odot}=8.5\\) the positive bias is similar to the ones of all other configurations. We track the motivation of this systematics to some bias of the GaZNet redshifts for a group of objects at very low redshifts (\\(z<0.05\\) see Fig. 2), which turn-out to have also low masses. This can be due to some residual contamination from stars, not picked in the spectra classification, or just a failure of the GaZNet predictions at very low-\\(z\\), which clearly impact the mass predictions. We will come back to this on Sect. 4. However, still looking at the LP/B03/ExD/morphoto-\\(z\\) vs. spec-\\(z\\), above \\(\\log M_{*}/M_{\\odot}=8.5\\), the bias is almost absent and the only relevant effect is the GaZNet redshift scatter that, from the NMAD, is quantified in 0.09. This is confirmed by noticing that the general increase of the NMAD from the spectroscopic sample to the morphoto-metric sample, in Table 2, is compatible with the sum in quadrature of the NMAD of the former with 0.09 coming from the latter, consistently with some pseudo-Gaussian distributions. This is consistent with a log-normal distribution of the uncertainties of the stellar masses, which are confirmed by the outlier fractions that are all of the order of 5-6% above \\(2\\sigma\\) of the\n\nFigure 4: Stellar mass estimates as for Fig. 3 but using the GaZNet morphoto-metric redshifts.\n\n "], "nougat": ["corresponding statistical estimators. In this case, we also use the LP / B03 / ExD model from the spec-\\(z\\) as reference to check the impact of the GaZNet redshift in terms of accuracy and scatter. Basically, the results show that, for the same correlations seen in Fig. 3 , the relative bias of the di ff erent configurations is not worsened, meaning that the accuracy of the mass estimates is not a ff ected by the use of the morphoto-\\(z\\). This is eventually a consequence of the good accuracy of these latter as seen in Fig. 2 . On the other hand, we register an evident increase of the NMAD as a consequence of the morphoto-\\(z\\) intrinsic statistical errors and outlier fractions, which is also mirrored by the scatter of the residual, at the bottom of the 1-to-1 relations, which is now of the order of 0.23 dex, for \\(\\log M_{*}/M_{\\odot}>9\\), and 0.49 dex for \\(\\log M_{*}/M_{\\odot}<9\\), on average. These large scatter at low stellar masses are mainly caused by the trend we see that below \\(\\log M_{*}/M_{\\odot}=8.5\\), where stellar masses are systematically overestimated compared to those obtained with the spec-\\(z\\). This is not an e ff ect that comes from the particular set-up of the fitting procedure, as shown by the comparison of the LP / B03 / ExD / morphoto-\\(z\\) against the same set-up with spec-\\(z\\) (bottom / left plot in Fig. 4 ). Even in this latter case, we see that below \\(\\log M_{*}/M_{\\odot}=8.5\\) the positive bias is similar to the ones of all other configurations. We track the motivation of this systematics to some bias of the GaZNet redshifts for a group of objects at very low redshifts (\\(z<0.05\\) see Fig. 2 ), which turn-out to have also low masses. This can be due to some residual contamination from stars, not picked in the spectra classification, or just a failure of the GaZNet predictions at very low-\\(z\\), which clearly impact the mass predictions. We will come back to this on Sect. 4 . However, still looking at the LP / B03 / ExD morphoto-\\(z\\) vs. spec-\\(z\\), above \\(\\log M_{*}/M_{\\odot}=8.5\\), the bias is almost absent and the only relevant ef ff ect is the GaZNet redshift scatter that, from the NMAD, is quantified in 0.09. This is confirmed by noticing that the general increase of the NMAD from the spectroscopic sample to the morphoto-metric sample, in Table 2 , is compatible with the sum in quadrature of the NMAD of the former with 0.09 coming from the latter, consistently with some pseudo-Gaussian distributions. This is consistent with a log-normal distribution of the uncertainties of the stellar masses, which are confirmed by the outlier fractions that are all of the order of 5-6% above 2\\(\\sigma\\) of the\n\nFigure 4: Stellar mass estimates as for Fig. 3 but using the GaZNet morphoto-metric redshifts.\n\n "]}, {"edit": ["\n\n# Improving Generalization in Visual Reinforcement Learning via\n\nConflict-aware Gradient Agreement Augmentation\n\nSiao Liu   Zhaoyu Chen   Yang Liu   Yuzheng Wang   Dingkang Yang   Zhile Zhao\n\nZiqing Zhou   Xie Yi   Wei Li   Wenqiang Zhang   Zhongxue Gan\n\nAcademy for Engineering & Technology, Fudan University\n\n{saliu20, zhaoyuchen20, yang_liu20, yzwang20, dkyang20, fd_liwei, wqzhang, ganzhongxue}@fudan.edu.cn\n\n{zhilezhao21, ziqingzhou21, yixie22}@m.fudan.edu.cn\n\n###### Abstract\n\nLearning a policy with great generalization to unseen environments remains challenging but critical in visual reinforcement learning. Despite the success of augmentation combination in the supervised learning generalization, naively applying it to visual RL algorithms may damage the training efficiency, suffering from serve performance degradation. In this paper, we first conduct qualitative analysis and illuminate the main causes: (i) high-variance gradient magnitudes and (ii) gradient conflicts existed in various augmentation methods. To alleviate these issues, we propose a general policy gradient optimization framework, named Conflict-aware Gradient Agreement Augmentation (CG2A), and better integrate augmentation combination into visual RL algorithms to address the generalization bias. In particular, CG2A develops a Gradient Agreement Solver to adaptively balance the varying gradient magnitudes, and introduces a Soft Gradient Surgery strategy to alleviate the gradient conflicts. Extensive experiments demonstrate that CG2A significantly improves the generalization performance and sample efficiency of visual RL algorithms.\n\n## 1 Introduction\n\nWith the development of deep learning in various tasks [28, 26, 25, 27, 7, 6, 38, 40, 39, 24], visual Reinforcement Learning (RL) has achieved impressive success in various fields such as robotic control [11], autonomous driving [17], and game-playing [35]. Previous works usually formulate it as a Partially Observable Markov Decision Process (POMDP) [33], and the agent receives high-dimensional image observations as inputs. As depicted in [15, 14], visual RL generalization refers to the ability of a pretrained RL agent to perform well in unseen environments. Due to the dynamic nature of the real world, even minor perturbations in the environment can result in significant semantic shifts in the visual observations, which makes visual RL generalization challenging.\n\nTo improve generalization performance, data augmentation [29] is a widely adopted technique in reinforcement learning. Numerous studies [22, 13] utilize data augmentation methods to generate synthetic data and diversify the training environments, yielding considerable performance improvements. However, recent methods [14, 3, 44] mostly select a single augmentation technique to improve the generalization capability, resulting in a poor performance in the environments with observations varying far from the augmented images. For instance, ColorJitter [23] is the preferred choice for addressing color variations, but agents trained with such augmentation still hard to cope with intricate texture patterns. In other words, the generalization ability heavily relies on the selection of specific data augmentation technique, which is so-called generalization bias.\n\nCompared to single data augmentation, Augmentation Combination (AC) [16] integrates multiple data augmentation methods to enhance the diversity of augmentations and alleviate the generalization bias, which is a more promising pre-processing solution. Unfortunately, there is a dilemma in incorporating AC into visual RL. Although data augmentation combination can effectively improve generalization capability in the supervised visual tasks, RL algorithms are quite sensitive to excessive variations, resulting in performance degradation and training sample inefficiency. Therefore, it is necessary to rethink why visual RL algorithms cannot benefit from AC as much as supervised learning.\n\nFrom the perspective of gradient optimization, we conduct numerous qualitative analysis to illustrate the causes of performance degradation and training collapse that occur when employing augmentation combinations during train"], "nougat": ["\n\n# Improving Generalization in Visual Reinforcement Learning via\n\nConflict-aware Gradient Agreement Augmentation\n\nSiao Liu   Zhaoyu Chen   Yang Liu   Yuzheng Wang   Dingkang Yang   Zhile Zhao\n\nZiqing Zhou   Xie Yi   Wei Li   Wenqiang Zhang   Zhongxue Gan\n\nAcademy for Engineering & Technology, Fudan University\n\n{saliu20, zhaoyuchen20, yang liu20, yzwang20, dkyang20, fd liwei, wqzhang, ganzhongxue}@fudan.edu.cn\n\n{zhilezhao21, ziqingzhou21, yixie22}@m.fudan.edu.cn\n\n###### Abstract\n\nLearning a policy with great generalization to unseen environments remains challenging but critical in visual re-inforcement learning. Despite the success of augmenta- tion combination in the supervised learning generalization, naively applying it to visual RL algorithms may damage the training efficiency, suffering from serve performance degradation. In this paper, we first conduct qualitative analysis and illuminate the main causes: (i) high-variance gradient magnitudes and (ii) gradient conflicts existed in various augmentation methods. To alleviate these issues, we propose a general policy gradient optimization frame- work, named Conflict-aware Gradient Agreement Augmen-tation (CG2A), and better integrate augmentation combina- tion into visual RL algorithms to address the generalization bias. In particular, CG2A develops a Gradient Agreement Solver to adaptively balance the varying gradient magni- tudes, and introduces a Soft Gradient Surgery strategy to al- leviate the gradient conflicts. Extensive experiments demon-strate that CG2A significantly improves the generalization performance and sample efficiency of visual RL algorithms.\n\n## 1 Introduction\n\nWith the development of deep learning in various tasks [28, 26, 25, 27, 7, 6, 38, 40, 39, 24], visual Reinforcement Learning (RL) has achieved impressive success in various fields such as robotic control [11], autonomous driving [17], and game-playing [35]. Previous works usually formulate it as a Partially Observable Markov Decision Process (POMDP) [33], and the agent receives highdimensional image observations as inputs. As depicted in [15, 14], visual RL generalization refers to the ability of a pretrained RL agent to perform well in unseen environments. Due to the dynamic nature of the real world, even minor perturbations in the environment can result in significant semantic shifts in the visual observations, which makes visual RL generalization challenging.\n\nTo improve generalization performance, data augmentation [29] is a widely adopted technique in reinforcement learning. Numerous studies [22, 13] utilize data augmentation methods to generate synthetic data and diversify the training environments, yielding considerable performance improvements. However, recent methods [14, 3, 44] mostly select a single augmentation technique to improve the generalization capability, resulting in a poor performance in the environments with observations varying far from the augmented images. For instance, ColorJitter [23] is the preferred choice for addressing color variations, but agents trained with such augmentation still hard to cope with intricate texture patterns. In other words, the generalization ability heavily relies on the selection of specific data augmentation technique, which is so-called generalization bias.\n\nCompared to single data augmentation, Augmentation Combination (AC) [16] integrates multiple data augmentation methods to enhance the diversity of augmentations and alleviate the generalization bias, which is a more promising pre-processing solution. Unfortunately, there is a dilemma in incorporating AC into visual RL. Although data augmentation combination can effectively improve generalization capability in the supervised visual tasks, RL algorithms are quite sensitive to excessive variations, resulting in performance degradation and training sample inefficiency. Therefore, it is necessary to rethink why visual RL algorithms cannot benefit from AC as much as supervised learning.\n\nFrom the perspective of gradient optimization, we conduct numerous qualitative analysis to illustrate the causes of performance degradation and training collapse that occur when employing augmentation combinations during train-"]}, {"edit": ["\n\n**Lemma 2.9**.: _Let \\(x=\\Delta^{k}\\) for some nonzero integer \\(k\\). We have \\(\\operatorname{SSS}(\\Delta^{k})=\\{\\Delta^{k}\\}\\). The centralizer of \\(\\Delta^{k}\\) in \\(G(m,\\ell)\\) is either \\(G(m,\\ell)\\) if \\(k\\ell\\) is a multiple of \\(m\\), or cyclic and generated by \\(\\Delta\\) otherwise._\n\nProof.: We have that \\(y\\in G(m,\\ell)\\) lies in \\(\\operatorname{SSS}(\\Delta^{k})\\) only if \\(\\inf(y)=k=\\sup(y)\\). The only element satisfying this is \\(\\Delta^{k}\\), which is conjugate to itself. Thus we have \\(\\operatorname{SSS}(\\Delta^{k})=\\{\\Delta^{k}\\}\\). Now, let \\(s(j,q)\\) be a simple element in \\(M(m,\\ell)\\). Since both \\(1\\) and \\(\\Delta\\) conjugate \\(\\Delta^{k}\\) to itself, we can assume that \\(q\\in\\llbracket 1,m-1\\rrbracket\\). We have\n\n\\[s(j,q)^{-1}\\Delta^{k}s(j,q) =\\overline{s(j,q)}\\Delta^{k-1}s(j,q)\\] \\[=s(j+q,\\ell-q)\\Delta^{k-1}s(j,q)\\] \\[=\\Delta^{k-1}s(j+q+(k-1)\\ell,\\ell-q)s(j,q).\\]\n\nIn order for this element to lie in \\(\\operatorname{SSS}(\\Delta^{k})\\), the word \\(s(j+q+(k-1)\\ell,\\ell-q)s(j,q)\\) must not be greedy. This is equivalent to \\(j+k\\ell\\equiv j[m]\\). If \\(k\\ell\\) is a multiple of \\(m\\), this is true for all \\(j\\in\\llbracket 0,m-1\\rrbracket\\), and we obtain \\(s(j,q)^{-1}\\Delta^{k}s(j,q)=\\Delta^{k}\\): the arrows from \\(\\Delta^{k}\\) to itself in \\(\\operatorname{CG}(\\Delta^{k})\\) are given by all the simple elements. Otherwise, \\(j+k\\ell\\equiv j[m]\\) is never true for \\(j\\in\\llbracket 1,m-1\\rrbracket\\) and the only arrows from \\(\\Delta^{k}\\) to itself in \\(\\operatorname{CG}(\\Delta^{k})\\) are given by \\(1\\) and \\(\\Delta\\). \n\n**Lemma 2.10**.: _Let \\(x=\\Delta^{k}s(i,p)\\) be a periodic element in \\(M(m,\\ell)\\) with \\(p\\in\\llbracket 1,m-1\\rrbracket\\). We have \\(\\operatorname{SSS}(x)=\\{\\Delta^{k}s(n,p)\\ |\\ n\\in\\llbracket 0,m-1\\rrbracket\\}\\). The centralizer of \\(\\Delta^{k}s(0,p)\\) in \\(G(m,\\ell)\\) is cyclic and generated by \\(s(p,m)\\)._\n\nProof.: The assumption that \\(x\\) is periodic is equivalent to \\(k\\ell+p\\equiv 0[m]\\) by Lemma 2.8. Let \\(s(j,q)\\) be"], "nougat": ["\n\n**Lemma 2.9**.: _Let \\(x=\\Delta^{k}\\) for some nonzero integer \\(k\\). We have \\(\\operatorname{SSS}(\\Delta^{k})=\\{\\Delta^{k}\\}\\). The centralizer of \\(\\Delta^{k}\\) in \\(G(m,\\ell)\\) is either \\(G(m,\\ell)\\) if \\(k\\ell\\) is a multiple of \\(m\\), or cyclic and generated by \\(\\Delta\\) otherwise._\n\nProof.: We have that \\(y\\in G(m,\\ell)\\) lies in \\(\\operatorname{SSS}(\\Delta^{k})\\) only if \\(\\inf(y)=k=\\sup(y)\\). The only element satisfying this is \\(\\Delta^{k}\\), which is conjugate to itself. Thus we have \\(\\operatorname{SSS}(\\Delta^{k})=\\{\\Delta^{k}\\}\\). Now, let \\(s(j,q)\\) be a simple element in \\(M(m,\\ell)\\). Since both \\(1\\) and \\(\\Delta\\) conjugate \\(\\Delta^{k}\\) to itself, we can assume that \\(q\\in\\llbracket 1,m-1\\rrbracket\\). We have\n\n\\[s(j,q)^{-1}\\Delta^{k}s(j,q) =\\overline{s(j,q)}\\Delta^{k-1}s(j,q)\\] \\[=s(j+q,\\ell-q)\\Delta^{k-1}s(j,q)\\] \\[=\\Delta^{k-1}s(j+q+(k-1)\\ell,\\ell-q)s(j,q).\\]\n\nIn order for this element to lie in \\(\\operatorname{SSS}(\\Delta^{k})\\), the word \\(s(j+q+(k-1)\\ell,\\ell-q)s(j,q)\\) must not be greedy. This is equivalent to \\(j+k\\ell\\equiv j[m]\\). If \\(k\\ell\\) is a multiple of \\(m\\), this is true for all \\(j\\in\\llbracket 0,m-1\\rrbracket\\), and we obtain \\(s(j,q)^{-1}\\Delta^{k}s(j,q)=\\Delta^{k}\\): the arrows from \\(\\Delta^{k}\\) to itself in \\(\\operatorname{CG}(\\Delta^{k})\\) are given by all the simple elements. Otherwise, \\(j+k\\ell\\equiv j[m]\\) is never true for \\(j\\in\\llbracket 1,m-1\\rrbracket\\) and the only arrows from \\(\\Delta^{k}\\) to itself in \\(\\operatorname{CG}(\\Delta^{k})\\) are given by \\(1\\) and \\(\\Delta\\). \n\n**Lemma 2.10**.: _Let \\(x=\\Delta^{k}s(i,p)\\) be a periodic element in \\(M(m,\\ell)\\) with \\(p\\in\\llbracket 1,m-1\\rrbracket\\). We have \\(\\operatorname{SSS}(x)=\\{\\Delta^{k}s(n,p)\\ |\\ n\\in\\llbracket 0,m-1\\rrbracket\\}\\). The centralizer of \\(\\Delta^{k}s(0,p)\\) in \\(G(m,\\ell)\\) is cyclic and generated by \\(s(p,m)\\)._\n\nProof.: The assumption that \\(x\\) is periodic is equivalent to \\(k\\ell+p\\equiv 0[m]\\) by Lemma 2.8. Let \\(s(j,q)\\"]}, {"edit": ["ture during a significant decrease in disk contribution, as well as the study of timing characteristics in that state.\n\n## 6 Acknowledgements\n\nWe would like to thank the anonymous referee for their valuable suggestions, which improved the quality of this work. This work has utilized data from _AstroSat_ mission which is archived at Indian Space Science Data Centre (ISSDC). We are grateful to the SXT and LAXPC POC teams for providing the data and requisite softwares to perform data analysis. NH acknowledges the financial support provided by Department of Science and Technology (DST) under the INSPIRE fellowship scheme. AG, RM and SS acknowledges the financial support provided by Department of Space, Govt of India (No.DS_2B-13012(2)/2/2022-Sec.2).\n\n## 7 Data Availability\n\nThe data used in the publication is publicly available for download at [https://astrobrowse.issdc.gov.in/astro_archive/archive/Home.jsp](https://astrobrowse.issdc.gov.in/astro_archive/archive/Home.jsp) using the observation IDs mentioned in Tab 1.\n\n## References\n\n* Agrawal et al. (2017) Agrawal P., et al., 2017, Journal of Astrophysics and Astronomy, 38, 30\n* Antia et al. (2017) Antia H., et al., 2017, The Astrophysical Journal Supplement Series, 231, 10\n* Asplund et al. (2009) Asplund M., et al., 2009, Annual review of astronomy and astrophysics, 47, 481\n* Belloni et al. (2011) Belloni T. M., et al., 2011, arXiv preprint arXiv:1109.3388\n* Bhargava et al. (2022) Bhargava Y., et al., 2022, Monthly Notices of the Royal Astronomical Society, 512, 6067\n* Capitanio et al. (2009) Capitanio F., et al., 2009, Monthly Notices of the Royal Astronomical Society, 398, 1194\n* Chevalier & Ilovaisky (1992) Chevalier C., Ilovaisky S., 1992, International Astronomical Union Circular, 5520, 1\n* Connors et al. (2021) Connors R., et al., 2021, The Astronomer's Telegram, 14725, 1\n* Dauser et al. (2014) Dauser T., et al., 2014, Monthly Notices of the Royal Astronomical Society: Letters, 444, L100\n* Davis et al. (2005) Davis S. W., et al., 2005, The Astrophysical Journal, 621, 372\n* Done et al. (2007) Done C., et al., 2007, The Astronomy and Astrophysics Review, 15, 1\n* Draghis et al. (2022) Draghis P. A., et al., 2022, arXiv preprint arXiv:2210.02479, DR22\n* Ebisawa et al. (1993) Ebisawa K., et al., 1993, Astrophysical Journal, Part 1 (ISSN 0004-637X), vol. 403, no. 2, p. 684-689., 403, 684\n* Ebisawa et al. (2003) Ebisawa K., et al., 2003, The Astrophysical Journal, 597, 780\n* Garcia & Kallman (2010) Garcia J., Kallman T. R., 2010, The Astrophysical Journal, 718, 695\n* Garg et al. (2022) Garg A., et al., 2022, Monthly Notices of the Royal Astronomical Society\n* Gierlinski & Done ( "], "nougat": ["ture during a significant decrease in disk contribution, as well as the study of timing characteristics in that state.\n\n## 6 Acknowledgements\n\nWe would like to thank the anonymous referee for their valuable suggestions, which improved the quality of this work. This work has utilized data from _AstroSat_ mission which is archived at Indian Space Science Data Centre (ISSDC). We are grateful to the SXT and LAXPC POC teams for providing the data and requisite softwares to perform data analysis. NH acknowledges the financial support provided by Department of Science and Technology (DST) under the INSPIRE fellowship scheme. AG, RM and SS acknowledges the financial support provided by Department of Space, Govt of India (No.DS_2B-13012(2)/ / 2 / 2022-Sec.2).\n\n## 7 Data Availability\n\nThe data used in the publication is publicly available for download at [https://astrobrowse.issdc.gov.in/astro_archive/archive/Home.jsp](https://astrobrowse.issdc.gov.in/astro_archive/archive/Home.jsp) using the observation IDs mentioned in Tab 1.\n\n## References\n\n* Agrawal et al. (2017) Agrawal P., et al., 2017, Journal of Astrophysics and Astronomy, 38, 30\n* Antia et al. (2017) Antia H., et al., 2017, The Astrophysical Journal Supplement Series, 231, 10\n* Asplund et al. (2009) Asplund M., et al., 2009, Annual review of astronomy and astrophysics, 47, 481\n* Belloni et al. (2011) Belloni T. M., et al., 2011, arXiv preprint arXiv:1109.3388\n* Bhargava et al. (2022) Bhargava Y., et al., 2022, Monthly Notices of the Royal Astronomical Society, 512, 6067\n* Capitanio et al. (2009) Capitanio F., et al., 2009, Monthly Notices of the Royal Astronomical Society, 398, 1194\n* Chevalier & Ilovaisky (1992) Chevalier C., Ilovaisky S., 1992, International Astronomical Union Circular, 5520, 1\n* Connors et al. (2021) Connors R., et al., 2021, The Astronomer's Telegram, 14725, 1\n* Dauser et al. (2014) Dauser T., et al., 2014, Monthly Notices of the Royal Astronomical Society: Letters, 444, L100\n* Davis et al. (2005) Davis S. W., et al., 2005, The Astrophysical Journal, 621, 372\n* Done et al. (2007) Done C., et al., 2007, The Astronomy and Astrophysics Review, 15, 1\n* Draghis et al. (2022) Draghis P. A., et al., 2022, arXiv preprint arXiv:2210.02479, DR22\n* Ebisawa et al. (1993) Ebisawa K., et al., 1993, Astrophysical Journal, Part 1 (ISSN 0004-637X), vol. 403, no. 2, p. 684-689., 403, 684\n* Ebisawa et al. (2003) Ebisawa K., et al., 2003, The Astrophysical Journal, 597, 780\n* Garcia & Kallman (2010) Garcia J., Kallman T. R., 2010, The Astrophysical Journal, 718, 695\n* Garg et al. (2022) Garg A., et al., 2022, Monthly Notices of the Royal Astronomical Society\n* Gierlinski "]}, {"edit": ["the presynaptic and postsynaptic neuron activities have low correlation their connection are likely to be removed. The latter process is called synaptic pruning and it is considered essential for optimizing activity propagation and memory capacity(Chklovskii et al., 2004; Knoblauch et al., 2014; Knoblauch and Sommer, 2016). Furthermore, it is commonly believed that synaptic pruning and rewiring dysfunction are one of the neural correlate of developmental disorders such as autism or schizophrenia (Bourgeron, 2009; Moyer et al., 2015), leading to, respectively, an higher or lower synaptic density with respect to neurotypical subjects(Hutsler and Zhang, 2010; Pagani et al., 2021; Glantz and Lewis, 2000).\n\nIn the last decades computational neuroscience has investigated brain dynamics at different scales, from cellular (Markram et al., 2015) to mesoscopic and macroscopic through mean-field approaches (Wilson and Cowan, 1972; Amit and Brunel, 1997; Hopfield, 1984; Renart et al., 2004; Leon et al., 2013; di Santo et al., 2018; Capone et al., 2019; Carlu et al., 2020). Regarding synaptic plasticity, computational models were mostly focused on plasticity mechanisms that involve strengthening or weakening of existing synapses, like short-term plasticity (STP) (Tsodyks et al., 1998) or spike timing-dependent plasticity (STDP) (Gutig et al., 2003) and on their role in short-term, long-term, working memory and learning (Mongillo et al., 2008; Tiddia et al., 2022b; Song et al., 2000; qiang Bi and ming Poo, 2001; Golosio et al., 2021; Capone et al., 2022). Only in recent times computational models of structural plasticity and connectivity rearrangements during learning were developed, showing intriguing results. Knoblauch et al. (2014) and Knoblauch and Sommer (2016) describe a model of structural plasticity based on \"effectual connectivity\", defined in these works as the fraction of synapses able to represent a memory stored in a network. By structural plasticity, effectual connectivity is improved, since synapses that do not code for the memory are moved in order to optimize network's connectivity. Their model defines synapses using a Markov model of three states: potential (i.e. not instantiated), instantiated but silent or instantiated and consolidated. Structural plasticity is thus related to the passage of the synapses from a potential state to an instantiated state (and vice versa), whereas changes only related to the synaptic weight are described by the consolidation of the instantiated synapses. With such a model, it is possible to show that networks with structural plasticity have higher or comparable memory capacity to networks with dense connectivity and it is possible to explain some cognitive mechanism such as the spacing effect (Knoblauch et al., 2014).\n\nSpieess et al. (2016) simulated a spiking neural network with structural plasticity and STDP, showing that structural plasticity reduces the amount of noise of the network after a learning process, thus making the network able to have a clearer output. Furthermore, such a network with structural plasticity shows higher learning speed than the same network with only STDP implemented.\n\nSome new insights about the importance of synaptic pruning are also shown in Navlakha et al. (2015), in which different pruning rates were studied suggesting that a slowly decreasing rate of pruning over time leads to more efficient network architectures.\n\nAs discussed above, the biochemical and biophysical mechanisms underlying structural plasticity are extremely complex and only partially understood to date. For this reason, rather than attempting to build a biologically detailed model, this work exploits a relatively simple phenomenological model, including both the activity-driven and the homeostatic contributions; despite the lower complexity, this model accounts for the effects of structural plasticity in terms of the consolidation of synaptic connections between neurons with a high activity correlation as well as those of pruning and rewiring the connections for which this correlation is lower. This approach is also justified by the requirement for a simple and effective computational model suitable for simulating networks with a relatively large number of neurons and connections and for representing learning processes with sizable numbers of training and validation patterns. This model will then serve as the "], "nougat": ["the presynaptic and postsynaptic neuron activities have low correlation their connection are likely to be removed. The latter process is called synaptic pruning and it is considered essential for optimizing activity propagation and memory capacity(Chklovskii, Mel, and Svoboda, 2004; Knoblauch _et al._, 2014; Knoblauch and Sommer, 2016). Furthermore, it is commonly believed that synaptic pruning and rewiring dysfunction are one of the neural correlate of developmental disorders such as autism or schizophrenia (Bourgeron, 2009; Moyer, Shelton, and Sweet, 2015), leading to, respectively, an higher or lower synaptic density with respect to neurotypical subjects(Hutsler and Zhang, 2010; Pagani _et al._, 2021; Glantz and Lewis, 2000). In the last decades computational neuroscience has investigated brain dynamics at different scales, from cellular (Markram _et al._, 2015) to mesoscopic and macroscopic through mean-field approaches (Wilson and Cowan, 1972; Amit and Brunel, 1997; Hopfield, 1984; Renart, Brunel, and Wang, 2004; Leon _et al._, 2013; di Santo _et al._, 2018; Capone _et al._, 2019; Carlu _et al._, 2020). Regarding synaptic plasticity, computational models were mostly focused on plasticity mechanisms that involve strengthening or weakening of existing synapses, like short-term plasticity (STP) (Tsodyks, Pawelzik, and Markram, 1998) or spike timing-dependent plasticity (STDP) (G \u0308utig _et al._, 2003) and on their role in short-term, long-term, working memory and learning (Mongillo, Barak, and Tsodyks, 2008; Tiddia _et al._, 2022b; Song, Miller, and Abbott, 2000; qiang Bi and ming Poo, 2001; Golosio _et al._, 2021; Capone _et al._, 2022). Only in recent times computational models of structural plasticity and connectivity rearrangements during learning were developed, showing intriguing results. Knoblauch _et al._ (2014) and Knoblauch and Sommer (2016) describe a model of structural plasticity based on \u201deffectual connectivity\u201d, defined in these works as the fraction of synapses able to represent a memory stored in a network. By structural plasticity, effectual connectivity is improved, since synapses that do not code for the memory are moved in order to optimize network\u2019s connectivity. Their model defines synapses using a Markov model of three states: potential (i.e. not instantiated), instantiated but silent or instantiated and consolidated. Structural plasticity is thus related to the passage of the synapses from a potential state to an instantiated state (and vice versa), whereas changes only related to the synaptic weight are described by the consolidation of the instantiated synapses. With such a model, it is possible to show that networks with structural plasticity have higher or comparable memory capacity to networks with dense connectivity and it is possible to explain some cognitive mechanism such as the spacing effect (Knoblauch _et al._, 2014). Spiess _et al._ (2016) simulated a spiking neural network with structural plasticity and STDP, showing that structural plasticity reduces the amount of noise of the network after a learning process, thus making the network able to have a clearer output. Furthermore, such a network with structural plasticity shows higher learning speed than the same network with only STDP implemented.\n\nSome new insights about "]}, {"edit": ["length \\(\\xi_{L}\\) which is given by\n\n\\[\\xi_{L}^{2}\\equiv\\frac{1}{4\\sin^{2}(k/2)}\\Bigg{(}\\frac{\\langle q^{2}\\rangle}{ \\langle|\\ q(\\vec{k})\\ |^{2}\\rangle}-1\\Bigg{)}\\,, \\tag{11}\\]\n\nwhere \\(q(\\vec{k})\\) is\n\n\\[q(\\vec{k})\\equiv\\frac{1}{N}\\sum_{j}s_{j}^{(1)}s_{j}^{(2)}e^{\\mathrm{i}\\vec{k} \\cdot\\vec{r}_{j}}\\ , \\tag{12}\\]\n\nwith \\(\\vec{r}_{j}\\) the position of the \\(j\\)-th NP, \\(\\vec{k}=(2\\pi/L,0,0)\\) and \\(k=\\|\\vec{k}\\|=2\\pi/L\\). [39].\n\nErrors in the measurements of these quantities have been calculated as the mean squared deviations of the sample-to-sample fluctuations.\n\n## III Results\n\n### Phase diagram for isotropic HS-like configurations\n\nIn this section we investigate the magnetic order as a function of the volume fraction \\(\\Phi\\) for frozen configurations obtained from equilibrium states of hard sphere fluids in the range \\(0<\\Phi\\leq 0.49\\). \\(\\Phi\\) measures the degree of spatial disorder on such configurations. We will show that for decreasing \\(\\Phi\\) (which means increasing disorder) SG order replaces the FM order.\n\nA first overview can be grasped from Figs. 1-2. Fig. 1(a) displays plots of the specific heat \\(c\\) vs \\(T\\) for \\(\\Phi=0.4\\). The curves exhibit a marked lambda-shaped peak. Their evident dependence on the number of NP indicates the presence of a singular point in the curve that corresponds to \\(N\\to\\infty\\) at \\(T_{c}\\approx 1.9\\). That singular behavior is expected in PM-FM second order transitions. Data are consistent with a logarithmic divergence of \\(c\\) with \\(N\\). Fig. 1(b) shows the plots obtained for \\(\\Phi=0.1\\). In contrast to the previous ones, these plots are smooth and depend little on the sample size. So, there is no sign of any singular behavior. This is expected in PM-SG transitions with strong structural disorder.\n\nFM order entails the presence of non-vanishing magnetization \\(m\\). Fig. 2(a) displays \\(m_{1}\\) vs \\(T\\) for \\(\\Phi=0.4\\) at several \\(N\\). They show that \\(m_{1}\\) tends to non-zero values for \\(N\\to\\infty\\) and low \\(T\\), revealing the existence of strong FM order. The curves plotted in Fig. 2(b) for the magnetic susceptibility \\(\\chi_{m}\\) vs \\(T\\) confirm this conclusion as they show peaks that become sharper for large \\(N\\). An extrapolation of the positions of the maxima of those peaks vs \\(1/N\\) provides a value for the transition temperature, \\(T_{c}(\\Phi=0.4)\\simeq 1.9(1)\\), in agreement with the estimated \\(T_{c}\\) obtained from the analysis of Fig. 1(a). For \\(T<T_{c}\\) we find that \\(\\chi_{m}\\) does not diverge with \\(N\\), a fact that validates the above conclusions on FM order. All that is in contrast to the results obtained for \\(\\Phi=0.1\\), shown in Fig. 2(c) where we see how the values of \\(\\chi_{m}\\) increase with \\(N\\) for low \\(T\\). Data are consistent with "], "nougat": ["length \\(\\xi_{L}\\) which is given by\n\n\\[\\xi_{L}^{2}\\equiv\\frac{1}{4\\sin^{2}(k/2)}\\Bigg{(}\\frac{\\langle q^{2}\\rangle}{ \\langle|\\ q(\\vec{k})\\ |^{2}\\rangle}-1\\Bigg{)}\\,, \\tag{11}\\]\n\nwhere \\(q(\\vec{k})\\) is\n\n\\[q(\\vec{k})\\equiv\\frac{1}{N}\\sum_{j}s_{j}^{(1)}s_{j}^{(2)}e^{\\mathrm{i}\\vec{k} \\cdot\\vec{r}_{j}}\\ , \\tag{12}\\]\n\nwith \\(\\vec{r}_{j}\\) the position of the \\(j\\)-th NP, \\(\\vec{k}=(2\\pi/L,0,0)\\) and \\(k=\\|\\vec{k}\\|=2\\pi/L\\). [39].\n\nErrors in the measurements of these quantities have been calculated as the mean squared deviations of the sample-to-sample fluctuations.\n\n## III Results\n\n### Phase diagram for isotropic HS-like configurations\n\nIn this section we investigate the magnetic order as a function of the volume fraction \\(\\Phi\\) for frozen configurations obtained from equilibrium states of hard sphere fluids in the range \\(0<\\Phi\\leq 0.49\\). \\(\\Phi\\) measures the degree of spatial disorder on such configurations. We will show that for decreasing \\(\\Phi\\) (which means increasing disorder) SG order replaces the FM order.\n\nA first overview can be grasped from Figs. 1-2. Fig. 1(a) displays plots of the specific heat \\(c\\) vs \\(T\\) for \\(\\Phi=0.4\\). The curves exhibit a marked lambda-shaped peak. Their evident dependence on the number of NP indicates the presence of a singular point in the curve that corresponds to \\(N\\to\\infty\\) at \\(T_{c}\\approx 1.9\\). That singular behavior is expected in PM-FM second order transitions. Data are consistent with a logarithmic divergence of \\(c\\) with \\(N\\). Fig. 1(b) shows the plots obtained for \\(\\Phi=0.1\\). In contrast to the previous ones, these plots are smooth and depend little on the sample size. So, there is no sign of any singular behavior. This is expected in PM-SG transitions with strong structural disorder.\n\nFM order entails the presence of non-vanishing magnetization \\(m\\). Fig. 2(a) displays \\(m_{1}\\) vs \\(T\\) for \\(\\Phi=0.4\\) at several \\(N\\). They show that \\(m_{1}\\) tends to non-zero values for \\(N\\to\\infty\\) and low \\(T\\), revealing the existence of strong FM order. The curves plotted in Fig. 2(b) for the magnetic susceptibility \\(\\chi_{m}\\) vs \\(T\\) confirm this conclusion as they show peaks that become sharper for large \\(N\\). An extrapolation of the positions of the maxima of those peaks vs \\(1/N\\) provides a value for the transition temperature, \\(T_{c}(\\Phi=0.4)\\simeq 1.9(1)\\), in agreement with the estimated \\(T_{c}\\) obtained from the analysis of Fig. 1(a). For \\(T<T_{c}\\) we find that \\(\\chi_{m}\\) does not diverge with \\(N\\), a fact that validates the above conclusions on FM order. All that is in contrast to the results obtained for \\(\\Phi=0.1\\), shown in Fig. 2(c) where we see how the values of \\(\\chi_{m}\\) increase with \\(N\\) for low \\(T\\). Data are consistent with "]}, {"edit": ["Let us now informally discuss the case when \\(\\alpha>0\\). For simplicity, we consider (13). The limit on the right-hand side is non-zero, which suggests that there is a residual dependence between the \\(k(N)\\) spins under the Gibbs measure. The reason for the non-zero limit is the fact that the distribution of \\(\\mathcal{P}_{k(N)}\\) and the corresponding binomial distribution satisfy central limit theorems with different variances, the variance of \\(\\mathcal{P}_{k(N)}\\) being strictly larger, which comes from the fact that the spins are positively correlated under the Gibbs measure. The distance between these normal distributions appears on the right-hand side of (13). In Theorem 3.5, we shall determine a _mixed_ binomial distribution which approximates the distribution of \\(\\mathcal{P}_{k(N)}\\) under \\(\\mu_{N}\\). In some sense, this describes the residual dependence between the spins under the Gibbs measure.\n\n_Remark 1.2_.: The exchangeability of the measure \\(\\mu_{N}\\) has been used to investigate the Curie-Weiss model for example, in [17, Section 5.2] and [2]. In particular, an explicit representation of \\(\\mu_{N}\\) as a mixture of Bernoulli measures (valid for each fixed \\(N\\)) can be found in [17, Theorem 5.6]. A general propagation of chaos principle stating that the distribution of \\(k\\) entries in a finite exchangeable vector of length \\(n\\) can be approximated by a mixture of i.i.d. distributions can found in [7].\n\nThe paper is organized as follows. Our proof relies on local limit theorems for the magnetization \\(m_{N}\\) and also for the total number of positive spins \\(\\mathcal{P}_{N}\\) under \\(\\mu_{N}\\). In some regimes those are known. We collect the corresponding results in Section 2 below. The proofs of these local limit theorems, which we have not been able to locate in the literature, are given in Section 4. The proof of Theorem 1.1 is given in Section 3, including the statement of residual dependence. Two auxiliary technical results related to calculations of the total variation distance are presented in Section 5.\n\n## 2. Local limit theorem for the magnetization\n\nDenote by \\(\\mathcal{N}(\\mathfrak{m},\\mathrm{v}^{2})\\) a Gaussian distribution with mean \\(\\mathfrak{m}\\) and variance \\(\\mathrm{v}^{2}\\), so\n\n\\[\\mathcal{N}(\\mathfrak{m},\\mathrm{v}^{2})(A)=\\int_{A}\\varphi(t;\\mathfrak{m}, \\mathrm{v}^{2})\\mathrm{d}t,\\quad A\\in\\mathcal{B}(\\mathbb{R}).\\]\n\nPut \\(\\delta_{N}:=(1-(-1)^{N})/2\\). This correction term appears below in the local limit theorems for \\(m_{N}\\), since \\(Nm_{N}\\) always has the same parity as \\(N\\).\n\n**Proposition 2.1**.: _Assume that \\(h\\neq 0\\) or \\(0<\\beta<1\\). Then_\n\n\\[\\mu_{N}\\left(\\sqrt{N}(m_{N}-\\mathfrak{m}(\\beta,h))\\in\\cdot\\right)\\implies \\mathcal{N}\\left(0,\\mathrm{v}_{\\beta,h}^{2}\\right),\\quad N\\longrightarrow \\infty,\\]\n\n_and the following local limit theorem holds true:_\n\n\\[\\lim_{N\\longrightarrow\\infty}\\sqrt{N}\\sup_{\\ell\\in\\mathbb{Z}}\\left|\\mu_{N} \\left(\\frac{Nm_{N}+\\delta_{N}}{2}=\\ell\\right)-\\varphi\\left(\\ell;\\frac{ "], "nougat": ["Let us now informally discuss the case when \\(\\alpha>0\\). For simplicity, we consider (13). The limit on the right-hand side is non-zero, which suggests that there is a residual dependence between the \\(k(N)\\) spins under the Gibbs measure. The reason for the non-zero limit is the fact that the distribution of \\(\\mathcal{P}_{k(N)}\\) and the corresponding binomial distribution satisfy central limit theorems with different variances, the variance of \\(\\mathcal{P}_{k(N)}\\) being strictly larger, which comes from the fact that the spins are positively correlated under the Gibbs measure. The distance between these normal distributions appears on the right-hand side of (13). In Theorem 3.5, we shall determine a _mixed_ binomial distribution which approximates the distribution of \\(\\mathcal{P}_{k(N)}\\) under \\(\\mu_{N}\\). In some sense, this describes the residual dependence between the spins under the Gibbs measure.\n\n_Remark 1.2_.: The exchangeability of the measure \\(\\mu_{N}\\) has been used to investigate the Curie-Weiss model for example, in [17, Section 5.2] and [2]. In particular, an explicit representation of \\(\\mu_{N}\\) as a mixture of Bernoulli measures (valid for each fixed \\(N\\)) can be found in [17, Theorem 5.6]. A general propagation of chaos principle stating that the distribution of \\(k\\) entries in a finite exchangeable vector of length \\(n\\) can be approximated by a mixture of i.i.d. distributions can found in [7].\n\nThe paper is organized as follows. Our proof relies on local limit theorems for the magnetization \\(m_{N}\\) and also for the total number of positive spins \\(\\mathcal{P}_{N}\\) under \\(\\mu_{N}\\). In some regimes those are known. We collect the corresponding results in Section 2 below. The proofs of these local limit theorems, which we have not been able to locate in the literature, are given in Section 4. The proof of Theorem 1.1 is given in Section 3, including the statement of residual dependence. Two auxiliary technical results related to calculations of the total variation distance are presented in Section 5.\n\n## 2. Local limit theorem for the magnetization\n\nDenote by \\(\\mathcal{N}(\\mathfrak{m},\\mathrm{v}^{2})\\) a Gaussian distribution with mean \\(\\mathfrak{m}\\) and variance \\(\\mathrm{v}^{2}\\), so\n\n\\[\\mathcal{N}(\\mathfrak{m},\\mathrm{v}^{2})(A)=\\int_{A}\\varphi(t;\\mathfrak{m}, \\mathrm{v}^{2})\\mathrm{d}t,\\quad A\\in\\mathcal{B}(\\mathbb{R}).\\]\n\nPut \\(\\delta_{N}:=(1-(-1)^{N})/2\\). This correction term appears below in the local limit theorems for \\(m_{N}\\), since \\(Nm_{N}\\) always has the same parity as \\(N\\).\n\n**Proposition 2.1**.: _Assume that \\(h\\neq 0\\) or \\(0<\\beta<1\\). Then_\n\n\\[\\mu_{N}\\left(\\sqrt{N}(m_{N}-\\mathfrak{m}(\\beta,h))\\in\\cdot\\right)\\implies \\mathcal{N}\\left(0,\\mathrm{v}_{\\beta,h}^{2}\\right),\\quad N\\longrightarrow \\infty,\\]\n\n_and the following local limit theorem holds true:_\n\n\\[\\lim_{N\\longrightarrow\\infty}\\sqrt{N}\\sup_{\\ell\\in\\mathbb{Z}}\\left|\\mu_{N} \\left(\\frac{Nm_{N}+\\delta_{N}}{2}=\\ell\\right)-\\varphi\\left(\\ell;\\frac{ "]}, {"edit": ["Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing with Non-Learnable Primitives\n\nChuntao Ding\\({}^{1}\\)  Zhichao Lu\\({}^{2}\\)  Shangguang Wang\\({}^{3}\\)  Ran Cheng\\({}^{4}\\)  Vishnu N. Boddeti\\({}^{5}\\)\n\n\\({}^{1}\\) Beijing Jiaotong University  \\({}^{2}\\) Sun Yat-sen University  \\({}^{3}\\) Beijing University of Posts and Telecommunications\n\n\\({}^{4}\\) Southern University of Science and Technology  \\({}^{5}\\) Michigan State University\n\nchuntaoding@163.com {luzhichaocn, ranchengcn}@gmail.com sgwang@bupt.edu.cn vishnu@msu.edu\n\nWork done as a visiting scholar at Michigan State University.Corresponding author\n\n###### Abstract\n\nMulti-task learning (MTL) seeks to learn a single model to accomplish multiple tasks by leveraging shared information among the tasks. Existing MTL models, however, have been known to suffer from negative interference among tasks. Efforts to mitigate task interference have focused on either loss/gradient balancing or implicit parameter partitioning with partial overlaps among the tasks. In this paper, we propose ETR-NLP to mitigate task interference through a synergistic combination of non-learnable primitives (NLPs) and explicit task routing (). Our key idea is to employ non-learnable primitives to extract a diverse set of task-agnostic features and recombine them into a shared branch common to all tasks and explicit task-specific branches reserved for each task. The non-learnable primitives and the explicit decoupling of learnable parameters into shared and task-specific ones afford the flexibility needed for minimizing task interference. We evaluate the efficacy of ETR-NLP networks for both image-level classification and pixel-level dense prediction MTL problems. Experimental results indicate that ETR-NLP significantly outperforms state-of-the-art baselines with fewer learnable parameters and similar FLOPs across all datasets. Code is available at this URL.\n\n## 1 Introduction\n\nMulti-task learning (MTL) is commonly employed to improve learning efficiency and performance of multiple tasks by using supervised signals from other related tasks [33, 49, 6]. These models have led to impressive results across numerous tasks. However, there is well-documented evidence [18, 28, 41, 53] that these models are suffering from _task interference_[53], thereby limiting multi-task networks (MTNs) from realizing their full potential.\n\nFor instance, consider the learning progression of an MTN with a standard learnable convolutional layer in Figure 1a (blue curve). Observe that the model learns rapidly, we posit, by exploiting all the shared information between the tasks, i.e., gradients pointing in similar directions. However, the performance starts degrading on further training since the model needs to exploit dissimilar information between the tasks for further improvement, i.e., gradients point in different directions. The latter can be verified by observing the similarity (centered kernel alignment [19]), or the lack thereof, between the gradients for each pair of tasks in Figure 1b.\n\nSeveral approaches were proposed for mitigating task interference in MTNs, including loss/gradient balancing [17, 21, 34, 52, 34], parameter partitioning [28, 30, 37, 2] and architectural design [29, 18, 1]. Despite the diversity of these approaches, they share two common characteristics, (i) all parameters are learned, either for a pre-trained task or for the multiple tasks at hand, (ii) the learned parameters are either fully shared across all tasks or are shared across a partial set of tasks through implicit partitioning, i.e., with no direct control over which parameters are shared across which tasks. Both of these features limit the flexibility of existing\n\nFigure 1: (a) Learning progression of multi-task networks (MTNs) on CelebA for eight tasks. Hard-sharing models with fully learnable parameters (gray) learn rapidly and then suffer from performance degradation due to conflicting gradients from task interference. Networks with non-learnable primitives (NLPs; blue) do not suffer from task interference by design, while explicit task routing (ETR; green), and\n\n"], "nougat": ["Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing with Non-Learnable Primitives\n\nChuntao Ding\\({}^{1}\\)  Zhichao Lu\\({}^{2}\\)  Shangguang Wang\\({}^{3}\\)  Ran Cheng\\({}^{4}\\)  Vishnu N. Boddeti\\({}^{5}\\)\n\n\\({}^{1}\\) Beijing Jiaotong University  \\({}^{2}\\) Sun Yat-sen University  \\({}^{3}\\) Beijing University of Posts and Telecommunications\n\n\\({}^{4}\\) Southern University of Science and Technology  \\({}^{5}\\) Michigan State University. chuntaoding@163.com {luzhichaocn, ranchengcn}@gmail.com sgwang@bupt.edu.cn vishnu@msu.edu\n\nWork done as a visiting scholar at Michigan State University.Corresponding author\n\n###### Abstract\n\nMulti-task learning (MTL) seeks to learn a single model to accomplish multiple tasks by leveraging shared information among the tasks. Existing MTL models, however, have been known to suffer from negative interference among tasks. Efforts to mitigate task interference have focused on either loss/gradient balancing or implicit parameter partitioning with partial overlaps among the tasks. In this paper, we propose ETR-NLP to mitigate task interference through a synergistic combination of non-learnable primitives (NLPs) and explicit task routing (). Our key idea is to employ non-learnable primitives to extract a diverse set of task-agnostic features and recombine them into a shared branch common to all tasks and explicit task-specific branches reserved for each task. The non-learnable primitives and the explicit decoupling of learnable parameters into shared and task-specific ones afford the flexibility needed for minimizing task interference. We evaluate the efficacy of ETR-NLP networks for both image-level classification and pixel-level dense prediction MTL problems. Experimental results indicate that ETR-NLP significantly outperforms state-of-the-art baselines with fewer learnable parameters and similar FLOPs across all datasets. Code is available at this URL.\n\n## 1 Introduction\n\nMulti-task learning (MTL) is commonly employed to improve learning efficiency and performance of multiple tasks by using supervised signals from other related tasks [33, 49, 6]. These models have led to impressive results across numerous tasks. However, there is well-documented evidence [18, 28, 41, 53] that these models are suffering from _task interference_[53], thereby limiting multi-task networks (MTNs) from realizing their full potential.\n\nFor instance, consider the learning progression of an MTN with a standard learnable convolutional layer in Figure 1a (blue curve). Observe that the model learns rapidly, we posit, by exploiting all the shared information between the tasks, i.e., gradients pointing in similar directions. However, the performance starts degrading on further training since the model needs to exploit dissimilar information between the tasks for further improvement, i.e., gradients point in different directions. The latter can be verified by observing the similarity (centered kernel alignment [19]), or the lack thereof, between the gradients for each pair of tasks in Figure 1b.\n\nSeveral approaches were proposed for mitigating task interference in MTNs, including loss/gradient balancing [17, 21, 34, 52, 34], parameter partitioning [28, 30, 37, 2] and architectural design [29, 18, 1]. Despite the diversity of these approaches, they share two common characteristics, (i) all parameters are learned, either for a pre-trained task or for the multiple tasks at hand, (ii) the learned parameters are either fully shared across all tasks or are shared across a partial set of tasks through implicit partitioning, i.e., with no direct control over which parameters are shared across which tasks. Both of these features limit the flexibility of existing\n\nFigure 1: (a) Learning progression of multi-task networks (MTNs) on CelebA for eight tasks. Hard-sharing models with fully learnable parameters (gray) learn rapidly and then suffer from performance degradation due to conflicting gradients from task interference. Networks with non-learnable primitives (NLPs; blue) do not suffer from task interference by design, while explicit task routing (ETR; green), and E\n\n"]}, {"edit": ["\n\n### The control group\n\nWe refer to [8] for a discussion about this section. Consider \\(\\Gamma\\subset\\operatorname{PSL}(3,\\mathbb{C})\\) a (discrete or not) subgroup which acts on \\(\\mathbb{P}^{2}_{\\mathbb{C}}\\) with a point \\(p\\) which is fixed by all of \\(\\Gamma\\). Choose an arbitrary line \\(\\ell\\) in \\(\\mathbb{P}^{2}_{\\mathbb{C}}\\setminus\\{p\\}\\), and notice we have a canonical projection:\n\n\\[\\pi=\\pi_{p,\\ell}:\\mathbb{P}^{2}_{\\mathbb{C}}\\setminus\\{p\\}\\longrightarrow \\ell\\,,\\]\n\ngiven by \\(\\pi(x)=\\overleftrightarrow{x},\\overleftrightarrow{p}\\cap\\ell\\). It is clear that this map is holomorphic and it allows us to define a group homomorphism:\n\n\\[\\Pi=\\Pi_{p,\\ell}:\\Gamma\\longrightarrow Bihol(\\ell)\\cong\\operatorname{PSL}(2, \\mathbb{C})\\,,\\]\n\nby \\(\\Pi(g)(x)=\\pi(g(x))\\). If we choose another line, say \\(\\ell^{\\prime}\\), one gets similarly a projection \\(\\pi^{\\prime}=\\pi_{p,\\ell^{\\prime}}:\\mathbb{P}^{2}_{\\mathbb{C}}\\setminus\\{p\\} \\rightarrow\\ell^{\\prime}\\,,\\) and a group homomorphism \\(\\Pi^{\\prime}=\\Pi_{p,\\ell^{\\prime}}:\\Gamma\\rightarrow\\operatorname{PSL}(2, \\mathbb{C})\\). It is an exercise to see that \\(\\Pi\\) and \\(\\Pi^{\\prime}\\) are equivalent in the sense that there is a biholomorphism \\(h:\\ell\\rightarrow\\ell^{\\prime}\\) inducing an automorphism \\(H\\) of \\(\\operatorname{PSL}(2,\\mathbb{C})\\) such that \\(H\\circ\\Pi=\\Pi^{\\prime}\\). As before, the line \\(\\ell\\) is called _the horizon_.\n\nThis leads to the following definition:\n\n**Definition 1.7**.: Let \\(\\Gamma\\subset\\operatorname{PSL}(3,\\mathbb{C})\\) be a discrete group as above. We call \\(\\Pi=\\Pi_{p,\\ell}\\) the control morphism (or map) and its image \\(\\Pi(\\Gamma)\\subset\\operatorname{PSL}(2,\\mathbb{C})\\), is the _control group_. These are well-defined and independent of \\(\\ell\\) up to an automorphism of \\(\\operatorname{PSL}(2,\\mathbb{C})\\).\n\nThe control map and the control group allow us to get information about the dynamics of \\(\\Gamma\\) by looking at a subgroup of \\(\\operatorname{PSL}(2,\\mathbb{C})\\), which is far easier to handle. The prize we pay is that the control group in \\(\\operatorname{PSL}(2,\\mathbb{C})\\) may not be discrete.\n\n## 2. Purely parabolic groups\n\nWe now follow [5] and look at the discrete subgroups in \\(\\operatorname{PSL}(3,\\mathbb{C})\\) that, besides the identity, have only parabolic elements. These are called purely parabolic and there are five families of such groups; three of them split into various subfamilies according to their limit set (and their control group, see [8]). All of these are elementary.\n\nThe simplest purely parabolic groups are cyclic, generated by a parabolic element. As described above, there are three types of such elements in \\(\\operatorname{PSL}(3,\\mathbb{C})\\), described by the Jordan normal form of their lifts to \\(\\operatorname{SL}(3"], "nougat": ["\n\n### The control group\n\nWe refer to [8] for a discussion about this section. Consider \\(\\Gamma\\subset\\mathrm{PSL}(3,\\mathbb{C})\\) a (discrete or not) subgroup which acts on \\(\\mathbb{P}^{2}_{\\mathbb{C}}\\) with a point \\(p\\) which is fixed by all of \\(\\Gamma\\). Choose an arbitrary line \\(\\ell\\) in \\(\\mathbb{P}^{2}_{\\mathbb{C}}\\setminus\\{p\\}\\), and notice we have a canonical projection:\n\n\\[\\pi=\\pi_{p,\\ell}:\\mathbb{P}^{2}_{\\mathbb{C}}\\setminus\\{p\\}\\longrightarrow \\ell\\,,\\]\n\ngiven by \\(\\pi(x)=\\overleftrightarrow{x},\\overleftrightarrow{p}\\cap\\ell\\). It is clear that this map is holomorphic and it allows us to define a group homomorphism:\n\n\\[\\Pi=\\Pi_{p,\\ell}:\\Gamma\\longrightarrow Bihol(\\ell)\\cong\\mathrm{PSL}(2, \\mathbb{C})\\,,\\]\n\nby \\(\\Pi(g)(x)=\\pi(g(x))\\). If we choose another line, say \\(\\ell^{\\prime}\\), one gets similarly a projection \\(\\pi^{\\prime}=\\pi_{p,\\ell^{\\prime}}:\\mathbb{P}^{2}_{\\mathbb{C}}\\setminus\\{p\\} \\rightarrow\\ell^{\\prime}\\,,\\) and a group homomorphism \\(\\Pi^{\\prime}=\\Pi_{p,\\ell^{\\prime}}:\\Gamma\\rightarrow\\mathrm{PSL}(2,\\mathbb{C})\\). It is an exercise to see that \\(\\Pi\\) and \\(\\Pi^{\\prime}\\) are equivalent in the sense that there is a biholomorphism \\(h:\\ell\\rightarrow\\ell^{\\prime}\\) inducing an automorphism \\(H\\) of \\(\\mathrm{PSL}(2,\\mathbb{C})\\) such that \\(H\\circ\\Pi=\\Pi^{\\prime}\\). As before, the line \\(\\ell\\) is called _the horizon_.\n\nThis leads to the following definition:\n\n**Definition 1.7.** Let \\(\\Gamma\\subset\\mathrm{PSL}(3,\\mathbb{C})\\) be a discrete group as above. We call \\(\\Pi=\\Pi_{p,\\ell}\\) the control morphism (or map) and its image \\(\\Pi(\\Gamma)\\subset\\mathrm{PSL}(2,\\mathbb{C})\\), is the _control group_. These are well-defined and independent of \\(\\ell\\) up to an automorphism of \\(\\mathrm{PSL}(2,\\mathbb{C})\\).\n\nThe control map and the control group allow us to get information about the dynamics of \\(\\Gamma\\) by looking at a subgroup of \\(\\mathrm{PSL}(2,\\mathbb{C})\\), which is far easier to handle. The prize we pay is that the control group in \\(\\mathrm{PSL}(2,\\mathbb{C})\\) may not be discrete.\n\n## 2. Purely parabolic groups\n\nWe now follow [5] and look at the discrete subgroups in \\(\\mathrm{PSL}(3,\\mathbb{C})\\) that, besides the identity, have only parabolic elements. These are called purely parabolic and there are five families of such groups; three of them split into various subfamilies according to their limit set (and their control group, see [8]). All of these are elementary.\n\nThe simplest purely parabolic groups are cyclic, generated by a parabolic element. As described above, there are three types of such elements in \\(\\mathrm{PSL}(3,\\mathbb{C})\\), described by the Jordan normal form of their lifts to \\(\\mathrm{SL}(3,\\"]}, {"edit": ["\n\n# Topportunities at the LHC:\n\nRare Top Decays with Light Singlets\n\nHenning Bahl\n\nhbahl@uchicago.edu Department of Physics and Enrico Fermi Institute, University of Chicago,\n\n5720 South Ellis Avenue, Chicago, IL 60637 USA\n\nSeth Koren\n\nsethk@uchicago.edu Department of Physics and Enrico Fermi Institute, University of Chicago,\n\n5720 South Ellis Avenue, Chicago, IL 60637 USA\n\nLian-Tao Wang\n\nliantaow@uchicago.edu Department of Physics and Enrico Fermi Institute, University of Chicago,\n\n5720 South Ellis Avenue, Chicago, IL 60637 USA\n\n###### Abstract\n\nThe discovery of the top quark, the most massive elementary particle yet known, has given us a distinct window into investigating the physics of the Standard Model and Beyond. With a plethora of top quarks to be produced in the High Luminosity era of the LHC, the exploration of its rare decays holds great promise in revealing potential new physics phenomena. We consider higher-dimensional operators contributing to top decays in the SMEFT and its extension by a light singlet species of spin 0, 1/2, or 1, and exhibit that the HL-LHC may observe many exotic top decays in a variety of channels. Light singlets which primarily talk to the SM through such a top interaction may also lead to distinctive long-lived particle signals. Searching for such long-lived particles in top-quark decays has the additional advantage that the SM decay of the other top quark in the same event provides a natural trigger.\n\n"], "nougat": ["\n\n# Topportunities at the LHC: Rare Top Decays with Light Singlets\n\nHenning Bahl\n\nhbahl@uchicago.edu\n\nSeth Koren\n\nsethk@uchicago.edu\n\nLian-Tao Wang\n\nliantaow@uchicago.edu Department of Physics and Enrico Fermi Institute, University of Chicago,\n\n5720 South Ellis Avenue, Chicago, IL 60637 USA\n\n###### Abstract\n\nThe discovery of the top quark, the most massive elementary particle yet known, has given us a distinct window into investigating the physics of the Standard Model and Beyond. With a plethora of top quarks to be produced in the High Luminosity era of the LHC, the exploration of its rare decays holds great promise in revealing potential new physics phenomena. We consider higher-dimensional operators contributing to top decays in the SMEFT and its extension by a light singlet species of spin 0, 1/2, or 1, and exhibit that the HL-LHC may observe many exotic top decays in a variety of channels. Light singlets which primarily talk to the SM through such a top interaction may also lead to distinctive long-lived particle signals. Searching for such long-lived particles in top-quark decays has the additional advantage that the SM decay of the other top quark in the same event provides a natural trigger.\n\n"]}, {"edit": ["\n\n### Computational Performance of the Optimization Proxies\n\nThis section presents numerical experiments used to assess the performance of the proposed optimization proxies (Proxies) against the optimization models (GDO) and the greedy heuristic (GH).\n\n**Optimality Gap:**\n\nTable 3 presents the optimality gaps of various approaches, including the results of Model (1) under various time constraints. In the table, the columns under \"Gap of Model (1)\" denote the optimality gaps of the model under various time limits. Similarly, columns _Gap_ for GH and Proxies denote optimality gaps for GH and the optimization proxies. In addition, columns _Time(s)_ denote the solving times for GH and Proxies.\n\nRecall that Model (1) produces solutions that exhibit considerable variability when the total commodity volume is perturbed as detailed in Table 4 and 5. As such, it is unlikely to be practical in scenarios with planners in the loop. Hence, the table compares the optimization proxies and the heuristics GH with an \"idealized\" benchmark. With this caveat in place, observe the performance of the optimization proxies under tight time constraints. Proxies generate solutions with low optimality gaps and may be up to 10 to 50 times faster than GH, and around 10 times faster than Model (1) solved with Gurobi. Second, although Model (1) efficiently produces solutions with low optimality gaps, closing the optimizality gap proves to be a significant challenge due to the poor LP relaxation. The performance of GH is also impeded by the inefficiencies of the LP relaxation, as it solves the LP relaxations over many iterations; it takes the GH around 30 iterations for terminal M, 200 iterations for terminal L, and more than 1000 iterations for terminal XL to generate a feasible solution.\n\n**Consistency:**\n\nTables 4 and 5 report the consistency of solutions obtained from different models in terms of the normalized distance to the reference load plan and the total variation of the generated solutions. As GDO requires running Model (1) and Model (2) sequentially, these experiments set the same time limits for the two stages. For example, if a time limit of 30 seconds is set, GDO runs Model (1) for 30 seconds and subsequently runs Model (2) using the best upper bound obtained from Model (1) for another 30 seconds.\n\n_The high-level result is that proxies are ideally suited to produce consistent plans._ Table 4 shows that the proxies accurately predict, in a few seconds, the results produced by GDO after an hour. Furthermore, Table 5\n\n\\begin{table}\n\\begin{tabular}{l|r r r r r r r r r} \\hline \\hline \\multirow{2}{*}{Instance} & \\multicolumn{6}{c}{Model (1)} & \\multicolumn{6}{c}{GH} & \\multicolumn{2}{c}{Proxies} \\\\ \\cline{2-10}  & 1s & 5s & 10s & 30s & 60s & 1800s & Gap & Time (s) & Gap & Time (s) \\\\ \\hline M & 2.59 & 0.55 & 0.48 & 0.48 & 0.48 & 0.48 & 3.84 & 3.12 & 1.14 & 0.33 \\\\ L & 51.15 & 5.22 & 2.18 & 1.71 & 1.41 & 1.39 & 12.85 & 13.28 & 3.80 & 1.10 \\\\ XL & 77.35 & 14.02 & 10.41 & 2.93 & 2.07 & 0.93 & 17.01 & 121.55 & 5.21 & 2.49 \\\\ \\hline \\hline \\end{tabular}\n\\end{table}\nTable 3: Optimality Gap (%) with respect to the Total Trailer Cost"], "nougat": ["\n\n### Computational Performance of the Optimization Proxies\n\nThis section presents numerical experiments used to assess the performance of the proposed optimization proxies (Proxies) against the optimization models (GDO) and the greedy heuristic (GH).\n\n**Optimality Gap:**\n\nTable 3 presents the optimality gaps of various approaches, including the results of Model (1) under various time constraints. In the table, the columns under \u201cGap of Model (1) \u201d denote the optimality gaps of the model under various time limits. Similarly, columns _Gap_ for GH and Proxies denote optimality gaps for GH and the optimization proxies. In addition, columns _Time(s)_ denote the solving times for GH and Proxies.\n\nRecall that Model (1) produces solutions that exhibit considerable variability when the total commodity volume is perturbed as detailed in Table 4 and 5. As such, it is unlikely to be practical in scenarios with planners in the loop. Hence, the table compares the optimization proxies and the heuristics GH with an \u201cidealized\u201d benchmark. With this caveat in place, observe the performance of the optimization proxies under tight time constraints. Proxies generate solutions with low optimality gaps and may be up to 10 to 50 times faster than GH, and around 10 times faster than Model (1) solved with Gurobi. Second, although Model (1) efficiently produces solutions with low optimality gaps, closing the optimizality gap proves to be a significant challenge due to the poor LP relaxation. The performance of GH is also impeded by the inefficiencies of the LP relaxation, as it solves the LP relaxations over many iterations; it takes the GH around 30 iterations for terminal L, and more than 1000 iterations for terminal XL to generate a feasible solution.\n\n**Consistency:**\n\nTables 4 and 5 report the consistency of solutions obtained from different models in terms of the normalized distance to the reference load plan and the total variation of the generated solutions. As GDO requires running Model (2) sequentially, these experiments set the same time limits for the two stages. For example, if a time limit of 30 seconds is set, GDO runs Model (1) for another 30 seconds. subsequently runs Model (2) using the best upper bound obtained from Model (1) for another 30 seconds.\n\n_The high-level result is that proxies are ideally suited to produce consistent plans. Table 4 shows that the proxies accurately predict, in a few seconds, the results produced by GDO after an hour. Furthermore, Table 5\n\n\\begin{table}\n\\begin{tabular}{l|r r r r r r r r r} \\hline \\hline \\multirow{2}{*}{Instance} & \\multicolumn{6}{c}{Model (1)} & \\multicolumn{6}{c}{GH} & \\multicolumn{2}{c}{Proxies} \\\\ \\cline{2-10}  & 1s & 5s & 10s & 30s & 60s & 1800s & Gap & Time (s) & Gap & Time (s) \\\\ \\hline M & 2.59 & 0.55 & 0.48 & 0.48 & 0.48 & 0.48 & 3.84 & 3.12 & 1.14 & 0.33 \\\\ L & 51.15 & 5.22 & 2.18 & 1.71 & 1.41 & 1.39 & 12.85 & 13.28 & 3.80 & 1.10 \\\\ XL & 77.35 & 14.02 & 10.41 & 2.93 & 2.07 & 0.93 & 17.01 & 121.55 & 5.21 & 2.49 \\\\ \\hline \\hline \\end{tabular}\n\\end{table}\nTable 3: Optimality Gap (%) with respect to the Total Trailer Cost"]}, {"edit": ["\n\n\\begin{table}\n\\begin{tabular}{l c c c c c} \\hline  & & \\multicolumn{4}{c}{Outcome} \\\\  & Emp. & p10 & p25 & p50 & p90 \\\\ \\hline _Panel A: Regions differ in location parameter, initial min. wage is small_ & & & & \\\\ Mean causal effect & -0.011 & 0.026 & 0.013 & 0.007 & 0.003 \\\\ Fraction affected & -0.013 & 0.028 & 0.014 & 0.008 & 0.004 \\\\  & (0.000) & (0.001) & (0.000) & (0.000) & (0.000) \\\\ Gap measure & -0.010 & 0.021 & 0.011 & 0.006 & 0.003 \\\\  & (0.000) & (0.001) & (0.000) & (0.000) & (0.000) \\\\ _Panel B: Regions differ in location parameter, initial min. wage is large_ & & & & \\\\ Mean causal effect & -0.076 & 0.155 & 0.079 & 0.049 & 0.025 \\\\ Fraction affected & -0.079 & 0.112 & 0.081 & 0.054 & 0.031 \\\\  & (0.002) & (0.014) & (0.003) & (0.000) & (0.001) \\\\ Gap measure & -0.055 & 0.074 & 0.058 & 0.038 & 0.022 \\\\  & (0.003) & (0.012) & (0.002) & (0.001) & (0.000) \\\\ _Panel C: Identical regions receive different location shocks, initial min. wage is small_ & & & & \\\\ Mean causal effect & -0.006 & 0.015 & 0.007 & 0.004 & 0.002 \\\\ Effective min. wage & -0.006 & 0.011 & 0.003 & 0.000 & -0.002 \\\\  & (0.001) & (0.001) & (0.000) & (0.000) & (0.000) \\\\ Effective min. wage, p90 & -0.006 & 0.013 & 0.005 & 0.002 & 0.000 \\\\  & (0.001) & (0.001) & (0.001) & (0.000) & (0.000) \\\\ _Panel D: Identical regions receive different location shocks, initial min. wage is large_ & & & & \\\\ Mean causal effect & -0.058 & 0.129 & 0.062 & 0.037 & 0.018 \\\\ Effective min. wage & -0.059 & 0.092 & 0.024 & 0.000 & -0.019 \\\\  & (0.0"], "nougat": ["\n\n\\begin{table}\n\\begin{tabular}{l c c c c c} \\hline  & & \\multicolumn{4}{c}{Outcome} \\\\  & Emp. & p10 & p25 & p50 & p90 \\\\ \\hline _Panel A: Regions differ in location parameter, initial min. wage is small_ & & & & \\\\ Mean causal effect & -0.011 & 0.026 & 0.013 & 0.007 & 0.003 \\\\ Fraction affected & -0.013 & 0.028 & 0.014 & 0.008 & 0.004 \\\\  & (0.000) & (0.001) & (0.000) & (0.000) & (0.000) \\\\ Gap measure & -0.010 & 0.021 & 0.011 & 0.006 & 0.003 \\\\  & (0.000) & (0.001) & (0.000) & (0.000) & (0.000) \\\\ _Panel B: Regions differ in location parameter, initial min. wage is large_ & & & & \\\\ Mean causal effect & -0.076 & 0.155 & 0.079 & 0.049 & 0.025 \\\\ Fraction affected & -0.079 & 0.112 & 0.081 & 0.054 & 0.031 \\\\  & (0.002) & (0.014) & (0.003) & (0.000) & (0.001) \\\\ Gap measure & -0.055 & 0.074 & 0.058 & 0.038 & 0.022 \\\\  & (0.003) & (0.012) & (0.002) & (0.001) & (0.000) \\\\ _Panel C: Identical regions receive different location shocks, initial min. wage is small_ & & & & \\\\ Mean causal effect & -0.006 & 0.015 & 0.007 & 0.004 & 0.002 \\\\ Effective min. wage & -0.006 & 0.011 & 0.003 & 0.000 & -0.002 \\\\  & (0.001) & (0.001) & (0.000) & (0.000) & (0.000) \\\\ Effective min. wage, p90 & -0.006 & 0.013 & 0.005 & 0.002 & 0.000 \\\\  & (0.001) & (0.001) & (0.001) & (0.000) & (0.000) \\\\ _Panel D: Identical regions receive different location shocks, initial min. wage is large_ & & & & \\\\ Mean causal effect & -0.058 & 0.129 & 0.062 & 0.037 & 0.018 \\\\ Effective min. wage & -0.059 & 0.092 & 0.024 & 0.000 & -0.019 \\\\  & (0.0"]}, {"edit": ["\n\n**Lemma 3.4**.: _Under the hypothesis of Theorem 3.1, for every \\(p_{0}>2\\) there exists a constant \\(C_{1}\\) such that for all \\(p\\geqslant p_{0}\\) and \\(\\zeta\\in C_{0}^{\\infty}(\\mathbb{R}^{n};\\,\\mathbb{R}_{\\geqslant 0})\\)_\n\n\\[\\|\\nabla(\\zeta|\\psi|^{p/2})\\|_{2}\\leqslant C_{1}\\,p^{\\alpha}\\|(|\\nabla\\zeta|+ \\zeta)|\\psi|^{p/2}\\|_{2}, \\tag{29}\\]\n\n_where \\(\\alpha=(\\mu+1)/2\\)._\n\nProof.: The proof of this lemma is based on (22) and it follows the proof of (Theorem 5.1 in [0]). Note that (22) becomes useless for \\(p=2\\). Agmon works with the stronger inequality (19), which doesn't degenerate at \\(p=2\\). By fixing a number \\(p_{0}>2\\) and looking at numbers \\(p\\geqslant p_{0}\\) we can reuse Agmon's proof. From (22) and Holder's inequality it follows that\n\n\\[(p-2)\\int\\!\\!dx\\,\\zeta^{2}|\\psi|^{p-2}|\\nabla|\\psi||^{2}\\leqslant 2 \\left(\\int\\!\\!dx\\,\\zeta^{2}|\\psi|^{p-2}|\\nabla|\\psi||^{2}\\right)^{1/2}\\left( \\int\\!\\!dx\\,|\\nabla\\zeta|^{2}|\\psi|^{p}\\right)^{1/2}\\] \\[+\\int\\!\\!dx\\,\\zeta^{2}q_{-}|\\psi|^{p}\\]\n\nUsing (26) this can be rewritten as\n\n\\[(p-2)\\frac{4}{p^{2}}\\underbrace{\\int\\!\\!dx\\,\\zeta^{2}|\\nabla| \\psi|^{p/2}|^{2}}_{=A^{2}}\\leqslant 2\\frac{2}{p}\\underbrace{\\left(\\int\\!\\!dx\\, \\zeta^{2}|\\nabla|\\psi|^{p/2}|^{2}\\right)^{1/2}}_{=A}\\underbrace{\\left(\\int\\! \\!dx\\,|\\nabla\\zeta|^{2}(|\\psi|^{p/2})^{2}\\right)^{1/2}}_{=B}\\] \\[+\\underbrace{\\int\\!\\!dx\\,\\zeta^{2}q_{-}(|\\psi|^{p/2})^{2}}_{=C}.\\]\n\nNote that by Lemma 3.3 and assumption (13) all integrals are finite. The above inequality is equivalent to\n\n\\[A^{2}\\leqslant\\frac{p}{p-2}AB+\\frac{p^{2}}{4(p-2)}C.\\]\n\nNext, we use that \\(p/(p-2)\\leqslant p_{0}/(p_{0}-2)\\rightleftharpoons:D_{0}\\) for \\(p\\geqslant p_{0}\\) and \\(AB\\leqslant\\delta A^{2}+B^{2}/\\delta\\) for all \\(\\delta>0\\):\n\n\\[A^{2} \\leqslant D_{0}\\left(\\delta A^{2}+\\frac{B^{2}}{\\delta}\\right)+ \\frac{p}{4}D_{0}C\\"], "nougat": ["\n\n**Lemma 3.4.**_Under the hypothesis of Theorem 3.1, for every \\(p_{0}>2\\) there exists a constant \\(C_{1}\\) such that for all \\(p\\geqslant p_{0}\\) and \\(\\zeta\\in C_{0}^{\\infty}(\\mathbb{R}^{n};\\,\\mathbb{R}_{\\geqslant 0})\\)_\n\n\\[\\|\\nabla(\\zeta|\\psi|^{p/2})\\|_{2}\\leqslant C_{1}\\,p^{\\alpha}\\|(|\\nabla\\zeta|+ \\zeta)|\\psi|^{p/2}\\|_{2},\\] (22) and it follows the proof of (Theorem 5.1 in [0]). Note that (22) becomes useless for \\(p=2\\). Agmon works with the stronger inequality (19), which doesn\u2019t degenerate at \\(p=2\\). By fixing a number \\(p_{0}>2\\) and looking at numbers \\(p\\geqslant p_{0}\\) we can reuse Agmon\u2019s proof. From (22) and H \u0308older\u2019s inequality it follows that\n\n\\[(p-2)\\int\\!\\!dx\\,\\zeta^{2}|\\psi|^{p-2}|\\nabla|\\psi||^{2}\\leqslant 2 \\left(\\int\\!\\!dx\\,\\zeta^{2}|\\psi|^{p-2}|\\nabla|\\psi||^{2}\\right)^{1/2}\\left( \\int\\!\\!dx\\,|\\nabla\\zeta|^{2}|\\psi|^{p}\\right)^{1/2}\\] \\[+\\int\\!\\!dx\\,\\zeta^{2}q_{-}|\\psi|^{p}\\]\n\nUsing (26) this can be rewritten as\n\n\\[(p-2)\\frac{4}{p^{2}}\\underbrace{\\int\\!\\!dx\\,\\zeta^{2}|\\nabla| \\psi|^{p/2}|^{2}}_{=A^{2}}\\leqslant 2\\frac{2}{p}\\underbrace{\\left(\\int\\!\\!dx\\, \\zeta^{2}|\\nabla|\\psi|^{p/2}|^{2}\\right)^{1/2}}_{=A}\\underbrace{\\left(\\int\\! \\!dx\\,|\\nabla\\zeta|^{2}(|\\psi|^{p/2})^{2}\\right)^{1/2}}_{=B}\\] \\[+\\underbrace{\\int\\!\\!dx\\,\\zeta^{2}q_{-}(|\\psi|^{p/2})^{2}}_{=C}.\\]\n\nNote that by Lemma 3.3 and assumption (13) all integrals are finite. The above inequality is equivalent to\n\n\\[A^{2}\\leqslant\\frac{p}{p-2}AB+\\frac{p^{2}}{4(p-2)}C.\\]\n\nNext, we use that \\(p/(p-2)\\leqslant p_{0}/(p_{0}-2)\\rightleftharpoons:D_{0}\\) for \\(p\\geqslant p_{0}\\) and \\(AB\\leqslant\\delta A^{2}+B^{2}/\\delta\\) for all \\(\\delta>0\\):\n\n\\[A^{2} \\leqslant D_{0}\\left(\\delta A^{2}+\\frac{B^{2}}{\\delta}\\right)+ \\frac{p}{4}D_{0}C\\] \\[\\Longleftrightarrow\\ (1-D_{0}\\delta)A^{2} \\leqslant D_{0}\\frac{B^{2}\n\n"]}, {"edit": ["\n\n### 4 language pairs\n\nA single encoder-decoder model is trained on 4 language pairs. We run experiments with 3 types of language pair combinations:\n\n* **Model 1**: 2 unique language pairs in forward and reverse direction: kn\\(\\leftrightarrow\\) ml,te \\(\\leftrightarrow\\) ta. Results documented in Table 3\n* **Model 2**: 4 unique language pairs: kn\\(\\rightarrow\\)ml, ml\\(\\rightarrow\\)te, te\\(\\rightarrow\\)ta, ta\\(\\rightarrow\\)kn. Results documented in Table 4\n* **Model 3**: 4 unique language pairs with VOLT: kn\\(\\rightarrow\\)ml, ml\\(\\rightarrow\\)te, te\\(\\rightarrow\\)ta, ta\\(\\rightarrow\\)kn. Results documented in Table 5\n\nBoth techniques expose the model to 1/3 of the total translation directions during training but the first technique is built to test model performance in very low resource conditions; when there are only 2 sources of parallel corpora available. In comparison, the second model is exposed to 1/3 of the total translation directions with each source-target language combination being unique. We ensure that in every model, both the encoder and decoder see each language atleast once during training.\n\nWe observe that BLEU score for zero-shot translation lags by 5.03 on average compared to the performance of trained language pairs when we train on both directions of 2 language pairs only. In comparison, the zero-shot translation BLEU score lags by 5.98 BLEU on average for the model trained on 4 unique language pairs. The BLEU score for trained translation directions is always in the 6-8 BLEU range. The 4 language pair model trained with VOLT outperforms both the 32000 vocabulary models in zero-shot translation performance with zero shot scores lagging by 3.53 on average from the trained directions.\n\n### 6 language pairs\n\n2 additional language pairs are added to the 4 unique language pairs of model 2 and a transformer model is trained on all 6 language pairs. The model now sees 1/2 of all possible translation directions during training. Table 6 shows the results obtained from the 6 language pair model.\n\nWe observe that zero-shot translation performance increases drastically. Zero-shot directions now lag by 1.76 BLEU to the trained translation directions.\n\n\\begin{table}\n\\begin{tabular}{c|c c c c} \\hline  & kn & ml & te & ta \\\\ \\hline kn & - & **7.4** & 0.4 & 0.5 \\\\ ml & 1.0 & - & **7.0** & 0.4 \\\\ te & 0.8 & 4.7 & - & **7.1** \\\\ ta & **8.9** & 4.5 & 0.6 & - \\\\ \\hline \\end{tabular}\n\\end{table}\nTable 4: BLEU score for 4 language pair model 2. The rows are the source language and the columns are the target language. Cells in bold represent the translation directions used in training\n\n\\begin{table}\n\\begin{tabular}{c|c c c c} \\hline  & kn & ml & te & ta \\\\ \\hline kn & - & **7.7** & 1.0 & 0.8 \\\\ ml & **8.9** & - & 5.7 & 0.6 \\\\ te & 0.5 & 3.2 & - & **7.4** \\\\ ta & 5.8 & 4.9 & **7.4** & - \\\\ \\hline \\end{tabular}\n\\end{table}\nTable 3: BLEU score for 4 language pair model 1. The rows are the source language and the columns are the target language. Cells in bold represent the translation directions used in training\n\n\\begin{table}\n\\begin{tabular}{c|c c c c} \\hline  & kn & ml & te & ta \\\\ \\hline kn & - & **6.5** & 4.5 & 0.8 \\\\ ml & 6.8 &"], "nougat": ["\n\n### 4 language pairs. We run experiments with 3 types of language pair combinations:\n\n* **Model 1**: 2 unique language pairs in forward and reverse direction: kn\\(\\leftrightarrow\\) ml,te \\(\\leftrightarrow\\) ta. Results documented in Table 3\n* **Model 2**: 4 unique language pairs: kn\\(\\rightarrow\\)ml, ml\\(\\rightarrow\\)te, te\\(\\rightarrow\\)ta, ta\\(\\rightarrow\\)kn. Results documented in Table 4\n* **Model 3**: 4 unique language pairs with VOLT: kn\\(\\rightarrow\\)ml, ml\\(\\rightarrow\\)te, te\\(\\rightarrow\\)ta, ta\\(\\rightarrow\\)kn. Results documented in Table 5 Both techniques expose the model to 1/3 of the total translation directions during training but the first technique is built to test model performance in very low resource conditions; when there are only 2 sources of parallel corpora available. In comparison, the second model is exposed to 1/3 of the total translation directions with each source-target language combination being unique. We ensure that in every model, both the encoder and decoder see each language atleast once during training. We observe that BLEU score for zero-shot translation lags by 5.03 on average compared to the performance of trained language pairs when we train on both directions of 2 language pairs only. In comparison, the zero-shot translation BLEU score lags by 5.98 BLEU on average for the model trained on 4 unique language pairs. The BLEU score for trained translation directions is always in the 6-8 BLEU range. The 4 language pair model trained with VOLT outperforms both the 32000 vocabulary models in zero-shot translation performance with zero shot scores lagging by 3.53 on average from the trained directions.\n\nlanguage pairs. The model now sees 1/2 of all possible translation directions during training. Table 6 shows the results obtained from the 6 language pair model.\n\nWe observe that zero-shot translation performance increases drastically. Zero-shot directions now lag by 1.76 BLEU to the trained translation directions.\n\n\\begin{table}\n\\begin{tabular}{c|c c c c} \\hline  & kn & ml & te & ta \\\\ \\hline kn & - & **7.4** & 0.4 & 0.5 \\\\ ml & 1.0 & - & **7.0** & 0.4 \\\\ te & 0.8 & 4.7 & - & **7.1** \\\\ ta & **8.9** & 4.5 & 0.6 & - \\\\ \\hline \\end{tabular}\n\\end{table}\nTable 4: BLEU score for 4 language pair model 2. The rows are the source language and the columns are the target language. Cells in bold represent the translation directions used in training\n\n\\begin{table}\n\\begin{tabular}{c|c c c c} \\hline  & kn & ml & te & ta \\\\ \\hline kn & - & **7.7** & 1.0 & 0.8 \\\\ ml & **8.9** & - & 5.7 & 0.6 \\\\ te & 0.5 & 3.2 & - & **7.4** \\\\ ta & 5.8 & 4.9 & **7.4** & - \\\\ \\hline \\end{tabular}\n\\end{table}\nTable 3: BLEU score for 4 language pair model 1. The rows are the source language and the columns are the target language. Cells in bold represent the translation directions used in training\n\n\\begin{table}\n\\begin{tabular}{c|c c c c} \\hline  & kn & ml & te & ta \\\\ \\hline kn & - & **6.5** & 4.5 & 0.8 \\\\ ml & 6.8 & - & **6.4** & 5.5 \\\\ te & 0.7 & 2.4 & - & **6.6** \\\\ ta & **"]}, {"edit": ["states. As shown in Fig. 8 (c) and (d), the qubit gradually relaxes to \\(|0\\rangle\\) after the SFQ operation. Therefore, when the \\(\\pi\\) pulse is applied to the qubit immediately after the SFQ operation, the qubit cannot be completely excited to the \\(|1\\rangle\\) state. As the recovery time prolongs, the population of \\(|1\\rangle\\) after the \\(\\pi\\) pulse also gradually increases to 1.\n\n## Appendix C Extraction of Quasiparticle Density Near Qubit\n\nThe \\(x_{\\rm QP,qubit}\\) extracted using \\(x_{\\rm QP,qubit}=(\\Gamma(t)-\\Gamma_{0})/C\\) is a kind of average QP density during the \\(T_{1}\\) measurement, which can be regarded as the average density of QP over a period of time \\(t_{\\rm avg}\\) (\\(\\sim\\,T_{1}\\)) after \\(t_{\\rm R}\\). To intuitively illustrate the validity of the \\(x_{\\rm QP,qubit}\\) evolution extracted in this way to analyze the QP propagation mechanism, we compare (i) the \\(x_{\\rm QP,qubit}\\) extracted with a very short measurement time (\\(t_{\\rm avg}\\,=\\,0\\)) in an ideal measurement, and (ii) the \\(x_{\\rm QP,qubit}\\) extracted with several microsecond measurement times (\\(t_{\\rm avg}=3,\\,6,\\,12,\\,18\\,\\mu s\\)) in a real experiment. We assume that the gray curve (\\(t_{\\rm avg}=0\\)) in the Fig. 9 below, which is similar to the trend of the extracted \\(x_{\\rm QP,qubit}\\) evolution in the main-text, is the real QP density evolution. The trends and timescales (\\(\\sim\\)several microseconds) of \\(x_{\\rm QP,qubit}\\) evolution vary little over all values of \\(t_{\\rm avg}\\) listed.\n\n## Appendix D Diffusion of Quasiparticles in Superconducting Quantum-Classical Hybrid Circuits\n\nMost QPs propagate diffusively when the local QP density is relatively low. The local QP density is \\(x_{\\rm QP}=n_{\\rm QP}/n_{\\rm CP}\\), where \\(n_{\\rm QP}\\) is the QP density and \\(n_{\\rm CP}\\) the Cooper pair density. As the local QP density increases, QPs have a greater probability of recombination, and phonon-mediated propagation becomes the leading mechanism. We calculated the diffusion equation by finite element simulation, thereby estimating the contribution of QP diffusion to the QP propagation in the device described in the paper. We set a boundary of\n\nFigure 8: **(a)** Pulse sequence applied to measure qubits state after an SFQ pulse train. The transmission of the readout resonator is measured at \\(f_{\\rm r}\\). **(b)** Qubit readout resonator spectroscopy vs. DC/SFQ converter drive pulse duration \\(t_{\\rm SFQ}\\). **(c)** Pulse sequence applied to measure qubits state with variable delay \\(t_{\\rm delay}\\) after a 25\\(\\mu\\)s SFQ pulse train. The transmission of the readout resonator is measured at \\(f_{\\rm r}\\). **(d)** Qubit readout resonator spectroscopy vs. delay between SFQ circuit operation and readout pulse \\(t_{\\rm delay}\\). The frequencies corresponding to the readout resonator when the state of qubit is \\(|0\\rangle\\), \\(|1\\rangle\\), and punch out have been marked.\n\nFigure 9: Effect of measurement time on the extracted \\(x_{\\rm QP,qubit}\\) evolution\n\n"], "nougat": ["states. As shown in Fig. 8 (c) and (d), the qubit gradually relaxes to \\(|0\\rangle\\) after the SFQ operation. Therefore, when the \\(\\pi\\) pulse is applied to the qubit immediately after the SFQ operation, the qubit cannot be completely excited to the \\(|1\\rangle\\) state. As the recovery time prolongs, the population of \\(|1\\rangle\\) after the \\(\\pi\\) pulse also gradually increases to 1.\n\n## Appendix C Extraction of Quasiparticle Density Near Qubit\n\nThe \\(x_{\\rm QP,qubit}\\) extracted using \\(x_{\\rm QP,qubit}=(\\Gamma(t)-\\Gamma_{0})/C\\) is a kind of average QP density during the \\(T_{1}\\) measurement, which can be regarded as the average density of QP over a period of time \\(t_{\\rm avg}\\) (\\(\\sim\\,T_{1}\\)) after \\(t_{\\rm R}\\). To intuitively illustrate the validity of the \\(x_{\\rm QP,qubit}\\) evolution extracted in this way to analyze the QP propagation mechanism, we compare (i) the \\(x_{\\rm QP,qubit}\\) extracted with a very short measurement time (\\(t_{\\rm avg}\\,=\\,0\\)) in an ideal measurement, and (ii) the \\(x_{\\rm QP,qubit}\\) extracted with several microsecond measurement times (\\(t_{\\rm avg}=3,\\,6,\\,12,\\,18\\,\\mu s\\)) in a real experiment. We assume that the gray curve (\\(t_{\\rm avg}=0\\)) in the Fig. 9 below, which is similar to the trend of the extracted \\(x_{\\rm QP,qubit}\\) evolution in the maintext, is the real QP density evolution. The trends and timescales (\\(\\sim\\)several microseconds) of \\(x_{\\rm QP,qubit}\\) evolution vary little over all values of \\(t_{\\rm avg}\\) listed.\n\n## Appendix D Diffusion of Quasiparticles in Superconducting Quantum-Classical Hybrid Circuits\n\nMost QPs propagate diffusively when the local QP density is relatively low. The local QP density is \\(x_{\\rm QP}=n_{\\rm QP}/n_{\\rm CP}\\), where \\(n_{\\rm QP}\\) is the QP density and \\(n_{\\rm CP}\\) the Cooper pair density. As the local QP density increases, QPs have a greater probability of recombination, and phonon-mediated propagation becomes the leading mechanism. We calculated the diffusion equation by finite element simulation, thereby estimating the contribution of QP diffusion to the QP propagation in the device described in the paper. We set a boundary of\n\nFigure 8: **(a)** Pulse sequence applied to measure qubits state after an SFQ pulse train. The transmission of the readout resonator is measured at \\(f_{\\rm r}\\). **(b)** Qubit readout resonator spectroscopy vs. DC/SFQ converter drive pulse duration \\(t_{\\rm SFQ}\\). **(c)** Pulse sequence applied to measure qubits state with variable delay \\(t_{\\rm delay}\\) after a 25\\(\\mu\\)s SFQ pulse train. The transmission of the readout resonator is measured at \\(f_{\\rm r}\\). **(d)** Qubit readout resonator spectroscopy vs. delay between SFQ circuit operation and readout pulse \\(t_{\\rm delay}\\). The frequencies corresponding to the readout resonator when the state of qubit is \\(|0\\rangle\\), \\(|1\\rangle\\), and punch out have been marked.\n\nFigure 9: Effect of measurement time on the extracted \\(x_{\\rm QP,qubit}\\) evolution\n\n"]}, {"edit": ["the other cliques, ensuring a fully connected graph. All nodes and edges are associated with information through embeddings, described below.\n\n**GNN stage** The second stage employs a generalized message-passing GNN following [5] to perform several prediction tasks on this graph simultaneously:\n\n1. **keypoint association prediction:** we model association between body keypoints and their corresponding pedicle keypoints as binary edge classification on the over-connected \\(k\\)-NN graph.\n2. **body keypoint level prediction:** for body keypoints, we model the spine level prediction as multi-class node classification.\n3. **keypoint legitimacy prediction:** to filter out false-positive keypoints, we additionally compute an binary legitimacy prediction for each node.\n\nTo perform these task, our message-passing GNN maintains edge and node embeddings which are updated in each layer. A message-passing layer performs a node update and edge update operation. Denoting the feature vector of a node \\(v\\) by \\(x_{v}\\), and the feature vector of a directed edge \\((u,v)\\) by \\(x_{uv}\\), the node and edge features are updated as follows:\n\n\\[\\underbrace{x^{\\prime}_{u}=\\bigoplus_{v\\in\\mathcal{N}_{u}\\cup\\{u\\}}\\psi_{\\text{ node}}(x_{u},x_{v},x_{uv})}_{\\text{Node update}},\\qquad\\underbrace{x^{\\prime}_{uv}=\\psi_{\\text{edge}}(x_{u},x_{v},x_{uv})}_{ \\text{Edge update}} \\tag{1}\\]\n\nHere \\(\\bigoplus\\) denotes a symmetric pooling operation (in our case max pooling) over the neighborhood \\(\\mathcal{N}_{u}\\). \\(\\psi_{\\text{node}}\\) and edge are trainable parametric functions: in our case, two distinct two-layer MLPs with ReLU nonlinearities. After \\(N\\) such message-passing layers we obtain an embedding vector for each node and edge. Each node/edge embedding is passed through a linear layer (distinct for nodes and edges) to obtain a vector of node class logits or a single edge prediction logit, respectively. The last entry in the node prediction vector is interpreted as a node legitimacy prediction score: nodes predicted as illegitimate are discarded for the output.\n\nThe node input features \\(x_{u}\\in\\mathbb{R}^{7}\\) consist of the one-hot encoded keypoint type (body, left or right pedicle) and the segment input information (a pseudo-probability in \\([0,1]\\) for each of the four spine segments of belonging to that segment, computed by applying a sigmoid to the heatmap network's output channels which represent the different spine segments). The edge input features \\(x_{uv}\\in\\mathbb{R}^{4}\\) consist of the normalized direction vector of the edge and the distance between the two endpoints.\n\nThe output of the GNN contains finer spine-level classification (i.e. C1-C7, T1-T13, L1-L6, S1-S2), keypoint-level legitimacy (legitimate vs. false-positive detection) and body-pedicle association via edge prediction, implicitly defining the orientation of each vertebra. Prediction scores of corresponding directed edges \\((u,v)\\) and \\((v,u)\\) are symmetrized by taking the mean.\n\nIn our experiments we consider variations to our architecture: weight sharing between consecutive GNN layers, multiple heads with a shared backbone (jointly trained) and dedicated networks (separately trained) for edge/node prediction.\n\n "], "nougat": ["the other cliques, ensuring a fully connected graph. All nodes and edges are associated with information through embeddings, described below.\n\n**GNN stage** The second stage employs a generalized message-passing GNN following [5] to perform several prediction tasks on this graph simultaneously:\n\n1. **keypoint association prediction:** we model association between body keypoints and their corresponding pedicle keypoints as binary edge classification on the over-connected \\(k\\)-NN graph.\n2. **body keypoint level prediction:** for body keypoints, we model the spine level prediction as multi-class node classification.\n3. **keypoint legitimacy prediction:** to filter out false-positive keypoints, we additionally compute an binary legitimacy prediction for each node.\n\nTo perform these task, our message-passing GNN maintains edge and node embeddings which are updated in each layer. A message-passing layer performs a node update and edge update operation. Denoting the feature vector of a node \\(v\\) by \\(x_{v}\\), and the feature vector of a directed edge \\((u,v)\\) by \\(x_{uv}\\), the node and edge features are updated as follows:\n\n\\[\\underbrace{x^{\\prime}_{u}=\\bigoplus_{v\\in\\mathcal{N}_{u}\\cup\\{u\\}}\\psi_{\\text{ node}}(x_{u},x_{v},x_{uv})}_{\\text{Node update}},\\qquad\\underbrace{x^{\\prime}_{uv}=\\psi_{\\text{edge}}(x_{u},x_{v},x_{uv})}_{ \\text{Edge update}} \\tag{1}\\]\n\nHere \\(\\bigoplus\\) denotes a symmetric pooling operation (in our case max pooling) over the neighborhood \\(\\mathcal{N}_{u}\\). \\(\\psi_{\\text{node}}\\) and edge are trainable parametric functions: in our case, two distinct two-layer MLPs with ReLU nonlinearities. After \\(N\\) such message-passing layers we obtain an embedding vector for each node and edge. Each node/edge embedding is passed through a linear layer (distinct for nodes and edges) to obtain a vector of node class logits or a single edge prediction logit, respectively. The last entry in the node prediction vector is interpreted as a node legitimacy prediction score: nodes predicted as illegitimate are discarded for the output.\n\nThe node input features \\(x_{u}\\in\\mathbb{R}^{7}\\) consist of the one-hot encoded keypoint type (body, left or right pedicle) and the segment input information (a pseudoprobability in \\([0,1]\\) for each of the four spine segments of belonging to that segment, computed by applying a sigmoid to the heatmap network\u2019s output channels which represent the different spine segments). The edge input features \\(x_{uv}\\in\\mathbb{R}^{4}\\) consist of the normalized direction vector of the edge and the distance between the two endpoints.\n\nThe output of the GNN contains finer spine-level classification (i.e. C1-C7, T1-T13, L1-L6, S1-S2), keypoint-level legitimacy (legitimate vs. false-positive detection) and body-pedicle association via edge prediction, implicitly defining the orientation of each vertebra. Prediction scores of corresponding directed edges \\((u,v)\\) and \\((v,u)\\) are symmetrized by taking the mean.\n\nIn our experiments we consider variations to our architecture: weight sharing between consecutive GNN layers, multiple heads with a shared backbone (jointly trained) and dedicated networks (separately trained) for edge/node prediction.\n\n "]}, {"edit": ["taking \\(\\Omega\\to\\Omega+\\imath 0^{+}\\), where we add infinitesimally small imaginary part as it is required in the retarded Green's function. Then \\(a=(\\Omega+\\imath 0^{+})^{2}+...\\) gets small imaginary part \\(2\\imath\\Omega 0^{+}\\) defining the value of csgn. Taking the limit of infinitisimal \\(0^{+}\\) we arrive at the following expression for the trace of the Green's function\n\n\\[\\mathrm{Tr}\\mathbf{G}(\\Omega)=\\frac{2\\Omega}{\\pi}\\int_{0}^{\\pi}dk_{y}\\begin{cases} \\frac{\\mathrm{sgn}\\,a}{\\sqrt{a^{2}-b^{2}}}&\\mathrm{for}\\;\\;a^{2}>b^{2},\\\\ \\frac{\\mathrm{sgn}\\,b}{\\sqrt{b^{2}-a^{2}}}&\\mathrm{for}\\;\\;a^{2}\\leqslant b^{2} ,\\end{cases} \\tag{13}\\]\n\nwhere we used the fact that the integrand is an even function of \\(k_{y}\\). Here the values \\(a\\) and \\(b\\) are the functions of the variables \\(k_{y}\\) and \\(\\Omega\\) and the parameter \\(m\\) as determined by Eqs. (10).\n\nThe DOS is proportional to the imaginary part of \\(\\mathrm{Tr}G(\\Omega)\\), therefore it is nonzero only in the region of \\(\\Omega\\) where\n\n\\[a^{2}-b^{2}\\leqslant 0. \\tag{14}\\]\n\nOutside the region (14) \\(\\mathrm{Tr}G\\) is real and the DOS is equal to zero, i.e., \\(\\rho(\\Omega)=0\\).\n\nThe next step in evaluation of Eq. (13) is to perform integration over \\(k_{y}\\). It is convenient to replace \\(y=\\cos k_{y}\\) and \\(dk_{y}=-dy(1-y^{2})^{-1/2}\\). The boundaries of the integration are \\(-1\\leqslant y\\leqslant 1\\), where changing the order of the integration boundaries results an additional minus sign. The integration over \\(y\\) is performed differently depending on the value of the parameter \\(m\\). Therefore, in what follows we are considering three cases with \\(|m|=1\\), \\(|m|>1\\), and \\(|m|<1\\), separately.\n\n#### ii.2.1 Case \\(|m|=1\\)\n\nCalculations are simpler in the special case of \\(|m|=1\\). The function in the denominator of the integral (13) can be written explicitly as \\(a^{2}-b^{2}=(\\Omega^{2}-1)(\\Omega^{2}-5-4my)\\). It is linear in \\(y\\) and changes the sign only once at the point \\(y=y_{0}\\,\\mathrm{sgn}\\,m\\), where \\(y_{0}=(\\Omega^{2}-5)/4\\). Solving the condition (14) with respect to \\(\\Omega\\) and using the fact that \\(|y|\\leq 1\\) we obtain that DOS is nonzero only for \\(1\\leq|\\Omega|\\leq 3\\).\n\nThe condition (14) considered with respect to \\(y\\) determines the boundaries of integration in Eq. (13). For \\(m=1\\) it is satisfied for \\(y_{0}\\leq y\\leq 1\\).Then the imaginary part of Eq. (13) in terms of variable \\(y\\) gives the DOS in the implicit form\n\n\\[\\rho( "], "nougat": ["taking \\(\\Omega\\to\\Omega+\\imath 0^{+}\\), where we add infinitesimally small imaginary part as it is required in the retarded Green\u2019s function. Then \\(a=(\\Omega+\\imath 0^{+})^{2}+...\\) gets small imaginary part \\(2\\imath\\Omega 0^{+}\\) defining the value of csgn. Taking the limit of infinitisimal \\(0^{+}\\) we arrive at the following expression for the trace of the Green\u2019s function\n\n\\[\\mathrm{Tr}\\mathbf{G}(\\Omega)=\\frac{2\\Omega}{\\pi}\\int_{0}^{\\pi}dk_{y}\\begin{cases} \\frac{\\mathrm{sgn}\\,a}{\\sqrt{a^{2}-b^{2}}}&\\mathrm{for}\\;\\;a^{2}>b^{2},\\\\ \\frac{\\mathrm{sgn}\\,b}{\\sqrt{b^{2}-a^{2}}}&\\mathrm{for}\\;\\;a^{2}\\leqslant b^{2} ,\\end{cases} \\tag{13}\\]\n\nwhere we used the fact that the integrand is an even function of \\(k_{y}\\). Here the values \\(a\\) and \\(b\\) are the functions of the variables \\(k_{y}\\) and \\(\\Omega\\) and the parameter \\(m\\) as determined by Eqs. (10).\n\nThe DOS is proportional to the imaginary part of \\(\\mathrm{Tr}G(\\Omega)\\), therefore it is nonzero only in the region of \\(\\Omega\\) where\n\n\\[a^{2}-b^{2}\\leqslant 0. \\tag{14}\\]\n\nOutside the region (14) \\(\\mathrm{Tr}G\\) is real and the DOS is equal to zero, i.e., \\(\\rho(\\Omega)=0\\).\n\nThe next step in evaluation of Eq. (13) is to perform integration over \\(k_{y}\\). It is convenient to replace \\(y=\\cos k_{y}\\) and \\(dk_{y}=-dy(1-y^{2})^{-1/2}\\). The boundaries of the integration are \\(-1\\leqslant y\\leqslant 1\\), where changing the order of the integration boundaries results an additional minus sign. The integration over \\(y\\) is performed differently depending on the value of the parameter \\(m\\). Therefore, in what follows we are considering three cases with \\(|m|=1\\), \\(|m|>1\\), and \\(|m|<1\\), separately.\n\n#### ii.2.1 Case \\(|m|=1\\)\n\nCalculations are simpler in the special case of \\(|m|=1\\). The function in the denominator of the integral (13) can be written explicitly as \\(a^{2}-b^{2}=(\\Omega^{2}-1)(\\Omega^{2}-5-4my)\\). It is linear in \\(y\\) and changes the sign only once at the point \\(y=y_{0}\\,\\mathrm{sgn}\\,m\\), where \\(y_{0}=(\\Omega^{2}-5)/4\\). Solving the condition (14) with respect to \\(\\Omega\\) and using the fact that \\(|y|\\leq 1\\) we obtain that DOS is nonzero only for \\(1\\leq|\\Omega|\\leq 3\\).\n\nThe condition (14) considered with respect to \\(y\\) determines the boundaries of integration in Eq. (13). For \\(m=1\\) it is satisfied for \\(y_{0}\\leq y\\leq 1\\).Then the imaginary part of Eq. (13) in terms of variable \\(y\\) gives the DOS in the implicit form\n\n\\[\\rho( "]}, {"edit": ["In the context of high-order schemes, mathematical frameworks have been developed in order to construct schemes of arbitrary spatial discretization order [4, 5]. Among the well-documented ones are the Discontinuous Galerkin [6, 7], the Spectral Differences [8] and the Spectral Volumes [9] class of schemes. More recently, another approach for constructing high-order schemes has been introduced in the literature: the Flux Reconstruction [10], also referred to as FR. This framework is capable of creating compact high-order schemes for solving partial differential equations. Special focus was given to the treatment of advection terms [10] and diffusion terms [11], which are integral components of high-fidelity mathematical models used in fluid dynamics, such as the Navier-Stokes equations.\n\nIn the Flux Reconstruction approach, the solution is known at multiple discrete points, also known as nodes, within a discrete domain element: a cell. A continuous solution is reconstructed within the cell by performing an interpolation using a basis of Lagrange polynomials. In general, the overall resulting solution will be continuous only within a cell, usually displaying discontinuities across the interfaces between two adjacent cells. Therefore, if nothing else is done, there is no interaction between nearby cells. The main idea behind the FR framework is to introduce means for information to propagate through the domain. This is done by defining common values for the property fluxes (and its derivatives, if needed) across a cell interface. For the advection terms, the interface fluxes are usually reconstructed in an upwind manner, with the Roe flux [12] being a popular procedure to be used. For the diffusion terms, the common fluxes are taken to be, in its most general case, a weighted average between the fluxes of the immediate adjacent cells. A corrected, continuous, flux function can, then, be reconstructed within a cell, in such a way that the previously defined common interface values are respected. Finally, the time-derivatives of the solution properties can be evaluated by using the corrected fluxes and, then, integrated by using an appropriate time-march scheme selected by the user.\n\nAn interesting characteristic of the FR framework is that the continuous reconstruction of the flux terms is performed by using a special set of functions, called \"correction functions\", and, depending on the function(s) used, different high-order schemes are achieved. For instance, schemes such as the nodal Discontinuous Galerkin [6] and the Staggered Grid [13] can be recovered by using the Flux Reconstruction in combination with an appropriate correction function. One important step that must be made towards the comprehension of the properties of a scheme is the assessment of its stability bounds and effective order of accuracy. In the case of the FR framework, part of this process has already been done by Huynh [10] and documented for a limited number of scheme orders. The present paper aims to expand this analysis by using the FR framework with 5 different correction functions and using cells with up to 10 internal nodes. The resulting schemes are, then, used to solve the 1-D advection model equation. Observations are done regarding the effects of the artificial dissipation over the transient solution, obtained by using a forth-order Runge-Kutta time-marching procedure (RK4). Further insights regarding the stability and accuracy of the resulting schemes are obtained by performing a Fourier analysis on each one of them.\n\n## 2 Numerical Formulation\n\nIn this section, a brief explanation is given regarding the Flux Reconstruction approach for the construction of high-order schemes. The intention is to give the reader sufficient background knowledge regarding the inner workings of the FR approach before performing the Fourier stability analysis in the next section.\n\nThe equation of interest is the initial value problem (IVP) given by the 1-D advection model equation:\n\n\\[\\begin{cases}\\frac{\\partial u}{\\partial t}+\\frac{\\partial f}{\\partial x}=0,\\quad \\text{ with }\\quad f=au\\\\ u(x,t)|_{t=0}=u_{0}(x)\\end{cases} \\tag{1}\\]\n\nwhere \\(t\\) is the time coordinate, \\(x\\) is the space coordinate, \\(u(x,t)\\) is the property being transported by a wave, whose dynamics is given by Eq. (1), with constant speed \\(a\\). Finally, \\(f\\) is the flux term and the main subject of interest to the FR approach. The discrete domain can be constructed by dividing its continuous counterpart into multiple cells \\(E\\), where the \\(j\\)-th cell is denoted by \\(E_{j}\\) and has a length \\(h "], "nougat": ["In the context of high-order schemes, mathematical frameworks have been developed in order to construct schemes of arbitrary spatial discretization order [4, 5]. Among the well-documented ones are the Discontinuous Galerkin [6, 7], the Spectral Differences [8] and the Spectral Volumes [9] class of schemes. More recently, another approach for constructing high-order schemes has been introduced in the literature: the Flux Reconstruction [10], also referred to as FR. This framework is capable of creating compact high-order schemes for solving partial differential equations. Special focus was given to the treatment of advection terms [10] and diffusion terms [11], which are integral components of high-fidelity mathematical models used in fluid dynamics, such as the Navier-Stokes equations.\n\nIn the Flux Reconstruction approach, the solution is known at multiple discrete points, also known as nodes, within a discrete domain element: a cell. A continuous solution is reconstructed within the cell by performing an interpolation using a basis of Lagrange polynomials. In general, the overall resulting solution will be continuous only within a cell, usually displaying discontinuities across the interfaces between two adjacent cells. Therefore, if nothing else is done, there is no interaction between nearby cells. The main idea behind the FR framework is to introduce means for information to propagate through the domain. This is done by defining common values for the property fluxes (and its derivatives, if needed) across a cell interface. For the advection terms, the interface fluxes are usually reconstructed in an upwind manner, with the Roe flux [12] being a popular procedure to be used. For the diffusion terms, the common fluxes are taken to be, in its most general case, a weighted average between the fluxes of the immediate adjacent cells. A corrected, continuous, flux function can, then, be reconstructed within a cell, in such a way that the previously defined common interface values are respected. Finally, the time-derivatives of the solution properties can be evaluated by using the corrected fluxes and, then, integrated by using an appropriate time-march scheme selected by the user.\n\nAn interesting characteristic of the FR framework is that the continuous reconstruction of the flux terms is performed by using a special set of functions, called \u201ccorrection functions\u201d, and, depending on the function(s) used, different high-order schemes are achieved. For instance, schemes such as the nodal Discontinuous Galerkin [6] and the Staggered Grid [13] can be recovered by using the Flux Reconstruction in combination with an appropriate correction function. One important step that must be made towards the comprehension of the properties of a scheme is the assessment of its stability bounds and effective order of accuracy. In the case of the FR framework, part of this process has already been done by Huynh [10] and documented for a limited number of scheme orders. The present paper aims to expand this analysis by using the FR framework with 5 different correction functions and using cells with up to 10 internal nodes. The resulting schemes are, then, used to solve the 1-D advection model equation. Observations are done regarding the effects of the artificial dissipation over the transient solution, obtained by using a forth-order Runge-Kutta time-marching procedure (RK4). Further insights regarding the stability and accuracy of the resulting schemes are obtained by performing a Fourier analysis on each one of them.\n\n## 2 Numerical Formulation\n\nIn this section, a brief explanation is given regarding the Flux Reconstruction approach for the construction of high-order schemes. The intention is to give the reader sufficient background knowledge regarding the inner workings of the FR approach before performing the Fourier stability analysis in the next section.\n\nThe equation of interest is the initial value problem (IVP) given by the 1-D advection model equation:\n\n\\[\\begin{cases}\\frac{\\partial u}{\\partial t}+\\frac{\\partial f}{\\partial x}=0, \\quad\\text{with}\\quad f=au\\\\ u(x,t)|_{t=0}=u_{0}(x)\\end{cases}\\] (1), with constant speed \\[a\\] .\n\nwhere \\(t\\) is the time coordinate, \\(x\\) is the space coordinate, \\(u(x,t)\\) is the property being transported by a wave, whose dynamics is given by Eq. (1), with constant speed \\(a\\). Finally, \\(f\\) is the flux term and the main subject of interest to the FR approach. The discrete domain can be constructed by dividing its continuous counterpart into multiple cells \\(E\\), where the \\(j\\)-th cell is denoted by \\(E_{j}\n\n "]}, {"edit": ["further, the time scales becomes sequenced as\n\n\\[t_{c}-\\hat{t}<t_{i}<t_{f}<t_{c}+\\hat{t}, \\tag{48}\\]\n\nwhere the system enters into the S regime. The scenario of the adiabatic-impulse approximation is illustrated in Fig. 7.\n\n###### Acknowledgements.\n\n We thank Yan He for useful discussion. This work is supported by NSFC under Grants No. 11074177.\n\n## Appendix A Solution of TDBdG equations\n\nWe can solve the TDBdG equations given by Eq. (6) exactly by mapping them to the Landau-Zener problem. Then, the time-dependent Bogoliubov coefficients can be given by\n\n\\[v_{q}(z)= C_{1}D_{-s_{q}-1}(iz)+C_{2}D_{-s_{q}-1}(-iz), \\tag{49}\\] \\[u_{q}(z)= \\frac{e^{i\\pi/4}}{\\sqrt{\\tau_{Q}}\\sin q}\\left(i\\frac{\\mathrm{d}}{ \\mathrm{d}z}+\\frac{iz}{2}\\right)v_{q}(z), \\tag{50}\\]\n\nwith free complex parameters \\(C_{1}\\) and \\(C_{2}\\). Here, \\(D_{m}(z)\\) is the complex parabolic cylinder function, \\(z=2\\sqrt{\\tau_{Q}}\\left(\\frac{t}{\\tau_{Q}}+\\cos q\\right)e^{i\\pi/4}\\), and \\(s_{q}=-i\\tau_{Q}\\sin^{2}q\\). To reduce the above rigorous solution, we need to apply the asymptotes of \\(D_{m}(z)\\) that are given by [89]\n\n\\[D_{m}(z)=e^{-z^{2}/4}z^{m},\\ \\forall|\\arg(z)|<3\\pi/4, \\tag{51}\\]\n\n\\[D_{m}(z)= e^{-z^{2}/4}z^{m}-\\frac{\\sqrt{2\\pi}}{\\Gamma(-m)}e^{-im\\pi}e^{z^{2 }/4}z^{-m-1},\\] \\[\\forall-5\\pi/4<\\arg(z)<-\\pi/4, \\tag{52}\\]\n\nfor \\(|z|\\gg 1\\) and\n\n\\[D_{m}(z)=\\frac{2^{m/2}\\sqrt{\\pi}}{\\Gamma(\\frac{1}{2}-\\frac{m}{2})}-\\frac{2^{ \\frac{1}{2}+\\frac{m}{2}}\\sqrt{\\pi}z}{\\Gamma(-\\frac{m}{2})}+O(z^{2}), \\tag{53}\\]\n\nfor \\(|z|\\to 0\\).\n\nFurthermore, in numerical simulations, the time-dependent parameter should start at a finite value. We choose a sufficiently large but finite initial transverse field, so the initial conditions of Eqs. (49) and (50) can be expanded into a powers of \\(1/g_{i}\\),\n\n\\[u_{q}(t_{i})^{2} =1-\\frac{\\sin^{2}q}{4g_{i}^{2}}+O(\\frac{1}{g_{i}^{3}}), \\tag{54}\\] \\[v_{q}(t_{i})^{2} =1-u_{q}(t_{i})^{2}. \\tag{55}\\]\n\nBased on this approximation, the two constants, \\(C_{1}\n\n "], "nougat": ["further, the time scales becomes sequenced as\n\n\\[t_{c}-\\hat{t}<t_{i}<t_{f}<t_{c}+\\hat{t}, \\tag{48}\\]\n\nwhere the system enters into the S regime. The scenario of the adiabatic-impulse approximation is illustrated in Fig. 7 .\n\n###### Acknowledgements.\n\n We thank Yan He for useful discussion. This work is supported by NSFC under Grants No. 11074177.\n\n## Appendix A Solution of TDBdG equations\n\nWe can solve the TDBdG equations given by Eq. ( 6 ) exactly by mapping them to the Landau-Zener problem. Then, the time-dependent Bogoliubov coefficients can be given by\n\n\\[v_{q}(z)= C_{1}D_{-s_{q}-1}(iz)+C_{2}D_{-s_{q}-1}(-iz), \\tag{49}\\] \\[u_{q}(z)= \\frac{e^{i\\pi/4}}{\\sqrt{\\tau_{Q}}\\sin q}\\left(i\\frac{\\mathrm{d}}{ \\mathrm{d}z}+\\frac{iz}{2}\\right)v_{q}(z), \\tag{50}\\]\n\nwith free complex parameters \\(C_{1}\\) and \\(C_{2}\\). Here, \\(D_{m}(z)\\) is the complex parabolic cylinder function, \\(z=2\\sqrt{\\tau_{Q}}\\left(\\frac{t}{\\tau_{Q}}+\\cos q\\right)e^{i\\pi/4}\\), and \\(s_{q}=-i\\tau_{Q}\\sin^{2}q\\). To reduce the above rigorous solution, we need to apply the asymptotes of \\(D_{m}(z)\\) that are given by [ 89 ]\n\n\\[D_{m}(z)=e^{-z^{2}/4}z^{m},\\ \\forall|\\arg(z)|<3\\pi/4, \\tag{51}\\]\n\n\\[D_{m}(z)= e^{-z^{2}/4}z^{m}-\\frac{\\sqrt{2\\pi}}{\\Gamma(-m)}e^{-im\\pi}e^{z^{2 }/4}z^{-m-1},\\] \\[\\forall-5\\pi/4<\\arg(z)<-\\pi/4, \\tag{52}\\]\n\nfor \\(|z|\\gg 1\\) and\n\n\\[D_{m}(z)=\\frac{2^{m/2}\\sqrt{\\pi}}{\\Gamma(\\frac{1}{2}-\\frac{m}{2})}-\\frac{2^{ \\frac{1}{2}+\\frac{m}{2}}\\sqrt{\\pi}z}{\\Gamma(-\\frac{m}{2})}+O(z^{2}), \\tag{53}\\]\n\nfor \\(|z|\\to 0\\).\n\nFurthermore, in numerical simulations, the time-dependent parameter should start at a finite value. We choose a sufficiently large but finite initial transverse field, so the initial conditions of Eqs. ( A1 ) and ( A2 ) can be expanded into a powers of \\(1/g_{i}\\),\n\n\\[u_{q}(t_{i})^{2} =1-\\frac{\\sin^{2}q}{4g_{i}^{2}}+O(\\frac{1}{g_{i}^{3}}), \\tag{54}\\] \\[v_{q}(t_{i})^{2} =1-u_{q}(t_{i})^{2}. \\tag{55}\\]\n\nBased on this approximation "]}, {"edit": ["\n\n**Proposition 2**.: _Given integers \\(l\\) and \\(k\\) where \\(l,k\\geq 2\\), the shortest negative cycle of \\(\\widehat{BQ}(\\ell,2k-1)\\) is of length \\(\\min\\{2l,2k\\}\\)._\n\nProof.: We first present two natural choices for a negative cycle, one of length \\(2k\\) and another of length \\(2l\\). The first is a negative cycle on the first two layers. Take a positive edge and connect its two ends with one of the two paths using only the negative edges that connect the two layers. This would result in a negative cycle of length \\(2k\\). The second negative cycle we consider is by taking a positive edge and connecting each of its ends to the vertex \\(u\\) by a shortest path (all edges negative). One of these paths will be of length \\(l\\) and the other would be of length \\(l-1\\). Together with the first chosen edge itself then, they form a negative cycle of length \\(2l\\).\n\nIt remains to show that the shortest of these two types of cycles gives us the negative girth. To that end, we will first show that a shortest negative cycle can only use one positive edge of \\(\\widehat{BQ}(\\ell,2k-1)\\). Towards a contradiction, let \\(C\\) be a negative cycle with more than two positive edges. We aim to present a negative cycle \\(C^{\\prime}\\) whose length is at most \\(|C|-2\\). We take two positive edges of \\(C\\) that come consecutively on the cyclic order. Assume \\(xy\\) and \\(x^{\\prime}y^{\\prime}\\) are these two edges and that \\(x^{\\prime}\\) is followed by \\(y\\) in the cyclic order of \\(C\\) (that is to say, there is no positive edge in the \\(x^{\\prime}-y\\) path in \\(C\\)). We remove the two positive edges \\(xy\\) and \\(x^{\\prime}y^{\\prime}\\) and the \\(x^{\\prime}y\\) path connecting them in \\(C\\), but then we add a \\(xy^{\\prime}\\) copy of this path (which also has no positive edge). The result is a closed walk whose sign is the same as that of \\(C\\), and whose length is \\(|C|-2\\). But then this closed walk must contain a negative cycle, whose length then is also at most \\(|C|-2\\), a contradiction.\n\nFigure 4: \\(\\widehat{BQ}(2,3)\\), presented two different ways.\n\nFigure 3: Construction of \\(\\widehat{BQ}(\\ell,2k-1)\\).\n\n"], "nougat": ["\n\n**Proposition 2**.: _Given integers \\(l\\) and \\(k\\) where \\(l,k\\geq 2\\), the shortest negative cycle of \\(\\widehat{BQ}(\\ell,2k-1)\\) is of length \\(\\min\\{2l,2k\\}\\)._\n\nProof.: We first present two natural choices for a negative cycle, one of length \\(2k\\) and another of length \\(2l\\). The first is a negative cycle on the first two layers. Take a positive edge and connect its two ends with one of the two paths using only the negative edges that connect the two layers. This would result in a negative cycle of length \\(2k\\). The second negative cycle we consider is by taking a positive edge and connecting each of its ends to the vertex \\(u\\) by a shortest path (all edges negative). One of these paths will be of length \\(l\\) and the other would be of length \\(l-1\\). Together with the first chosen edge itself then, they form a negative cycle of length \\(2l\\).\n\nIt remains to show that the shortest of these two types of cycles gives us the negative girth. To that end, we will first show that a shortest negative cycle can only use one positive edge of \\(\\widehat{BQ}(\\ell,2k-1)\\). Towards a contradiction, let \\(C\\) be a negative cycle with more than two positive edges. We aim to present a negative cycle \\(C^{\\prime}\\) whose length is at most \\(|C|-2\\). We take two positive edges of \\(C\\) that come consecutively on the cyclic order. Assume \\(xy\\) and \\(x^{\\prime}y^{\\prime}\\) are these two edges and that \\(x^{\\prime}\\) is followed by \\(y\\) in the cyclic order of \\(C\\) (that is to say, there is no positive edge in the \\(x^{\\prime}-y\\) path in \\(C\\)). We remove the two positive edges \\(xy\\) and \\(x^{\\prime}y^{\\prime}\\) and the \\(x^{\\prime}y\\) path connecting them in \\(C\\), but then we add a \\(xy^{\\prime}\\) copy of this path (which also has no positive edge). The result is a closed walk whose sign is the same as that of \\(C\\), and whose length is \\(|C|-2\\). But then this closed walk must contain a negative cycle, whose length then is also at most \\(|C|-2\\), a contradiction.\n\nFigure 4: \\(\\widehat{BQ}(2,3)\\), presented two different ways.\n\nFigure 3: Construction of \\(\\widehat{BQ}(\\ell,2k-1)\\).\n\n"]}, {"edit": ["For a set \\(E\\), \\(\\overset{\\circ}{E}\\) denotes its interior and \\(\\overline{E}\\) its closure.\n\nFor \\(E\\subseteq d\\), \\(\\mathcal{C}_{\\overline{E}}[0,\\infty)\\) denotes the set of continuous functions from \\([0,\\infty)\\) to \\(\\overline{E}\\) and \\(\\mathcal{D}_{\\overline{E}}[0,\\infty)\\) denotes the set of cadlag functions from \\([0,\\infty)\\) to \\(\\overline{E}\\).\n\nFor \\(E\\subseteq d\\), \\(\\mathcal{P}(\\overline{E})\\) denotes the set of probability measures on \\(\\overline{E}\\). For a stochastic process \\(Z\\), \\(\\{\\mathcal{F}_{t}^{Z}\\}\\) denotes the filtration generated by \\(Z\\), i.e. \\(\\mathcal{F}_{t}^{Z}:=\\sigma\\{Z(s),\\,s\\leq t\\}\\).\n\n## 2 Formulation of the problem and preliminaries\n\nLet \\(\\mathcal{W}\\subseteq d\\) be a piecewise smooth cone with vertex at the origin i.e.\n\n\\[\\mathcal{W}:=\\bigcap_{j=1}^{m}\\mathcal{W}_{j},\\quad\\mathcal{W}_{j}:=\\{x=rz,\\, z\\in\\mathcal{S}_{j},\\,r>0\\}, \\tag{1}\\]\n\nwhere \\(\\mathcal{S}_{j}\\) is a nonempty domain in the unit sphere \\(S^{d-1}\\) with \\(\\mathcal{C}^{2}\\) boundary. Clearly\n\n\\[\\mathcal{W}=\\{x=rz,\\,z\\in\\mathcal{S},\\,r>0\\},\\quad\\text{where }\\mathcal{S}:= \\bigcap_{j=1}^{m}\\mathcal{S}_{j}. \\tag{2}\\]\n\nIt is supposed that\n\n\\[\\overline{\\mathcal{S}}=\\bigcap_{j=1}^{m}\\overline{\\mathcal{S}_{j}}.\\]\n\nThe object of this work is the semimartingale obliquely reflecting Brownian motion (ORBM) in \\(\\overline{\\mathcal{W}}\\) with drift \\(b\\), dispersion matrix \\(\\sigma\\) and radially constant direction of reflection \\(g^{j}\\) on each face, i.e. for \\(x\\in\\partial\\mathcal{W}_{j}-\\{0\\}\\), \\(x=rz\\), \\(z\\in\\partial\\mathcal{S}_{j}\\),\n\n\\[g^{j}(x)=g^{j}(z), \\tag{3}\\]\n\nfor some unit vector field \\(g^{j}\\) defined on \\(\\partial\\mathcal{S}_{j}\\). This process can be defined as the solution of the following stochastic differential equation with reflection:\n\n\\[\\begin{array}{l}X(t)=X(0)+b\\,t+\\sigma\\,W(t)+\\int_{0}^{t}\\gamma(s)\\,d\\lambda( s),\\quad t\\geq 0,\\\\ \\gamma(t)\\in G(X(t)),\\quad|\\gamma(t)|=1,\\quad d\\lambda-a.e.,\\quad t\\geq 0,\\\\ X(t)\\in\\overline{\\mathcal{W}},\\quad\\lambda(t)=\\int_{0}^{t}11_{\\partial \\mathcal{W}}(X(s))d\\lambda(s),\\quad t\\geq 0,\\end{array} \\tag{4}\\]\n\nwhere\n\n\\ "], "nougat": ["For a set \\(E\\), \\(\\overset{\\circ}{E}\\) denotes its interior and \\(\\overline{E}\\) its closure.\n\nFor \\(E\\subseteq d\\), \\(\\mathcal{C}_{\\overline{E}}[0,\\infty)\\) denotes the set of probability measures on \\(\\overline{E}\\).\n\nFor \\(E\\subseteq d\\), \\(\\mathcal{P}(\\overline{E})\\) denotes the set of probability measures on \\(\\overline{E}\\). For a stochastic process \\(Z\\), \\(\\{\\mathcal{F}_{t}^{Z}\\}\\) denotes the filtration generated by \\(Z\\), i.e. \\(\\mathcal{F}_{t}^{Z}:=\\sigma\\{Z(s),\\,s\\leq t\\}\\).\n\n## 2 Formulation of the problem and preliminaries\n\nLet \\(\\mathcal{W}\\subseteq d\\) be a piecewise smooth cone with vertex at the origin i.e.\n\n\\[\\mathcal{W}:=\\bigcap_{j=1}^{m}\\mathcal{W}_{j},\\quad\\mathcal{W}_{j}:=\\{x=rz,\\, z\\in\\mathcal{S}_{j},\\,r>0\\}, \\tag{1}\\]\n\nwhere \\(\\mathcal{S}_{j}\\) is a nonempty domain in the unit sphere \\(S^{d-1}\\) with \\(\\mathcal{C}^{2}\\) boundary. Clearly\n\n\\[\\mathcal{W}=\\{x=rz,\\,z\\in\\mathcal{S},\\,r>0\\},\\quad\\text{where }\\mathcal{S}:= \\bigcap_{j=1}^{m}\\mathcal{S}_{j}. \\tag{2}\\]\n\nIt is supposed that\n\n\\[\\overline{\\mathcal{S}}=\\bigcap_{j=1}^{m}\\overline{\\mathcal{S}_{j}}.\\]\n\nThe object of this work is the semimartingale obliquely reflecting Brownian motion (ORBM) in \\(\\overline{\\mathcal{W}}\\) with drift \\(b\\), dispersion matrix \\(\\sigma\\) and radially constant direction of reflection \\(g^{j}\\) on each face, i.e. for \\(x\\in\\partial\\mathcal{W}_{j}-\\{0\\}\\), \\(x=rz\\), \\(z\\in\\partial\\mathcal{S}_{j}\\),\n\n\\[g^{j}(x)=g^{j}(z), \\tag{3}\\]\n\nfor some unit vector field \\(g^{j}\\) defined on \\(\\partial\\mathcal{S}_{j}\\). This process can be defined as the solution of the following stochastic differential equation with reflection:\n\n\\[\\begin{array}{l}X(t)=X(0) := the closed convex cone generated by \\big{\\{}g^{j}(x),\\;x\\in\\big{(}\\partial\\mathcal{W}_{j}\\cap\\overline{\\mathcal{W}} \\big{)}-\\{0\\},\\;j=1,\\cdots,m\\big{\\}}.\\end{array}\\] "]}, {"edit": ["\n\n**Lemma 1.1.**_[0] If \\(G\\) is a graph such that for \\(u,v\\in V(G)\\), \\(uv\\notin E(G)\\), then \\(\\rho(G)<\\rho(G+uv)\\)._\n\nLet \\(A\\) be a real symmetric matrix whose rows and columns are indexed by \\(V=\\{1,2,\\cdots,n\\}\\). Let \\(\\{V_{1},V_{2},\\cdots,V_{k}\\}\\) be a partition of \\(V\\) such that the block partition of the matrix \\(A\\) according to \\(\\{V_{1},V_{2},\\cdots,V_{k}\\}\\) can be expressed as\n\n\\[A=\\begin{bmatrix}A_{11}&A_{12}&\\ldots&A_{1k}\\\\ A_{21}&A_{22}&\\ldots&A_{2k}\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\ A_{k1}&A_{k2}&\\ldots&A_{kk}\\end{bmatrix},\\]\n\nwhere \\(A_{ij}\\) denotes the block formed by intersection of the rows in \\(V_{i}\\) and the columns in \\(V_{j}\\). Let \\(q_{ij}(A)\\) denote the average row sum of \\(A_{ij}\\). Then, the quotient matrix of \\(A\\) with respect to the partition \\(\\{V_{1},V_{2},\\cdots,V_{k}\\}\\) is given by\n\n\\[Q(A)=[q_{ij}(A)].\\]\n\nMoreover, if the row sum of each block \\(A_{ij}\\) is constant then we say that the partition is equitable and \\(Q(A)\\) is called an equitable quotient matrix of \\(A\\). There is a nice relation between the spectrum of \\(A\\) and that of \\(Q(A)\\), which is stated now as a theorem.\n\n**Theorem 1.2.**_[0] Let \\(A\\) be a real symmetric matrix such that it has an equitable quotient matrix \\(Q(A)\\), then, \\(\\sigma(Q(A))\\subset\\sigma(Q(A))\\). Moreover, if \\(A\\) is nonnegative, then \\(\\rho(A)=\\rho(Q(A))\\), i.e., the spectral radius of \\(Q(A)\\) is actually the spectral radius of \\(A\\)._\n\nA vertex \\(v\\) of a connected graph \\(G\\) is a cut vertex of \\(G\\) if \\(G-v\\) is disconnected. A block of the graph \\(G\\) is a maximal connected subgraph of G that has no cut-vertex. Given two blocks \\(F\\) and \\(H\\) of graph \\(G\\) are said to be adjacent if they are connected via a cut-vertex. We denote \\(F\\vartriangleleft H\\), to represent the induced subgraph on the vertex set of two adjacent blocks \\(F\\) and \\(H\\).\n\nA complete graph is a graph where each vertex is adjacent to every other vertex. A complete graph on \\(n\\) vertices is denoted by \\(K_{n}\\). A connected graph is called a clique tree if each of its blocks is a clique. Let \\(G\\) be a clique tree with \\(d_{1}\\) blocks of \\(K_{n_{1}}\\), \\(d_{2}\\) blocks of \\(K_{n_{2}}\\), so on up to \\(d_{b}\\) blocks of \\(K_{n_{b}}\\), then we write \\(G\\) with blocks \\(K_{n_{1}}^{(d_{1}\n\n"], "nougat": ["\n\n**Lemma 1.1.**_[0] If \\(G\\) is a graph such that for \\(u,v\\in V(G)\\), \\(uv\\notin E(G)\\), then \\(\\rho(G)<\\rho(G+uv)\\)._\n\nLet \\(A\\) be a real symmetric matrix whose rows and columns are indexed by \\(V=\\{1,2,\\cdots,n\\}\\). Let \\(\\{V_{1},V_{2},\\cdots,V_{k}\\}\\) be a partition of \\(V\\) such that the block partition of the matrix \\(A\\) according to \\(\\{V_{1},V_{2},\\cdots,V_{k}\\}\\) can be expressed as\n\n\\[A=\\begin{bmatrix}A_{11}&A_{12}&\\ldots&A_{1k}\\\\ A_{21}&A_{22}&\\ldots&A_{2k}\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\ A_{k1}&A_{k2}&\\ldots&A_{kk}\\end{bmatrix},\\]\n\nwhere \\(A_{ij}\\) denotes the block formed by intersection of the rows in \\(V_{i}\\) and the columns in \\(V_{j}\\). Let \\(q_{ij}(A)\\) denote the average row sum of \\(A_{ij}\\). Then, the quotient matrix of \\(A\\) with respect to the partition \\(\\{V_{1},V_{2},\\cdots,V_{k}\\}\\) is given by\n\n\\[Q(A)=[q_{ij}(A)].\\]\n\nMoreover, if the row sum of each block \\(A_{ij}\\) is constant then we say that the partition is equitable and \\(Q(A)\\) is called an equitable quotient matrix of \\(A\\). There is a nice relation between the spectrum of \\(A\\) and that of \\(Q(A)\\), which is stated now as a theorem.\n\n**Theorem 1.2. [ 0 ] Let \\(A\\) be a real symmetric matrix such that it has an equitable quotient matrix \\(Q(A)\\), then, \\(\\sigma(Q(A))\\subset\\sigma(Q(A))\\). Moreover, if \\(A\\) is nonnegative, then \\(\\rho(A)=\\rho(Q(A))\\), i.e., the spectral radius of \\(Q(A)\\) is actually the spectral radius of \\(A\\).**\n\nA vertex \\(v\\) of a connected graph \\(G\\) is a cut vertex of \\(G\\) if \\(G-v\\) is disconnected. A block of the graph \\(G\\) is a maximal connected subgraph of G that has no cut-vertex. Given two blocks \\(F\\) and \\(H\\) of graph \\(G\\) are said to be adjacent if they are connected via a cut-vertex. We denote \\(F\\vartriangleleft H\\), to represent the induced subgraph on the vertex set of two adjacent blocks \\(F\\) and \\(H\\).\n\nA complete graph is a graph where each vertex is adjacent to every other vertex. A complete graph on \\(n\\) vertices is denoted by \\(K_{n}\\). A connected graph is called a clique tree if each of its blocks is a clique. Let \\(G\\) be a clique tree with \\(d_{1}\\) blocks of \\(K_{n_{1}}\\), \\(d_{2}\\) blocks of \\(K_{n_{2}}\\), so on up to \\(d_{b}\\) blocks of \\(K_{n_{b}}\\), then we write \\(G\\) with blocks \\(K_{n_{1}}^{(d_{1"]}, {"edit": ["where \\(\\rho_{w}\\) is the density of water and \\(C\\) the ocean function that equals one where water is present and zero otherwise. In the case of ice loading, the direct term is given by\n\n\\[\\zeta=\\rho_{i}(1-C)\\Delta I, \\tag{2.12}\\]\n\nwhere \\(\\rho_{i}\\) is the density of ice, and \\(\\Delta I\\) the change in ice thickness. The factor, \\(1-C\\), within this expression accounts for the possibility of floating ice (e.g. Crawford et al. 2018, equations 31-34).\n\nConsider again a pair of solutions \\((\\mathbf{u},\\phi)\\) and \\((\\mathbf{u}^{\\dagger},\\phi^{\\dagger})\\) of the loading problem associated, respectively, to with loads \\(\\sigma\\) and \\(\\sigma^{\\dagger}\\). Here, however, we assume that these loads are decomposed as in eq.(2.11) into water and direct terms that share a common ocean function. From the above expression for sea level change we can write\n\n\\[\\mathbf{u}\\cdot\\nabla\\Phi+\\phi=-g\\,\\Delta SL+\\Phi_{g},\\quad\\mathbf{u}^{\\dagger }\\cdot\\nabla\\Phi+\\phi^{\\dagger}=-g\\,\\Delta SL^{\\dagger}+\\Phi_{g}^{\\dagger}, \\tag{2.13}\\]\n\nand hence eq.(2.8) becomes\n\n\\[\\int_{\\partial M}(-g\\,\\Delta SL^{\\dagger}+\\Phi_{g}^{\\dagger})\\,\\sigma\\,{\\rm d }S=\\int_{\\partial M}(-g\\,\\Delta SL+\\Phi_{g})\\,\\sigma^{\\dagger}\\,{\\rm d}S. \\tag{2.14}\\]\n\nThe terms involving the constants \\(\\Phi_{g}\\) and \\(\\Phi_{g}^{\\dagger}\\) vanish due to conservation of mass, and so the identity simplifies to\n\n\\[\\int_{\\partial M}\\Delta SL^{\\dagger}\\,\\sigma\\,{\\rm d}S=\\int_{\\partial M}\\Delta SL \\,\\sigma^{\\dagger}\\,{\\rm d}S. \\tag{2.15}\\]\n\nIf we now substitute into the equality the decompositions of the loads, \\(\\sigma\\) and \\(\\sigma^{\\dagger}\\), we find\n\n\\[\\int_{\\partial M}\\Delta SL^{\\dagger}(\\rho_{w}C\\,\\Delta SL+\\zeta)\\,\\,{\\rm d}S= \\int_{\\partial M}\\Delta SL\\,(\\rho_{w}C\\,\\Delta SL^{\\dagger}+\\zeta^{\\dagger}) \\,{\\rm d}S, \\tag{2.16}\\]\n\nand cancelling the terms symmetric in \\(\\Delta SL\\) and \\(\\Delta SL^{\\dagger}\\) we arrive at\n\n\\[\\int_{\\partial M}\\Delta SL^{\\dagger}\\,\\zeta\\,\\,{\\rm d}S=\\int_{\\partial M}\\Delta SL \\,\\zeta^{\\dagger}\\,{\\rm d}S. \\tag{2.17}\\]\n\nAgain, this is a known result, being implied as a special case by the adjoint theory of Crawford et al. (2018) for sea level change in a viscoelastic earth model. What is new is the explicit statement as a reciprocity theorem along with the more elementary derivation that has been facilitated by restricting attention to the elastic fingerprint problem.\n\n### Symmetry of the Green's function\n\nBecause the fingerprint problem is linear, its solution must take the form\n\n\\[\\Delta SL(\\mathbf{x})=\\int_{\\partial M}G(\\mathbf{x},\\mathbf{x}^{\\prime})\\zeta( \\mathbf{x}^ "], "nougat": ["where \\(\\rho_{w}\\) is the density of water and \\(C\\) the ocean function that equals one where water is present and zero otherwise. In the case of ice loading, the direct term is given by\n\n\\[\\zeta=\\rho_{i}(1-C)\\Delta I, \\tag{2.12}\\]\n\nwhere \\(\\rho_{i}\\) is the density of ice, and \\(\\Delta I\\) the change in ice thickness. The factor, \\(1-C\\), within this expression accounts for the possibility of floating ice (e.g. Crawford et al. 2018, equations 31\u201334).\n\nConsider again a pair of solutions \\((\\mathbf{u},\\phi)\\) and \\((\\mathbf{u}^{\\dagger},\\phi^{\\dagger})\\) of the loading problem associated, respectively, to with loads \\(\\sigma\\) and \\(\\sigma^{\\dagger}\\). Here, however, we assume that these loads are decomposed as in eq.(2.11) into water and direct terms that share a common ocean function. From the above expression for sea level change we can write\n\n\\[\\mathbf{u}\\cdot\\nabla\\Phi+\\phi=-g\\,\\Delta SL+\\Phi_{g},\\quad\\mathbf{u}^{\\dagger }\\cdot\\nabla\\Phi+\\phi^{\\dagger}=-g\\,\\Delta SL^{\\dagger}+\\Phi_{g}^{\\dagger},\\] (2.8) becomes \\[\\int_{\\partial M}(-g\\,\\Delta SL^{\\dagger}+\\Phi_{g}^{\\dagger})\\,\\sigma\\,{\\rm d }S=\\int_{\\partial M}(-g\\,\\Delta SL+\\Phi_{g})\\,\\sigma^{\\dagger}\\,{\\rm d}S. \\tag{2.9}\\]\n\nThe terms involving the constants \\(\\Phi_{g}\\) and \\(\\Phi_{g}^{\\dagger}\\) vanish due to conservation of mass, and so the identity simplifies to\n\n\\[\\int_{\\partial M}\\Delta SL^{\\dagger}\\,\\sigma\\,{\\rm d}S=\\int_{\\partial M}\\Delta SL \\,\\sigma^{\\dagger}\\,{\\rm d}S. \\tag{2.10}\\]\n\nIf we now substitute into the equality the decompositions of the loads, \\(\\sigma\\) and \\(\\sigma^{\\dagger}\\), we find\n\n\\[\\int_{\\partial M}\\Delta SL^{\\dagger}(\\rho_{w}C\\,\\Delta SL+\\zeta)\\,\\,{\\rm d}S= \\int_{\\partial M}\\Delta SL\\,(\\rho_{w}C\\,\\Delta SL^{\\dagger}+\\zeta^{\\dagger})\\, {\\rm d}S, \\tag{2.11}\\]\n\nand cancelling the terms symmetric in \\(\\Delta SL\\) and \\(\\Delta SL^{\\dagger}\\) we arrive at\n\n\\[\\int_{\\partial M}\\Delta SL^{\\dagger}\\,\\zeta\\,\\,{\\rm d}S=\\int_{\\partial M}\\Delta SL \\,\\zeta^{\\dagger}\\,{\\rm d}S.\\] (2018) for sea level change in a viscoelastic earth model. What is new is the explicit statement as a reciprocity theorem along with the more elementary derivation that has been facilitated by restricting attention to the elastic fingerprint problem.\n\n### Symmetry of the Green\u2019s function\n\nBecause the fingerprint problem is linear, its solution must take the form\n\n\\[\\Delta SL(\\mathbf{x})=\\int_{\\partial M}G(\\mathbf{x},\\mathbf{x}^{\\prime})\\zeta( \\mathbf{x}^{\\prime})\\,{\\rm d}S_{\\mathbf{x}^{\\prime}},\\] (2.17), we take \\[\\Delta SL( "]}, {"edit": ["decline in activity on Stack Overflow.\n\nWe report the estimated effect of our difference-in-differences model in Table 1 and visualize the weekly estimates of the relative change in the Stack Overflow activity in Figure 2. Table 1 indicates that ChatGPT decreased posting activity on Stack Overflow by 15.6% (\\(1-e^{-0.17}\\)). These results are robust to changes in the controls and starting point of the data time series. We also tested for heterogeneity in subsets of the data: considering only questions (rather than counting both questions and answers) and posts on weekdays. In both subsets our estimates did not deviate significantly from the main result: we estimate a 12% relative decrease in questions and 14% relative decrease in posts on weekdays.\n\nFigure 2 shows that the impact of ChatGPT is increasing over time and is by the end of our study greater in magnitude than the average post-ChatGPT effect estimated in Table 1. By the end of April 2023, the estimated effect stabilizes at around 25%. Interestingly, ChatGPT use, in general, peaked around this time.3\n\n\\begin{table}\n\\begin{tabular}{l c c c} \\hline  & (1) & (2) & (3) \\\\  & Number of posts & Number of questions & Weekday posts \\\\ \\hline Stack Overflow \\(\\times\\) Post-GPT & -0.170** & -0.112+ & -0.149* \\\\  & (0.0607) & (0.0619) & (0.0636) \\\\ Observations & 1,150 & 1,150 & 1,150 \\\\ R-squared & 0.995 & 0.994 & 0.993 \\\\ R-squared within & 0.290 & 0.315 & 0.232 \\\\ Outcome mean & 16363 & 7273 & 13191 \\\\ Outcome std. dev. & 29088 & 12661 & 23685 \\\\ \\hline \\end{tabular}\n\\end{table}\nTable 1: Results of a difference-in-differences model, estimating the change in activity observed weekly on Stack Overflow following the release of ChatGPT, relative to activity on four other platforms less likely to have been impacted. All regressions comprise platform fixed effects, week fixed effects, and platform-specific linear time-trends. The standard-error of the estimate clustered on month is reported in parentheses. Significance codes: ***: \\(p<0.001\\), **: \\(p<0.01\\), *: \\(p<0.05\\), +: \\(p<0.1\\).\n\n "], "nougat": ["decline in activity on Stack Overflow.\n\nWe report the estimated effect of our difference-in-differences model in Table 1 and visualize the weekly estimates of the relative change in the Stack Overflow activity in Figure 2 . Table 1 indicates that ChatGPT decreased posting activity on Stack Overflow by 15.6% (\\(1-e^{-0.17}\\)). These results are robust to changes in the controls and starting point of the data time series. We also tested for heterogeneity in subsets of the data: considering only questions (rather than counting both questions and answers) and posts on weekdays. In both subsets our estimates did not deviate significantly from the main result: we estimate a 12% relative decrease in questions and 14% relative decrease in posts on weekdays.\n\nFigure 2 shows that the impact of ChatGPT is increasing over time and is by the end of our study greater in magnitude than the average post-ChatGPT effect estimated in Table 1 . By the end of April 2023, the estimated effect stabilizes at around 25%. Interestingly, ChatGPT use, in general, peaked around this time.3\n\n\\begin{table}\n\\begin{tabular}{l c c c} \\hline  & (1) & (2) & (3) \\\\  & Number of posts & Number of questions & Weekday posts \\\\ \\hline Stack Overflow \\(\\times\\) Post-GPT & -0.170** & -0.112+ & -0.149* \\\\  & (0.0607) & (0.0619) & (0.0636) \\\\ Observations & 1,150 & 1,150 & 1,150 \\\\ R-squared & 0.995 & 0.994 & 0.993 \\\\ R-squared within & 0.290 & 0.315 & 0.232 \\\\ Outcome mean & 16363 & 7273 & 13191 \\\\ Outcome std. dev. & 29088 & 12661 & 23685 \\\\ \\hline \\end{tabular}\n\\end{table}\nTable 1: Results of a difference-in-differences model, estimating the change in activity observed weekly on Stack Overflow following the release of ChatGPT, relative to activity on four other platforms less likely to have been impacted. All regressions comprise platform fixed effects, week fixed effects, and platform-specific linear time-trends. The standard-error of the estimate clustered on month is reported in parentheses. Significance codes: ***: \\(p<0.001\\), **: \\(p<0.01\\), *: \\(p<0.05\\), +: \\(p<0.1\\).\n\n "]}, {"edit": ["A large topographic feature on the surface of the trans-Neptunian object (307261) 2002 MS\\({}_{4}\\) measured from stellar occultations\n\nF. L. Rommel\n\n12.3\n\n F. Braga-Ribas\n\n11.2\n\n J. L. Ortiz\n\n2. B. Sicardy\n\n5. Pantos-Sanz\n\n6.7\n\n J. Desmars\n\n1.2\n\n J. I. B. Camargo\n\n1.2\n\n R. Vieira-Martins\n\n1.2\n\n M. Assafin\n\n8.2\n\n B. E. Morgado\n\n8.2\n\n R. C. Boufleur\n\n1.2\n\n G. Benedetti-Rossi\n\n9.5.2\n\nA. R. Gomes-Junior\n\n10.9.2\n\n E. Fernandez-Valenzuela\n\n11.1\n\n B. J. Holler\n\n12. D. Souami\n\n5.13.14\n\n R. Duffard\n\n4. G. Margoti\n\n3.2\n\n M. Vara-Lubiano\n\n4. J. Lecacheux\n\n5. J. L. Plouvier\n\n15. N. Morales\n\n4. A. Maury\n\n16. J. Fabrega\n\n17. P. Ceravolo\n\n18. E. Jehin\n\n19. D. Albanese\n\n20. H. Mariey\n\n21. S. Cikota\n\n22.3\n\n D. Ruzdjak\n\n24. Cikota\n\n25. R. Szakats\n\n26.27\n\n D. Baba Aissa\n\n28. Z. Gringahcene\n\n28. V. Kashuba\n\n29. N. Koshkin\n\n29. V. Zhukov\n\n30. S. Fisek\n\n31.32\n\n O. Cakir\n\n33.34\n\n S. Ozer\n\n35.36\n\n C. Schnabel\n\n37.38\n\n M. Schnabel\n\n38. F. Signoret\n\n39. L. Morrone\n\n40.41\n\n T. Santana-Ros\n\n42.43\n\n C. L. Pereira\n\n1.2\n\n M. Emilio\n44.13\n\n A. Y. Burdanov\n\n45. J. de Wit\n\n5. R. Barkaoui\n\n46.65\n\n M. Gillon\n\n46. G. Leto\n\n48. Frasca\n\n48. G. Catanzaro\n\n48. R. Zanmar Sanchez\n\n48. U. Tagliaferri\n\n47. M. Di Sora\n\n49. G. Isopi\n\n49. Y. Krugly\n\n50.51\n\n I. Slyusarev\n\n50. Chiory\n\n6. Mikuiz\n\n53.30\n\n P. Bacci\n\n54. M. Masetrijeri\n\n54. M. D. Grazie\n\n45. I. de Luca\n\n55. M. Yuste-Moreno\n\n55.\n\n F. Ciabatrari\n\n6.0\n\n O. Kozhukhov\n\n57. M. Sre-Ricart\n\n47.58\n\n M. R. Alarcon\n\n47.58\n\n J. Licandro\n\n58. G. Massi\n\n8. Racci\n\n60. J. M. Bosch\n\n61. R. Behem\n\n62. J.-P. Prost\n\n62. S. Renner\n\n7.63\n\n M. Conjat\n\n21. Machin\n\n64. Succi\n\n4. L. Stoian\n\n65. A. Juravle\n\n65. D. Carosati\n\n66. B. Gouve\n\n67. Carrillo\n\n68. A. P. Zheleznyak\n\n50. N.\n\n "], "nougat": ["A large topographic feature on the surface of the trans-Neptunian object (307261) 2002 MS\\({}_{4}\\) measured from stellar occultations\n\nF. L. Rommel 3\n\nF. Braga-Ribas , 2 J. L. Ortiz 4\n\nB. Sicardy 5\n\nP. Santos-Sanz 4\n\nJ. Desmars 6 , 7 J. I. B. Camargo 2 , R. Vieira-Martins 2\n\nM. Assafin 8 , 2 B. E. Morgado , 1 R. C. Boufleur 1\n\nG. Benedetti-Rossi 9\n\nA. R. Gomes-J\u00fanior , 2 E. Fernandez-Valenzuela 11\n\nB. J. Holler 12\n\nD. Souami 5\n\n13 14 1 "]}, {"edit": ["Proposition 5.1 characterizes the slow dynamics of an \\(N\\)-spike quasi-equilibrium solution on the long \\(O\\left(\\epsilon^{-3}\\right)\\) time-scale. We remark that this time-scale is longer than the \\(O\\left(\\epsilon^{-2}\\right)\\) time-scale of slow spike dynamics for the GM and Gray-Scott models ([23], [8], [12]), where there are no chemotactic effects.\n\nIn Appendix H, we show that \\(\\beta_{j}\\), as given in (5.17), can be calculated asymptotically by retaining only the contribution from the sub-inner solution. In particular, in Appendix H we provide the leading order estimate\n\n\\[\\beta_{j}\\sim\\frac{2}{v_{\\max j}}\\,,\\qquad\\text{for}\\;\\;v_{\\max j}\\gg 1\\,. \\tag{5.26}\\]\n\nMoreover, in Appendix H we show at the steady-state spike locations that \\(\\beta_{j}=\\beta_{0}\\;\\forall j\\), with \\(\\beta_{0}\\) given in (4.28).\n\nTo illustrate our results, we now compare the dynamics computed from the DAE system (5.24) with corresponding numerical results computed from the full PDE system (1.2) using FLEXPDE7[14]. In our comparison, we computed the integrals defining \\(\\beta_{j}\\) numerically from (5.17). The results for a one- and two-spike dynamics are shown in Figure 8 for the parameter values in the figure caption. In Figure 7(a), where we chose the initial condition \\(x_{1}(0)=-0.1\\), the asymptotic and numerical spike trajectories are favorably compared for a one-spike quasi-equilibrium pattern. In Figure 7(b) a similar favorable comparison is shown for the case of two-spike dynamics starting from the initial condition \\(x_{1}(0)=-0.6\\) and \\(x_{2}(0)=0.6\\).\n\n### Computation of Jacobian Matrix for Balancing Conditions\n\nIn this subsection, and as remarked in SS4, we show that when \\(d_{1}\\in\\mathcal{T}_{e}\\) the matrix \\(\\mathcal{M}\\) in (4.29) arises from the linearization of the DAE dynamics (5.24) in Proposition 5.1 about the steady-state spike locations. Our approach below is inspired by a related analysis for the GM model in [58].\n\nTo this end, we use the Green's function in (2.24) together with its decomposition in (5.20) to define\n\n\\[\\partial_{x_{j}}G(x_{j};x_{k}):=\\left\\{\\begin{array}{ll}\\frac{\\partial R}{ \\partial x_{j}}(x;x_{j})|_{x=x_{j}}\\,,&j=k\\,,\\\\ \\frac{\\partial G}{\\partial x}(x;x_{k})|_{x=x_{j}}\\,,&j\\neq k\\,,\\end{array}\\right. \\qquad\\partial_{x_{j}}\\partial_{x_{k}}G(x_{j};x_{k})=\\left\\{\\begin{array}{ ll}\\frac{\\partial}{\\partial x}|_{x=x_{j}}\\frac{\\partial}{\\partial y}|_{y=x_{k}} R(x;y)\\,,&j=k\\,,\\\\ \\partial_{x_{j}}\\partial_{x_{k}}G(x_{j};x_{k})\\,,&j\\neq k\\,.\\end{array}\\right. \\tag{5.27}\\]\n\nFigure 8:\n\n"], "nougat": ["Proposition 5.1 characterizes the slow dynamics of an \\(N\\)-spike quasi-equilibrium solution on the long \\(O\\left(\\epsilon^{-3}\\right)\\) time-scale. We remark that this time-scale is longer than the \\(O\\left(\\epsilon^{-2}\\right)\\) time-scale of slow spike dynamics for the GM and Gray-Scott models ([ 23 ], [ 8 ], [ 12 ]), where there are no chemotactic e ff ects.\n\nIn Appendix H, we show that \\(\\beta_{j}\\), as given in ( 5.17 ), can be calculated asymptotically by retaining only the contribution from the sub-inner solution. In particular, in Appendix we provide the leading order estimate\n\n\\[\\beta_{j}\\sim\\frac{2}{v_{\\max j}}\\,,\\qquad\\text{for}\\;\\;v_{\\max j}\\gg 1\\,.\\] (5.26 )\n\nMoreover, in Appendix H we show at the steady-state spike locations that \\(\\beta_{j}=\\beta_{0}\\;\\forall j\\), with \\(\\beta_{0}\\) given in ( 4.28 ).\n\nTo illustrate our results, we now compare the dynamics computed from the DAE system ( 5.24 ) with corresponding numerical results computed from the full PDE system ( 1.2 ) using FLEXPDE7 [ 14 ]. In our comparison, we computed the integrals defining \\(\\beta_{j}\\) numerically from ( 5.17 ). The results for a oneand two-spike dynamics are shown in Figure 8 for the parameter values in the figure caption. In Figure 8a , where we chose the initial condition \\(x_{1}(0)=-0.1\\), the asymptotic and numerical spike trajectories are favorably compared for a one-spike quasi-equilibrium pattern. In Figure 8b a similar favorable comparison is shown for the case of two-spike dynamics starting from the initial condition \\(x_{1}(0)=-0.6\\) and \\(x_{2}(0)=0.6\\).\n\n### Computation of Jacobian Matrix for Balancing Conditions\n\nIn this subsection, and as remarked in \u00a7 4 , we show that when \\(d_{1}\\in\\mathcal{T}_{e}\\) the matrix \\(\\mathcal{M}\\) in (4.29 ) arises from the linearization of the DAE dynamics ( 5.24 ) in Proposition 5.1 about the steady-state spike locations. Our approach below is inspired by a related analysis for the GM model in [ 58 ].\n\nTo this end, we use the Green\u2019s function in ( 2.24 ) together with its decomposition in ( 5.20 ) to define\n\n\\[\\partial_{x_{j}}G(x_{j};x_{k}):=\\left\\{\\begin{array}{ll}\\frac{\\partial R}{ \\partial x_{j}}(x;x_{j})|_{x=x_{j}}\\,,&j=k\\,,\\\\ \\frac{\\partial G}{\\partial x}(x;x_{k})|_{x=x_{j}}\\,,&j\\neq k\\,,\\end{array}\\right. \\qquad\\partial_{x_{j}}\\partial_{x_{k}}G(x_{j};x_{k})=\\left\\{\\begin{array}{ ll}\\frac{\\partial}{\\partial x}|_{x=x_{j}}\\frac{\\partial}{\\partial y}|_{y=x_{k}}R(x ;y)\\,,&j=k\\,,\\\\ \\partial_{x_{j}}\\partial_{x_{k}}G(x_{j};x_{k})\\,,&j\\neq k\\,.\\end{array}\\right.\\] "]}, {"edit": ["where \\(S_{n}(u)=e^{-B_{S}}\\psi_{s_{n}}(u)\\). To get the normalization constant we first consider the asymptotic expansion of the normalizable solution, \\(S_{n}(u)\\), close to the boundary, \\(S_{n}(u)=N_{s_{n}}u^{3}+\\cdots\\), where \\(N_{s_{n}}\\) is the normalization constant, which is obtained by plugging \\(S_{n}(u)\\) in (4.3). Then decay constants of the scalar mesons are given by the following dictionary [40]\n\n\\[F_{s_{n}}=\\zeta\\,u\\,e^{3A_{s}-\\Phi}\\partial_{u}S_{n}\\bigg{|}_{u= \\epsilon}=3\\frac{\\sqrt{N_{c}}}{2\\pi}\\,N_{s_{n}}. \\tag{4.4}\\]\n\nWe first investigate the behavior of the scalar meson decay constants as a function of the parameter \\(a_{0}\\) in the chiral limit with the other parameters fixed as in model A. Our numerical results are displayed on the left panel of Fig. 14. As can be seen from the plot, the results show a smooth behavior of the decay constants in the region of interest, i.e., \\(a_{0}<a_{0}^{c}\\), where \\(a_{0}^{c}\\approx 0.0974\\). However, the behavior of the decay constant increases close to \\(a_{0}^{c}\\) for the ground state while it decreases for the scalar resonances.\n\nIn addition, we calculate the scalar meson decay constants as a function of the quark mass. Our numerical results are displayed on the right panel of Fig. 14. As can be seen from the plot, the decay constant increases in the region of small quark mass until it reaches some maximum value and then decreases when the quark mass grows. Finally, we calculate the decay constant using the final set of parameters for models A and B displayed in Table 1.\n\n\\begin{table}\n\\begin{tabular}{c|c|c|c} \\hline \\hline  & Models A and B & SW [13] & Experimental [78] \\\\ \\hline \\(F_{V_{0}}^{1/2}\\) & 276 & 261 & \\(346.2\\pm 1.4\\) \\\\ \\(F_{V_{1}}^{1/2}\\) & 341 & & \\(433\\pm 13\\) \\\\ \\(F_{V_{2}}^{1/2}\\) & 384 & & \\\\ \\hline \\hline \\end{tabular}\n\\end{table}\nTable 6: The decay constants of the vector mesons (in MeV) obtained in our work (same result for models A and B), compared against the result obtained using the soft wall model (SW) [13]. In order to compare with experimental results of [78], we need to identify \\(F_{V}\\) with \\(g_{\\rho}\\).\n\nFigure 14: Left panel: The decay constants of the scalar mesons as a function of \\(a_{0}\\) in the chiral limit. Right panel: The decay constants of the scalar mesons as a function of the quark mass for \\(\\lambda=27\\), \\(b_{0}=1.7\\) and \\(a_{0}=0.02\\).\n\n "], "nougat": ["where \\(S_{n}(u)=e^{-B_{S}}\\psi_{s_{n}}(u)\\). To get the normalization constant we first consider the asymptotic expansion of the normalizable solution, \\(S_{n}(u)\\), close to the boundary, \\(S_{n}(u)=N_{s_{n}}u^{3}+\\cdots\\), where \\(N_{s_{n}}\\) is the normalization constant, which is obtained by plugging \\(S_{n}(u)\\) in (4.3). Then decay constants of the scalar mesons are given by the following dictionary [ 40 ]\n\n\\[F_{s_{n}}=\\zeta\\,u\\,e^{3A_{s}-\\Phi}\\partial_{u}S_{n}\\bigg{|}_{u= \\epsilon}=3\\frac{\\sqrt{N_{c}}}{2\\pi}\\,N_{s_{n}}. \\tag{4.4}\\]\n\nWe first investigate the behavior of the scalar meson decay constants as a function of the parameter \\(a_{0}\\) in the chiral limit with the other parameters fixed as in model A. Our numerical results are displayed on the left panel of Fig. 14 . As can be seen from the plot, the results show a smooth behavior of the decay constants in the region of interest, i.e., \\(a_{0}<a_{0}^{c}\\), where \\(a_{0}^{c}\\approx 0.0974\\). However, the behavior of the decay constant increases close to \\(a_{0}^{c}\\) for the ground state while it decreases for the scalar resonances.\n\nIn addition, we calculate the scalar meson decay constants as a function of the quark mass. Our numerical results are displayed on the right panel of Fig. 14 . As can be seen from the plot, the decay constant increases in the region of small quark mass until it reaches some maximum value and then decreases when the quark mass grows. Finally, we calculate the decay constant using the final set of parameters for models A and B displayed in Table 1 .\n\n\\begin{table}\n\\begin{tabular}{c|c|c|c} \\hline \\hline  & Models A and B & SW [13] & Experimental [78] \\\\ \\hline \\(F_{V_{0}}^{1/2}\\) & 276 & 261 & \\(346.2\\pm 1.4\\) \\\\ \\(F_{V_{1}}^{1/2}\\) & 341 & & \\(433\\pm 13\\) \\\\ \\(F_{V_{2}}^{1/2}\\) & 384 & & \\\\ \\hline \\hline \\end{tabular}\n\\end{table}\nTable 6: The decay constants of the vector mesons (in MeV) obtained in our work (same result for models A and B), compared against the result obtained using the soft wall model (SW) [13]. In order to compare with experimental results of [78], we need to identify \\(F_{V}\\) with \\(g_{\\rho}\\).\n\nFigure 14: Left panel: The decay constants of the scalar mesons as a function of \\(a_{0}\\) in the chiral limit. Right panel: The decay constants of the scalar mesons as a function of the quark mass for \\(\\lambda=27\\), \\(b_{0}=1.7\\) and \\(a_{0}=0.02\\).\n\n "]}, {"edit": ["detection methods (Kumar et al., 2017; Li et al., 2018; Li et al., 2019; Li et al., 2019) only focus on coping with presentation attacks, which construct artificial fingers (Shi et al., 2019) to circumvent the fingerprint recognition system. To model the physiological differences between the artificial and real fingers, existing liveness detection methods usually rely on collecting the pulse rate, skin odor, finger elasticity, _etc._, from the sensor of the fingerprint recognition system. Unfortunately, the replacement attack, _i.e._, replacing the real fingerprint images with the GAN-generated fingerprint images, usually happens after the fingerprint collection step and the fake fingerprint images are generated without the defects of artificial fingers. Besides, the GAN-generated fingerprint images also exist some generation artifacts, which are induced by specific processing operations in GANs and does not exist in the captured impressions. Under such circumstance, the existing fingerprint liveness detection methods can hardly be directly applied to detect the GAN-generated fingerprint images.\n\nTo efficiently and effectively identify the GAN-generated fingerprint images with decent robustness against the existing anti-forensic method (Li et al., 2019), in this paper, we propose a **R**obust deep **F**orgery **D**etection method **for** GAN-generated **F**ingerrprint images (RFDforFin). To take full advantage of the fingerprint characteristics and the generation artifacts of the fake fingerprint images, we construct a lightweight yet robust two-stream neural network, by exploiting the unique features of the fingerprint images and generation artifacts in frequency domain. Considering the impact of sweat on grayscale variations along the ridges of fingerprints (Li et al., 2019), we construct a special ridge stream which utilizes this unique fingerprint characteristic. In the generation artifact stream, inspired by (Li et al., 2019), which discovers the obvious generation artifacts in the Discrete Cosine Transform (DCT) frequency domain, we transform the input fingerprint image into different frequency domains, analyze the generation artifacts in these spectrum, and construct a simple yet effective convolutional neural network to learn more robust generation artifacts between the generated and real images from the FFT frequency spectrum. To simultaneously utilize the features extracted from the ridges and generation artifacts in the final prediction, we build a simple yet effective fusion module. Since typical CNNs can learn certain frequency information from the input image or its frequency transformed spectrum, by utilizing the unique 1D ridge features jointly with the 2D generation artifact features, our method can avoid overfitting to certain frequency information, which can improve the robustness of our method.\n\nOur main contributions are summarized as follows:\n\n* We propose the first deep forgery detection method for GAN-generated fingerprint images, by jointly exploiting the unique 1D ridge features and 2D generation artifact features via a lightweight two-stream neural network to ensure the robustness and efficiency of the proposed work.\n* We propose to exploit fingerprint-related characteristic and construct a ridge stream, which exploits the grayscale variations along the ridges. With this ridge stream, our method can avoid overfitting to certain frequency information, which can be easily interfered by existing anti-forensic method (Li et al., 2019) and thus improves the overall robustness.\n* We analyze the frequency spectrum of the real and generated fingerprint images and construct a simple yet effective generation artifact stream, _i.e._, a shallow convolutional neural network, to extract frequency-domain inconsistencies.\n* Comprehensive experiments demonstrate that our method is effective, efficient, and robust to the anti-forensic method (Li et al., 2019).\n\n## 2. Related Work\n\n### Fingerprint Image Synthesis\n\nSeveral recent methods have been proposed for generating realistic fingerprint images automatically (Shi et al., 2019; Li et al., 2019; Li et al., 2019; Li et al., 2019; Li et al., 2019; Li et al., 2019; Li et al., 2019). (Shi et al., 2019) and (Li et al., 2019) combine a convolution autoencoder (CAE) and a GAN-based method (_e.g._, DCGAN (Li et al., 2019), WGAN (Li et al., 2019)) directly. (Li et al., 2019) presents a GAN-based pipeline followed by a stochastic search algorithm over the latent variable "], "nougat": ["detection methods (Kumar et al., 2017; Li et al., 2018; Li et al., 2019; Li et al., 2019) only focus on coping with presentation attacks, which construct artificial fingers (Shi et al., 2019) to circumvent the fingerprint recognition system. To model the physiological differences between the artificial and real fingers, existing liveness detection methods usually rely on collecting the pulse rate, skin odor, finger elasticity, _etc._, from the sensor of the fingerprint recognition system. Unfortunately, the replacement attack, _i.e._, replacing the real fingerprint images with the GAN-generated fingerprint images, usually happens after the fingerprint collection step and the fake fingerprint images are generated without the defects of artificial fingers. Besides, the GAN-generated fingerprint images also exist some generation artifacts, which are induced by specific processing operations in GANs and does not exist in the captured impressions. Under such circumstance, the existing fingerprint liveness detection methods can hardly be directly applied to detect the GAN-generated fingerprint images.\n\nTo efficiently and effectively identify the GAN-generated fingerprint images with decent robustness against the existing anti-forensic method (Li et al., 2019), in this paper, we propose a **R**obust deep **F**orgery **D**etection method **for** GAN-generated **F**ingerrprint images (RFDforFin). To take full advantage of the fingerprint characteristics and the generation artifacts of the fake fingerprint images, we construct a lightweight yet robust two-stream neural network, by exploiting the unique features of the fingerprint images and generation artifacts in frequency domain. Considering the impact of sweat on grayscale variations along the ridges of fingerprints (Li et al., 2019), we construct a special ridge stream which utilizes this unique fingerprint characteristic. In the generation artifact stream, inspired by (Li et al., 2019), which discovers the obvious generation artifacts in the Discrete Cosine Transform (DCT) frequency domain, we transform the input fingerprint image into different frequency domains, analyze the generation artifacts in these spectrum, and construct a simple yet effective convolutional neural network to learn more robust generation artifacts between the generated and real images from the FFT frequency spectrum. To simultaneously utilize the features extracted from the ridges and generation artifacts in the final prediction, we build a simple yet effective fusion module. Since typical CNNs can learn certain frequency information from the input image or its frequency transformed spectrum, by utilizing the unique 1D ridge features jointly with the 2D generation artifact features, our method can avoid overfitting to certain frequency information, which can improve the robustness of our method.\n\nOur main contributions are summarized as follows:\n\n* We propose the first deep forgery detection method for GAN-generated fingerprint images, by jointly exploiting the unique 1D ridge features and 2D generation artifact features via a lightweight two-stream neural network to ensure the robustness and efficiency of the proposed work.\n* We propose to exploit fingerprint-related characteristic and construct a ridge stream, which exploits the grayscale variations along the ridges. With this ridge stream, our method can avoid overfitting to certain frequency information, which can be easily interfered by existing anti-forensic method (Li et al., 2019) and thus improves the overall robustness.\n* We analyze the frequency spectrum of the real and generated fingerprint images and construct a simple yet effective generation artifact stream, _i.e._, a shallow convolutional neural network, to extract frequency-domain inconsistencies.\n* Comprehensive experiments demonstrate that our method is effective, efficient, and robust to the anti-forensic method (Li et al., 2019).\n\n## 2. Related Work\n\n### Fingerprint Image Synthesis\n\nSeveral recent methods have been proposed for generating realistic fingerprint images automatically (Shi et al., 2019; Li et al., 2019; Li et al., 2019; Li et al., 2019; Li et al., 2019; Li et al., 2019; Li et al., 2019). (Shi et al., 2019) and (Li et al., 2019) combine a convolution autoencoder (CAE) and a GAN-based method (_e.g._, DCGAN (Li et al., 2019), WGAN (Li et al., 2019)) directly. (Li et al., 2019) presents a GAN-based pipeline followed by a stochastic search algorithm over the latent variable "]}, {"edit": ["_Remark 2.1_.:\n1. It should be noted that for any potential \\(u\\), the eigenvalues \\((\\nu_{n})\\) of \\(L_{u}\\) cannot be all simple. For instance, take \\(u(x)=\\mathrm{e}^{ix}\\) , one can easily check that for \\(L_{u}=D-T_{u}T_{\\overline{u}}\\) , \\[L_{u}1=L_{u}e^{ix}=0\\,.\\]\n2. Inequality (2.7) implies that as \\(n>\\!>0\\), the lower bound of the distance between two consecutive eigenvalues \\(\\nu_{n}\\) gets closer to \\(1\\) .\n\nProof.: All the presented inequalities are a direct consequence of the max-min principle\n\n\\[\\lambda_{n}=\\max_{\\begin{subarray}{c}F\\subseteq L_{+}^{2}\\\\ \\dim F\\leq n\\end{subarray}}\\,\\min\\Big{\\{}\\Big{\\langle}\\tilde{L}_{u}h\\,|\\,h \\Big{\\rangle}\\ ;\\,h\\in F^{\\perp}\\cap H_{+}^{\\frac{1}{2}}(\\mathbb{T})\\,,\\ \\|h\\|_{L^{2} }=1\\Big{\\}}\\ .\\] \\[\\nu_{n}=\\max_{\\begin{subarray}{c}F\\subseteq L_{+}^{2}\\\\ \\dim F\\leq n\\end{subarray}}\\,\\min\\Big{\\{}\\langle L_{u}h\\,|\\,h\\rangle\\ ;\\,h\\in F^{ \\perp}\\cap H_{+}^{\\frac{1}{2}}(\\mathbb{T})\\,,\\ \\|h\\|_{L^{2}}=1\\Big{\\}}\\ .\\]\n\n**Spectrum of \\(\\tilde{L}_{u}\\) .** Let \\(F\\) be any subspace of \\(L_{+}^{2}(\\mathbb{T})\\) of dimension \\(n\\) , and consider \\(E:=\\mathbb{C}1\\oplus S(F)\\) , where \\(S\\) is the shift operator, then\n\n\\[\\lambda_{n+1}\\geq\\min\\{\\langle\\tilde{L}_{u}h\\mid h\\rangle\\ ;\\|h\\|_{L^{2}}=1,\\,h \\in E^{\\perp}\\cap H_{+}^{\\frac{1}{2}}\\}\\]\n\nObserve that \\(E^{\\perp}=S\\left(F^{\\perp}\\right)\\) , thus by (2.2) ,\n\n\\[\\lambda_{n+1}\\geq\\min\\Big{\\{}\\langle\\tilde{L}_{u}g\\mid g\\rangle+1+|\\langle Sg \\,|\\,u\\rangle|^{2}\\ ;\\ \\|g\\|_{L^{2}}=1,\\ g\\in F^{\\perp}\\cap H_{+}^{\\frac{1}{2}} \\Big{\\}}\\ .\\]\n\nIn addition, since \\(|\\langle Sg\\,|\\,u\\rangle|^{2}\\geq 0\\) , we infer for all \\(n\\in\\mathbb{N}_{\\geq 0}\\) ,\n\n\\[\\lambda_{n+1}\\geq\\lambda_{n}+1\\,.\\]\n\n**Spectrum of \\(L_{u}\\) -Inequality (2.6).** let \\(F\\) be any subspace of \\(L_{+}^{2}(\\mathbb{T}) "], "nougat": ["_Remark 2.1_.:\n1. It should be noted that for any potential \\(u\\), the eigenvalues \\((\\nu_{n})\\) of \\(L_{u}\\) cannot be all simple. For instance, take \\(u(x)=\\mathrm{e}^{ix}\\) , one can easily check that for \\(L_{u}=D-T_{u}T_{\\overline{u}}\\) , \\[L_{u}1=L_{u}e^{ix}=0\\,.\\]\n2. Inequality (2.7) implies that as \\(n>\\!>0\\), the lower bound of the distance between two consecutive eigenvalues \\(\\nu_{n}\\) gets closer to \\(1\\) .\n\nProof.: All the presented inequalities are a direct consequence of the max-min principle\n\n\\[\\lambda_{n}=\\max_{\\begin{subarray}{c}F\\subseteq L_{+}^{2}\\\\ \\dim F\\leq n\\end{subarray}}\\,\\min\\Big{\\{}\\Big{\\langle}\\tilde{L}_{u}h\\,|\\,h \\Big{\\rangle}\\ ;\\,h\\in F^{\\perp}\\cap H_{+}^{\\frac{1}{2}}(\\mathbb{T})\\,,\\ \\|h\\|_{L^{2} }=1\\Big{\\}}\\ .\\] \\[\\nu_{n}=\\max_{\\begin{subarray}{c}F\\subseteq L_{+}^{2}\\\\ \\dim F\\leq n\\end{subarray}}\\,\\min\\Big{\\{}\\langle L_{u}h\\,|\\,h\\rangle\\ ;\\,h\\in F^{ \\perp}\\cap H_{+}^{\\frac{1}{2}}(\\mathbb{T})\\,,\\ \\|h\\|_{L^{2}}=1\\Big{\\}}\\ .\\]\n\n**Spectrum of \\(\\tilde{L}_{u}\\) .** Let \\(F\\) be any subspace of \\(L_{+}^{2}(\\mathbb{T})\\) of dimension \\(n\\) , and consider \\(E:=\\mathbb{C}1\\oplus S(F)\\) , where \\(S\\) is the shift operator, then\n\n\\[\\lambda_{n+1}\\geq\\min\\{\\langle\\tilde{L}_{u}h\\mid h\\rangle\\ ;\\|h\\|_{L^{2}}=1,\\,h \\in E^{\\perp}\\cap H_{+}^{\\frac{1}{2}}\\}\\]\n\nObserve that \\(E^{\\perp}=S\\left(F^{\\perp}\\right)\\) , thus by (2.2) ,\n\n\\[\\lambda_{n+1}\\geq\\min\\Big{\\{}\\langle\\tilde{L}_{u}g\\mid g\\rangle+1+|\\langle Sg \\,|\\,u\\rangle|^{2}\\ ;\\ \\|g\\|_{L^{2}}=1,\\ g\\in F^{\\perp}\\cap H_{+}^{\\frac{1}{2}} \\Big{\\}}\\ .\\]\n\nIn addition, since \\(|\\langle Sg\\,|\\,u\\rangle|^{2}\\geq 0\\) , we infer for all \\(n\\in\\mathbb{N}_{\\geq 0}\\) ,\n\n\\[\\lambda_{n+1}\\geq\\lambda_{n}+1\\,.\\]\n\n**Spectrum of \\(L_{u}\\) -Inequality (2.6).** let \\(F\\) be any subspace of \\(L_{+}^{2}(\\mathbb{T}) "]}, {"edit": ["From embedding \\(L^{\\frac{2n}{n-2}}(\\Omega)\\hookrightarrow L^{\\frac{2n\\gamma}{n-2(1-\\sigma)}}(\\Omega)\\) and \\(D(A^{\\frac{1-\\sigma}{2}})\\hookrightarrow L^{\\frac{2n}{n-2(1+\\sigma)}}(\\Omega)\\), (1.10) and the Young inequality, we conclude\n\n\\[\\left(f_{1}\\left(v_{1}(t)\\right),A^{\\sigma}v_{2}(t)\\right) \\leqslant C\\int_{\\Omega}\\left(1+\\left|v_{1}(t)\\right|^{\\gamma} \\right)|A^{\\sigma}v_{2}(t)|\\,dx\\] \\[\\leqslant C\\left(1+\\left\\|v_{1}(t)\\right\\|_{L^{\\frac{2n\\gamma}{n -2}2\\sigma}(\\Omega)}^{\\gamma}\\right)\\|A^{\\sigma}v_{2}(t)\\|_{L^{\\frac{2n}{n-2+2 \\sigma}}(\\Omega)}\\] \\[\\leqslant C\\left(1+\\left\\|v_{1}(t)\\right\\|_{L^{\\frac{2n}{n-2}}( \\Omega)}^{\\gamma}\\right)\\|A^{\\sigma}v_{2}(t)\\|_{L^{\\frac{2n}{n-2+2\\sigma}}( \\Omega)}\\] \\[\\leqslant C\\left(1+\\left\\|v_{1}(t)\\right\\|_{L^{\\frac{2n}{n-2}}( \\Omega)}^{\\gamma}\\right)\\|A^{\\frac{1+\\sigma}{2}}v_{2}(t)\\|\\] \\[\\leqslant C\\left(1+\\left\\|v_{1}(t)\\right\\|_{L^{\\frac{2n}{n-2}}( \\Omega)}^{2\\gamma}\\right)+\\frac{1}{16}\\|A^{\\frac{1+\\sigma}{2}}v_{2}(t)\\|^{2}. \\tag{4.72}\\]\n\nUsing (1.7), we derive\n\n\\[\\left|\\left(f(u(t))-f\\left(v_{1}(t)\\right),A^{\\sigma}v_{2}(t) \\right)\\right| \\leqslant C\\int_{\\Omega}\\left|\\left(f^{\\prime}\\left((1-\\mu)u(t) \\right)+\\mu v_{1}(t)\\right)\\right|\\left|u(t)-v_{1}(t)\\right|\\left|A^{\\sigma}v_{ 2}(t)\\right|dx \\tag{4.73}\\] \\[\\leqslant C\\int_{\\Omega}\\left(1+\\left|u(t)\\right|^{\\frac{4}{n-2}} +\\left|v_{1}(t)\\right|^{\\frac{4}{n-2}}\\right)\\left|v_{2}(t)\\right|\\left|A^{ \\sigma}v_{2}(t)\\right|dx,\\]\n\nwhere \\(0<\\mu<1\\).\n\nNoticing \\(u(t)=v_{1}(t)+v_{2}(t)\\), it follows that\n\n\\[\\int_{\\Omega}\\left|u(t)\\right|^{\\frac{4}{n-2}}\\left|v_{2}(t)\\right|\\ "], "nougat": ["From embedding \\(L^{\\frac{2n}{n-2}}(\\Omega)\\hookrightarrow L^{\\frac{2n\\gamma}{n-2(1-\\sigma)}}(\\Omega)\\) and \\(D(A^{\\frac{1-\\sigma}{2}})\\hookrightarrow L^{\\frac{2n}{n-2(1+\\sigma)}}(\\Omega)\\), (1.10) and the Young inequality, we conclude\n\n\\[\\left(f_{1}\\left(v_{1}(t)\\right),A^{\\sigma}v_{2}(t)\\right) \\leqslant C\\int_{\\Omega}\\left(1+\\left|v_{1}(t)\\right|^{\\gamma} \\right)|A^{\\sigma}v_{2}(t)|\\,dx\\] \\[\\leqslant C\\left(1+\\left\\|v_{1}(t)\\right\\|_{L^{\\frac{2n\\gamma}{n -2}2\\sigma}(\\Omega)}^{\\gamma}\\right)\\|A^{\\sigma}v_{2}(t)\\|_{L^{\\frac{2n}{n-2+2 \\sigma}}(\\Omega)}\\] \\[\\leqslant C\\left(1+\\left\\|v_{1}(t)\\right\\|_{L^{\\frac{2n}{n-2}}( \\Omega)}^{\\gamma}\\right)\\|A^{\\sigma}v_{2}(t)\\|_{L^{\\frac{2n}{n-2+2\\sigma}}( \\Omega)}\\] \\[\\leqslant C\\left(1+\\left\\|v_{1}(t)\\right\\|_{L^{\\frac{2n}{n-2}}( \\Omega)}^{\\gamma}\\right)\\|A^{\\frac{1+\\sigma}{2}}v_{2}(t)\\|\\] \\[\\leqslant C\\left(1+\\left\\|v_{1}(t)\\right\\|_{L^{\\frac{2n}{n-2}}( \\Omega)}^{2\\gamma}\\right)+\\frac{1}{16}\\|A^{\\frac{1+\\sigma}{2}}v_{2}(t)\\|^{2}. \\tag{4.72}\\]\n\nUsing (1.7 ), we derive\n\n\\[\\begin{split}|(f(u(t))-f\\left(v_{1}(t)\\right),A^{\\sigma}v_{2}(t))| &\\leqslant C\\int_{\\Omega}\\left|(f^{\\prime}\\left((1-\\mu)u(t))+\\mu v _{1}(t)\\right)\\right|\\left|u(t)-v_{1}(t)\\right|\\left|A^{\\sigma}v_{2}(t)\\right| dx\\\\ &\\leqslant C\\int_{\\Omega}\\left(1+\\left|u(t)\\right|^{\\frac{4}{n-2} }+\\left|v_{1}(t)\\right|^{\\frac{4}{n-2}}\\right)\\left|v_{2}(t)\\right|\\left|A^{ \\sigma}v_{2}(t)\\right|dx,\\end{split} \\tag{4.73}\\]\n\nwhere \\(0<\\mu<1\\).\n\nNoticing \\(u(t)=v_{1}(t)+v_{2}(t)\\), it follows that\n\n\\[\\int_{\\Omega}\\left|u(t)\\right|^{\\frac{4}{n-2}}\\left|v_{2}(t)\\right|\\left|A^ "]}, {"edit": ["\n\n## 4 TinySiamese Network\n\nThe proposed TinySiamese neural network takes on a new look and a new way of working which is different from the standard Siamese network. The difference first appears in the input processing of the network. Instead of having images as input, the input was the output feature vector of a pre-trained CNN model. In other words, all input images would be transformed into feature vectors using a feature extractor (such as a pre-trained CNN model) as illustrated in Fig. 3. Then, the Tiny-Siamese encoded the features in a small set of layers and finally calculated the distance between two encoded feature vectors and generated similarity score. Using this score, the model was trained from scratch with the Adam optimization algorithm and binary cross-entropy loss function.\n\n### Architecture\n\nUnlike the standard Siamese, the input of the TinySiamese was the encoded image as a feature vector. The backbone layers first aimed to extract relevant features using a linear fully-connected layer and a ReLU layer and then amplify them using another linear fully-connected layer and Sigmoid layer. The output size of the first linear layer had the half size of the input (n, n/2) and was followed by a non-linear ReLU layer. The second linear layer took n/2 features in input and came back to the same first input size in output (n/2, n). This layer was followed by a non-linear Sigmoid layer. The outputs of the TinySiamese sub-networks were encoded into an n-dimensional vector using inputs of a size equal to n. Siamese networks are usually trained\n\nFigure 3: The Proposed Architecture Based on TinySiamese Network for Verification.\n\n"], "nougat": ["\n\n## 4 TinySiamese Network\n\nThe proposed TinySiamese neural network takes on a new look and a new way of working which is different from the standard Siamese network. The difference first appears in the input processing of the network. Instead of having images as input, the input was the output feature vector of a pretrained CNN model. In other words, all input images would be transformed into feature vectors using a feature extractor (such as a pre-trained CNN model) as illustrated in Fig. 3. Then, the Tiny-Siamese encoded the features in a small set of layers and finally calculated the distance between two encoded feature vectors and generated similarity score. Using this score, the model was trained from scratch with the Adam optimization algorithm and binary cross-entropy loss function.\n\n### Architecture\n\nUnlike the standard Siamese, the input of the TinySiamese was the encoded image as a feature vector. The backbone layers first aimed to extract relevant features using a linear fully-connected layer and a ReLU layer and then amplify them using another linear fully-connected layer and Sigmoid layer. The output size of the first linear layer had the half size of the input (n, n/2) and was followed by a non-linear ReLU layer. The second linear layer took n/2 features in input and came back to the same first input size in output (n/2, n). This layer was followed by a non-linear Sigmoid layer. The outputs of the TinySiamese sub-networks were encoded into an n-dimensional vector using inputs of a size equal to n. Siamese networks are usually trained\n\nFigure 3: The Proposed Architecture Based on TinySiamese Network for Verification.\n\n"]}, {"edit": ["\n\n# Natural Language is All a Graph Needs\n\n Ruosong Ye\n\nRutgers University\n\nruosong.ye@rutgers.edu\n\n&Caiqi Zhang\n\nUniversity of Cambridge\n\ncz391@cam.ac.uk\n\n&Runhui Wang\n\nRutgers University\n\nrunhui.wang@rutgers.edu\n\n&Shuyuan Xu\n\nRutgers University\n\nshuyuan.xu@rutgers.edu\n\n&Yongfeng Zhang\n\nRutgers University\n\nyongfeng.zhang@rutgers.edu\n\n###### Abstract\n\nThe emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose **InstructGLM (Instruction-finetuned Graph L**anguage Model**), systematically design highly scalable prompts based on natural language instructions, and use natural language to describe the geometric structure and node features of the graph for instruction tuning an LLM to perform learning and inference on graphs in a generative manner. Our method exceeds all competitive GNN baselines on ogbn-arxiv, Cora and PubMed datasets, which demonstrates the effectiveness of our method and sheds light on generative large language models as the foundation model for graph machine learning.\n\n## 1 Introduction\n\nBefore the advent of Transformers [1], various artificial intelligence domains with different inductive biases had diverse foundational model architectures. For instance, CNNs [2; 3] were designed with considerations for spatial invariance in images, leading to superior performance in computer vision tasks [4; 5]. Memory-enhanced models like RNNs [6] and LSTM [7; 8] were widely used for handling sequential data such as natural language [9] and audio [10]. Graph Neural Networks (GNNs) excel in capturing topological information by employing message passing and aggregation mechanisms, making them a preferred choice in the field of graph learning for a long time [11; 12; 13].\n\nIn recent years, the AI community has witnessed the emergence of numerous powerful pre-trained Large Language Models (LLMs) [14; 15; 16; 17; 18], which are driving huge advancements and lead to the pursuit of possible Artificial General Intelligence (AGI) [19]. Under this background, there is a trend towards unification in model architectures across different domains. Specifically, pre-trained Transformers have demonstrated remarkable performance on various modalities, such as images [20] and videos [21] in computer vision, text in natural language processing [22], structured data in graph machine learning [23], decision sequences in reinforcement learning [24], and visual-text pairs in multimodal tasks [25]. There has even been Transformers capable of handling twelve modalities [26].\n\nBesides model architecture, the unification of processing method in handling multimodal data is also a significant trend worth attention. T5 [15] established a text-to-text framework, unifying all NLP"], "nougat": ["\n\n# Natural Language is All a Graph Needs\n\n Ruosong Ye\n\nRutgers University\n\nruosong.ye@rutgers.edu\n\n&Caiqi Zhang\n\nUniversity of Cambridge\n\ncz391@cam.ac.uk\n\n&Runhui Wang\n\nRutgers University\n\nrunhui.wang@rutgers.edu\n\n&Shuyuan Xu\n\nRutgers University\n\nshuyuan.xu@rutgers.edu\n\n&Yongfeng Zhang\n\nRutgers University\n\nyongfeng.zhang@rutgers.edu\n\n###### Abstract\n\nThe emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose **InstructGLM (Instruction-finetuned Graph L**anguage Model**), systematically design highly scalable prompts based on natural language instructions, and use natural language to describe the geometric structure and node features of the graph for instruction tuning an LLM to perform learning and inference on graphs in a generative manner. Our method exceeds all competitive GNN baselines on ogbn-arxiv, Cora and PubMed datasets, which demonstrates the effectiveness of our method and sheds light on generative large language models as the foundation model for graph machine learning.\n\n## 1 Introduction\n\nBefore the advent of Transformers [1], various artificial intelligence domains with different inductive biases had diverse foundational model architectures. For instance, CNNs [2; 3] were designed with considerations for spatial invariance in images, leading to superior performance in computer vision tasks [4; 5]. Memory-enhanced models like RNNs [6] and LSTM [7; 8] were widely used for handling sequential data such as natural language [9] and audio [10]. Graph Neural Networks (GNNs) excel in capturing topological information by employing message passing and aggregation mechanisms, making them a preferred choice in the field of graph learning for a long time [11; 12; 13].\n\nIn recent years, the AI community has witnessed the emergence of numerous powerful pre-trained Large Language Models (LLMs) [14; 15; 16; 17; 18], which are driving huge advancements and lead to the pursuit of possible Artificial General Intelligence (AGI) [19]. Under this background, there is a trend towards unification in model architectures across different domains. Specifically, pre-trained Transformers have demonstrated remarkable performance on various modalities, such as images [20] and videos [21] in computer vision, text in natural language processing [22], structured data in graph machine learning [23], decision sequences in reinforcement learning [24], and visual-text pairs in multimodal tasks [25]. There has even been Transformers capable of handling twelve modalities [26].\n\nBesides model architecture, the unification of processing method in handling multimodal data is also a significant trend worth attention. T5 [15] established a text-to-text framework, unifying all NLP"]}, {"edit": ["each root subgroup may be expressed in terms of commutators of other root subgroups. If some commutative unital ring \\(K\\) acts on the root subgroups in a natural way, then the resulting odd form ring \\((R,\\Delta)\\) is an augmented odd form \\(K\\)-algebra and in the last claim of the theorem the maps of the root subgroups are isomorphisms.\n\nThe Chevalley commutator formula is\n\n\\[[G_{\\alpha},G_{\\beta}]\\leq\\prod_{\\begin{subarray}{c}i\\alpha+j\\beta\\in\\Phi\\\\ i,j\\in\\{1,2,\\ldots\\}\\end{subarray}}G_{i\\alpha+j\\beta},\\]\n\nwe also assume that \\(G_{2\\alpha}\\leq G_{\\alpha}\\) are \\(2\\)-step nilpotent filtrations for any ultrashort root \\(\\alpha\\). In other words, \\(G\\) is a group with \\(\\mathsf{BC}_{\\ell}\\)-commutator relations in the sense of [8]. Alternatively, we may assume only that \\(G\\) contains groups indexed by a root system of type \\(\\mathsf{B}_{\\ell}\\) and\n\n\\[[G_{\\alpha},G_{\\beta}]\\leq\\prod_{\\begin{subarray}{c}i\\alpha+j\\beta\\in\\Phi\\\\ i,j\\in\\mathbb{R}_{+}\\end{subarray}}G_{i\\alpha+j\\beta}.\\]\n\nThese formulas turn out to be equivalent (modulo other natural conditions) up to a choice of the nilpotent filtrations \\(G_{2\\alpha}\\leq G_{\\alpha}\\).\n\nThe paper is organized as follows. In section 2 we recall the definitions of odd form rings and associated unitary groups. In sections 3 and 4 we list the precise conditions on \\(G\\) and its subgroups. Namely, the conditions are (C1)-(C5) without using a commutative unital ring \\(K\\) or (C1)-(C8) involving \\(K\\). In section 4 we also discuss the case \\(\\ell=3\\): under additional \"associativity conditions\" (A1)-(A4) the main results still hold, otherwise there are counterexamples (e.g. Chevalley groups of types \\(\\mathsf{E}_{6}\\) and \\(\\mathsf{E}_{7}\\)). These associativity conditions always hold for \\(\\ell\\geq 4\\) by theorem 1. Sections 5 and 6 contain the proof of the main theorem 2 for groups satisfying (C1)-(C8). In the last section 7 we prove theorem 3 for groups satisfying only (C1)-(C5).\n\n## 2 Odd form groups and odd form rings\n\nIn this paper we build a lot of \\(2\\)-step nilpotent groups with various operations, so it is useful to develop some technique to simplify such constructions. The group operation of \\(2\\)-step nilpotent groups is usually denoted by \\(\\dotplus\\). All lemmas in this section may be checked directly or using the machinery of polyquadratic maps [14, SS1.3].\n\nWe say that \\((M,H)\\) is a _hermitian group_ if \\(M\\) and \\(H\\) are abelian groups, there is an automorphism\n\n\\(\\overline{(-)}\\colon H\\to H\\) of order at most \\(2\\), and there is a biadditive pairing \\(\\langle-,=\\rangle\\colon M\\times M\\to H\\) such that\n\n\\(\\overline{\\langle m,m^{\\prime}\\rangle}\\)\n\n\\(=\\langle m^{\\prime},m\\rangle\\) for all \\(m,m^{\\prime}\\in M\\).\n\n "], "nougat": ["each root subgroup may be expressed in terms of commutators of other root subgroups. If some commutative unital ring \\(K\\) acts on the root subgroups in a natural way, then the resulting odd form ring \\((R,\\Delta)\\) is an augmented odd form \\(K\\)-algebra and in the last claim of the theorem the maps of the root subgroups are isomorphisms.\n\nThe Chevalley commutator formula is\n\n\\[[G_{\\alpha},G_{\\beta}]\\leq\\prod_{\\begin{subarray}{c}i\\alpha+j\\beta\\in\\Phi\\\\ i,j\\in\\{1,2,\\ldots\\}\\end{subarray}}G_{i\\alpha+j\\beta},\\]\n\nwe also assume that \\(G_{2\\alpha}\\leq G_{\\alpha}\\) are \\(2\\)-step nilpotent filtrations for any ultrashort root \\(\\alpha\\). In other words, \\(G\\) is a group with \\(\\mathsf{BC}_{\\ell}\\)-commutator relations in the sense of [8]. Alternatively, we may assume only that \\(G\\) contains groups indexed by a root system of type \\(\\mathsf{B}_{\\ell}\\) and\n\n\\[[G_{\\alpha},G_{\\beta}]\\leq\\prod_{\\begin{subarray}{c}i\\alpha+j\\beta\\in\\Phi\\\\ i,j\\in\\mathbb{R}_{+}\\end{subarray}}G_{i\\alpha+j\\beta}.\\]\n\nThese formulas turn out to be equivalent (modulo other natural conditions) up to a choice of the nilpotent filtrations \\(G_{2\\alpha}\\leq G_{\\alpha}\\).\n\nThe paper is organized as follows. In section 2 we recall the definitions of odd form rings and associated unitary groups. In sections 3 and 4 we list the precise conditions on \\(G\\) and its subgroups. Namely, the conditions are (C1)\u2013(C5) without using a commutative unital ring \\(K\\) or (C1)\u2013(C8) involving \\(K\\). In section 4 we also discuss the case \\(\\ell=3\\): under additional \u201cassociativity conditions\u201d (A1)\u2013(A4) the main results still hold, otherwise there are counterexamples (e.g. Chevalley groups of types \\(\\mathsf{E}_{6}\\) and \\(\\mathsf{E}_{7}\\)). These associativity conditions always hold for \\(\\ell\\geq 4\\) by theorem 1. Sections 5 and 6 contain the proof of the main theorem 7 we prove theorem 3 for groups satisfying only (C1)\u2013(C5).\n\n## 2 Odd form groups and odd form rings\n\nIn this paper we build a lot of \\(2\\)-step nilpotent groups with various operations, so it is useful to develop some technique to simplify such constructions. The group operation of \\(2\\)-step nilpotent groups is usually denoted by \\(\\dot{+}\\). All lemmas in this section may be checked directly or using the machinery of polyquadratic maps [14, \u00a7 1.3].\n\nWe say that \\((M,H)\\) is a _hermitian group_ if \\(M\\) and \\(H\\) are abelian groups, there is an automorphism\n\n\\(\\overline{(-)}\\colon H\\to H\\) of order at most \\(2\\), and there is a biadditive pairing \\(\\langle-,=\\rangle\\colon M\\times M\\to H\\) such that\n\n\\(\\overline{\\langle m,m^{\\prime}\\rangle}\\)\n\n\\(=\\langle m^{\\prime},m\\rangle\\) for all \\(m,m^{\\prime}\\in M\\).\n\n "]}, {"edit": ["the spectral coefficients. We can fit this dependency of the coefficients in order to obtain the _spectral functions_\n\n\\[a(r_{\\mathrm{ant}},X_{\\mathrm{slice}},\\ X_{\\mathrm{max}})\\;,\\] \\[b(r_{\\mathrm{ant}},X_{\\mathrm{slice}},\\ X_{\\mathrm{max}})\\;,\\] \\[c(r_{\\mathrm{ant}},X_{\\mathrm{slice}},\\ X_{\\mathrm{max}})\\;,\\]\n\nin every slice. This procedure is applied to each antenna independently, indicated by the explicit dependency on the antenna distance. Therefore in the current version of template synthesis \\(r_{\\mathrm{ant}}\\), as well as \\(X_{\\mathrm{slice}}\\), can only take values on a fixed grid. We note here that while we write the functions as both a function of \\(X_{\\mathrm{slice}}\\) and \\(X_{\\mathrm{max}}\\), a more accurate description should probably take some combination of the two that could serve as a proxy for shower age in the slice. We come back to this in Section VI.\n\nWe fit the spectral parameters as a function of \\(X_{\\mathrm{max}}\\) using a parabola. In order to better deal with the large scatter for some slices, especially the very early and late ones where there are only a few particles, we opt to first bin the data points by \\(X_{\\mathrm{max}}\\). In each bin we calculate the mean value of the spectral parameter and the corresponding standard deviation. If a bin contains less than two data points, which can occur because not all showers might contain particles in that slice, we do not consider it for the parabolic fit. All the other bins are then fed to a least-squares fitting routine. We end up with a spectral function describing the spectral parameter value as function of the shower \\(X_{\\mathrm{max}}\\) in a given slice, for a particular antenna,\n\n\\[a(r_{\\mathrm{ant}},X_{\\mathrm{slice}},\\ X_{\\mathrm{max}}) =p_{0}^{a}+p_{1}^{a}\\cdot\\ X_{\\mathrm{max}}+p_{2}^{a}\\cdot\\ X_{ \\mathrm{max}}^{2}\\] \\[b(r_{\\mathrm{ant}},X_{\\mathrm{slice}},\\ X_{\\mathrm{max}}) =p_{0}^{b}+p_{1}^{b}\\cdot\\ X_{\\mathrm{max}}+p_{2}^{b}\\cdot\\ X_{ \\mathrm{max}}^{2}\\] \\[c(r_{\\mathrm{ant}},X_{\\mathrm{slice}},\\ X_{\\mathrm{max}}) =p_{0}^{c}+p_{1}^{c}\\cdot\\ X_{\\mathrm{max}}+p_{2}^{c}\\cdot\\ X_{ \\mathrm{max}}^{2}\\;.\\]\n\nIn Figure 3 we present a schematic overview of how we extract the spectral functions. These need to be determined only once for the air shower geometry under consideration. Generalising them to arbitrary geometries will be the subject of future work.\n\n### Construction of the template\n\nThe final ingredient of template synthesis, is the _template_ itself. With this object and the spectral functions, we have all the necessary information to synthesise the emission from an air shower with arbitrary longitudinal profile.\n\nIn order to construct our template, we use a single microscopic simulation called the _origin_ shower. The origin is a microscopic simulation, sliced using the same procedure as the simulation set that was used to extract the spectral functions. From the origin shower we calculate the amplitude frequency spectrum \\(A_{\\mathrm{origin}}\\) and phase frequency spectrum \\(\\phi_{\\mathrm{origin}}\\) in each antenna and every slice, as shown in Figure 4. Using the spectral functions with the \\(X_{\\mathrm{max}}\\) of the origin shower, we can normalise these "], "nougat": ["the spectral coefficients. We can fit this dependency of the coefficients in order to obtain the _spectral functions_\n\n\\[a(r_{\\mathrm{ant}},X_{\\mathrm{slice}},\\;X_{\\mathrm{max}})\\;,\\] \\[b(r_{\\mathrm{ant}},X_{\\mathrm{slice}},\\;X_{\\mathrm{max}})\\;,\\] \\[c(r_{\\mathrm{ant}},X_{\\mathrm{slice}},\\;X_{\\mathrm{max}})\\;,\\]\n\nin every slice. This procedure is applied to each antenna independently, indicated by the explicit dependency on the antenna distance. Therefore in the current version of template synthesis \\(r_{\\mathrm{ant}}\\), as well as \\(X_{\\mathrm{slice}}\\), can only take values on a fixed grid. We note here that while we write the functions as both a function of \\(X_{\\mathrm{slice}}\\) and \\(X_{\\mathrm{max}}\\), a more accurate description should probably take some combination of the two that could serve as a proxy for shower age in the slice. We come back to this in Section VI.\n\nWe fit the spectral parameters as a function of \\(X_{\\mathrm{max}}\\) using a parabola. In order to better deal with the large scatter for some slices, especially the very early and late ones where there are only a few particles, we opt to first bin the data points by \\(X_{\\mathrm{max}}\\). In each bin we calculate the mean value of the spectral parameter and the corresponding standard deviation. If a bin contains less than two data points, which can occur because not all showers might contain particles in that slice, we do not consider it for the parabolic fit. All the other bins are then fed to a least-squares fitting routine. We end up with a spectral function describing the spectral parameter value as function of the shower \\(X_{\\mathrm{max}}\\) in a given slice, for a particular antenna,\n\n\\[a(r_{\\mathrm{ant}},X_{\\mathrm{slice}},\\;X_{\\mathrm{max}}) =p_{0}^{a}+p_{1}^{a}\\cdot\\;X_{\\mathrm{max}}+p_{2}^{a}\\cdot\\;X_{ \\mathrm{max}}^{2}\\] \\[b(r_{\\mathrm{ant}},X_{\\mathrm{slice}},\\;X_{\\mathrm{max}}) =p_{0}^{b}+p_{1}^{b}\\cdot\\;X_{\\mathrm{max}}+p_{2}^{b}\\cdot\\;X_{ \\mathrm{max}}^{2}\\] \\[c(r_{\\mathrm{ant}},X_{\\mathrm{slice}},\\;X_{\\mathrm{max}}) =p_{0}^{c}+p_{1}^{c}\\cdot\\;X_{\\mathrm{max}}+p_{2}^{c}\\cdot\\;X_{ \\mathrm{max}}^{2}\\;.\\]\n\nIn Figure 3 we present a schematic overview of how we extract the spectral functions. These need to be determined only once for the air shower geometry under consideration. Generalising them to arbitrary geometries will be the subject of future work.\n\n### Construction of the template\n\nThe final ingredient of template synthesis, is the _template_ itself. With this object and the spectral functions, we have all the necessary information to synthesise the emission from an air shower with arbitrary longitudinal profile.\n\nIn order to construct our template, we use a single microscopic simulation called the _origin_ shower. The origin is a microscopic simulation, sliced using the same procedure as the simulation set that was used to extract the spectral functions. From the origin shower we calculate the amplitude frequency spectrum \\(A_{\\mathrm{origin}}\\) and phase frequency spectrum \\(\\phi_{\\mathrm{origin}}\\) in each antenna and every slice, as shown in Figure 4. Using the spectral functions with the \\(X_{\\mathrm{max}}\n\n "]}, {"edit": ["\n\n### Effectiveness of different features in the physical world\n\n#### 4.3.1 Color transfer and texture blurring\n\nIn physical-world deployments, attack patches are often affected by color transfer due to lighting conditions or blurring caused by camera focus or smudging. To analyze if these changes will affect the performance of the attack patches, we compare the performance of an original patch, a color-adjusted patch, and a local texture adjusted patch. The color-adjusted patch is applied by adding a value (\\(\\delta\\)) to all values in RGB channels and making sure \\(\\delta\\) will not lead to an overflow. This color transfer will not change the texture information of the patch. The local texture adjustment is applied by using a 3x3 Gaussian blur. Figure 4 demonstrates the two types of feature adjustments applying to a patch with brightness range=0.24.\n\nThe performances of different patches are shown in Table 1. Regardless of the lightness restriction, the color transfer patch achieves almost the same success rate as the original patch. This performance shows that the patch does not need to maintain a specific color to deceive the target network. On the other hand, the blurred patch exhibits a significant decrease in success rate, suggesting that local texture is the key feature in deceiving target networks. Using these findings, we can apply the proposed hue mapping method to adjust the color of the patch and enhance its integration with the target environment, resulting in further reduced visibility. This process does not require any learning and can be quickly applied when deploying the patch in the physical world.\n\n#### 4.3.2 Random color variations\n\nWhen printing an attack patch, it is important to consider that normal printers are not able to produce a patch with precisely the same color as the digital version. Therefore, the patch's robustness to random color variations must be evaluated.\n\nTo replicate the color drift that commonly occurs during printing, we generate random noise within a restricted range that corresponds to a percentage of the original value. This approach allows us to simulate different levels of drift, and the results are shown in Table 2.\n\n\\begin{table}\n\\begin{tabular}{c c c c} \\hline \\hline Brightness range & Original & Color transfer & Gaussian blur \\\\ \\hline \\(1\\) (AdvPatch) & 89.4\\% & 90.8\\% & 47.8\\% \\\\ \\(0.35\\) & 89.5\\% & 87.9\\% & 22.7\\% \\\\ \\(0.24\\) & 74.2\\% & 75.3\\% & 10.1\\% \\\\ \\hline \\hline \\end{tabular}\n\\end{table}\nTable 1: Performance with color transfer and Gaussian blur\n\n\\begin{table}\n\\begin{tabular}{c c c c c} \\hline \\hline Brightness range & Original & 10\\% drift & 15\\% drift & 20\\% drift \\\\ \\hline \\(1\\) (AdvPatch) & 89.4\\% & 87.6\\% & 85.9\\% & 83.3\\% \\\\ \\(0.35\\) & 89.5\\% & 84.2\\% & 77.6\\% & 67.2\\% \\\\ \\(0.24\\) & 74.2\\% & 68.6\\% & 43.2\\% & 27.3\\% \\\\ \\hline \\hline \\end{tabular}\n\\end{table}\nTable 2: Performance with different color drift\n\nFigure 4: Patch with different feature changes. (left: original patch; middle: color transfer patch; right: texture blurring patch)"], "nougat": ["\n\n### Effectiveness of different features in the physical world\n\n#### 4.3.1 Color transfer and texture blurring\n\nIn physical-world deployments, attack patches are often affected by color transfer due to lighting conditions or blurring caused by camera focus or smudging. To analyze if these changes will affect the performance of the attack patches, we compare the performance of an original patch, a color-adjusted patch, and a local texture adjusted patch. The color-adjusted patch is applied by adding a value (\\(\\delta\\)) to all values in RGB channels and making sure \\(\\delta\\) will not lead to an overflow. This color transfer will not change the texture information of the patch. The local texture adjustment is applied by using a 3x3 Gaussian blur. Figure 4 demonstrates the two types of feature adjustments applying to a patch with brightness range=0.24.\n\nThe performances of different patches are shown in Table 1. Regardless of the lightness restriction, the color transfer patch achieves almost the same success rate as the original patch. This performance shows that the patch does not need to maintain a specific color to deceive the target network. On the other hand, the blurred patch exhibits a significant decrease in success rate, suggesting that local texture is the key feature in deceiving target networks. Using these findings, we can apply the proposed hue mapping method to adjust the color of the patch and enhance its integration with the target environment, resulting in further reduced visibility. This process does not require any learning and can be quickly applied when deploying the patch in the physical world.\n\n#### 4.3.2 Random color variations\n\nWhen printing an attack patch, it is important to consider that normal printers are not able to produce a patch with precisely the same color as the digital version. Therefore, the patch\u2019s robustness to random color variations must be evaluated.\n\nTo replicate the color drift that commonly occurs during printing, we generate random noise within a restricted range that corresponds to a percentage of the original value. This approach allows us to simulate different levels of drift, and the results are shown in Table 2.\n\n\\begin{table}\n\\begin{tabular}{c c c c} \\hline \\hline Brightness range & Original & Color transfer & Gaussian blur \\\\ \\hline \\(1\\) (AdvPatch) & 89.4\\% & 90.8\\% & 47.8\\% \\\\ \\(0.35\\) & 89.5\\% & 87.9\\% & 22.7\\% \\\\ \\(0.24\\) & 74.2\\% & 75.3\\% & 10.1\\% \\\\ \\hline \\hline \\end{tabular}\n\\end{table}\nTable 1: Performance with color transfer and Gaussian blur\n\n\\begin{table}\n\\begin{tabular}{c c c c c} \\hline \\hline Brightness range & Original & 10\\% drift & 15\\% drift & 20\\% drift \\\\ \\hline \\(1\\) (AdvPatch) & 89.4\\% & 87.6\\% & 85.9\\% & 83.3\\% \\\\ \\(0.35\\) & 89.5\\% & 84.2\\% & 77.6\\% & 67.2\\% \\\\ \\(0.24\\) & 74.2\\% & 68.6\\% & 43.2\\% & 27.3\\% \\\\ \\hline \\hline \\end{tabular}\n\\end{table}\nTable 2: Performance with different color drift\n\nFigure 4: Patch with different feature changes. (left: original patch; middle: color transfer patch; right: texture blurring patch)"]}, {"edit": ["The cyan dashed line represents the linear least-squares best fit performed on the logarithm of the points for high densities (\\(\\rho_{\\rm thr}>1.1\\times 10^{-21}\\) g cm\\({}^{-3}\\)). The best fit of \\(\\kappa=0.47\\pm 0.03\\) is consistent with the strong-field limit of \\(B\\propto\\rho^{0.5}\\). We have already shown in the previous section (Section 5) that our structures are on average highly elongated, and magnetic fields clearly help to deform the shape of the forming structures. It is therefore not unexpected that we find a shallower scaling compared to the weak field limit (\\(\\kappa=0.67\\)).\n\nWe see that, while there is no clear transition from the sub- to the super-Alfvenic regime, there is clearly a trend that higher Alfvenic Mach numbers are preferentially obtained at the higher density end. This is confirmed by a Kolmogorov-Smirnov (KS) two-sample test, which compares if two distributions belong to the same population. In this case, we compare the \\(\\rho_{\\rm thr}\\)-distributions of structures with \\(\\mathcal{M}_{A}>1\\) and \\(\\mathcal{M}_{A}\\leq 1\\). We find the \\(p\\)-values3 to be very low: \\(6\\times 10^{-4}\\) at 2 Myr and \\(5.2\\times 10^{-15}\\) at 3.5 Myr (see Table 6).\n\nFootnote 3: If the \\(p\\)-value is larger than a certain value (typically 0.05), this means that we cannot reject the null hypothesis that the sub-Alfvenic and super-Alfvenic structures have the same underlying density distribution.\n\nCrutcher et al. (2010) found that the observed magnetic field distribution is rather flat at low density, in agreement with the idea that denser clouds are swept up along the magnetic field lines on large scales, while at higher density there is a power-law increase of the magnetic field strength. If spherical clouds start to collapse and the magnetic field is not strong enough to stop the collapse, one expects a power-law slope of \\(\\kappa=0.5-0.67\\) (see above).\n\nIn the case of our clouds, we find that the high-density end is well consistent with \\(\\kappa=0.5\\), and the lower-density end clearly shows a much shallower slope. Nonetheless, there does not seem to be a clear single density at which there is a sharp change in slope. Simulations by Li et al. (2015), Mocz et al. (2017), Girichidis et al. (2018), Zhang et al. (2019) find similarly the lack of a sharp transition density. Auddy et al. (2022) predict that the transition density depends on the fourth power of \\(\\mathcal{M}_{A}\\). While of potential interest, this is unfortunately not demonstrable from the present analysis.\n\n### Impact of magnetic fields on the energetics of sub-structures\n\nWe are also interested in assessing the energetic relevance of magnetic fields over different length scales in the MCs, especially with respect to potentially star-forming structures. For this purpose, we compute the volume term of the magnetic energy and compare it with the kinetic and potential energies. Similar work for the same simulations has been performed by Ganguly et al. (2022), who assess the virial balance of the cloud sub-structures. Here, we extend the range of our analysis to include the dynamics of lower-density gas (between \\(10^{-24}\\) and \\(10^{-22}\\) g cm\\({}^{-3}\\); _low-den_ dendrogram analysis, see Table 2).\n\nThe magnetic energy of a given structure is computed as\n\n\\[E_{\\rm B}=\\int_{V}\\frac{1}{8\\pi}|\\mathbf{B}|^{2}\\mathrm{d}^{3}r, \\tag{21}\\]\n\nwhere the integration is computed over the entire volume \\( "], "nougat": ["The cyan dashed line represents the linear least-squares best fit performed on the logarithm of the points for high densities (\\(\\rho_{\\rm thr}>1.1\\times 10^{-21}\\) g cm\\({}^{-3}\\)). The best fit of \\(\\kappa=0.47\\pm 0.03\\) is consistent with the strong-field limit of \\(B\\propto\\rho^{0.5}\\). We have already shown in the previous section (Section 5 ) that our structures are on average highly elongated, and magnetic fields clearly help to deform the shape of the forming structures. It is therefore not unexpected that we find a shallower scaling compared to the weak field limit (\\(\\kappa=0.67\\)).\n\nWe see that, while there is no clear transition from the subto the super-Alfv\u00e9nic regime, there is clearly a trend that higher Alfv\u00e9nic Mach numbers are preferentially obtained at the higher density end. This is confirmed by a Kolmogorov-Smirnov (KS) two-sample test, which compares if two distributions belong to the same population. In this case, we compare the \\(\\rho_{\\rm thr}\\)-distributions of structures with \\(\\mathcal{M}_{A}>1\\) and \\(\\mathcal{M}_{A}\\leq 1\\). We find the \\(p\\)-values3 to be very low: \\(6\\times 10^{-4}\\) at 2 Myr and \\(5.2\\times 10^{-15}\\) at 3.5 Myr (see Table 6).\n\nFootnote 3: If the \\(p\\)-value is larger than a certain value (typically 0.05), this means that we cannot reject the null hypothesis that the sub-Alfv\u00e9nic and super-Alfv\u00e9nic structures have the same underlying density distribution.\n\nCrutcher et al. (2010) found that the observed magnetic field distribution is rather flat at low density, in agreement with the idea that denser clouds are swept up along the magnetic field lines on large scales, while at higher density there is a power-law increase of the magnetic field strength. If spherical clouds start to collapse and the magnetic field is not strong enough to stop the collapse, one expects a power-law slope of \\(\\kappa=0.5-0.67\\) (see Table 6 ).\n\nIn the case of our clouds, we find that the high-density end is well consistent with \\(\\kappa=0.5\\), and the lower-density end clearly shows a much shallower slope. Nonetheless, there does not seem to be a clear single density at which there is a sharp change in slope. Simulations by Li et al. ( 2010 ) found that the observed magnetic field disstructures have the same underlying density distribution.\n\n "]}, {"edit": ["instructions given in the prompt. Past research has shown the importance of stating the actions a model can take, such as outputting \"I don't know.\" (Zhou et al., 2023). Similarly, how strongly the prompt encourages a model to incorporate feedback can favor overoptimization.\n\nIntroducing ErrorsFinally, effective feedback may communicate information on where the learner is failing, requiring an understanding of the possible error modes for a given task, and which ones the learner is likely in. For example, guessing and committing systematic reasoning mistakes are reflections of differing understandings. Exploring the error space and identifying the mistakes made by a learner is an important extension to the base framework directly derived from pedagogical and psychology of education research.\n\n### Feedback Integration\n\nThe method used to transmit the feedback to the model influences how it is subsequently processed. Fernandes et al. (2023) identify three common feedback integration mechanisms: feedback-based imitation learning, joint-feedback modeling, and reinforcement learning. In addition to this, we also consider feedback use in in-context learning (Brown et al., 2020). The training objective will necessarily influence how the model is processing and incorporating feedback. Typically, the training relies upon either scalar feedback (a single number encoding how much the model should be rewarded for its output) or a ranking (how well a given output did in relation to other candidate answers). However, this is simple information, and does not leverage the rich and complex information encoded in natural language feedback. Section 5 therefore comprehensively explores the different types of information that can be encoded in feedback.\n\n## 5 Feedback Content Taxonomy\n\nIn Section 4, we presented an overview of the complex ecosystem of feedback, including an expansion specifically for LLMs (i.e., FELT) that connects various background elements (e.g.,the learner, the task, the error types) to the actual feedback that must be given. In this section, we expand on our analysis of the _content_ dimension of feedback in FELT. Specifically, we present a taxonomy of feedback content under two different forms: a set of 10 broad axes along which feedback can vary, and a more concrete set of nine emergent categories for feedback topic. Figure 4 presents an overview of the two different presentations of this taxonomy, and the mapping between them.\n\nWe motivate this taxonomy to finely categorize current approaches to textual feedback that implicitly formulate feedback solely for _utility_ (i.e.,how useful is the feedback for guiding a model toward a suitable response). However, they do not categorize its content, leaving a conceptual gap about _what_ makes feedback useful. Our taxonomy stratifies the feedback space, allowing a deliberate and systematic study of feedback content.\n\n### General Taxonomy\n\nWe break down feedback content along ten dimensions that influence how feedback is formulated:\n\n1. _length_, an indication of how much feedback feedback is given, possibly measured by counting its number of tokens,\n2. _granularity_, a measure of the level of detail with which the feedback addresses the original answer -- it is not a measure of how much of the answer is being considered, but rather of the level of detail with which it is being considered,8 Footnote 8: For an open-answer example task, feedback might range from global learning meta-feedback, to global but task-specific, to paragraph-level, to sentence-level, to word-level, to token-level feedback.\n3. _applicability of instructions_, expressing both whether the feedback contains instructions, as well as how applicable those instructions are for the learner and their current understanding and approach to solving the task,\n4. _answer coverage_, which registers how much of the learner's answer is considered to generate the given feedback. The feedback could be independent of the answer, or only relate to parts of the answer (e.g.,, focusing on a particular mistake), or the feedback might take the complete answer into consideration,\n5. _criteria_, denoting which criteria the answer is being evaluated on: global evaluation, specific dimensions (e.g., fluency, engagement, etc.), or, alternatively, no dimensions (the answer is not being evaluated),\n6. _information novelty_, indicating the degree to which learner already had access to the information provided in the feedback, ranging from all information being previously known "], "nougat": ["instructions given in the prompt. Past research has shown the importance of stating the actions a model can take, such as outputting \u201cI don\u2019t know.\u201d ( Zhou et al. , 2023 ). Similarly, how strongly the prompt encourages a model to incorporate feedback can favor overoptimization.\n\nIntroducing ErrorsFinally, effective feedback may communicate information on where the learner is failing, requiring an understanding of the possible error modes for a given task, and which ones the learner is likely in. For example, guessing and committing systematic reasoning mistakes are reflections of differing understandings. Exploring the error space and identifying the mistakes made by a learner is an important extension to the base framework directly derived from pedagogical and psychology of education research.\n\n### Feedback Integration\n\nThe method used to transmit the feedback to the model influences how it is subsequently processed. Fernandes et al. ( 2023 ) identify three common feedback integration mechanisms: feedback-based imitation learning, joint-feedback modeling, and reinforcement learning. In addition to this, we also consider feedback use in in-context learning ( Brown et al. , 2020 ). The training objective will necessarily influence how the model is processing and incorporating feedback. Typically, the training relies upon either scalar feedback (a single number encoding how much the model should be rewarded for its output) or a ranking (how well a given output did in relation to other candidate answers). However, this is simple information, and does not leverage the rich and complex information encoded in natural language feedback. Section 5 therefore comprehensively explores the different types of information that can be encoded in feedback.\n\n## 5 Feedback Content Taxonomy\n\nIn Section 4 , we presented an overview of the complex ecosystem of feedback, including an expansion specifically for LLMs (i.e., FELT) that connects various background elements (e.g.,the learner, the task, the error types) to the actual feedback that must be given. In this section, we expand on our analysis of the _content_ dimension of feedback in FELT. Specifically, we present a taxonomy of feedback content under two different forms: a set of 10 broad axes along which feedback can vary, and a more concrete set of nine emergent categories for feedback topic. Figure 4 presents an overview of the two different presentations of this taxonomy, and the mapping between them.\n\nWe motivate this taxonomy to finely categorize current approaches to textual feedback that implicitly formulate feedback solely for _utility_ (i.e.,how useful is the feedback for guiding a model toward a suitable response). However, they do not categorize its content, leaving a conceptual gap about _what_ makes feedback useful. Our taxonomy stratifies the feedback space, allowing a deliberate and systematic study of feedback content.\n\n### General Taxonomy\n\nWe break down feedback content along ten dimensions that influence how feedback is formulated: 1. length , an indication of how much feedback feedback is given, possibly measured by counting its number of tokens, 2. granularity , a measure of the level of detail with which the feedback addresses the original answer \u2014 it is not a measure of how much of the answer is being considered, but rather of the level of detail with which it is being considered, 8 For an open-answer example task, feedback might range from global learning meta-feedback, to global but taskspecific, to paragraph-level, to sentence-level, to word-level, to token-level feedback. 3. applicability of instructions , expressing both whether the feedback contains instructions, as well as how applicable those instructions are for the learner and their current understanding and approach to solving the task, 4. answer coverage , which registers how much of the learner\u2019s answer is considered to generate the given feedback. The feedback could be independent of the answer, or only relate to parts of the answer (e.g.,, focusing on a particular mistake), or the feedback might take the complete answer into consideration, 5. criteria , denoting which criteria the answer is being evaluated on: global evaluation, specific dimensions (e.g., fluency, engagement, etc.), or, alternatively, no dimensions (the answer is not being evaluated), 6. information novelty , indicating the degree to which learner already had access to the information provided in the feedback, ranging from all information being previously known "]}, {"edit": ["are likely to be inaccurate. Indeed, the unconstrained estimate \\(\\tilde{f}_{\\rm esc}\\) predicts high escape fraction values in the high stellar mass range and hence does not match our findings in fig. 4. We therefore artificially set the \\(f_{\\rm esc}\\) value to 0 for galaxies with \\(x_{\\star}>8.5\\), obtaining:\n\n\\[f_{\\rm esc}=\\begin{cases}0&\\tilde{f}_{\\rm esc}<0\\ {\\rm or}\\ M_{\\star,\\rm log}<8.5\\\\ 1&\\tilde{f}_{\\rm esc}>1\\\\ \\tilde{f}_{\\rm esc}&\\text{otherwise}.\\end{cases} \\tag{14}\\]\n\nIn order to determine the accuracy of the model in predicting individual escape fractions, we select a subsample of galaxies not used for the fitting in order to avoid problems due to over-fitting. As a measure for this accuracy we use the average relative deviation\n\n\\[r=\\frac{1}{N_{\\rm test}}\\sum_{i=1}^{N_{\\rm test}}\\frac{|f_{\\rm esc,pred,\\,}-f_{ \\rm esc,i}|}{f_{\\rm esc,\\,}i}, \\tag{15}\\]\n\nwith \\(f_{\\rm esc,pred,\\,}i\\) and \\(f_{\\rm esc,\\,}i\\) being the predicted and modelled escape fraction of the \\(i\\)-th galaxy respectively, and \\(N_{\\rm test}\\) the number of test galaxies. We only used galaxies with \\(f_{\\rm esc}>0.01\\), as this measure is not useful for \\(f_{\\rm esc}\\) approaching 0. We find a value of \\(r\\approx 1.2\\), i.e. the average estimation error is of the order of a factor of 2, and as such the accuracy of predicting the escape fraction of a single halo is limited. However, for large scale studies where the statistical distribution of the escape fraction is more important, this model performs significantly better. Indeed, the average escape fraction obtained with the fitting formula is \\(\\tilde{f}_{\\rm esc,pred,\\,}i=0.121\\pm 0.086\\), with the modelled escape fraction being \\(\\tilde{f}_{\\rm esc,\\,}i=0.117\\pm 0.1334\\).\n\nIn fig. 9 we show how well the fitting formula is able to reproduce the behaviour of the escape fraction in relation to the stellar mass and redshift that we examined in fig. 2. We see that the evolution of the escape fraction with redshift is successfully reproduced. However, the large gradients in \\(\\langle f_{\\rm esc}\\rangle\\) that are seen in fig. 8 are smoothed out. The reason for this likely lies in the optimization process used to find the fitting formula, as the mean squared error was used for optimization, and thus large gradients in the fitting function were disfavored because they led to large errors for the outer mass ranges.\n\nFig. 10 shows that the fitting formula is able to successfully predict the bimodality in the escape fraction, as seen in fig. 4. However the boundary between the two modes is less pronounced. This is likely caused by the smoothing effect of the optimization process of the fitting function discussed above.\n\nFinally, by comparing fig. 11 to fig. 3, we see that the fitting formula is able to reproduce all important trends, namely, the decrease in peak escape fraction with redshift and the approximate locations and values of the peaks. We also reproduce both the minima and maxima in the dependence of \\(f_{\\rm esc}\\) on \\(M_{\\rm gas}\\).\n\nAs mentioned earlier, it is important to emphasize that our modeling aims to capture the overall trends of LyC escape with galactic properties. Considering the inherent limitations in resolution and simplifications involved in estimating the LyC flux, it is crucial to scale the absolute value predicted by the fitting formula using a free parameter, which should be determined based on the specific ionizing photon budget required for reionization. We intend to investigate the large scale implication of these results and to determine scaling parameters in subsequent work.\n\n "], "nougat": ["are likely to be inaccurate. Indeed, the unconstrained estimate \\(\\tilde{f}_{\\rm esc}\\) predicts high escape fraction values in the high stellar mass range and hence does not match our findings in fig. 4 . We therefore artificially set the \\(f_{\\rm esc}\\) value to 0 for galaxies with \\(x_{\\star}>8.5\\), obtaining:\n\n\\[f_{\\rm esc}=\\begin{cases}0&\\tilde{f}_{\\rm esc}<0\\ {\\rm or}\\ M_{\\star,\\rm log}<8.5\\\\ 1&\\tilde{f}_{\\rm esc}>1\\\\ \\tilde{f}_{\\rm esc}&\\text{otherwise}.\\end{cases} \\tag{14}\\]\n\nIn order to determine the accuracy of the model in predicting individual escape fractions, we select a subsample of galaxies not used for the fitting in order to avoid problems due to over-fitting. As a measure for this accuracy we use the average relative deviation\n\n\\[r=\\frac{1}{N_{\\rm test}}\\sum_{i=1}^{N_{\\rm test}}\\frac{|f_{\\rm esc,pred,\\,}-f_{ \\rm esc,i}|}{f_{\\rm esc,\\,}i}, \\tag{15}\\]\n\nwith \\(f_{\\rm esc,pred,\\,}i\\) and \\(f_{\\rm esc,\\,}i\\) being the predicted and modelled escape fraction of the \\(i\\)-th galaxy respectively, and \\(N_{\\rm test}\\) the number of test galaxies. We only used galaxies with \\(f_{\\rm esc}>0.01\\), as this measure is not useful for \\(f_{\\rm esc}\\) approaching 0. We find a value of \\(r\\approx 1.2\\), i.e. the average estimation error is of the order of a factor of 2, and as such the accuracy of predicting the escape fraction of a single halo is limited. However, for large scale studies where the statistical distribution of the escape fraction is more important, this model performs significantly better. Indeed, the average escape fraction obtained with the fitting formula is \\(\\tilde{f}_{\\rm esc,pred,\\,}i=0.121\\pm 0.086\\), with the modelled escape fraction being \\(\\tilde{f}_{\\rm esc,\\,}i=0.117\\pm 0.1334\\).\n\nIn fig. 9 we show how well the fitting formula is able to reproduce the behaviour of the escape fraction in relation to the stellar mass and redshift that we examined in fig. 2 . We see that the evolution of the escape fraction with redshift is successfully reproduced. However, the large gradients in \\(\\langle f_{\\rm esc}\\rangle\\) that are seen in fig. 8 are smoothed out. The reason for this likely lies in the optimization process used to find the fitting formula, as the mean squared error was used for optimization, and thus large gradients in the fitting function were disfavored because they led to large errors for the outer mass ranges.\n\nFig. 10 shows that the fitting formula is able to successfully predict the bimodality in the escape fraction, as seen in fig. 4 . However the boundary between the two modes is less pronounced. This is likely caused by the smoothing effect of the optimization process of the fitting function discussed above.\n\nFinally, by comparing fig. 11 to fig. 3 , we see that the fitting formula is able to reproduce all important trends, namely, the decrease in peak escape fraction with redshift and the approximate locations and values of the peaks. We also reproduce both the minima and maxima in the dependence of \\(f_{\\rm esc}\\) on \\(M_{\\rm gas}\\).\n\nAs mentioned earlier, it is important to emphasize that our modeling aims to capture the overall trends of LyC escape with galactic properties. Considering the inherent limitations in resolution and simplifications involved in estimating the LyC flux, it is crucial to scale the absolute value predicted by the fitting formula using a free parameter, which should be determined based on the specific ionizing photon budget required for reionization. We intend to investigate the large scale implication of these results and to determine scaling parameters in subsequent work.\n\n "]}, {"edit": ["level accuracy, we can obtain the message that LeViLM is not capable of performing complicated (multi-hop) reasoning over the scene knowledge and producing accurate predictions. Besides, the prediction process is black-box and can not be explainable, which can be further studied in the future. The answer is that (i) The current baselines can only achieve strong results on easy or medium tasks and are unable to perform well on the hard task; (ii) The interpretability of the baselines is poor.\n\n### Case Study\n\nTo further investigate the effects of knowledge, we perform qualitative analysis on four cases in the SK-VG dataset. Figure 5 shows the grounding results of four baselines on four referring expressions. It is observed that in the first case, all the baselines can ground the \"_cane_\" in the image even without the knowledge since there is only one cane presented. In the second case, the finetuned LeViLM can detect the target object even without knowledge, while it can not detect the \"_Brandon's servant_\" without knowledge in the third case. In the last case, all the baselines can not ground the referred object correctly, and the last three baselines all treat the \"_Spider-Man_\" as the \"_enemy_\". This shows that the baseline models can not perform accurate reasoning in some complicated cases, demonstrating the challenges.\n\n## 5 Concluding Remarks\n\nThe visual grounding field has emerged as a prominent attractive research direction, where the models are required to reason over vision and language to ground the target objects. Yet, the language part of the existing VG benchmarks is only simple description texts, which can not evaluate the reasoning capability of the models comprehensively. To take a step in this direction, we propose a new benchmark dataset called SK-VG, which requires models to reason over the (image, scene knowledge, query) triples to perform accurate reasoning. We propose two approaches to perform this new task: Knowledge-embedded Vision-Language Interaction and Linguistic-enhanced Vision-Language Matching. Experimental results confirm the validity of the proposed approaches but also show that there is still substantial room for improvement, e.g., reasoning and interpretability.\n\n## Acknowledgement\n\nThis work was supported in part by the Chinese Key-Area Research and Development Program of Guangdong Province (2020B0101350001), in part by the Guangdong Basic and Applied Basic Research Foundation (NO. 2020B1515020048), in part by the National Natural Science Foundation of China (NO. 61976250), in part by the Shenzhen Science and Technology Program (NO. JCYJ20220530141211024, NO. JCYJ20220818103001002), in part by the Fundamental Research Funds for the Central Universities under Grant 22lgqb25 and in part by the Guangdong Provincial Key Laboratory of Big Data Computing, The Chinese University of Hong Kong, Shenzhen. This work was also sponsored by Tencent CCF Open Fund (NO. RBFR2022009).\n\nFigure 5: The illustration of samples from the proposed SK-VG dataset, where a scene story and its four referring expressions are shown with the grounding results from four baseline methods.\n\n "], "nougat": ["level accuracy, we can obtain the message that LeViLM is not capable of performing complicated (multi-hop) reasoning over the scene knowledge and producing accurate predictions. Besides, the prediction process is black-box and can not be explainable, which can be further studied in the future. The answer is that (i) The current baselines can only achieve strong results on easy or medium tasks and are unable to perform well on the hard task; (ii) The interpretability of the baselines is poor.\n\n### Case Study\n\nTo further investigate the effects of knowledge, we perform qualitative analysis on four cases in the SK-VG dataset. Figure 5 shows the grounding results of four baselines on four referring expressions. It is observed that in the first case, all the baselines can ground the \"_cane_\" in the image even without the knowledge since there is only one cane presented. In the second case, the finetuned LeViLM can detect the target object even without knowledge, while it can not detect the \"_Brandon's servant_\" without knowledge in the third case. In the last case, all the baselines can not ground the referred object correctly, and the last three baselines all treat the \"_Spider-Man_\" as the \"_enemy_\". This shows that the baseline models can not perform accurate reasoning in some complicated cases, demonstrating the challenges.\n\n## 5 Concluding Remarks\n\nThe visual grounding field has emerged as a prominent attractive research direction, where the models are required to reason over vision and language to ground the target objects. Yet, the language part of the existing VG benchmarks is only simple description texts, which can not evaluate the reasoning capability of the models comprehensively. To take a step in this direction, we propose a new benchmark dataset called SK-VG, which requires models to reason over the (image, scene knowledge, query) triples to perform accurate reasoning. We propose two approaches to perform this new task: Knowledge-embedded Vision-Language Interaction and Linguistic-enhanced Vision-Language Matching. Experimental results confirm the validity of the proposed approaches but also show that there is still substantial room for improvement, e.g., reasoning and interpretability.\n\n## Acknowledgement\n\nThis work was supported in part by the Chinese Key-Area Research and Development Program of Guangdong Province (2020B0101350001), in part by the Guangdong Basic and Applied Basic Research Foundation (NO. 2020B1515020048), in part by the National Natural Science Foundation of China (NO. 61976250), in part by the Shenzhen Science and Technology Program (NO. JCYJ20220530141211024, NO. JCYJ20220818103001002), in part by the Fundamental Research Funds for the Central Universities under Grant 22lgqb25 and in part by the Guangdong Provincial Key Laboratory of Big Data Computing, The Chinese University of Hong Kong, Shenzhen. This work was also sponsored by Tencent CCF Open Fund (NO. RBFR2022009).\n\nFigure 5: The illustration of samples from the proposed SK-VG dataset, where a scene story and its four referring expressions are shown with the grounding results from four baseline methods.\n\n "]}, {"edit": ["\n\n# Unique continuation for an elliptic interface problem using unfitted isoparametric finite elements\n\nErik\n\nDepartment of Mathematics, University College London, Gower Street, London, WC1E 6BT, United Kingdom.\n\nJanosch\n\nCorresponding author(s). E-mail(s): j.preuss@ucl.ac.uk;\n\nContributing authors: e.burman@ucl.ac.uk;\n\n###### Abstract\n\nWe study unique continuation over an interface using a stabilized unfitted finite element method tailored to the conditional stability of the problem. The interface is approximated using an isoparametric transformation of the background mesh and the corresponding geometrical error is included in our error analysis. To counter possible destabilizing effects caused by non-conformity of the discretization and cope with the interface conditions, we introduce adapted regularization terms. This allows to derive error estimates based on conditional stability. Numerical experiments suggest that the presence of an interface seems to be of minor importance for the continuation of the solution beyond the data domain. On the other hand, certain convexity properties of the geometry are crucial as has already been observed for many other problems without interfaces.\n\nunfitted finite element method, unique continuation, interface problems, isoparametric finite element method, geometry errors, conditional Holder stability\n\n**MSC Classification:** 35J15 , 65N12 , 65N20 , 65N30 , 86-08\n\n## 1 Introduction\n"], "nougat": ["\n\n# Unique continuation for an elliptic interface problem using unfitted isoparametric finite elements\n\nErik\n\nDepartment of Mathematics, University College London, Gower Street, London, WC1E 6BT, United Kingdom.\n\nJanosch\n\nCorresponding author(s). E-mail(s): j.preuss@ucl.ac.uk;\n\nContributing authors: e.burman@ucl.ac.uk;\n\n###### Abstract\n\nWe study unique continuation over an interface using a stabilized unfitted finite element method tailored to the conditional stability of the problem. The interface is approximated using an isoparametric transformation of the background mesh and the corresponding geometrical error is included in our error analysis. To counter possible destabilizing effects caused by non-conformity of the discretization and cope with the interface conditions, we introduce adapted regularization terms. This allows to derive error estimates based on conditional stability. Numerical experiments suggest that the presence of an interface seems to be of minor importance for the continuation of the solution beyond the data domain. On the other hand, certain convexity properties of the geometry are crucial as has already been observed for many other problems without interfaces.\n\nunfitted finite element method, unique continuation, interface problems, isoparametric finite element method, geometry errors, conditional H \u0308older stability\n\n**MSC Classification:** 35J15 , 65N12 , 65N20 , 65N30 , 86-08\n\n## 1 Introduction\n"]}]