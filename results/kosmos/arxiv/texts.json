[
  {
    "edit": [
      "In Figure 9, we show the Leavitt laws for a sample of Cepheids in the LMC (Persson et al. 2004) and in NGC 7250 (Owens et al. 2023). The scatter in the JWST F115W data for NGC 7250 is a factor of two smaller than in the SHoES F160W data; i.e., the improved resolution and higher signal-to-noise ratio of the JWST data results in a lower variance ( \u03c3 2) for the F115W relation by almost a factor of four. This is all the more remarkable since the J-band data are single-phase observations only, while the HST observations have been corrected to mean light. The HST data exhibit more than three times the scatter of the H-band data for the LMC, the latter of which reflects the expected scatter for that band, as exemplified by the LMC data.\n\n## 12 Summary\n\nThe accuracy of the Cepheid distance scale has continued to improve over the century during which it has been used to measure the distances to nearby galaxies and set the scale for the determination of H\\({}_{0}\\). Still, challenges remain in overcoming systematic uncertainties. Many of these challenges will be overcome with new capabilities provided by the JWST.\n\nNew JWST data for the nearby galaxy NGC 7250 already demonstrate that (1) many of the Cepheids observed with HST/WFC3 are significantly crowded (and biased to brighter apparent magnitudes) by nearby neighbors. A re-analysis of the SH0ES optical data, then coupled with the new high-resolution and higher signal-to-noise JWST F115W data, leads to significantly reduced effects of crowding and smaller photometric uncertainties. (2) These improvements result in a factor of two lower scatter in the near-infrared Leavitt law for JWST F115W compared with HST F160W, even with single-epoch F115W JWST photometry.\n\nThe galaxies in our JWST CCHP program sample have all been selected to have with distances \u227220 Mpc, close enough to minimize crowding effects. As for the case of NGC 7250 presented here, these data will be combined with a re-analysis of the SH0ES HST optical data for the Cepheids. TRGB, carbon star, and Cepheid distances to the same sample of galaxies being observed as part of the CCHP will allow measurement of three independent distances to each\n\nFigure 8: Four Cepheids in NGC 7250 discovered as part of the SH0ES project (bottom row: HST F160W/ H -band exposures; top row: JWST F115W/ J band). Each postage-stamp image is 2 \u00d7 2 arcsec on a side. The red circles are centered at the position of the Cepheid determined from the HST optical (F350LP white light) photometry. It is immediately evident that the crowding for these Cepheids is quite severe and the signal-to-noise ratio (SNR) for the H -band data tends to be low, ranging from 1 to 23. In contrast, the SNR for the J -band data ranges from 36 to 121. On average, the JWST data have almost an order of magnitude greater SNR, and a four times better angular resolution, allowing the Cepheids to be distinguished clearly from the background.\n\n "
    ],
    "kosmos": [
      "In Figure 9, we show the Leavitt laws for a sample of Cepheids in the LMC (Persson et al. 2004) and in NGC 7250 (Owens et al. 2023). The scatter in the JWST F115W data for NGC 7250 is a factor of two smaller than in the SHoES F160W data; i.e., the improved resolution and higher signal-to-noise ratio of the JWST data results in a lower variance ( \u03c3 2) for the F115W relation by almost a factor of four. This is all the more remarkable since the J-band data are single-phase observations only, while the HST observations have been corrected to mean light. The HST data exhibit more than three times the scatter of the H-band data for the LMC, the latter of which reflects the expected scatter for that band, as exemplified by the LMC data.\n\n## 12 Summary\n\nThe accuracy of the Cepheid distance scale has continued to improve over the century during which it has been used to measure the distances to nearby galaxies and set the scale for the determination of H\\({}_{0}\\). Still, challenges remain in overcoming systematic uncertainties. Many of these challenges will be overcome with new capabilities provided by the JWST.\n\nNew JWST data for the nearby galaxy NGC 7250 already demonstrate that (1) many of the Cepheids observed with HST/WFC3 are significantly crowded (and biased to brighter apparent magnitudes) by nearby neighbors. A re-analysis of the SH0ES optical data, then coupled with the new high-resolution and higher signal-to-noise JWST F115W data, leads to significantly reduced effects of crowding and smaller photometric uncertainties. (2) These improvements result in a factor of two lower scatter in the near-infrared Leavitt law for JWST F115W compared with HST F160W, even with single-epoch F115W JWST photometry.\n\nThe galaxies in our JWST CCHP program sample have all been selected to have with distances \u227220 Mpc, close enough to minimize crowding effects. As for the case of NGC 7250 presented here, these data will be combined with a re-analysis of the SH0ES HST optical data for the Cepheids. TRGB, carbon star, and Cepheid distances to the same sample of galaxies being observed as part of the CCHP will allow measurement of three independent distances to each\n\nFigure 8: Four Cepheids in NGC 7250 discovered as part of the SH0ES project (bottom row: HST F160W/ H -band exposures; top row: JWST F115W/ J band). Each postage-stamp image is 2 \u00d7 2 arcsec on a side. The red circles are centered at the position of the Cepheid determined from the HST optical (F350LP white light) photometry. It is immediately evident that the crowding for these Cepheids is quite severe and the signal-to-noise ratio (SNR) for the H -band data tends to be low, ranging from 1 to 23. In contrast, the SNR for the J -band data ranges from 36 to 121. On average, the JWST data have almost an order of magnitude greater SNR, and a four times better angular resolution, allowing the Cepheids to be distinguished clearly from the background.\n\n "
    ]
  },
  {
    "edit": [
      "\\[\\alpha_{\\rm eff}=\\beta_{\\rm eff} \\tag{50}\\] \\[= \u21d2\\alpha\\frac{N_{1}}{L}=\\beta\\bigg{(}1-\\frac{N_{2}}{L}\\bigg{)} \\tag{51}\\]\n\nBelow we obtain the exact location \\(x_{w}\\) and height \u2206 of the LDW in terms of the control parameters. In the DW phase, particle number in T can be expressed as\n\n\\[N_{T}=L\\int_{0}^{1}\\rho(x)dx, \\tag{52}\\]\n\nwhere a multiplicative factor L is introduced in the righthand side to rescale the integration limit of the position variable x. Using ( 48 ) and ( 51 ) in ( 52 ), we get: \\[N_{T}=L\\bigg{[}\\alpha\\frac{N_{1}}{L}(2x_{w}-1)+1-x_{w}\\bigg{]}.\\] (53)\n\nIdentifying the steady state TASEP current in the DW phase as \\(J_{T}=\\rho_{\\rm LD}(1-\\rho_{\\rm LD})\\) or \\(J_{T}=\\rho_{\\rm HD}(1-\\rho_{\\rm HD})\\) and substituting the expressions of \\(N_{T}\\) [see eq. ( 53 )] and \\(J_{T}\\) in eq. ( 17a ) together with PNC, we obtain the following two equations coupled in \\(N_{1}/L\\) and \\(x_{w}\\):\n\n\\[\\frac{N_{1}}{L}\\bigg{[}1-\\frac{\\alpha}{\\beta}+\\alpha(2x_{w}-1)\\bigg{]}-x_{w}+1= \\mu-1, \\tag{54}\\] \\[\\frac{N_{1}}{L}=\\frac{k_{2}}{k_{1}+k_{2}}\\bigg{[}\\mu-\\alpha\\frac{N _{1}}{L}(2x_{w}-1)-1+x_{w}\\bigg{]}\\] (55) \\[\\qquad-\\frac{1}{L(k_{1}+k_{2})}\\alpha\\frac{N_{1}}{L}\\bigg{(}1- \\alpha\\frac{N_{1}}{L}\\bigg{)}.\\]\n\nWhile solving eqs. ( 54 ) and ( 55 ) for \\(N_{1}/L\\) and \\(x_{w}\\), we get a qudratic equation for \\(N_{1}/L\\) with two solutions:\n\n\\[\\bigg{(}\\frac{N_{1}}{L}\\bigg{)}^{\\pm}=\\bigg{(}\\frac{1}{2\\alpha}+\\frac{k_{10}}{2 \\alpha^{2}}+\\frac{k_{20}}{2\\alpha\\beta}\\bigg{)} \\tag{56}\\] \\[\\qquad\\pm\\bigg{[}\\bigg{(}\\frac{1}{2\\alpha}+\\frac{k_{10}}{2\\alpha^{ 2}}+\\frac{k_{20}}{2\\alpha\\beta}\\bigg{)}^{2}-\\frac{k_{20}}{\\alpha^{2}}\\bigg{]}^{ 1\\over 2}.\\]\n\nThe density in the LD part of the DW is thus\n\n\\[\\rho_{\\rm LD}=\\alpha\\frac{N_{1}}{L}=\\bigg{(}\\frac{1}{2}+\\frac{k_{10}}{2\\alpha}+ \\frac{k_{20}}{2\\beta}\\bigg{)}\\pm\\bigg{[}\\bigg{(}\\frac{1}{2}+\\frac{k_{10}}{2 \\alpha}+\\frac{k_{20}}{2\\beta}\\bigg{)}^{2}-k_{20}\\bigg{]}^{1\\over 2}. \\tag{57}\\]\n\nAt the boundary between the LD and the DW phases, MFT must predict identical (low) density in the bulk of T. We now argue that in ( 57 ) the solution with a negative discriminant is actually the physically acceptable solution. Equating the density in LD phase [eq. ( 25 )] and density in the LD domain of DW phase [eq. ( 57 ) with negative discriminant], we obtain the LD-DW boundary as follows:\n\n\\[\\bigg{(}\\frac{1+k_{20}}{2}+\\frac{k_{10}+k_{20}}{2\\alpha}\\bigg{)}\\] (58) \\[\\qquad-\\bigg{[}\\bigg{(}\\frac{1+k_{20}}{2}+\\frac{k_{10}+k_{20 "
    ],
    "kosmos": [
      "\\[\\alpha_{\\rm eff}=\\beta_{\\rm eff} \\tag{50}\\] \\[= \u21d2\\alpha\\frac{N_{1}}{L}=\\beta\\bigg{(}1-\\frac{N_{2}}{L}\\bigg{)} \\tag{51}\\]\n\nBelow we obtain the exact location \\(x_{w}\\) and height \u2206 of the LDW in terms of the control parameters.\n\nIn the DW phase, particle number in T can be expressed as\n\n\\[N_{T}=L\\int_{0}^{1}\\rho(x)dx, \\tag{52}\\]\n\nwhere a multiplicative factor L is introduced in the right-hand side to rescale the integration limit of the position variable x. Using ( 48 ) and ( 51 ) in ( 52 ), we get:\n\n\\[N_{T}=L\\bigg{[}\\alpha\\frac{N_{1}}{L}(2x_{w}-1)+1-x_{w}\\bigg{]}. \\tag{53}\\]\n\nIdentifying the steady state TASEP current in the DW phase as \\(J_{T}=\\rho_{\\rm LD}(1-\\rho_{\\rm LD})\\) or \\(J_{T}=\\rho_{\\rm HD}(1-\\rho_{\\rm HD})\\) and substituting the expressions of \\(N_{T}\\) [see eq. ( 53 )] and \\(J_{T}\\) in eq. ( 17a ) together with PNC, we obtain the following two equations coupled in \\(N_{1}/L\\) and \\(x_{w}\\):\n\n\\[\\frac{N_{1}}{L}\\bigg{[}1-\\frac{\\alpha}{\\beta}+\\alpha(2x_{w}-1)\\bigg{]}-x_{w}+1= \\mu-1, \\tag{54}\\] \\[\\frac{N_{1}}{L}=\\frac{k_{2}}{k_{1}+k_{2}}\\bigg{[}\\mu-\\alpha\\frac{N _{1}}{L}(2x_{w}-1)-1+x_{w}\\bigg{]}\\] (55) \\[\\qquad-\\frac{1}{L(k_{1}+k_{2})}\\alpha\\frac{N_{1}}{L}\\bigg{(}1- \\alpha\\frac{N_{1}}{L}\\bigg{)}.\\]\n\nWhile solving eqs. ( 54 ) and ( 55 ) for \\(N_{1}/L\\) and \\(x_{w}\\), we get a qudratic equation for \\(N_{1}/L\\) with two solutions:\n\n\\[\\bigg{(}\\frac{N_{1}}{L}\\bigg{)}^{\\pm}=\\bigg{(}\\frac{1}{2\\alpha}+\\frac{k_{10}}{2 \\alpha^{2}}+\\frac{k_{20}}{2\\alpha\\beta}\\bigg{)} \\tag{56}\\] \\[\\qquad\\pm\\bigg{[}\\bigg{(}\\frac{1}{2\\alpha}+\\frac{k_{10}}{2\\alpha^{ 2}}+\\frac{k_{20}}{2\\alpha\\beta}\\bigg{)}^{2}-\\frac{k_{20}}{\\alpha^{2}}\\bigg{]}^{ 1\\over 2}.\\]\n\nThe density in the LD part of the DW is thus\n\n\\[\\rho_{\\rm LD}=\\alpha\\frac{N_{1}}{L}=\\bigg{(}\\frac{1}{2}+\\frac{k_{10}}{2\\alpha}+ \\frac{k_{20}}{2\\beta}\\bigg{)}\\pm\\bigg{[}\\bigg{(}\\frac{1}{2}+\\frac{k_{10}}{2 \\alpha}+\\frac{k_{20}}{2\\beta}\\bigg{)}^{2}-k_{20}\\bigg{]}^{1\\over 2}. \\tag{57}\\]\n\nAt the boundary between the LD and the DW phases, MFT must predict identical (low) density in the bulk of T. We now argue that in ( 57 ) the solution with a negative discriminant is actually the physically acceptable solution. Equating the density in LD phase [eq. ( 25 )] and density in the LD domain of DW phase [eq. ( 57 ) with negative discriminant], we obtain the LD-DW boundary as follows:\n\n\\[\\bigg{(}\\frac{1+k_{20}}{2}+\\frac{k_{10}+k_{20}}{2\\alpha}\\bigg{)}\\] (58) \\[\\qquad-\\bigg{[}\\bigg{(}\\frac{1+k_{20}}{2}+\\frac{k_{10}+k_{20}}{ "
    ]
  },
  {
    "edit": [
      "For our experiments, ondemand mode with baseload cores 3 has an optimum energy usage when calculating same number of samples compared to other baseload and samples per second values for different powermodes and CPU core baseloads. For our experiments, ondemand mode with baseload cores 3 has an optimum energy usage when calculating same number of samples compared to other baseload and samples per second combinations.\n\n## IV Conclusion and Future Work\n\nRecent research studies have focused on energy-efficient and carbon-efficient FL scheduling and client selection. However, most of the research assumes simplistic energy consumption models for underlying FL clients. In this work, we showed that how energy per sample values were calculated using Eq. 1. We report mean energy per sample and 95% confidence intervals for each powermode, based on 10 repeated measurements. We observe significant difference in energy per sample and samples per second values when there is no nonFL base load (0 baseload cores) compared to a scenario when non-FL baseload is executing and utilizing all CPU cores. We also observe that while samples per second (Figure 1b) doesn\u2019t vary significantly when non-FL baseload is co-running with FL, energy per sample values fluctuate for baseloads 3 and 4. For our experiments, ondemand mode with baseload cores 3 has an optimum energy usage when calculating same number of samples compared to other baseload and samples per second combinations. This repetitive FL training execution could be leveraged to collect more data about power-performance behavior patterns of FL clients.\n\n## References\n\n* [1]B. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas (2016) Communication-efficient learning of deep networks from decentralized data. In AISTATS, Cited by: SSI.\n* [2]Y. G. Kim and C. J. Wu (2021) AutoFL: enabling heterogeneity-aware energy efficient federated learning. In IEEE/ACM International Symposium on Microarchitecture (MICRO), pp. 183-198. Cited by: SSI.\n* [3]H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas (2016) Communication-efficient learning of deep networks from decentralized data. In AISTATS, Cited by: SSI.\n* [4]X. Qiu, T. Parcollet, D. J. Beutel, T. Topal, A. Mathur, and N. D. Lane (2020) A first look into the carbon footprint of federated learning. CoRRabs/2010.06537. Cited by: SSI.\n* [5]P. Wiesner, R. Khalili, D. Grinwald, P. Agrawal, L. Thamsen, and O. Kao (2023) FedZero: leveraging renewable excess energy in federated learning. arXiv preprint arXiv:2305.15092. Cited by: SSI.\n* [6]C. W. Zaw, S. R. Pandey, K. Kim, and C. S. Hong (2021) Energy-aware resource management for federated learning in multi-access edge computing systems. IEEE Access9, pp. 34938-34950. Cited by: SSI.\n* [7]X. Zhou, J. Zhao, H. Han, and C. Guet (2022) Joint optimization of energy consumption and completion time in federated learning. In ICDCS, Cited by: SSI.\n* [8]B. G\u00fcler and A. Yener (2021) A framework for sustainable federated learning. In International Symposium on Modeling and Optimization in Mobile, Ad hoc, and Wireless Networks (WiOpt), pp. . Cited by: SSI.\n* [9]X. Zhou, J. Zhao, H. Han, and C. Guet (2022) Joint optimization of energy consumption and completion time in federated learning. In ICDCS, Cited by: SSI.\n* [10]C. Zeng, Y. Zhang, and Y. Liu (2022) Energy-aware federated learning. IEEE Transactions on Wireless Communications21 (11), pp. 6991-7005. Cited by: SSI.\n* [11]X. Zhou, J. Zhao, H. Han, and C. Guet (2022) Joint optimization of energy consumption and completion time in federated learning. In ICDCS, Cited by: SSI.\n* [12]X. Zhou, J. Zhao, H. Han, and C. Guet (2022) Joint optimization of energy consumption and completion time in federated learning. In ICDCS, Cited by: SSI.\n* [13]X. Zhou, J "
    ],
    "kosmos": [
      "For our experiments, ondemand mode with baseload cores 3 has an optimum energy usage when calculating same number of samples compared to other baseload and samples per second combinations.\n\n## IV Conclusion and Future Work\n\nRecent research studies have focused on energy-efficient and carbon-efficient FL scheduling and client selection. However, most of the research assumes simplistic energy consumption models for underlying FL clients. In this work, we showed that how energy per sample values under real-world scenarios such as different power modes and non-FL baseloads at CPU cores can vary and exhibit complex operational behavior patterns.\n\nFor future work, following open research questions and possibilities could be explored further,\n\n* How do current FL systems communicate FL clients\u2019 energy related information? How to collect energy per sample, throughput per second and uncertainty related information at runtime?\n* How can we predict the power-performance characteristics, what are the relevant metrics? With more data about real-world impact factors affecting energy footprint of edge devices, can we build predictive models for forecasting?\n* How often do we need to measure before we can be certain? Can we report the uncertainty to be used in scheduling? FL trainings are usually executed multiple times due to data distribution drifts and hyperparameter search. This repetitive FL training execution could be leveraged to collect more data about power-performance behavior patterns of FL clients.\n* What\u2019s the impact of hardware accelerated edge devices such as jetson nano on energy related metrics? What are the energy efficiency opportunities in FL and non-FL co-running scenarios?\n\n## References\n\n* [1] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, \u201cCommunication-efficient learning of deep networks from decentralized data,\u201d in _AISTATS_, 2016.\n* [2] X. Qiu, T. Parcollet, D. J. Beutel, T. Topal, A. Mathur, and N. D. Lane, \u201cA first look into the carbon footprint of federated learning,\u201d _CoRR_, vol. abs/2010.06537, 2020.\n* [3] X. Zhou, J. Zhao, H. Han, and C. Guet, \u201cJoint optimization of energy consumption and completion time in federated learning,\u201d in _ICDCS_, IEEE, 2022.\n* [4] C. W. Zaw, S. R. Pandey, K. Kim, and C. S. Hong, \u201cEnergy-aware resource management for federated learning in multi-access edge computing systems,\u201d _IEEE Access_, vol. 9, pp. 34938-34950, 2021.\n* [5] Y. G. Kim and C.-J. Wu, \u201cAutofl: Enabling heterogeneity-aware energy efficient federated learning,\u201d in _IEEE/ACM International Symposium on Microarchitecture (MICRO)_, pp. 183-198, 2021.\n* [6] P. Wiesner, R. Khalili, D. Grinwald, P. Agrawal, L. Thamsen, and O. Kao, \u201cFedzero: Leveraging renewable excess energy in federated learning,\u201d _arXiv preprint arXiv:2305.15092_, 2023.\n* [7] B. Guler and A. Yener, \u201cA framework for sustainable federated learning,\u201d in _International Symposium on Modeling and Optimization in Mobile, Ad hoc, and Wireless Networks (WiOpt)_, IEEE, 2021.\n* [8] B. Rupprecht, D. Hujo, and B. Vogel-Heuser, \u201cPerformance evaluation of ai algorithms on heterogeneous edge devices for manufacturing,\u201d in _International Conference on Automation Science and Engineering (CASE)_, pp. 2132-2139, IEEE, 2022.\n\n "
    ]
  },
  {
    "edit": [
      "Figure 6: Lyric Query Detection\n\nIn the process of scanning the database, the system identifies all the songs that have the word \u2019good\u2019 either in their titles or in their lyrics. The first song that meets this criterion is \u2019Good Life\u2019 by Kehlani and G-Eazy. This song contains the word \u2019good\u2019 not only in its title but also within its lyrical content. It\u2019s important to note that the search doesn\u2019t stop at \u2019Good Life.\u2019 There might be many other songs with the word \u2019good\u2019 in their titles or lyrics within the database. The given information only indicates the first result that the system presents in response to the query \u2019good.\u2019 To summarize, the system is a music-centric search tool that uses the provided query to locate songs in its database containing the term \u2019good\u2019 either in their titles or lyrics. The first song it finds is \u2019Good Life\u2019 by Kehlani and G-Eazy, which contains \u2019good\u2019 both in its title and lyrics. The system can provide more results if the user wishes to explore other songs with the word \u2019good.\u2019 In Figure 7, the dataset consists of song lyrics from the year 1950 to 2019. This provides a wellspread dataset which allows us to get results from over 60 years, thus making the search engine efficient enough to provide songs from every year. The year 2017 has the most songs amounting upto 660 songs from various different genres and emotions. \n\nFigure 7: Distribution of songs throughout the years\n\nIn Figure 8, the dataset has been categorized into 7 major genres namely pop, country, blues, rock, jazz, reggae and hip hop. There are about 7042 songs under the genre pop. This includes pop "
    ],
    "kosmos": [
      "Figure 6: Lyric Query Detection\n\nIn the process of scanning the database, the system identifies all the songs that have the word \u2019good\u2019 either in their titles or in their lyrics. The first song that meets this criterion is \u2019Good Life\u2019 by Kehlani and G-Eazy. This song contains the word \u2019good\u2019 not only in its title but also within its lyrical content.\n\nIt\u2019s important to note that the search doesn\u2019t stop at \u2019Good Life.\u2019 There might be many other songs with the word \u2019good\u2019 in their titles or lyrics within the database. The given information only indicates the first result that the system presents in response to the query \u2019good.\u2019\n\nTo summarize, the system is a music-centric search tool that uses the provided query to locate songs in its database containing the term \u2019good\u2019 either in their titles or lyrics. The first song it finds is \u2019Good Life\u2019 by Kehlani and G-Eazy, which contains \u2019good\u2019 both in its title and lyrics. The system can provide more results if the user wishes to explore other songs with the word \u2019good.\u2019\n\nIn Figure 7, the dataset consists of song lyrics from the year 1950 to 2019. This provides a wellspread dataset which allows us to get results from over 60 years, thus making the search engine efficient enough to provide songs from every year. The year 2017 has the most songs amounting upto 660 songs from various different genres and emotions.\n\nFigure 7: Distribution of songs throughout the years\n\nIn Figure 8, the dataset has been categorized into 7 major genres namely - pop, country, blues, rock, jazz, reggae and hip hop. There are about 7042 songs under the genre pop. This includes pop "
    ]
  },
  {
    "edit": [
      "\n\n## 3 Experiments with C++ data\n\nTo answer the first research question, we used the original C++ method-level vulnerability dataset from [1]. After parsing, we obtained the following statistics of the input graphs:\n\n11788 train graphs (956 vulnerable), 1667 validation graphs (133 vulnerable), 3385 test graphs (286 vulnerable)\n\nTo test each dimension of RQ 1, we performed 10 trials of training the model. In each trial, the dataset was split into train, validation, and test parts anew. The results can be found in Table 1.\n\n<table>\n<thead>\n<tr>\n<th>\nConfiguration\n</th>\n<th>\nMedian F1\n</th>\n<th>\nMedian ROC AUC\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\nBaseline\n</td>\n<td>\n27.29\n</td>\n<td>\n0.696\n</td>\n</tr>\n<tr>\n<td>\nWithout SMOTE &amp; RL\n</td>\n<td>\n21.45\n</td>\n<td>\n0.730\n</td>\n</tr>\n<tr>\n<td>\nWithout AST edges\n</td>\n<td>\n27.65\n</td>\n<td>\n0.706\n</td>\n</tr>\n<tr>\n<td>\nWith pruning\n</td>\n<td>\n30.83\n</td>\n<td>\n0.724\n</td>\n</tr>\n<tr>\n<td>\nMajority downsampling\n</td>\n<td>\n26.61\n</td>\n<td>\n0.678\n</td>\n</tr>\n</tbody>\n</table>\n\nTable 1. Results of experiments for research question 1\n\ngraph embedding size 200, batch size 128, maximum number of batches 10000, number of gradient accumulation steps 8, maximum patience of 50 for C++ data and 20 for Java data.\n\n## 3 Experiments with C++ data\n\nTo answer the rest of research questions, we trained and tested the model on different parts of the Java dataset (1): \\(P_{1}\\), \\(P_{2}\\), and \\(P_{3}\\). In particular, we varied \\(k\\) in the range from 1 to 14. Then, we plotted the resulting ROC AUC scores against \\(k\\), and draw conclusions based on the observed dynamics. To make set \\(P_{3}\\) to be independent of \\(k\\), we fixed it to be the complement of \\(P_{1}\\). That is, \\(P_{3}\\) consisted of functions that remained unchanged in the commits where only one function was changed. Also, in order to balance different parts involved in training and testing, we restricted the size of \\(P_{3}\\):\n\n\\(|P_{3}|=|P_{1}|+|P_{2}|\\)\n\nDuring the data cleaning phase, we ensured that in each experiment, \\(P_{3}\\) did not contain functions that are contained in \\(P_{1}\\cup P_{2}\\). Also, we removed any duplicate functions from each of the parts \\(P_{1}\\), \\(P_{2}\\), and \\(P_{3}\\), and removed methods contained in the training data from the test data.\n\nTable 2 shows the distribution of the collected Java methods after stratification by \\(k\\) and cleaning the data:\n\n<table>\n<thead>\n<tr>\n<th>\n</th>\n<th>\nP1\n</th>\n<th>\n</th>\n<th>\nP2\n</th>\n<th>\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\nk\n</td>\n<td>\ntrain\n</td>\n<td>\ntest\n</td>\n<td>\ntrain\n</td>\n<td>\ntest\n</td>\n</tr>\n<tr>\n<td>\n1\n</td>\n<td>\n410 (205)\n</td>\n<td>\n135 (68)\n</td>\n<td>\n0 (0)\n</td>\n<td>\n0 (0)\n</td>\n</tr>\n<tr>\n<td>\n2\n</td>\n<td>\n399 (200)\n</td>\n<td>\n145 (73)\n</td>\n<td>\n343 (171)\n</td>\n<td>\n122 (61)\n</td>\n</tr>\n<tr>\n<td>\n3\n</td>\n<td>\n416 (208)\n</td>\n<td>\n132 (66)\n</td>\n<td>\n696 (347)\n</td>\n<td>\n228 (113)\n</td>\n</tr>\n<tr>\n<td>\n4\n</td>\n<td>\n414 (207)\n</td>\n<td>\n128 (65)\n</td>\n<td>\n960 (479)\n</td>\n<td>\n346 (172)\n</td>\n</tr>\n<tr>\n<td>\n5\n</td>\n<td>\n415 (210)\n</td>\n<td>\n129 (64)\n</td>\n<td>\n1159 (575)\n</td>\n<td>\n433 (217)\n</td>\n</tr>\n<tr>\n<td>\n6\n</td>\n<td>\n414 (208)"
    ],
    "kosmos": [
      "\n\n## 3 Experiments with C++ data\n\nTo answer the first research question, we used the original C++ method-level vulnerability dataset from [1]. After parsing, we obtained the following statistics of the input graphs:\n\n11788 train graphs (956 vulnerable), 1667 validation graphs (133 vulnerable), 3385 test graphs (286 vulnerable)\n\nTo test each dimension of RQ 1, we performed 10 trials of training the model. In each trial, the dataset was split into train, validation, and test parts anew. The results can be found in Table 1.\n\n<table>\n<thead>\n<tr>\n<th>\nConfiguration\n</th>\n<th>\nMedian F1\n</th>\n<th>\nMedian ROC AUC\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\nBaseline\n</td>\n<td>\n27.29\n</td>\n<td>\n0.696\n</td>\n</tr>\n<tr>\n<td>\nWithout SMOTE &amp; RL\n</td>\n<td>\n21.45\n</td>\n<td>\n0.730\n</td>\n</tr>\n<tr>\n<td>\nWithout AST edges\n</td>\n<td>\n27.65\n</td>\n<td>\n0.706\n</td>\n</tr>\n<tr>\n<td>\nWith pruning\n</td>\n<td>\n30.83\n</td>\n<td>\n0.724\n</td>\n</tr>\n<tr>\n<td>\nMajority downsampling\n</td>\n<td>\n26.61\n</td>\n<td>\n0.678\n</td>\n</tr>\n</tbody>\n</table>\n\nTable 1. Results of experiments for research question 1\n\ngraph embedding size 200, batch size 128, maximum number of batches 10000, number of gradient accumulation steps 8, maximum patience of 50 for C++ data and 20 for Java data.\n\n## 3 Experiments with C++ data\n\nTo answer the rest of research questions, we trained and tested the model on different parts of the Java dataset (1): \\(P_{1}\\), \\(P_{2}\\), and \\(P_{3}\\). In particular, we varied \\(k\\) in the range from 1 to 14. Then, we plotted the resulting ROC AUC scores against \\(k\\), and draw conclusions based on the observed dynamics. To make set \\(P_{3}\\) to be independent of \\(k\\), we fixed it to be the complement of \\(P_{1}\\). That is, \\(P_{3}\\) consisted of functions that remained unchanged in the commits where only one function was changed. Also, in order to balance different parts involved in training and testing, we restricted the size of \\(P_{3}\\):\n\n\\(|P_{3}|=|P_{1}|+|P_{2}|\\)\n\nDuring the data cleaning phase, we ensured that in each experiment, \\(P_{3}\\) did not contain functions that are contained in \\(P_{1}\\cup P_{2}\\). Also, we removed any duplicate functions from each of the parts \\(P_{1}\\), \\(P_{2}\\), and \\(P_{3}\\), and removed methods contained in the training data from the test data.\n\nTable 2 shows the distribution of the collected Java methods after stratification by \\(k\\) and cleaning the data:\n\n<table>\n<thead>\n<tr>\n<th>\n</th>\n<th>\nP1\n</th>\n<th>\n</th>\n<th>\nP2\n</th>\n<th>\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\nk\n</td>\n<td>\ntrain\n</td>\n<td>\ntest\n</td>\n<td>\ntrain\n</td>\n<td>\ntest\n</td>\n</tr>\n<tr>\n<td>\n1\n</td>\n<td>\n410 (205)\n</td>\n<td>\n135 (68)\n</td>\n<td>\n0 (0)\n</td>\n<td>\n0 (0)\n</td>\n</tr>\n<tr>\n<td>\n2\n</td>\n<td>\n399 (200)\n</td>\n<td>\n145 (73)\n</td>\n<td>\n343 (171)\n</td>\n<td>\n122 (61)\n</td>\n</tr>\n<tr>\n<td>\n3\n</td>\n<td>\n416 (208)\n</td>\n<td>\n132 (66)\n</td>\n<td>\n696 (347)\n</td>\n<td>\n228 (113)\n</td>\n</tr>\n<tr>\n<td>\n4\n</td>\n<td>\n414 (207)\n</td>\n<td>\n128 (65)\n</td>\n<td>\n960 (479)\n</td>\n<td>\n346 (172)\n</td>\n</tr>\n<tr>\n<td>\n5\n</td>\n<td>\n415 (210)\n</td>\n<td>\n129 (64)\n</td>\n<td>\n1159 (575)\n</td>\n<td>\n433 (217)\n</td>\n</tr>\n<tr>\n<td>\n6\n</td>\n<td>\n414 (208)"
    ]
  },
  {
    "edit": [
      "&gt; 4\u00b7 2 1 3 2 0 5 3 1 5 1 0 3 2 1 3 3 0 2 0 3 0 4 5 3 2 2 5 5 0 2 3 5 5\n&gt;\n&gt; 0\u00b7 2 5 0 326.\n\nAfter expanding further the minors of above determinant based on Theorem 1, we see that this result is the same as the result of Example 2. For i = 2, we have:\n\n&gt; A = 3 0 4 2 4 0 5 1 0 2 5 1 3 0 3 3 1 2 0 3 2 5 0 4 3\n\n= 2\u00b7 4 0 1 0 5 1 5 0 2 4 5 1 0 3 3 1 2 0 3 2 5 0 4 3\n\n&gt; 0\u00b7 3 5 1 5 1 5 1 5 1 0 3 3 1 2 0 3 2 5 0 4 3\n\n= 0\u00b7 4 0 1 0 5 1 5 0 2 4 5 1 0 3 3 1 2 0 3 2 5 0 4 3\n\n&gt; 0\u00b7 3 5 1 5 1 5 1 5 1 0 3 3 1 2 0 3 2 5 0 4 3\n\nAfter expanding further the minors of above determinant based on Theorem 1, we see that this result is the same as the result of Example 2. For i = 3, we have:\n\n&gt; A = 3 0 4 2 4 0 5 1 0 2 5 1 3 0 3 3 1 2 0 3 2 5 0 4 3\n\n= 0\u00b7 4 0 1 0 5 1 5 0 2 4 5 1 0 3 3 1 2 0 3 2 5 0 4 3\n\n&gt; 0\u00b7 3 5 1 5 1 5 1 5 1 0 3 3 1 2 0 3 2 5 0 4 3\n\n= 0\u00b7 4 0 1 0 5 1 5 0 2 4 5 1 0 3 3 1 2 0 3 2 5 0 4 3\n\n&gt; 0\u00b7 3 5 1 5 1 5 1 5 1 0 3 3 1 2 0 3 2 5 0 4 3\n\nAfter expanding further the minors of above determinant based on Theorem 1, we see that this result is the same as the result of Example 2. For j = 1, we have:\n\n&gt; A = 3 0 4 2 4 0 5 1 0 2 5 1 3 0 3 3 1 2 0 3 2 5 0 4 3\n\n= 3\u00b7 0 3 1 2 4 5 3 2 0 5 "
    ],
    "kosmos": [
      "&gt; 4\u00b7 2 1 3 2 0 5 3 1 5 1 0 3 2 1 3 3 0 2 0 3 0 4 5 3 2 2 5 5 0 2 3 5 5\n&gt;\n&gt; 0\u00b7 2 5 0 326.\n\nAfter expanding further the minors of above determinant based on Theorem 1, we see that this result is the same as the result of Example 2. For i = 2, we have:\n\n&gt; A = 3 0 4 2 4 0 5 1 0 2 5 1 3 0 3 3 1 2 0 3 2 5 0 4 3\n\n= 2\u00b7 4 0 1 0 5 1 5 0 4 5 1 0 2 5 1 3 0 3 3 1 2 0 3 2 5 0 4 3\n\n&gt; 0\u00b7 3 4 5 0 326.\n\nAfter expanding further the minors of above determinant based on Theorem 1, we see that this result is the same as the result of Example 2. For i = 3, we have:\n\n&gt; A = 3 0 4 2 4 0 5 1 0 2 5 1 3 0 3 3 1 2 0 3 2 5 0 4 3\n\n= 0\u00b7 4 0 1 0 5 1 5 0 4 5 1 0 2 5 1 3 0 3 3 1 2 0 3 2 5 0 4 3\n\n&gt; 0\u00b7 3 4 5 0 326.\n\nAfter expanding further the minors of above determinant based on Theorem 1, we see that this result is the same as the result of Example 2. For j = 1, we have:\n\n&gt; A = 3\u00b7 0 3 1 2 4 0 1 0 5 1 2 0 3 3 1 2 0 3 2 5 0 4 3\n\n= 3\u00b7 0 3 1 2 4 0 1 0 5 1 2 0 3 3 1 2 0 3 2 5 0 4 3\n\n&gt; 0\u00b7 3 1 2 4 5 0 326.\n\n "
    ]
  },
  {
    "edit": [
      "Let us denote by T = \\((\\kappa\\times\\omega\\times\\operatorname{acc}(\\kappa)\\times\\gamma\\times\\kappa \\times\\kappa\\times\\kappa\\times\\kappa)^{\\leq\\gamma}\\). For every \\(\\xi\\in\\mathcal{T}\\) there are functions \\(\\{\\xi_{i}\\in\\kappa^{\\leq\\omega}\\mid 0<i\\leq 8\\}\\)=\"\" 8\\),=\"\" 8\\}\\)=\"\" 8\\}\\).=\"\" 8\\}\\).\n\n "
    ],
    "kosmos": [
      "Let us denote by T = \\((\\kappa\\times\\omega\\times\\operatorname{acc}(\\kappa)\\times\\gamma\\times\\kappa \\times\\kappa\\times\\kappa\\times\\kappa)^{\\leq\\gamma}\\). For every \\(\\xi\\in\\mathcal{T}\\) there are functions \\(\\{\\xi_{i}\\in\\kappa^{\\leq\\omega}\\mid 0<i\\leq 8\\}\\)=\"\" 8\\),=\"\" 8\\}\\)=\"\" 8\\}\\).=\"\" 8\\}\\).\n\n "
    ]
  },
  {
    "edit": [
      "whenever tr(\u03c1^{2}) \u2264 2/d. Secondly, Lemma 7 and the vechints to the similarity of the role of \u03d1(G) in quantum contextuality and joint expectation values.\n\n## IV Selfadjoint unitary representations\n\nThe set of operators, where any pair either commutes or anticommutes, plays an important role as exemplified by the Pauli strings. The commutation and anticommutation relations of such a set {S i} can be encoded into a so-called frustration graph G [11, 12], where i \u223c j if {S i, S j} = 0, and i \u0338\u223c j if [S i, S j] = 0. By checking extensive examples, it is conjectured in Ref. [34] that\n\n\\[q\\left(\\left\\{S_{i}\\right\\}\\right)=\\alpha(G). \\tag{20}\\]\n\nWhether Eq. (20) can be violated is also an open question in Ref. [25]. Conversely, for a given graph G, we can consider its representation by a set {S i} of selfadjoint unitaries, in the sense that {S i, S j} = 0 if i \u223c j and [S i, S j] = 0 if i \u0338\u223c j. This representation is called selfadjoint unitary representation (SAUR) [41]. By taking the graph-theoretic approach instead of starting from a special set, we denote \\(\\beta(G,w)=q\\left(S_{ac}(G),w\\right)\\), where S ac (G) is the set of all SAURs of G. The conjecture in Eq. (20) is equivalent to \\(\\beta(G)=\\alpha(G)\\). In Ref. [25], no such an example is known that \\(\\beta(G)&gt;\\alpha(G)\\). To continue, we first introduce the standard SAUR of a given graph, which is defined deductively. The standard SAUR can help us to reduce the complexity of considerations, since we only need to focus on the standard SAUR to obtain \\(\\beta(G)\\) as we will see later.\n\n**Definition 12**.: For a given graph G and one of its edges (i0, j0), other vertices except i0 and j0 can be devided into four groups V0, V1, V2, V3, such that\n\n* i \u0338\u223c i0 and i \u0338\u223c j0 for any i \u2208 V0;\n* i \u0338\u223c i0 and i \u223c j0 for any i \u2208 V1;\n* i \u223c i0 and i \u223c j0 for any i \u2208 V2;\n* i \u223c i0 and i \u0338\u223c j0 for any i \u2208 V3.\n\nThe subgraph G' of G with vertices in \\(\\cup_{i=0}^{4}V_{i}\\) is said to be a Pauli-(i0, j0)-induced subgraph of G if\n\n* when i \u2208 Vk1, j \u2208 Vk2 where k1 \u0338= k2 \u2208 {1, 2, 3}, we have i \u223c j (or i \u0338\u223c j) in G' if and only if i \u0338\u223c j (or i \u223c j) in G;\n* otherwise, i \u223c j in G' if and only if i \u223c j in G.\n\n**Definition 13**.: For a given graph G and one of its edges (i0, j0), denote G' the Pauli-(i0, j0)-induced subgraph of G, from a standard SAUR {S' i} of G', we call the following SAUR a standard one of G:\n\n(21)\n\nIf G has no edge, we assign id or 1 to all its vertices.\n\nIf we take the pentagon C5 in Fig. 2 as the original graph, and (1, 3) as the edge, then the Pauli-(1, 3)-induced subgraph G' is a triangle. Continually, the Pauli-(4, 5)-induced subgraph G'' of G' is just the vertex 2. Hence, the standard SAURs {S'' i}, {S' i} and {S i} of G'', G' and G, respectively, are\n\n\\[S''_{2} =1,S''_{2} =Y,S''_{4} =X,S''_{5} =Z, \\tag{22}\\] \\[S_{1} =X\\mathrm{id},S_{2} =\\mathrm{id}Y,S_{3} =Z\\mathrm{id},S_{4} =XX,S_{5} =ZZ, \\tag{23}\\]\n\nwhere we have omitted "
    ],
    "kosmos": [
      "whenever tr(\u03c1^{2}) \u2264 2/d. Secondly, Lemma 7 and the vectorization of matrices, Theorem 8 and Theorem 11 give hints to the similarity of the role of \u03d1(G) in quantum contextuality and joint expectation values.\n\n## IV Selfadjoint unitary representations\n\nThe set of operators, where any pair either commutes or anticommutes, plays an important role as exemplified by the Pauli strings. The commutation and anticommutation relations of such a set {S~i} can be encoded into a so-called frustration graph G [11, 12], where i \\(\\sim j\\) if {S~i, S~j} = 0, and i \\(\\not\\sim j\\) if {S~i, S~j} = 0. By checking extensive examples, it is conjectured in Ref. [34] that\n\n\\[q\\left({\\left\\{{S~i}\\right\\}}\\right)=\\alpha(G). \\tag{20}\\]\n\nWhether Eq. (20) can be violated is also an open question in Ref. [25]. Conversely, for a given graph G, we can consider its representation by a set {S~i} of selfadjoint unitaries, in the sense that {S~i, S~j} = 0 if i \\(\\sim j\\) and {S~i, S~j} = 0 if i \\(\\not\\sim j\\). This representation is called selfadjoint unitary representation (SAUR) [41]. By taking the graph-theoretic approach instead of starting from a special set, we denote \\(\\beta(G,w)=q\\left({\\mathcal{S}}_{ac}(G),w\\right)\\), where \\(\\mathcal{S}_{ac}(G)\\) is the set of all SAURs of G. The conjecture in Eq. (20) is equivalent to \\(\\beta(G)=\\alpha(G)\\). In Ref. [25], no such an example is known that \\(\\beta(G)&gt;\\alpha(G)\\). To continue, we first introduce the standard SAUR of a given graph, which is defined deductively. The standard SAUR can help us to reduce the complexity of considerations, since we only need to focus on the standard SAUR to obtain \\(\\beta(G)\\) as we will see later.\n\n**Definition 12**.: For a given graph G and one of its edges (i~0, j~0), other vertices except i~0 and j~0 can be devided into four groups V~0, V~1, V~2, V~3, such that\n\n* i \\(\\not\\sim\\) i~0 and i \\(\\not\\sim\\) j~0 for any i \u2208 V~0;\n* i \\(\\not\\sim\\) i~0 and i \\(\\sim\\) j~0 for any i \u2208 V~1;\n* i \\(\\sim\\) i~0 and i \\(\\sim\\) j~0 for any i \u2208 V~2;\n* i \\(\\sim\\) i~0 and i \\(\\not\\sim\\) j~0 for any i \u2208 V~3.\n\nThe subgraph G' of G with vertices in \\(\\cup_{i=0}^{4}V~i\\) is said to be a Pauli-(i~0, j~0)-induced subgraph of G if\n\n* when i \u2208 V~k~1, j \u2208 V~k~2 where k~1 \u0338= k~2 \u2208 {1, 2, 3}, we have i \\(\\sim\\) j (or i \\(\\not\\sim\\) j) in G' if and only if i \\(\\not\\sim\\) j (or i \\(\\sim\\) j) in G;\n* otherwise, i \\(\\sim\\) j in G' if and only if i \\(\\sim\\) j in G.\n\n**Definition 13**.: For a given graph G and one of its edges (i~0, j~0), denote G' the Pauli-(i~0, j~0)-induced subgraph of G, from a standard SAUR {S'~i} of G', we call the following SAUR a standard one of G:\n\n\\[(\\bigcup_{k=0}^{3}\\left\\{{\\sigma~k}\\otimes S'~i}\\right\\}i\\in V~k) \\cup\\left\\{{X~i~0}\\otimes\\mathrm{id},Z~j~0}\\otimes\\mathrm{id}\\right). \\tag{21}\\]\n\nIf G has no edge, we assign id or 1 to all its vertices.\n\nIf we take the pentagon C~5 in Fig. 2 as the original graph, and (1, 3) as the edge "
    ]
  },
  {
    "edit": [
      "A.2 Checking SUSY for the Fibered Background\n\nHere we aim to compute how many supercharges are preserved by the the backgrounds presented in this paper. For the un-fibered background in eq.( 2.1 ) we refer the reader to [ 12 ], [ 19 ], where it is shown that this solution preserves 16 Supercharges in an interesting way: the anti-commutator of two supercharges includes the \\(R\\)-Symmetry generators. Now we present the analysis for the fibered background in eq.( 2.3 ). We perform all the analysis in the S-dual system, in terms of NS5 branes, where we only have \\(H_{3}\\) flux.\n\nFirst, note that the dilatino variation is a matrix equation of the form \\(M\\varepsilon=0\\). In order to have non-trivial solutions to this equation, we require \\(M\\) to be non-invertible, for which we need to impose det( M ) = 0. It is also possible to obtain a matrix equation from the gravitino variation. Noting that we can write the gravitino variation as a covariant derivative, for which we define the connection\n\n\\[W_{\\mu}=\\frac{1}{4}\\omega_{\\mu}^{\\ ab}\\Gamma_{ab}+\\frac{1}{4\\cdot 2!}H_{\\mu\\nu \\lambda}\\Gamma^{\\nu\\lambda}\\sigma^{3}+\\frac{e^{\\Phi}}{8}\\left(F_{\\mu}\\Gamma^{\\mu }(i\\sigma_{2})+\\frac{1}{3!}F_{\\mu\\nu\\lambda}\\Gamma^{\\mu\\nu\\lambda}\\sigma^{1}+ \\frac{1}{2\\cdot 5!}F_{\\mu\\nu\\lambda\\rho\\sigma}\\Gamma^{\\mu\\nu\\lambda\\rho\\sigma}( i\\sigma_{2})\\right)\\Gamma_{\\mu},\\] (A.10)\n\nthen we can write the gravitino variation as\n\n\\[\\delta\\psi_{\\mu}dx^{\\mu}=\\left\\langle\\partial_{\\mu}\\varepsilon+W_{\\mu} \\varepsilon\\right\\rangle dx^{\\mu}\\equiv\\mathcal{D}\\varepsilon.\\] (A.11)\n\nWe can get rid of the partial derivative of the spinor by acting with \\(\\mathcal{D}\\) a second time\n\n\\[\\mathcal{D}\\wedge\\mathcal{D}\\varepsilon=\\left(dW+W\\wedge W\\right)\\varepsilon= \\frac{1}{2}\\Theta_{\\mu\\nu}dx^{\\mu}\\wedge dx^{\\nu}\\varepsilon.\\] (A.12)\n\nEach of the components of \\(\\Theta_{\\mu\\nu}\\) defines a matrix equation, giving a total of 45 independent equations. We need to make sure that det(\u0398 \u00b5\\nu) = 0 for each of the components. The equations\n\n\\[M\\varepsilon=0,\\quad\\Theta_{\\mu\\nu}\\varepsilon=0,\\] (A.13)\n\nconstrain the number of independent components of the spinor. After this procedure we use the gravitino variation to solve the dependence of the spinor on the spacetime coordinates. Specialising to our background, the determinant of the Dilatino variation for the background in eq.( 2.3 ) reads\n\n\\[\\text{det}(M)\\sim\\left(4(e_{B}Q_{A}-e_{A}Q_{B})^{2}+m^{2}\\right)^{8}\\left(4(e_{B }Q_{A}+e_{A}Q_{B})^{2}+m^{2}\\right)^{8}.\\] (A.14)\n\nIn order to have non-trivial solutions we need to impose the following BPS conditions on the parameters of the background\n\n\\[e_{A}Q_{B}=\\pm e_{B}Q_{A},\\quad m=0.\\] (A.15)\n\nWith this conditions it is possible to check that det(\u0398 \u00b5\\nu) = 0 is also satisfied. Solving these matrix equations shows that the spinor has 8 independent components. Then, solving for the gravitino variation shows that these components are not independents, and in fact, the total number of independent components its reduced to 4. The solution for the spinor is\n\n\\[\\varepsilon_{1}=\\vec{0}\\] (A.16) "
    ],
    "kosmos": [
      "A.2 Checking SUSY for the Fibered Background\n\nHere we aim to compute how many supercharges are preserved by the the backgrounds presented in this paper. For the un-fibered background in eq.(2.1) we refer the reader to [12], [19], where it is shown that this solution preserves 16 Supercharges in an interesting way: the anti-commutator of two supercharges includes the \\(R\\)-Symmetry generators. Now we present the analysis for the fibered background in eq.(2.3). We perform all the analysis in the S-dual system, in terms of NS5 branes, where we only have \\(H_{3}\\) flux.\n\nFirst, note that the dilatino variation is a matrix equation of the form \\(M\\varepsilon=0\\). In order to have non-trivial solutions to this equation, we require \\(M\\) to be non-invertible, for which we need to impose det( M ) = 0. It is also possible to obtain a matrix equation from the gravitino variation. Noting that we can write the gravitino variation as a covariant derivative, for which we define the connection\n\n\\[W_{\\mu}=\\frac{1}{4}\\omega_{\\mu}^{\\ ab}\\Gamma_{ab}+\\frac{1}{4\\cdot 2!}H_{\\mu\\nu \\lambda}\\Gamma^{\\nu\\lambda}\\sigma^{3}+\\frac{e^{\\Phi}}{8}\\left(F_{\\mu}\\Gamma^{\\mu }(i\\sigma_{2})+\\frac{1}{3!}F_{\\mu\\nu\\lambda}\\Gamma^{\\mu\\nu\\lambda}\\sigma^{1}+ \\frac{1}{2\\cdot 5!}F_{\\mu\\nu\\lambda\\rho\\sigma}\\Gamma^{\\mu\\nu\\lambda\\rho\\sigma}( i\\sigma_{2})\\right)\\Gamma_{\\mu},\\] (A.10)\n\nthen we can write the gravitino variation as\n\n\\[\\delta\\psi_{\\mu}dx^{\\mu}=\\left\\langle\\partial_{\\mu}\\varepsilon+W_{\\mu} \\varepsilon\\right\\rangle dx^{\\mu}\\equiv\\mathcal{D}\\varepsilon.\\] (A.11)\n\nWe can get rid of the partial derivative of the spinor by acting with \\(\\mathcal{D}\\) a second time\n\n\\[\\mathcal{D}\\wedge\\mathcal{D}\\varepsilon=\\left(dW+W\\wedge W\\right)\\varepsilon= \\frac{1}{2}\\Theta_{\\mu\\nu}dx^{\\mu}\\wedge dx^{\\nu}\\varepsilon.\\] (A.12)\n\nEach of the components of \\(\\Theta_{\\mu\\nu}\\) defines a matrix equation, giving a total of 45 independent equations. We need to make sure that det(\u0398 \u00b5\\nu) = 0 for each of the components. The equations\n\n\\[M\\varepsilon=0,\\quad\\Theta_{\\mu\\nu}\\varepsilon=0,\\] (A.13)\n\nconstrain the number of independent components of the spinor. After this procedure we use the gravitino variation to solve the dependence of the spinor on the spacetime coordinates.\n\nSpecialising to our background, the determinant of the Dilatino variation for the background in eq.(2.3) reads\n\n\\[\\text{det}(M)\\sim\\left(4(e_{B}Q_{A}-e_{A}Q_{B})^{2}+m^{2}\\right)^{8}\\left(4(e_{B }Q_{A}+e_{A}Q_{B})^{2}+m^{2}\\right)^{8}.\\] (A.14)\n\nIn order to have non-trivial solutions we need to impose the following BPS conditions on the parameters of the background\n\n\\[e_{A}Q_{B}=\\pm e_{B}Q_{A},\\quad m=0.\\] (A.15)\n\nWith this conditions it is possible to check that det(\u0398 \u00b5\\nu) = 0 is also satisfied. Solving these matrix equations shows that the spinor has 8 independent components. Then, solving for the gravitino variation shows that these components are not independents, and in fact, the total number of independent components its reduced to 4. The solution for the spinor is\n\n\\[\\varepsilon_{1}=\\vec{0}\\] (A.16) "
    ]
  },
  {
    "edit": [
      "scenes (though they have similar objects such as computers, chairs, or tables, as shown in Figure 1(b)), which may confuse the recognition system. Furthermore, constructing a scene representation that captures crucial semantic information reflecting the complexity of the data is challenging, particularly when dealing with a large number of categories\n\nResearch in scene recognition frameworks can be categorized into ones that are based on hand-crafted engineering and methods that employ automatic feature extraction without human intervention. Hand-engineered features are based on manually designing and selecting features utilizing different techniques to capture spatial characteristics, local features, object-based concepts, and holistic representations of scenes [6, 7, 8, 9]. However, hand-crafted features require notable domain expertise and significant human effort, resulting in inefficiency. Consequently, Deep Convolutional Neural Networks (DCNNs) have largely replaced them due to their superior representation learning ability [10, 11, 12]. They have demonstrated that they attain superior classification performance when trained on extensive datasets. After AlexNet's [13] introduction, deep convolutional neural networks, such as VGG [14], Inception [15], ResNet [16], and DenseNet [17], impressively advanced the field of image classification due to their great ability of capturing locally correlated image values [18, 19, 20] and learning features that enhance efficiency compared to hand-crafted methods. Utilizing deep convolutional neural networks can enhance scene recognition performance, but it remains challenging due to the intricate spatial layout, intra-class variation, and inter-class similarity, which weaken discriminability among scenes [21, 22]. In addition, due to the rise of extensive scene-centric datasets, a simple CNN-generated representation model is inadequate to accurately discriminate large-scale scenes [23]. As a result, research has shifted its focus to developing more representative features by incorporating contextual information, such as objects, or proposing strategies to effectively extract features that facilitate decision-making boundaries. Furthermore, several studies have employed ensemble learning strategies to leverage the complementary strengths of multiple feature levels or recognition models [24, 25, 26].\n\nBesides improving accuracy, recognition systems based on deep neural networks face the challenge of ambiguity in the reasoning behind the model's predictions[27, 28, 29]. This boils down to the black-box nature of deep neural networks, which induces a lack of trust and ethical concerns regarding the model's decisions, especially in high-stakes applications such as medical analysis and self-driving vehicles [30, 31, 32]. Along this direction, numerous efforts have been made to help understand how a model solves a problem and makes decisions. Typically, an explanation should help us get the answers to the following questions: Why did the model predict this category? Is the prediction of the model reliable? Which parts of the input led\n\nFigure 1: Demonstrations of inter-class similarity and intra-class variation. a) Images from the auditorium and movie theater classes have a high degree of similarity (inter-class similarity). b) Images of the office demonstrate a considerable degree of intra-class diversity, suggesting a wide spectrum of visual features within the category.\n\n "
    ],
    "kosmos": [
      "scenes (though they have similar objects such as computers, chairs, or tables, as shown in Figure 1(b)), which may confuse the recognition system. Furthermore, constructing a scene representation that captures crucial semantic information reflecting the complexity of the data is challenging, particularly when dealing with a large number of categories\n\nResearch in scene recognition frameworks can be categorized into ones that are based on hand-crafted engineering and methods that employ automatic feature extraction without human intervention. Hand-engineered features are based on manually designing and selecting features utilizing different techniques to capture spatial characteristics, local features, object-based concepts, and holistic representations of scenes [6, 7, 8, 9]. However, hand-crafted features require notable domain expertise and significant human effort, resulting in inefficiency. Consequently, Deep Convolutional Neural Networks (DCNNs) have largely replaced them due to their superior representation learning ability [10, 11, 12]. They have demonstrated that they attain superior classification performance when trained on extensive datasets. After AlexNet's [13] introduction, deep convolutional neural networks, such as VGG [14], Inception [15], ResNet [16], and DenseNet [17], impressively advanced the field of image classification due to their great ability of capturing locally correlated image values [18, 19, 20] and learning features that enhance efficiency compared to hand-crafted methods. Utilizing deep convolutional neural networks can enhance scene recognition performance, but it remains challenging due to the intricate spatial layout, intra-class variation, and inter-class similarity, which weaken discriminability among scenes [21, 22]. In addition, due to the rise of extensive scene-centric datasets, a simple CNN-generated representation model is inadequate to accurately discriminate large-scale scenes [23]. As a result, research has shifted its focus to developing more representative features by incorporating contextual information, such as objects, or proposing strategies to effectively extract features that facilitate decision-making boundaries. Furthermore, several studies have employed ensemble learning strategies to leverage the complementary strengths of multiple feature levels or recognition models [24, 25, 26].\n\nBesides improving accuracy, recognition systems based on deep neural networks face the challenge of ambiguity in the reasoning behind the model's predictions[27, 28, 29]. This boils down to the black-box nature of deep neural networks, which induces a lack of trust and ethical concerns regarding the model's decisions, especially in high-stakes applications such as medical analysis and self-driving vehicles [30, 31, 32]. Along this direction, numerous efforts have been made to help understand how a model solves a problem and makes decisions. Typically, an explanation should help us get the answers to the following questions: Why did the model predict this category? Is the prediction of the model reliable? Which parts of the input led\n\nFigure 1: Demonstrations of inter-class similarity and intra-class variation. a) Images from the auditorium and movie theater classes have a high degree of similarity (inter-class similarity). b) Images of the office demonstrate a considerable degree of intra-class diversity, suggesting a wide spectrum of visual features within the category.\n\n "
    ]
  },
  {
    "edit": [
      "Database (NVD) by inferring new classes, enriching relations, and expanding conceptual coverage. The ontology is used to search for and query social media threads that contain cybersecurity-related information, and natural language processing techniques are used to relate unstructured information to concepts in the ontology. The paper highlights the advantages of Semantic Web technologies in integrating information from multiple and often heterogeneous sources, without human intervention. Rosa et.al. [8] presented a novel ontology-based approach to utilize ontology to identify and map threats to assets. With the support of formally sound approaches, this process can be streamlined and made more efficient. From an ontology perspective, the authors introduced ThreMA, an ontology-based approach for automating threat modeling in ICT infrastructures. ThreMA provides a standard metamodel that describes the infrastructure and a set of rules for threat modeling. The meta-model consists of three ontologies modules: ICT ontology for modeling the infrastructure, Data Flow ontology for representing data flow diagrams, and threat ontology for characterizing threats. The use of ontology and inference rules allows for a syntactical representation of the problem, mimicking expert thinking. This approach enhances extensibility, maintainability, and integration in a rapidly changing context. The paper emphasizes the importance of using ontologies to address the lack of context and low accuracy in threat modeling. Overall, ThreMA offers a comprehensive ontology-based solution for automating threat modeling in ICT infrastructures.**\n\n## III Methodology\n\nThis work presents an ontology for representing various data sources about cloud computing and security. This ontology enables a knowledge presentation framework for all cloud computing and its relationships. The ontology consists of several modules: Cloud Computing and services, Cloud Service underlying components, and CVE module.\n\n### _Cloud Computing Stack and Services Ontology Module_\n\nThis section represents our proposed ontology module that covers the cloud computing stack and services. Our extension ontology is provided as a separate ontology, which is an important design criteria in ontology engine engineering [13, 14]. This ontology can be used to unify and provide a primary baseline for cloud computing stack, threat understanding, and system diagram presentations. Firstly, we start by creating the ontology of the cloud computing stack. Figure 2 depicts the ontology of the three cloud stack: Software as a Service (SaaS), Platform as a Service (PaaS), Infrastructure as a Service (IaaS), Function as a Service (FaaS), Communication as a service (CaaS), and Desktop as a service (DaaS). Figure 1 depicts the cloud service model. Cloud service has nine layers, green colored layers mean these layers are managed by the client while the other color refers to layers managed by the cloud providers.\n\nFig. 1: Cloud computing stack. The green color depicts the layers managed by the used user. While the orange-colored boxes represent the layers managed by the cloud provider.\n\n "
    ],
    "kosmos": [
      "Database (NVD) by inferring new classes, enriching relations, and expanding conceptual coverage. The ontology is used to search for and query social media threads that contain cybersecurity-related information, and natural language processing techniques are used to relate unstructured information to concepts in the ontology. The paper highlights the advantages of Semantic Web technologies in integrating information from multiple and often heterogeneous sources, without human intervention. Rosa et.al. [8] presented a novel ontology-based approach to utilize ontology to identify and map threats to assets. With the support of formally sound approaches, this process can be streamlined and made more efficient. From an ontology perspective, the authors introduced ThreMA, an ontology-based approach for automating threat modeling in ICT infrastructures. ThreMA provides a standard meta-model that describes the infrastructure and a set of rules for threat modeling. The meta-model consists of three ontologies modules: ICT ontology for modeling the infrastructure, Data Flow ontology for representing data flow diagrams, and threat ontology for characterizing threats. The use of ontology and inference rules allows for a syntactical representation of the problem, mimicking expert thinking. This approach enhances extensibility, maintainability, and integration in a rapidly changing context. The paper emphasizes the importance of using ontologies to address the lack of context and low accuracy in threat modeling. Overall, ThreMA offers a comprehensive ontology-based solution for automating threat modeling in ICT infrastructures.\n\n## III Methodology\n\nThis work presents an ontology for representing various data sources about cloud computing and security. This ontology enables a knowledge presentation framework for all cloud computing and its relationships. The ontology consists of several modules: Cloud Computing and services, Cloud Service underlying components, and CVE module.\n\n### _Cloud Computing Stack and Services Ontology Module_\n\nThis section represents our proposed ontology module that covers the cloud computing stack and services. Our extension ontology is provided as a separate ontology, which is an important design criteria in ontology engine engineering [13, 14]. This ontology can be used to unify and provide a primary baseline for cloud computing stack, threat understanding, and system diagram presentations. Firstly, we start by creating the ontology of the cloud computing stack. Figure 2 depicts the ontology of the three cloud stack: Software as a Service (SaaS), Platform as a Service (PaaS), Infrastructure as a Service (IaaS), Function as a Service (FaaS), Communication as a service (CaaS), and Desktop as a service (DaaS). Figure 1 depicts the cloud service model. Cloud service has nine layers, green colored layers mean these layers are managed by the client while the other color refers to layers managed by the cloud providers.\n\nFig. 1: Cloud computing stack. The green color depicts the layers managed by the used user. While the orange-colored boxes represent the layers managed by the cloud provider.\n\n "
    ]
  },
  {
    "edit": [
      "for the \\(q\\)-analogue to \\(\\lambda\\), which reduces to\n\n\\[[n]_{q}=1+q+\\cdots+q^{n-1},\\qquad\\text{for }n\\in\\mathbb{N}^{+}.\\]\n\nWe have that \\([0]_{q}=0\\) and\n\n\\[[np]_{q}=\\frac{q^{p}-1}{q-1}\\frac{q^{np}-1}{q^{p}-1}=[n]_{q}\\cdot[n]_{q^{p}}, \\qquad n,p\\in\\mathbb{N}^{+}. \\tag{2.1}\\]\n\nFor future use, we note that\n\n\\[|[n]_{q}|\\leqslant[n]_{|q|},\\qquad\\text{for }n\\in\\mathbb{N},\\]\n\nand also that\n\n\\[\\lim_{n\\to+\\infty}\\frac{[n]_{q}}{q^{n}}=\\frac{1}{q-1}. \\tag{2.2}\\]\n\nThese constants appear naturally while considering Jackson\u2019s \\(q\\)-derivative of a function \\(f\\), which is given by\n\n\\[d_{q}(f)(x):=\\frac{f(qx)-f(x)}{qx-x}=\\frac{\\sigma_{q}(f)(x)-f(x)}{qx-x},\\]\n\nwhenever the expression is defined. As before, \\(\\sigma_{q}(f)(x):=f(qx)\\). For analytic functions \\(f\\in\\mathcal{O}(D_{r})\\), we see that\n\n\\[\\sigma_{q}(f),d_{q}(f)\\in\\mathcal{O}(D_{r/|q|})\\]\n\nand they can be computed term by term using its power series expansion according to the rules\n\n\\[d_{q}(x^{n})=[n]_{q}x^{n-1},\\qquad\\sigma_{q}(x^{n})=q^{n}x^{n},\\qquad n\\in \\mathbb{N}.\\]\n\nOn the other hand, this formula allows to consider \\(d_{q},\\sigma_{d}:\\mathbb{C}[[x]]\\to\\mathbb{C}[[x]]\\), also defined term by term. In this setting, Leibniz rule is replaced by\n\n\\[d_{q}(fg)(x)=d_{q}(f)(x)g(x)+f(qx)d_{q}(g)(x). \\tag{2.4}\\]\n\nWe recall the coefficients\n\n\\[(a;q)_{n}=\\prod_{j=0}^{n-1}(1-aq^{j}),\\qquad(a;q^{-1})_{\u221e}=\\prod_{j=0}^{\\infty }(1-aq^{-j}),\\qquad a\\in\\mathbb{C}. \\tag{2.5}\\]\n\nThe second one is convergent as we can compare it with a geometric series. The \\(q\\)-factorial is defined accordingly as\n\n\\[[n]_{q}^{\\dagger}=[1]_{q}[2]_{q}\\cdots[n]_{q}=\\frac{(q;q)_{n}}{(1+q)^{n}}.\\]\n\nIn general, for \\(|q|&gt;1\\), since \\(\\lambda\\in\\mathbb{R}\\longmapsto[\\lambda]_{|q|}\\) is a strictly increasing function, the same holds for the map \\(n\\in\\mathbb{N}\\longmapsto[n]_{|q|}^{\\dagger}\\). Therefore,\n\n\\[\\frac{[n-p]_{|q|}}{([n]_{|q|}^{\\dagger})^{1/p}}=\\frac{[n-p]_{|q|}}{([n]_{|q|} \\cdots[n-p+1]_{|q|})^{1/p}([n-p]_{|q|}^{\\dagger})^{1/p}}\\leqslant\\frac{[n-p]_{ |q|}}{[n-p+1]_{|q|}}\\frac{1}{([n-p]_{|q|}^{\\dagger})^{1/p}},\\]\n\nand thus\n\n\\[\\frac{[n-p]_{|q|}}{([n]_{|q|}^{\\dagger})^{1/p}}\\leqslant\\frac{1}{([n-p]_{|q|}^ {\\dagger})^{1/p}},\\qquad\\text{for integers }n&gt;p&gt;0.\\] (2.6) "
    ],
    "kosmos": [
      "for the \\(q\\)-analogue to \\(\\lambda\\), which reduces to\n\n\\[[n]_{q}=1+q+\\cdots+q^{n-1},\\qquad\\text{for }n\\in\\mathbb{N}^{+}.\\]\n\nWe have that \\([0]_{q}=0\\) and\n\n\\[[np]_{q}=\\frac{q^{p}-1}{q-1}\\frac{q^{np}-1}{q^{p}-1}=[n]_{q}\\cdot[n]_{q^{p}}, \\qquad n,p\\in\\mathbb{N}^{+}. \\tag{2.1}\\]\n\nFor future use, we note that\n\n\\[|[n]_{q}|\\leqslant[n]_{|q|},\\qquad\\text{for }n\\in\\mathbb{N},\\]\n\nand also that\n\n\\[\\lim_{n\\to+\\infty}\\frac{[n]_{q}}{q^{n}}=\\frac{1}{q-1}. \\tag{2.2}\\]\n\nThese constants appear naturally while considering Jackson\u2019s \\(q\\)-derivative of a function \\(f\\), which is given by\n\n\\[d_{q}(f)(x):=\\frac{f(qx)-f(x)}{qx-x}=\\frac{\\sigma_{q}(f)(x)-f(x)}{qx-x},\\]\n\nwhenever the expression is defined. As before, \\(\\sigma_{q}(f)(x):=f(qx)\\). For analytic functions \\(f\\in\\mathcal{O}(D_{r})\\), we see that\n\n\\[\\sigma_{q}(f),d_{q}(f)\\in\\mathcal{O}(D_{r/|q|})\\]\n\nand they can be computed term by term using its power series expansion according to the rules\n\n\\[d_{q}(x^{n})=[n]_{q}x^{n-1},\\qquad\\sigma_{q}(x^{n})=q^{n}x^{n},\\qquad n\\in \\mathbb{N}.\\]\n\nOn the other hand, this formula allows to consider \\(d_{q},\\sigma_{d}:\\mathbb{C}[[x]]\\to\\mathbb{C}[[x]]\\), also defined term by term. In this setting, Leibniz rule is replaced by\n\n\\[d_{q}(fg)(x)=d_{q}(f)(x)g(x)+f(qx)d_{q}(g)(x). \\tag{2.4}\\]\n\nWe recall the coefficients\n\n\\[(a;q)_{n}=\\prod_{j=0}^{n-1}(1-aq^{j}),\\qquad(a;q^{-1})_{\u221e}=\\prod_{j=0}^{\\infty }(1-aq^{-j}),\\qquad a\\in\\mathbb{C}. \\tag{2.5}\\]\n\nThe second one is convergent as we can compare it with a geometric series. The \\(q\\)-factorial is defined accordingly as\n\n\\[[n]_{q}^{\\dagger}=[1]_{q}[2]_{q}\\cdots[n]_{q}=\\frac{(q;q)_{n}}{(1+q)^{n}}.\\]\n\nIn general, for \\(|q|&gt;1\\), since \\(\\lambda\\in\\mathbb{R}\\longmapsto[\\lambda]_{|q|}\\) is a strictly increasing function, the same holds for the map \\(n\\in\\mathbb{N}\\longmapsto[n]_{|q|}^{\\dagger}\\). Therefore,\n\n\\[\\frac{[n-p]_{|q|}}{([n]_{|q|}^{\\dagger})^{1/p}}=\\frac{[n-p]_{|q|}}{([n]_{|q|} \\cdots[n-p+1]_{|q|})^{1/p}([n-p]_{|q|}^{\\dagger})^{1/p}}\\leqslant\\frac{[n-p]_{ |q|}}{[n-p+1]_{|q|}}\\frac{1}{([n-p]_{|q|}^{\\dagger})^{1/p}},\\]\n\nand thus\n\n\\[\\frac{[n-p]_{|q|}}{([n]_{|q|}^{\\dagger})^{1/p}}\\leqslant\\frac{1}{([n-p]_{|q|}^ {\\dagger})^{1/p}},\\qquad\\text{for integers }n&gt;p&gt;0.\\] (2.6) "
    ]
  },
  {
    "edit": [
      "doping. As a result we find the largest angle change in \u03b4. As the inter-layer distance stays the same, the inner and outer Cr atoms of the bilayer system experience a different change in their local environments, which we interpret as the reason for the intra-layer magnetic symmetry breaking as reflected in \u00b5^1^~B~ \u2260 \u00b5^2^~B~ and \u00b5^3^~B~ \u2260 \u00b5^4^~B~.\n\nN. D. Mermin and H. Wagner, Phys. Rev. Lett. **17**, 1133 (1966). S. Chakravarty, B. I. Halperin, and D. R. Nelson, Phys. Rev. B **39**, 2344 (1989). L. J. de Jongh, ed., *Magnetic Properties of Layered Transition Metal Compounds* (Springer, 1990). V. Y. Irkhin, A. A. Katanin, and M. I. Katsnelson, Phys. Rev. B **60**, 1082 (1999). M. Gibertini, M. Koperski, A. F. Morpurgo, and K. S. Novoselov, Nature Nanotechnology **14**, 408 (2019). K. Zollner, P. E. Faria Junior, and J. Fabian, Phys. Rev. B **100**, 085128 (2019). B. Huang, G. Clark, E. Navarro-Moratalla, D. R. Klein, R. Cheng, K. L. Seyler, D. Zhong, E. Schmidgall, M. A. McGuire, D. H. Cobden, et al., Nature **546**, 270 (2017). C. Gong, L. Li, Z. Li, H. Ji, A. Stern, Y. Xia, T. Cao, W. Bao, C. Wang, Y. Wang, et al., Nature **546**, 265 (2017). Y. Deng, Y. Yu, Y. Song, J. Zhang, N. Z. Wang, Z. Sun, Y. Yi, Y. Z. Wu, S. Wu, J. Zhu, et al., Nature **563**, 94 (2018). O. G\u00a8oser, W. Paul, and H. Kahle, Journal of Magnetism and Magnetic Materials **92**, 129 (1990). E. J. Telford, A. H. Dismukes, K. Lee, M. Cheng, A. Wieteska, A. K. Bartholomew, Y.-S. Chen, X. Xu, A. N. Pasupathy, X. Zhu, et al., Advanced Materials **32**, 2003240 (2020). Y. Guo, Y. Zhang, S. Yuan, B. Wang, and J. Wang, Nanoscale **10**, 18036 (2018). Z. Jiang, P. Wang, J. Xing, X. Jiang, and J. Zhao, ACS Applied Materials and Interfaces **10**, 39032 (2018). C. Wang, X. Zhou, L. Zhou, N.-H. Tong, Z.-Y. Lu, and W. Ji, Science Bulletin **64**, 293 (2019). N. P. Wilson, K. Lee, J. Cenker, K. Xie, A. H. Dismukes, E. J. Telford, J. Fonseca, S. Sivakumar, C. Dean, T. Cao, et al., Nature Materials **20**, 1657 (2021). J. Klein, B. Pingault, M. Florian, M.-C. Hei\u00dfenb\u00fcttel, A. Steinho\ufb00, Z. Song, K. Torres, F. Dirnberger, J. B. Curtis, M. Weile, et al., ACS Nano **17**, 5316 (2023). M. Bianchi, S. Acharya, F. Dirnberger, J. Klein, D. Pashov, K. Mosina, Z. Sofer, A. N. Rudenko, M. I. Katsnelson, M. van Schilfgaarde, et al., Phys. Rev. B **107**, 235107 (2023). A. N. Rudenko, M. R\u00a8osner, and M. I. Katsnelson, npj Computational Materials **9**, 1 (2023). J. Klein, T. Pham, J. D. Thomsen, J. B. Curtis, T. Denneulin, M. Lorke, M. Florian, A. Steinho\ufb00, R. A "
    ],
    "kosmos": [
      "doping. As a result we find the largest angle change in \u03b4. As the inter-layer distance stays the same, the inner and outer Cr atoms of the bilayer system experience a different change in their local environments, which we interpret as the reason for the intra-layer magnetic symmetry breaking as reflected in \u00b5^1^~B~ \u2260 \u00b5^2^~B~ and \u00b5^3^~B~ \u2260 \u00b5^4^~B~.\n\nN. D. Mermin and H. Wagner, Phys. Rev. Lett. **17**, 1133 (1966).\n\nS. Chakravarty, B. I. Halperin, and D. R. Nelson, Phys. Rev. B **39**, 2344 (1989).\n\nL. J. de Jongh, ed., Magnetic Properties of Layered Transition Metal Compounds (Springer, 1990).\n\nV. Y. Irkhin, A. A. Katanin, and M. I. Katsnelson, Phys. Rev. B **60**, 1082 (1999).\n\nM. Gibertini, M. Koperski, A. F. Morpurgo, and K. S. Novoselov, Nature Nanotechnology **14**, 408 (2019).\n\nK. Zollner, P. E. Faria Junior, and J. Fabian, Phys. Rev. B **100**, 085128 (2019).\n\nB. Huang, G. Clark, E. Navarro-Moratalla, D. R. Klein, R. Cheng, K. L. Seyler, D. Zhong, E. Schmidgall, M. A. McGuire, D. H. Cobden, et al., Nature **546**, 270 (2017).\n\nC. Gong, L. Li, Z. Li, H. Ji, A. Stern, Y. Xia, T. Cao, W. Bao, C. Wang, Y. Wang, et al., Nature **546**, 265 (2017).\n\nY. Deng, Y. Yu, Y. Song, J. Zhang, N. Z. Wang, Z. Sun, Y. Yi, Y. Z. Wu, S. Wu, J. Zhu, et al., Nature **563**, 94 (2018).\n\nO. G\u00a8oser, W. Paul, and H. Kahle, Journal of Magnetism and Magnetic Materials **92**, 129 (1990).\n\nE. J. Telford, A. H. Dismukes, K. Lee, M. Cheng, A. Wieteska, A. K. Bartholomew, Y.-S. Chen, X. Xu, A. N. Pasupathy, X. Zhu, et al., Advanced Materials **32**, 2003240 (2020).\n\nY. Guo, Y. Zhang, S. Yuan, B. Wang, and J. Wang, Nanoscale **10**, 18036 (2018).\n\nZ. Jiang, P. Wang, J. Xing, X. Jiang, and J. Zhao, ACS Applied Materials and Interfaces **10**, 39032 (2018).\n\nC. Wang, X. Zhou, L. Zhou, N.-H. Tong, Z.-Y. Lu, and W. Ji, Science Bulletin **64**, 293 (2019).\n\nN. P. Wilson, K. Lee, J. Cenker, K. Xie, A. H. Dismukes, E. J. Telford, J. Fonseca, S. Sivakumar, C. Dean, T. Cao, et al., Nature Materials **20**, 1657 (2021).\n\nJ. Klein, B. Pingault, M. Florian, M.-C. Hei\u00dfenb\u00fcttel, A. Steinho\ufb00, Z. Song, K. Torres, F. Dirnberger, J. B. Curtis, M. Weile, et al., ACS Nano **17**, 5316 (2023).\n\nM. Bianchi, S. Acharya, F. Dirnberger, J. Klein, D. Pashov, K. Mosina, Z. Sofer, A. N. Rudenko, M. I. Katsnelson, M. van Schilfgaarde, et al., Phys. Rev. B **107**, 235107 (2023).\n\nA. N. Rudenko, M. R\u00a8osner, and M. I. Katsnelson, npj Computational Materials **9**, 1 (2023).\n\nJ. Klein, T. Pham, J. D. Thomsen, J. B. Curtis, T. Denneulin, M. Lorke, M. Florian, A. Steinho\ufb00, R. A. Wis "
    ]
  },
  {
    "edit": [
      "describes the traveling-wave microwave photon transporting along the transmission line with \\(l=b,c\\) refering to its left, right side, and the relevant bosonic operators satisfy the communication relation: [\\(l(\\omega),l^{\\dagger}(\\omega^{\\prime})\\)] = \u03b4(\u03c9\u2212\u03c9^{\\prime})\\). Also, the flux operator of the traveling-wave photon reads [14\u201316]: \u02c6 \u03c6 l (x) = \\(\\sqrt{\\frac{\\hbar Z_{0}}{4\\pi}}\\int_{0}^{\\infty}\\frac{d\\omega}{\\sqrt{\\omega}} \\left[\\hat{l}(\\omega)e^{ikx}+\\hat{l}^{\\dagger}(\\omega)e^{-ikx}\\right],\\) (7)\n\nwith \\(Z_{0}\\) being the characteristic impedance of the transmission line, and thus\n\n\\[\\hat{\\phi}_{l}(x) = (-i)\\sqrt{\\frac{\\hbar Z_{0}}{4\\pi}}\\int_{0}^{\\infty}d\\omega\\sqrt{ \\omega}\\left[\\hat{l}(\\omega)e^{ikx}-\\hat{l}^{\\dagger}(\\omega)e^{-ikx}\\right]. \\tag{8}\\]\n\nUnder the sufficiently low current bias, the CBJJ Hamiltonian reads: \\(\\hat{H}_{CBJJ}\\approx\\hat{H}_{b}\\), shown in Eq. 4. The physical boundary condition at \\(x=0\\), i.e., the location of the device, reads: \\(\\hat{I}\\left(0_{b},t\\right)=\\hat{I}\\left(0_{c},t\\right)\\), \\(V_{J}=(\\Phi_{0}/2\\pi)\\,\\delta\\) + \\(\\left[\\hat{\\phi}(0_{b})-\\hat{\\phi}(0_{c})\\right]\\). Thus, under the low-excitation limit and rotating-wave approximation, i.e., the photon scattering is the desired elastic and any possibly created and annihilated of the photons at \\(x=0\\) is neglected, we have\n\n\\[\\hat{H}_{CBJJ-B} = C_{J}\\hat{p}_{\u03b8}\\left[\\hat{\\phi}(0_{b})-\\hat{\\phi}(0_{c})\\right] \\tag{9}\\] \\[=i\\hbar\\sqrt{\\frac{\\kappa_{l}}{2\\pi}}\\int d\\omega\\left[a^{\\dagger} l(\\omega)-l^{\\dagger}(\\omega)a\\right],\\]\n\nwhere \\(\\kappa_{l}=Z_{0}/4Z_{J}\\) (\\(l=b,c\\)) describes the interaction between the CBJJ and the left/right traveling-wave photons, \\(Z_{J}=\\sqrt{L_{J}/C_{J}}\\) is the characteristic impedance of the Josephson junction. As a consequence, the Hamiltonian (with \\(\\hbar=1\\)) of the system [16\u201318]: H_B = \\(\\left(\\omega_{P}-\\frac{i\\gamma}{2}\\right)a^{\\dagger}a\\] (10) \\[+ \\int d\\omega\\left[\\omega b(\\omega)^{\\dagger}b(\\omega)+i\\sqrt{\\frac {\\kappa_{1}}{2\\pi}}\\left(a^{\\dagger}b(\\omega)-ab(\\omega)^{\\dagger}\\right)\\right]\\] \\[+ \\int d\\omega\\left[\\omega c(\\omega)^{\\dagger}c(\\omega)+i\\sqrt{\\frac {\\kappa_{2}}{2\\pi}}\\left(a^{\\dagger}c(\\omega)-ac(\\omega)^{\\dagger}\\right)\\right],\\]\n\nwhere \u03b3 is decay rate of the cavity, \\(\\kappa_{1}\\) and \\(\\kappa_{2}\\) are the effective strengths of the boson coupled to the photons in the left and right sides of the transmission line, respectively. By using the standard input-output theory [18, 19], we get the relations: da dt = \\(-i\\omega_{p}-\\frac{\\kappa+\\gamma}{2}a+\\sqrt{\\kappa_{1}}b_{\\text{in}}(t)+ \\sqrt{\\kappa_{2}}c_{\\text{in}}(t),\\) (11) and da dt = \\(-i\\omega_{p}+\\frac{\\kappa-\\gamma}{2}a-\\sqrt{\\kappa_{1}}b_{\\text{out}}(t)- \\sqrt{\\kappa_{2}}c_{\\text{out}}(t),\\) (12) \n\nare the input- and output fields, respectively. After the Fourier transformation: \\( "
    ],
    "kosmos": [
      "describes the traveling-wave microwave photon transporting along the transmission line with \\(l=b,c\\) refering to its left, right side, and the relevant bosonic operators satisfy the communication relation: [\\(l(\\omega),l^{\\dagger}(\\omega^{\\prime})\\)] = \u03b4(\u03c9\u2212\u03c9^{\\prime})\\). Also, the flux operator of the traveling-wave photon reads [14, 15, 16]:\n\n\\[\\hat{\\phi}_{l}(x)=\\sqrt{\\frac{\\hbar Z_{0}}{4\\pi}}\\int_{0}^{\\infty}\\frac{d\\omega} {\\sqrt{\\omega}}\\left[\\hat{l}(\\omega)e ikx+\\hat{l}^{\\dagger}(\\omega)e \u2212 ikx\\right], \\tag{7}\\]\n\nwith \\(Z_{0}\\) being the characteristic impedance of the transmission line, and thus\n\n\\[\\hat{\\phi}_{l}(x)=(-i)\\sqrt{\\frac{\\hbar Z_{0}}{4\\pi}}\\int_{0}^{\\infty}d\\omega \\sqrt{\\omega}\\left[\\hat{l}(\\omega)e ikx-\\hat{l}^{\\dagger}(\\omega)e \u2212 ikx\\right]. \\tag{8}\\]\n\nUnder the sufficiently low current bias, the CBJJ Hamiltonian reads: \\(\\hat{H}_{CBJJ}\\approx\\hat{H}_{b}\\), shown in Eq. 4. The physical boundary condition at \\(x=0\\), i.e., the location of the device, reads: \\(\\hat{I}\\left(0_{b},t\\right)=\\hat{I}\\left(0_{c},t\\right)\\), \\(V_{J}=(\\Phi_{0}/2\\pi)\\,\\delta\\) + \\(\\left[\\hat{\\phi}(0_{b})-\\hat{\\phi}(0_{c})\\right]\\). Thus, under the low-excitation limit and rotating-wave approximation, i.e., the photon scattering is the desired elastic and any possibly created and annihilated of the photons at \\(x=0\\) is neglected, we have\n\n\\[\\hat{H}_{CBJJ-B}=C_{J}\\hat{p}_{\u03b8}\\left[\\hat{\\phi}(0_{b})-\\hat{\\phi}(0_{c})\\right]\\]\n\nwhere \\(\\kappa_{l}=Z_{0}/4Z_{J}\\) (\\(l=b,c\\)) describes the interaction between the CBJJ and the left/right traveling-wave photons, \\(Z_{J}=\\sqrt{L_{J}/C_{J}}\\) is the characteristic impedance of the Josephson junction. As a consequence, the Hamiltonian (with \\(\\hbar=1\\)) of the system [16, 17, 18]:\n\n\\[H_{B}=\\left(\\omega_{P}-\\frac{i\\gamma}{2}\\right)a^{\\dagger}a\\]\n\n\\[+\\int d\\omega\\left[\\omega b(\\omega)^{\\dagger}b(\\omega)+i\\sqrt{\\frac{\\kappa_{1}}{ 2\\pi}}\\left(a^{\\dagger}b(\\omega)-ab(\\omega)^{\\dagger}\\right)\\right]\\]\n\n\\[+\\int d\\omega\\left[\\omega c(\\omega)^{\\dagger}c(\\omega)+i\\sqrt{\\frac{\\kappa_{2}}{ 2\\pi}}\\left(a^{\\dagger}c(\\omega)-ac(\\omega)^{\\dagger}\\right)\\right], \\tag{9}\\]\n\nwhere \u03b3 is decay rate of the cavity, \\(\\kappa_{1}\\) and \\(\\kappa_{2}\\) are the effective strengths of the boson coupled to the photons in the left and right sides of the transmission line, respectively. By using the standard input-output theory [18, 19], we get the relations:\n\n\\[\\frac{da}{dt}=\\left(-i\\omega_{p}-\\frac{\\kappa+\\gamma}{2}\\right)a+\\sqrt{\\kappa_ {1}}b_{\\text{in}}(t)+\\sqrt{\\kappa_{2}}c_{\\text{in}}(t), \\tag{10}\\]\n\nand\n\n\\[\\hat{b}_{\\text{in/out}}=\\pm\\frac{1}{\\sqrt{2\\pi}}\\int d\\omega e^{-i\\omega(t-t^{ \\prime})}b_{0}(\\omega), \\tag{11}\\]\n\nand\n\n\\[\\hat{c}_{\\text{in/out}}=\\pm\\frac{1}{\\sqrt{2\\pi}}\\int d\\omega e^{-i\\omega(t-t^{ \\prime})}c_{0}(\\omega), \\tag{12}\\]\n\nare the input- and output fields, respectively. After the Fourier transformation: \\( "
    ]
  },
  {
    "edit": [
      "\n\n## 1 Introduction\n\nVectorlike (VL) fermions are key ingredients in many new physics models beyond the Standard Model (SM) adopted to resolve both theoretical and experimental issues. Since chiral fermions in the fourth family are excluded experimentally [1, 2], these are considered to be vectorlike and their masses are given independently to the Higgs mechanism in the SM. The VL fermions are introduced in, for instance, supersymmetric models [3, 4, 5, 6, 7, 8, 9, 10], gauge mediated supersymmetry breaking scenario [11, 12, 13, 14, 15, 16], composite Higgs models [17, 18, 19], KSVZ axion models [20, 21], axionlike particle models [22, 23, 24, 25], alternative solutions of the strong CP problem [26, 27], two Higgs doublet model augmented by VL fermions [28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38], and models for gauge coupling uni\ufb01cation [39, 40].\n\nAmong the fourth family fermions, VL leptons (VLLs) with nonzero lepton number play unique roles in constructing lepton-philic dark matter (DM) models [41, 42, 43, 44, 45, 46], mirror sector models [47, 48, 49], and explanations for the muon anomalies [50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65] 1. Interestingly, the lightest VLL is expected to be in the reach of the Large Hadron Collider (LHC) or High-Luminosity (HL)-LHC. Both ATLAS and CMS collaborations search for the pair production of VLLs each of which dominantly decays to a SM boson and a tau lepton [70, 71, 72]. For the doublet VLL 2, the ATLAS search excludes the mass range of 130"
    ],
    "kosmos": [
      "\n\n## 1 Introduction\n\nVectorlike (VL) fermions are key ingredients in many new physics models beyond the Standard Model (SM) adopted to resolve both theoretical and experimental issues. Since chiral fermions in the fourth family are excluded experimentally [1, 2], these are considered to be vectorlike and their masses are given independently to the Higgs mechanism in the SM. The VL fermions are introduced in, for instance, supersymmetric models [3, 4, 5, 6, 7, 8, 9, 10], gauge mediated supersymmetry breaking scenario [11, 12, 13, 14, 15, 16], composite Higgs models [17, 18, 19], KSVZ axion models [20, 21], axionlike particle models [22, 23, 24, 25], alternative solutions of the strong CP problem [26, 27], two Higgs doublet model augmented by VL fermions [28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38], and models for gauge coupling uni\ufb01cation [39, 40].\n\nAmong the fourth family fermions, VL leptons (VLLs) with nonzero lepton number play unique roles in constructing lepton-philic dark matter (DM) models [41, 42, 43, 44, 45, 46], mirror sector models [47, 48, 49], and explanations for the muon anomalies [50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65] 1. Interestingly, the lightest VLL is expected to be in the reach of the Large Hadron Collider (LHC) or High-Luminosity (HL)-LHC. Both ATLAS and CMS collaborations search for the pair production of VLLs each of which dominantly decays to a SM boson and a tau lepton [70, 71, 72]. For the doublet VLL 2, the ATLAS search excludes the mass range of 130 < m vll < 900 (neutrino)=\"\" 1045=\"\" 150=\"\" 71].=\"\" 72],=\"\" 73].=\"\" 74]=\"\" 76]=\"\" 77,=\"\" 78].=\"\" 79,=\"\" 80]=\"\" 81].=\"\" 82,=\"\" 83,=\"\" 84,=\"\" 85].=\"\" 86],=\"\" 87,=\"\" 88,=\"\" 89,=\"\" 90=\"\" 91,=\"\" 92,=\"\" 93,=\"\" 94,=\"\" 95,=\"\" 96,=\"\" 97,=\"\" 98,=\"\" 99,=\"\" 100,=\"\" 101,=\"\" 102,=\"\" 103,=\"\" 104,=\"\" 105,=\"\" 106,=\"\" 107,=\"\" 108,=\"\" 109,=\"\" 110,=\"\" 111,=\"\" 112,=\"\" 113,=\"\" 114,=\"\" 115,=\"\" 116,=\"\" 117,=\"\" 118,=\"\" 119,=\"\" 120,=\"\" 121,=\"\" 122,=\"\" 123,=\"\" 124,=\"\" 125,=\"\" 126,=\"\" 127,=\"\" 128,=\"\" 129,=\"\" 130,=\"\" 131,=\"\" 132,=\"\" 133,=\"\" 134,=\"\" 135,=\"\" 136,=\"\" 137,=\"\" 138,=\"\" 139,=\"\" 140,=\"\" 141,=\"\" 142,=\"\" 143,=\"\" 144,=\"\" 145,=\"\" 146,=\"\" 147,=\"\" 148,=\"\" 149,=\"\" 150,=\"\" 151,=\"\" 152,=\"\" 153,=\"\" 154,=\"\" 155,=\"\" 156,=\"\" 157,=\"\" 158,=\"\" 159,=\"\" 160,=\"\" 161,=\"\" 162,=\"\" 163,=\"\" 164,=\"\" 164,=\"\" 165,=\"\" 166,=\"\" 167,=\"\" 168,=\"\" 170,=\"\" 171,=\"\" 172,=\"\" 173,=\"\" 174,=\"\" 175,=\"\" 176,=\"\" 177,=\"\" 178,=\"\" 179,=\"\" 180,=\"\" 181,=\"\" 182,=\"\" 183,=\"\" 184,=\"\" 85,=\"\" 86,=\"\" 87,=\"\" 88,=\"\" 89,=\"\" 90,=\"\" 91,=\"\" 92,=\"\" 93,=\"\" 94,=\"\" 95,=\"\" 96,=\"\" 97,=\"\" 98,=\"\" 99,=\"\" 100,=\"\" 101,=\"\" 102,=\"\" 103,=\"\" 104,"
    ]
  },
  {
    "edit": [
      "\n\n#### 3.1.2 Even L with a fermion parity defect\n\nLet us introduce a G defect. A concrete Hamiltonian to keep in mind is\n\n\\[H_{\\mathsf{G}} =i\\sum_{\\ell=1}^{L-1}\\chi_{\\ell+1}\\chi_{\\ell}-i\\overline{\\chi_{1 }}\\chi_{L}\\,, \\tag{3.21}\\]\n\nwhere the defect is in the link connecting ( L, 1) . We use the subscript G for the Hamiltonian and symmetry operators in the system with a G defect. However, we emphasize that for most of our discussion the particular form of the Hamiltonian will not matter. Note that the defect can be moved to other links, e.g., to the link (1 , 2) , by conjugating \\(H_{\\mathsf{G}}\\) by a local G transformation, \\(\\chi_{1}\\) or g~1.\n\nLet us determine the symmetry operators of the theory with the defect. We use the same fermion parity operator G as in ( 3.10 ), because it commutes with \\(H_{\\mathsf{G}}\\). 21 On the other hand, instead 21 of ( 3.5 ), the translation operator now acts on the fermion fields as\n\nFootnote 21: We do not write G~ because it is the same as G.\n\n\\[T_{\\mathsf{G}}:\\qquad\\chi_{\\ell}\\to T_{\\mathsf{G}}\\chi_{\\ell}T_{\\mathsf{G}}^{-1 }=\\sum_{\\ell^{\\prime}}R(T_{\\mathsf{G}})_{\\ell,\\ell^{\\prime}}\\chi_{\\ell^{\\prime} }=\\begin{cases}\\chi_{\\ell+1}&amp;\\ell=1,2,\\cdots,L-1\\\\ -\\chi_{1}&amp;\\ell=L\\end{cases} \\tag{3.22}\\]\n\nThe algebra satisfied by these operators is\n\n\\[R(\\mathsf{G})^{2} =1\\,, R(T_{\\mathsf{G}})^{L} =R(\\mathsf{G})\\,, R(\\mathsf{G})\\ R(T_{\\mathsf{G}}) =R(T_{\\mathsf{G}})\\ R(\\mathsf{G})\\,. \\tag{3.23}\\]\n\nIn contrast to the case without the defect ( 3.7 ),now we have\n\n\\[\\det R(T_{\\mathsf{G}})_{\\ell,\\ell^{\\prime}} =+1\\,, \\tag{3.24}\\] \\[\\det R(\\mathsf{G})_{\\ell,\\ell^{\\prime}} =+1\\,.\\]\n\nThis means that the twisted translation operator is an \\(SO(L)\\) transformation and is constructed out of an even number of fermions, i.e., it is bosonic. Let us write \\(T_{\\mathsf{G}}\\) in terms of the fermion fields. Again, ( 3.22 ) does not determine its phase normalization, and we will make an arbitrary choice below. Later, in Section 3.3 , we will rescale it to T~NSNS and compare it to the continuum operators. Its action in ( 3.22 ) means that we should multiply \\(T\\) by an operator that maps \\(\\chi_{1}\\to-\\chi_{1}\\) and leaves the other fermions unchanged, i.e., we should multiply it by g~1 = -\\chi_{2}\\chi_{3}\\cdots\\chi_{L}\\). Therefore, we take [ 81 ] 22 22 Alternatively, the translation operator for even L with a defect can be written as\n\nFootnote 22: (Note that in this forms, \\(T_{\\mathsf{G}}\\) does not satisfy the locality condition ( 1.15 ).)\n\n\\[T_{\\mathsf{G}} =(-1)^{N}\\text{g~1}T=\\frac{1}{2^{\\frac{L-1}{2}}}(1-\\chi_{1}\\chi_{ 2})(1-\\chi_{2}\\chi_{3})\\cdots(1-\\chi_{L-1}\\chi_{L})\\,. \\tag{3.26}\\]"
    ],
    "kosmos": [
      "\n\n#### 3.1.2 Even L with a fermion parity defect\n\nLet us introduce a G defect. A concrete Hamiltonian to keep in mind is\n\n\\[H_{\\mathsf{G}} =i\\sum_{\\ell=1}^{L-1}\\chi_{\\ell+1}\\chi_{\\ell}-i\\overline{\\chi_{1 }}\\chi_{L}\\,, \\tag{3.21}\\]\n\nwhere the defect is in the link connecting ( L, 1) . We use the subscript G for the Hamiltonian and symmetry operators in the system with a G defect. However, we emphasize that for most of our discussion the particular form of the Hamiltonian will not matter. Note that the defect can be moved to other links, e.g., to the link (1 , 2) , by conjugating \\(H_{\\mathsf{G}}\\) by a local G transformation, \\(\\chi_{1}\\) or g~1.\n\nLet us determine the symmetry operators of the theory with the defect. We use the same fermion parity operator G as in (3.10), because it commutes with \\(H_{\\mathsf{G}}\\). 21 On the other hand, instead of (3.5), the translation operator now acts on the fermion fields as\n\nFootnote 21: We do not write G~ because it is the same as G.\n\n\\[T_{\\mathsf{G}}:\\qquad\\chi_{\\ell}\\to T_{\\mathsf{G}}\\chi_{\\ell}T_{\\mathsf{G}}^{-1 }=\\sum_{\\ell^{\\prime}}R(T_{\\mathsf{G}})_{\\ell,\\ell^{\\prime}}\\chi_{\\ell^{\\prime} }=\\begin{cases}\\chi_{\\ell+1}&amp;\\ell=1,2,\\cdots,L-1\\\\ -\\chi_{1}&amp;\\ell=L\\end{cases} \\tag{3.22}\\]\n\nThe algebra satisfied by these operators is\n\n\\[R(\\mathsf{G})^{2} =1\\,, R(T_{\\mathsf{G}})^{L} =R(\\mathsf{G})\\,, R(\\mathsf{G})\\ R(T_{\\mathsf{G}}) =R(T_{\\mathsf{G}})\\ R(\\mathsf{G})\\,. \\tag{3.23}\\]\n\nIn contrast to the case without the defect (3.7),now we have\n\n\\[\\det R(T_{\\mathsf{G}})_{\\ell,\\ell^{\\prime}} =+1\\,, \\tag{3.24}\\] \\[\\det R(\\mathsf{G})_{\\ell,\\ell^{\\prime}} =+1\\,.\\]\n\nThis means that the twisted translation operator is an \\(SO(L)\\) transformation and is constructed out of an even number of fermions, i.e., it is bosonic.\n\nLet us write \\(T_{\\mathsf{G}}\\) in terms of the fermion fields. Again, (3.22) does not determine its phase normalization, and we will make an arbitrary choice below. Later, in Section 3.3, we will rescale it to T~NSNSNS and compare it to the continuum operators. Its action in (3.22) means that we should multiply \\(T\\) by an operator that maps \\(\\chi_{1}\\to-\\chi_{1}\\) and leaves the other fermions unchanged, i.e., we should multiply it by g~1 = -\\chi_{2}\\chi_{3}\\cdots\\chi_{L}\\). Therefore, we take [81] 22\n\nFootnote 22: Alternatively, the translation operator for even L with a defect can be written as\n\\[T_{\\mathsf{G}} =\\frac{(-1)^{N}}{2^{\\frac{L-1}{2}}}(\\chi_{1}-\\chi_{2})(\\chi_{2}- \\chi_{3})\\cdots(\\chi_{L-1}-\\chi_{L})\\chi_{L}\\,. \\tag{3.25}\\]\n\n(Note that in this forms, \\(T_{\\mathsf{G}}\\) does not satisfy the locality condition (1.15).)\n\n\\[T_{\\mathsf{G}} =(-1)^{N}\\text{g~1}T=\\frac{1}{2^{\\frac{L-1}{2}}}(1-\\chi_{1}\\chi_{ 2})(1-\\chi_{2}\\chi_{3})\\cdots(1-\\chi_{L-1}\\chi_{L})\\,. \\tag{3.26}\\]"
    ]
  },
  {
    "edit": [
      "when measuring the correlation of cities. Although the correlation of cities can be measured from the aspect of POI distribution, the user behavioral transition pattern is a significant factor in the next POI recommendation task, we thus further explore such correlation from the angle of user sequential behaviors. **Correlation of Cities w.r.t Behavioral Patterns**. We examine the correlation of cities w.r.t. the categories of users\u2019 successive POI visits. In particular, given any two cities, \\(A^{cat}=[A^{cat}_{1},A^{cat}_{2}...A^{cat}_{|S|}]\\) and \\(B^{cat}=[B^{cat}_{1},B^{cat}_{2}...B^{cat}_{|S|}]\\) refer to the category transition distributions among \\(S\\) transition types, e.g., \\(A^{cat}_{1}\\) denotes the ratio of transition type \\(FO\\to SS\\) within city A. Analogously, the similarity among different cities can be calculated via the Pearson correlation coefficient, shown in Fig. 2(b). Interestingly, we observe that the correlation of cities w.r.t behavioral patterns is quite different from that w.r.t POI distribution. Specifically, PHO and CAL still keep higher similarity, whereas NYC shows comparably lower similarity with PHO and CAL. To further dig out how the four cities are correlated and different over the behavioral patterns, we compare the two most correlated cities (i.e., CAL and PHO) and the two least correlated cities (i.e., NYC and SIN). For ease of presentation, we select the 10 most frequent category transitions for comparison as shown in Fig. 2(c-d), where the \\(x\\)-axis denotes the category transitions, e.g., \\(AE\\to CU\\) (AE2CU), and the \\(y-\\)axis shows the proportion of such a transition within a city. We find that the more correlated cities possess consistent distributions over the frequent category transitions and _vice versa_. The above observations depict the various correlations between cities, which inspire us to differentiate their influence when transferring knowledge from auxiliary cities to the target city. **The Proposed MERec** This section presents the proposed MERec, which leverages the correlation of behavioral patterns when transferring knowledge from auxiliary cities to the target city, i.e., paying more attention to more correlated knowledge. **Problem Formulation.** Each city has its unique user set \\(\\mathcal{U}\\) and POI set \\(\\mathcal{P}\\) without sharing any common users and POIs. For user \\(u\\), all his check-in records, i.e., \\(r=(p,c,g,t)\\), are ordered by timestamps as in [22], where \\(p,c,g,t\\) denote POI \\(p\\), category \\(c\\), coordinate \\(g\\) (i.e., longitude and latitude) and timestamp \\(t\\). We then split his historical records into sequences by day and obtain two\n\nFigure 2: (a-b) the correlation of four cities w.r.t POI distribution and behavioral patterns at category level; (c-d) two most correlated and least correlated cities.\n\n "
    ],
    "kosmos": [
      "when measuring the correlation of cities. Although the correlation of cities can be measured from the aspect of POI distribution, the user behavioral transition pattern is a significant factor in the next POI recommendation task, we thus further explore such correlation from the angle of user sequential behaviors.\n\n**Correlation of Cities w.r.t Behavioral Patterns**. We examine the correlation of cities w.r.t. the categories of users' successive POI visits. In particular, given any two cities, \\(A^{cat}=[A^{cat}_{1},A^{cat}_{2}...A^{cat}_{|S|}]\\) and \\(B^{cat}=[B^{cat}_{1},B^{cat}_{2}...B^{cat}_{|S|}]\\) refer to the category transition distributions among S transition types, e.g., \\(A^{cat}_{1}\\) denotes the ratio of transition type \\(FO\\to SS\\) within city A. Analogously, the similarity among different cities can be calculated via the Pearson correlation coefficient, shown in Fig. 2(b). Interestingly, we observe that the correlation of cities w.r.t behavioral patterns is quite different from that w.r.t POI distribution. Specifically, PHO and CAL still keep higher similarity, whereas NYC shows comparably lower similarity with PHO and CAL. To further dig out how the four cities are correlated and different over the behavioral patterns, we compare the two most correlated cities (i.e., CAL and PHO) and the two least correlated cities (i.e., NYC and SIN). For ease of presentation, we select the 10 most frequent category transitions for comparison as shown in Fig. 2(c-d), where the \\(x\\)-axis denotes the category transitions, e.g., \\(AE\\to CU\\) (AE2CU), and the \\(y-\\)axis shows the proportion of such a transition within a city. We find that the more correlated cities possess consistent distributions over the frequent category transitions and _vice versa_. The above observations depict the various correlations between cities, which inspire us to differentiate their influence when transferring knowledge from auxiliary cities to the target city.\n\n## 4 The Proposed MERec\n\nThis section presents the proposed MERec, which leverages the correlation of behavioral patterns when transferring knowledge from auxiliary cities to the target city, i.e., paying more attention to more correlated knowledge.\n\n**Problem Formulation.** Each city has its unique user set \\(\\mathcal{U}\\) and POI set \\(\\mathcal{P}\\) without sharing any common users and POIs. For user \\(u\\), all his check-in records, i.e., \\(r=(p,c,g,t)\\), are ordered by timestamps as in [22], where \\(p,c,g,t\\) denote POI \\(p\\), category \\(c\\), coordinate \\(g\\) (i.e., longitude and latitude) and timestamp \\(t\\). We then split his historical records into sequences by day and obtain two\n\nFigure 2: (a-b) the correlation of four cities w.r.t POI distribution and behavioral patterns at category level; (c-d) two most correlated and least correlated cities.\n\n "
    ]
  },
  {
    "edit": [
      "\n\n### Symmetric and Schur-positive sets\n\nAs mentioned in Section 1, a set \\(\\mathcal{A}\\) is symmetric with respect to a statistic function \\(D:\\mathcal{A}\\to 2^{[N-1]}\\) if its generating function \\(\\mathcal{Q}_{N,D}(\\mathcal{A})\\) is a symmetric function. Moreover, it is Schur-positive if all Schur coefficients are nonnegative integers.\n\nOne of the fundamental constructions of Schur-positive sets, regarding sets of standard Young tableaux (SYT), is due to Gessel [11]. Let \\(\\text{SYT}(\\lambda)\\) denote the set of standard Young tableaux of shape \\(\\lambda\\). We draw tableaux in English notation, as in Figure 2. The _descent set_ of \\(T\\in\\text{SYT}(\\lambda)\\) is\n\n\\[\\text{Des}(T):=\\{i\\in[N-1]~{}|~{}i+1\\text{ appears in a lower row than }i\\text{ in }T\\}.\\]\n\nFor example, the descent set of the SYT in Figure 2 is \\(\\{2,4,7,8\\}\\).\n\nThe entry in row \\(i\\) and column \\(j\\) of a tableau \\(T\\in\\text{SYT}(\\lambda)\\) is denoted as \\(T_{i,j}\\). In addition, we define \\(\\text{row}_{i}(T):=\\{T_{i,j}~{}|~{}1\\leq j\\leq\\lambda_{i}\\}\\) as the set of entries in the \\(i\\)-th row of \\(T\\). For example, if we consider the SYT shown in Figure 2, then \\(T_{3,2}=8\\) and \\(\\text{row}_{3}(T)=\\{5,8\\}\\).\n\n**Theorem 2.4** (Gessel [11]).: _For every \\(\\lambda\\vdash N\\), the set \\(\\text{SYT}(\\lambda)\\) is Schur-positive with respect to \\(\\text{Des}\\). Moreover, \\(\\mathcal{Q}(\\text{SYT}(\\lambda))=s_{\\lambda}\\)._\n\nIn 2015, Adin and Roichman proved the following criterion.\n\n**Theorem 2.5** ([2, Prop. 9.1]).: _A set \\(\\mathcal{A}\\) is symmetric with respect to \\(D:S\\to 2^{[N-1]}\\) if and only if_\n\n\\[\\sum_{a\\in\\mathcal{A}}\\boldsymbol{t}^{D(a)}=\\sum_{\\lambda\\vdash N}c_{\\lambda} \\sum_{T\\in\\text{SYT}(\\lambda)}\\boldsymbol{t}^{\\text{Des}(T)}\\]\n\n_for some values \\(c_{\\lambda}\\), where \\(\\boldsymbol{t}^{J}:=\\prod_{j\\in J}t_{j}\\) for \\(J\\subseteq[N-1]\\). The coefficients \\(c_{\\lambda}\\) are the Schur-coefficients of \\(\\mathcal{A}\\). Moreover, \\(\\mathcal{A}\\) is Schur-positive if and only if \\(c_{\\lambda}\\in\\mathbb{N}_{0}\\) for all \\(\\lambda\\vdash N\\)._\n\nThis criterion implies that proving the Schur-positivity of a set is achievable by establishing a statistic-preserving bijection between the set and SYTs of shapes corresponding to a specific multiset.\n\nIn this paper, we will also apply a recently formulated criterion for symmetry [19].\n\n**Definition 2.6**.: Let \\(\\mathcal{A}\\) be a finite set with a statistic \\(D:\\mathcal{A}\\to 2^{[N-1]}\\). The set of elements that _respect_ a given composition \\(\\alpha\\vDash N\\), denoted \\(\\mathcal{A}_{D}(\\alpha)\\), consists of the elements \\(a\\in\\mathcal{A}\\) such that \\(D(a)\\subseteq S_{\\alpha}\\), where \\(S_{\\alpha}\\) is the set corresponding to the composition \\(\\alpha\\). When \\(D\\) is clear from the context, we may write \\(\\mathcal{A}(\\alpha)\\) instead.\n\n**Lemma 2.7**.: _A set \\(\\mathcal{A}\\) is symmetric if and only if \\(|\\mathcal{A}(\\alpha)|=|\\mathcal{A}(\\beta)|\\) for all \\(\\alpha\\sim\\beta\\vDash N\\)._\n\nNote that only sets of permutations are considered in [19"
    ],
    "kosmos": [
      "\n\n### Symmetric and Schur-positive sets\n\nAs mentioned in Section 1, a set \\(\\mathcal{A}\\) is symmetric with respect to a statistic function \\(D:\\mathcal{A}\\to 2^{[N-1]}\\) if its generating function \\(\\mathcal{Q}_{N,D}(\\mathcal{A})\\) is a symmetric function. Moreover, it is Schur-positive if all Schur coefficients are nonnegative integers.\n\nOne of the fundamental constructions of Schur-positive sets, regarding sets of standard Young tableaux (SYT), is due to Gessel [11]. Let \\(\\text{SYT}(\\lambda)\\) denote the set of standard Young tableaux of shape \\(\\lambda\\). We draw tableaux in English notation, as in Figure 2. The _descent set_ of \\(T\\in\\text{SYT}(\\lambda)\\) is\n\n\\[\\text{Des}(T):=\\{i\\in[N-1]~{}|~{}i+1\\text{ appears in a lower row than }i\\text{ in }T\\}.\\]\n\nFor example, the descent set of the SYT in Figure 2 is \\(\\{2,4,7,8\\}\\).\n\nThe entry in row \\(i\\) and column \\(j\\) of a tableau \\(T\\in\\text{SYT}(\\lambda)\\) is denoted as \\(T_{i,j}\\). In addition, we define \\(\\text{row}_{i}(T):=\\{T_{i,j}~{}|~{}1\\leq j\\leq\\lambda_{i}\\}\\) as the set of entries in the \\(i\\)-th row of \\(T\\). For example, if we consider the SYT shown in Figure 2, then \\(T_{3,2}=8\\) and \\(\\text{row}_{3}(T)=\\{5,8\\}\\).\n\n**Theorem 2.4** (Gessel [11]).: _For every \\(\\lambda\\vdash N\\), the set \\(\\text{SYT}(\\lambda)\\) is Schur-positive with respect to \\(\\text{Des}\\). Moreover, \\(\\mathcal{Q}(\\text{SYT}(\\lambda))=s_{\\lambda}\\)._\n\nIn 2015, Adin and Roichman proved the following criterion.\n\n**Theorem 2.5** ([2, Prop. 9.1]).: _A set \\(\\mathcal{A}\\) is symmetric with respect to \\(D:S\\to 2^{[N-1]}\\) if and only if_\n\n\\[\\sum_{a\\in\\mathcal{A}}\\boldsymbol{t}^{D(a)}=\\sum_{\\lambda\\vdash N}c_{\\lambda} \\sum_{T\\in\\text{SYT}(\\lambda)}\\boldsymbol{t}^{\\text{Des}(T)}\\]\n\n_for some values \\(c_{\\lambda}\\), where \\(\\boldsymbol{t}^{J}:=\\prod_{j\\in J}t_{j}\\) for \\(J\\subseteq[N-1]\\). The coefficients \\(c_{\\lambda}\\) are the Schur-coefficients of \\(\\mathcal{A}\\). Moreover, \\(\\mathcal{A}\\) is Schur-positive if and only if \\(c_{\\lambda}\\in\\mathbb{N}_{0}\\) for all \\(\\lambda\\vdash N\\)._\n\nThis criterion implies that proving the Schur-positivity of a set is achievable by establishing a statistic-preserving bijection between the set and SYTs of shapes corresponding to a specific multiset.\n\nIn this paper, we will also apply a recently formulated criterion for symmetry [19].\n\n**Definition 2.6**.: Let \\(\\mathcal{A}\\) be a finite set with a statistic \\(D:\\mathcal{A}\\to 2^{[N-1]}\\). The set of elements that _respect_ a given composition \\(\\alpha\\vDash N\\), denoted \\(\\mathcal{A}_{D}(\\alpha)\\), consists of the elements \\(a\\in\\mathcal{A}\\) such that \\(D(a)\\subseteq S_{\\alpha}\\), where \\(S_{\\alpha}\\) is the set corresponding to the composition \\(\\alpha\\). When \\(D\\) is clear from the context, we may write \\(\\mathcal{A}(\\alpha)\\) instead.\n\n**Lemma 2.7**.: _A set \\(\\mathcal{A}\\) is symmetric if and only if \\(|\\mathcal{A}(\\alpha)|=|\\mathcal{A}(\\beta)|\\) for all \\(\\alpha\\sim\\beta\\vDash N\\)._\n\nNote that only sets of permutations are considered in [19"
    ]
  },
  {
    "edit": [
      "where \u03b4 nm is the Kronecker delta symbol. Such relation leads to the definition of the Laguerre transform of order \u03bd:\n\nT^ = {c^ },\n\nit must be emphasize that the Laguerre transform is a sequence of numbers in C. The inverse Laguerre transform is defined by\n\nf(x) = (T^ )^ -1 [{c^ }] = ^ k=0^ c^ n^ L^ n^ (x).\n\n## 3 Examples of applications\n\n### Applications to the Schr\u00a8odinger equation\n\nIn this section is consider the equation\n\n- (V(r) + E)\u03c8(r) = 0 (13)\n\nwhich in appropriate units (h=M=1) is the steady state Sch\u00a8odinger equation defined in a one dimensional space, where V(r) is a potential function and E is the energy. Under the change of coordinates (see [0]), given by \u03bb^1 \u03be(x) = dx/dr, where \u03bb \u2265 0 has inverse length units, equation (13) becomes\n\n\u03bb^2 \u03be^2 \\[d^2 dx^2 \u03c8(x) + 1 \u03be dx^2 dx \u03c8(x) - 2 \u03bb^2 \u03be^2 W(x)\u03c8(x)\\] (14)\n\nwhere W(x) = V(r) - E. To obtain a Laguerre-type equation the change of coordinates must satisfy x(r) \u2265 0 and setting 1 \u03be dx^2 dx = a x, leads to \u03be(x) = x^a e^bx.\n\nIn this way, equation (14) becomes\n\n\u03bb^2 \u03be^2 \\[d^2 dx^2 \u03c8(x) + a x + b^ d dx^2 \u03c8(x) + A + + A - x^2 - A 0 x^2\\] (15)\n\nwhere A \u00b1 ,A 0 ,a,b are real parameters determined in terms of V(r) and E.\n\nTo solve equation (15) it is proposed a solution of the form\n\n\u03c8(x) = x^a e^bx y(x), (16)\n\nwhere y = ^ k=0^ c^ k^ L^ k^ (x), and L^ n^ (x) are the Laguerre polynomials of order \u03bd, and \u03b1,\u03b2,\u03bd are dimensionless parameters, free for the moment, but to be determined according to the concrete examples to be solved below. To solve (15) the use of the finite Laguerre transform is introduced. Many of the following transforms are known [0] or are obtained by direct calculation by using Laguerre polynomial properties found in [0] or in [0].\n\n "
    ],
    "kosmos": [
      "where \u03b4 nm is the Kronecker delta symbol. Such relation leads to the definition of the Laguerre transform of order \u03bd:\n\nT^ = {c^ },\n\nit must be emphasize that the Laguerre transform is a sequence of numbers in C. The inverse Laguerre transform is defined by\n\nf(x) = (T^ )^ -1 [{c^ }] = ^ k=0 c^ n^ L^ n^ (x).\n\n## 3 Examples of applications\n\n### Applications to the Schrodinger equation\n\nIn this section is consider the equation\n\n- 1 2 d^ 2 dr^ 2 (r)\n- (V(r) + E) (r) = 0 (13)\n\nwhich in appropriate units (h=M=1) is the steady state Schrodinger equation defined in a one dimensional space, where V(r) is a potential function and E is the energy. Under the change of coordinates (see [0]), given by \u03bb^1 \u03be(x) = dx/dr, where \u03bb \u2265 0 has inverse length units, equation (13) becomes\n\n- 1 2 d^ 2 dx^ 2 (x) + \u03be d^ 2 dx^ 2 (x)\n- \u03bb^2 \u03be^2 W(x) (x) = 0, (14)\n\nwhere W(x) = V(r) - E. To obtain a Laguerre-type equation the change of coordinates must satisfy x(r) \u2265 0 and setting 1 \u03be d^ 2 dx = a x, leads to \u03be(x) = x a e bx.\n\nIn this way, equation (14) becomes\n\n- 1 2 d^ 2 dx^ 2 (x) + a x + b d^ 2 dx^ 2 (x) + A + + A\n- A 0 x^2 = 0, (15)\n\nwhere A \u00b1 ,A 0 ,a,b are real parameters determined in terms of V(r) and E.\n\nTo solve equation (15) it is proposed a solution of the form\n\n- 1 2 y(x) = x^a e -bx y(x), (16)\n\nwhere y = ^ k=0 c^ k^ L^ k^ (x), and L^ n^ (x) are the Laguerre polynomials of order \u03bd, and \u03b1,\u03b2,\u03bd are dimensionless parameters, free for the moment, but to be determined according to the concrete examples to be solved below.\n\nTo solve (15) the use of the finite Laguerre transform is introduced. Many of the following transforms are known [0] or are obtained by direct calculation by using Laguerre polynomial properties found in [0] or in [0].\n\n "
    ]
  },
  {
    "edit": [
      "Magnetohydrodynamics of the transition between the buoyancy-dominated and Lorentz-force-dominated regimes in quasistatic magnetohydrodynamics\n\nAndrei Teimurazov 1 , Matthew McCormack 2,1,1, Moritz Linkmann 2,1,1, and Olga Shishkina A. Teimurazov and M. McCormack contributed equally. 1,2\n\n1 Max Planck Institute for Dynamics and Self-Organization, 37077 G\u02dcA\u00b8ttingen, Germany\n\n2 School of Mathematics and Maxwell Institute for Mathematical Sciences, University of Edinburgh, UK\n\n###### Abstract\n\nIn magnetoconvection, the flow of electromagnetically conductive fluid is driven by a combination of buoyancy forces, which create the fluid motion due to thermal expansion and contraction, and Lorentz forces, which distort the convective flow structure in the presence of a magnetic field. The differences in the global flow structures in the buoyancy-dominated and Lorentz-force-dominated regimes lead to different heat transport properties in these regimes, reflected in distinct dimensionless scaling relations of the global heat flux (Nusselt number Nu) versus the strength of buoyancy (Rayleigh number Ra) and electromagnetic forces (Hartmann number Ha). Here, we propose a theoretical model for the transition between these two regimes for the case of a quasistatic vertical magnetic field applied to a convective fluid layer confined between two isothermal, a lower warmer and an upper colder, horizontal surfaces. The model suggests that the scaling exponents \u03b3 in the buoyancy-dominated regime, Nu \u223c Ra \u03b3 , and \u03be in the Lorentz-force-dominated regime, Nu \u223c (Ha \u2212 2 Ra) \u03be , are related as \u03be = \u03b3/ (1 \u2212 2 \u03b3 ), and the onset of the transition scales with Ha \u2212 1 /\u03b3 Ra. These theoretical results are supported by our Direct Numerical Simulations for 10 \u2a7d Ha \u2a7d 2000, Prandtl number Pr = 0 . 025 and Ra up to 10 9 and data from the literature.\n\n## 1 Introduction\n\nMagnetoconvection (MC) governs most astro- and geophysical systems and is relevant to various engineering applications [25, 7]. The former include, for instance, outer layers of stars and liquid metal planetary cores [12], examples of the latter comprise liquid-metal batteries, induction heating, casting, liquid-metal cooling for nuclear fusion reactors and semiconductor crystal growth [6]. MC occurs in an electrically conducting fluid that is subjected both to a magnetic field and an imposed temperature gradient. The buoyancy forces induce convective fluid motion due to thermal expansion and contraction, while the magnetic field affects this motion and distorts the global flow structure through the Lorentz force, which eventually influences the heat transport in the system. The resulting main two control parameters, the strength of the imposed thermal driving and that of the external magnetic field, are encoded in independent dimensionless groups, the Rayleigh number Ra and Hartmann number Ha, respectively.\n\nOne of the key objectives in MC research is to provide scaling relations for the heat transport through the system, represented in dimensionless form by the Nusselt number Nu, as a function of Ra and Ha. However, the heat transport scaling relations also depend on the flow configuration, including the angle between the magnetic field and gravity, the geometry of the container and the boundary conditions (BCs), and on whether the buoyancy forces dominate over the Lorentz forces in the system or vice versa. This inherent complexity results in the need, at least in principle, to derive separate heat transport scaling relations to describe each specific flow regime itself and transitions between distinct regimes. The considerable difficulty of doing so in a coherent manner is exacerbated by non-universal scaling relations even within specific regimes - the scaling relations in the buoyancy-dominated and Lorentz-force-dominated regimes themselves change with the control parameters, and transitions between the different regimes are also non-universal.\n\nThe objective of this paper is to offer a unifying heat transport model for the transition between the buoyancy-dominated and Lorentz-force-dominated regimes in quasistatic MC. We focus on Rayleigh-B\u00b4enard "
    ],
    "kosmos": [
      "Magnetohydrodynamics of the transition between the buoyancy-dominated and Lorentz-force-dominated regimes in quasistatic magnetohydrodynamics\n\nAndrei Teimurazov 1 , Matthew McCormack 2,1,1, Moritz Linkmann 2,1,1, and Olga Shishkina 1,1,2\n\n1 Max Planck Institute for Dynamics and Self-Organization, 37077 G\u00a8A\u00a8ttingen, Germany\n\n2 School of Mathematics and Maxwell Institute for Mathematical Sciences, University of Edinburgh, UK\n\nandrei.teimurazov@mpid.de, matthew.mccormack@ed.ac.uk, olga.shishkina@ed.ac.uk\n\n###### Abstract\n\nIn magnetoconvection, the flow of electromagnetically conductive fluid is driven by a combination of buoyancy forces, which create the fluid motion due to thermal expansion and contraction, and Lorentz forces, which distort the convective flow structure in the presence of a magnetic field. The differences in the global flow structures in the buoyancy-dominated and Lorentz-force-dominated regimes lead to different heat transport properties in these regimes, reflected in distinct dimensionless scaling relations of the global heat flux (Nusselt number Nu) versus the strength of buoyancy (Rayleigh number Ra) and electromagnetic forces (Hartmann number Ha). Here, we propose a theoretical model for the transition between these two regimes for the case of a quasistatic vertical magnetic field applied to a convective fluid layer confined between two isothermal, a lower warmer and an upper colder, horizontal surfaces. The model suggests that the scaling exponents \u03b3 in the buoyancy-dominated regime, Nu \u223c Ra \u03b3 , and \u03be in the Lorentz-force-dominated regime, Nu \u223c (Ha \u2212 2 Ra) \u03be , are related as \u03be = \u03b3/ (1 \u2212 2 \u03b3 ), and the onset of the transition scales with Ha \u2212 1 /\u03b3 Ra. These theoretical results are supported by our Direct Numerical Simulations for 10 \u2a7d Ha \u2a7d 2000, Prandtl number Pr = 0 . 025 and Ra up to 10 9 and data from the literature.\n\n## 1 Introduction\n\nMagnetoconvection (MC) governs most astro- and geophysical systems and is relevant to various engineering applications [25, 7]. The former include, for instance, outer layers of stars and liquid metal planetary cores [12], examples of the latter comprise liquid-metal batteries, induction heating, casting, liquid-metal cooling for nuclear fusion reactors and semiconductor crystal growth [6]. MC occurs in an electrically conducting fluid that is subjected both to a magnetic field and an imposed temperature gradient. The buoyancy forces induce convective fluid motion due to thermal expansion and contraction, while the magnetic field affects this motion and distorts the global flow structure through the Lorentz force, which eventually influences the heat transport in the system. The resulting main two control parameters, the strength of the imposed thermal driving and that of the external magnetic field, are encoded in independent dimensionless groups, the Rayleigh number Ra and Hartmann number Ha, respectively.\n\nOne of the key objectives in MC research is to provide scaling relations for the heat transport through the system, represented in dimensionless form by the Nusselt number Nu, as a function of Ra and Ha. However, the heat transport scaling relations also depend on the flow configuration, including the angle between the magnetic field and gravity, the geometry of the container and the boundary conditions (BCs), and on whether the buoyancy forces dominate over the Lorentz forces in the system or vice versa. This inherent complexity results in the need, at least in principle, to derive separate heat transport scaling relations to describe each specific flow regime itself and transitions between distinct regimes. The considerable difficulty of doing so in a coherent manner is exacerbated by non-universal scaling relations even within specific regimes - the scaling relations in the buoyancy-dominated and Lorentz-force-dominated regimes themselves change with the control parameters, and transitions between the different regimes are also non-universal.\n\nThe objective of this paper is to offer a unifying heat transport model for the transition between the buoyancy-dominated and Lorentz-force-dominated regimes in quasistatic MC. We focus on Rayleigh-B\u00b4enard "
    ]
  },
  {
    "edit": [
      "\n\n### Compared Methods\n\nThe experiment includes a comparison of different models: \u2022 **I) MHA-LSTM [4]:** This model only takes as inputs the past trajectories of the agents in the scene and outputs \\(L\\) trajectories with their associated probabilities (see the architecture in the red rectangle in Fig. 1). We use \\(L=6\\) attention heads. \u2022 **II) G-MHA-LSTM [17]:** We add to the previous model a radial grid representation from which we extract potential goals. We predict the goal and then the trajectories conditioned on the predicted goal. (see the architecture in the orange rectangle in Fig. 1). \u2022 **III) ODCM-MHA-LSTM :** To predict the goal of the target agent, we combine the DCM and the neural network using the LMNL framework [15]. This model is described in Section III and the architecture is illustrated in the blue rectangle in Fig. 1. \u2022 **IV) ODCM-MHA-LSTM :** This model only uses the DCM to predict the goal of the target agent. **Goal set representations :** We also compare different types of radial grids. For the methods II), III) and IV), we compare our results for two types of radial grid : a **dynamic** grid (d) and a **fixed** one (f). Similar to [12], we build the dynamic grid by considering the target agent\u2019s current velocity \\(v_{T}^{t_{obs}}\\). If \\(v_{T}^{t_{obs}}=0\\), we replace it with an arbitrary value equals to \\(0.5\\)\\(m.s^{-1}\\). The fixed grid is built using the value \\(v=5.83m.s^{-1}\\), which corresponds to the mean of the velocities in the INTERACTION training set. ### Compared DCMs\n\nWe compare two types of DCMs for modelling the behavior of vehicle motion. For our case, the functions modelling vehicle motion phenomenon which we consider for goal selection in this work are: 1) *occupancy:* directions containing neighbours in the vicinity are less desirable. 2) *keep direction:* vehicles tend to maintain the same direction of motion. 3) *collision avoidance:* when a neighbour vehicle\u2019s trajectory is head-on towards a potential goal, this goal becomes less desirable due to the chance of a collision. \u2022 **1) DCM 1 :** For the first DCM configuration, we use a utility function defined as: \\[u_{k}(\\mathbf{X})=\\beta_{dir}dir_{k}+\\beta_{occ}occ_{k}+\\beta_{col}col_{k}\\] (13) Where the functions \\(dir_{k}\\), \\(occ_{k}\\), and \\(col_{k}\\) correspond respectively to *keep direction*, *occupancy* and *collision avoidance*. These functions are defined in [2] and [6]. \u2022 **2) DCM 2 :** For the second DCM, the utility function is defined as : \\[u_{k}(\\mathbf{X})=\\beta_{dir}dir_{k}+\\beta_{occup}occup_{k}\\] (14) Where the function \\(dir_{k}\\) is the same as in (IV-C). For \\(occup_{k}\\), we use the same mathematical formula as the occupancy function in (IV-C), however, we don\u2019t consider the position of the neighbors at time \\(t_{obs}\\). Instead, we consider their predicted position at time \\(t_{obs}+t_{f}\\) using a Constant velocity model. We assume that before predicting his goal, the target agent first predicts the future positions of his surroundings according to their headings and current velocitites, and then avoids the zones that are expected to be crowded. While training this model, we calculate the \\(occup_{k}\\) function using the grouth truth positions of the neighbors. ### Implementation details\n\nWe use \\(K=15\\) number of potential goals. Similar to [8], our interaction space is 40 m ahead of the target vehicle, 10 m behind and 25 m on each side. We consider the neighbors situated in the interaction space at \\(t_{obs}\\). We also take into account the neighbors that are susceptible of being in this space from time \\(t_{obs}\\) to \\(t_{f}\\). To do so, we predict the trajectories of all of the neighbors in the scene using a Constant Velocity model and if they have a predicted position in the interaction space, we consider them in our model. We argue that this representation allows to consider neighbors that are not situated in the grid at \\(t_{obs}\\) but that can appear in the grid from time \\(t=t_{obs}+1\\) to \\(t=t_{f}\\). without having to create a bigger interaction space which can be more computationally expensive. We use \\(L+K=6+15\\) parallel attention"
    ],
    "kosmos": [
      "\n\n### Compared Methods\n\nThe experiment includes a comparison of different models:\n\n* **I) MHA-LSTM [4]:** This model only takes as inputs the past trajectories of the agents in the scene and outputs \\(L\\) trajectories with their associated probabilities (see the architecture in the red rectangle in Fig. 1). We use \\(L=6\\) attention heads.\n* **II) G-MHA-LSTM [17]:** We add to the previous model a radial grid representation from which we extract potential goals. We predict the goal and then the trajectories conditioned on the predicted goal. (see the architecture in the orange rectangle in Fig. 1).\n* **III) ODCM-MHA-LSTM :** To predict the goal of the target agent, we combine the DCM and the neural network using the LMNL framework [15]. This model is described in Section III and the architecture is illustrated in the blue rectangle in Fig. 1.\n* **IV) ODCM-MHA-LSTM :** This model only uses the DCM to predict the goal of the target agent.\n\n**Goal set representations :** We also compare different types of radial grids. For the methods II), III) and IV), we compare our results for two types of radial grid : a **dynamic** grid (d) and a **fixed** one (f). Similar to [12], we build the dynamic grid by considering the target agent\u2019s current velocity \\(v_{T}^{t_{obs}}\\). If \\(v_{T}^{t_{obs}}=0\\), we replace it with an arbitrary value equals to \\(0.5\\)\\(m.s^{-1}\\). The fixed grid is built using the value \\(v=5.83m.s^{-1}\\), which corresponds to the mean of the velocities in the INTERACTION training set.\n\n### Compared DCMs\n\nWe compare two types of DCMs for modelling the behavior of vehicle motion. For our case, the functions modelling vehicle motion phenomenon which we consider for goal selection in this work are:\n\n1. *occupancy:* directions containing neighbours in the vicinity are less desirable.\n2. *keep direction:* vehicles tend to maintain the same direction of motion.\n3. *collision avoidance:* when a neighbour vehicle\u2019s trajectory is head-on towards a potential goal, this goal becomes less desirable due to the chance of a collision.\n\n* **1) DCM 1 :** For the first DCM configuration, we use a utility function defined as: *u*<k> (14)=\"\" (2)=\"\" (6).=\"\" (8),=\"\" (iv-c).=\"\" (iv-d).=\"\" (iv-f).=\"\" (iv-g).=\"\" (i)=\"\" (ii)=\"\" (iii)=\"\" (iv-k)=\"\" (iv-lstm)=\"\" (iv-mha-lstm)=\"\" (iv-nml)=\"\" (iv-nml):=\"\" (iv-nml),=\"\" (iv-nml).=\"\" (iv-nml).=\"\" (iv-nml),=\"\" (iv-nml).=\"\" (iv-nml).\n\n"
    ]
  },
  {
    "edit": [
      "that local interactions play a subordinate role and the hard-core constraint is largely inactive. As a result, the qualitative behavior of the system is dominated by the hopping processes, which naturally result in the formation of a Fermi surface similar to the one in graphene at high hole doping. The hard-core nature of the fermions and their density-density interactions on the enter as corrections to this pure Fermi-gas behavior. By measuring the lattice Green\u2019s function \\(G^{\\alpha,\\beta}_{i,j}\\equiv\\langle c^{\\dagger}_{\\beta,j}c_{\\alpha,i}\\rangle\\), we obtain the momentum-space occupation number \\(n({\\bf k})\\) of the ground state wave function as\n\n\\[n({\\bf k}) = 1\\over N\\sum_{\\alpha=A,B}\\sum_{i,j}e^{i{\\bf k}\\cdot({\\bf r}_{i}- {\\bf r}_{j})}G^{\\alpha,\\alpha}_{i,j}. \\tag{3}\\]\n\nFor non-interacting fermions on the honeycomb lattice, i.e. without density-density interactions or the hard-core constraint, \\(n({\\bf k})\\) should exhibit a clear step (whose magnitude is the quasiparticle residue, \\(Z\\)) as a function of the graphene dispersion \\(\\epsilon_{\\rm G}({\\bf k})\\). Fig. 3 (a) clearly displays a step-like behavior for the occupation at lower fillings \u03bd \u2272 0 . 2, consistent with the formation of a Fermi liquid. Increasing the fermion density leads to a gradual softening of the quasiparticle residue, so that the Fermi liquid appears to give way to a non-Fermi-liquid phase at \u03bd \u2273 0 . 25.\n\nInterestingly, we find that the Fermi liquid regime is interrupted by a charge ordered phase at filling \u03bd = 1 / 6, which triples the unit cell and spontaneously polarizes into either the A or B sublattice. The expected six-fold ground state degeneracy in the many-body spectrum as well as the clear energetic preference of simulation clusters supporting this form of order strongly points at spontaneous symmetry breaking as the root of this incompressible phase. Additionally, the static structure factor S C ( q ) obtained from Fourier transformation of the measured density-density correlation function\n\n\\[C^{\\alpha,\\beta}_{i}(a) \\equiv \\langle n^{\\alpha}_{i}n^{\\beta}_{i+a \u27e9, \\tag{4}\\]\n\nshown in Fig. 3 (b), is expected to exhibit pronounced peaks. Indeed, as can be seen in Fig. 3 (c), the peak at q = K extrapolates to finite values in the TDL while other signals vanish, indicative of long-range order.\n\nThese ED results are corroborated by our HF simulations, where the same type of symmetry breaking order prevails at \u03bd = 1 / 6 and ground state energies per site are relatively close to the ones obtained from ED (cf. Fig. 1 for \u03bd \u2272 1 / 6). HF also finds a charge density wave at \u03bd = 1 / 8 consisting of a fermion delocalized around a honeycomb for every enlarged 2 \u00d7 2 unit cell. This suggests a potentially more general instability towards charge order above some critical filling in the dilute fermion regime.\n\n## III Zero-energy state window\n\n1 / 4 \u2272 \u03bd \u2272 0 . 292\n\nStarting from \u03bd = 1 / 4, i.e. one fermion per two unit cells, the ground state of the SUSY model on the honeycomb lattice from Eq. ( 2 ) has exactly zero energy for a finite range of fillings. In fact, as illustrated in Fig. 4 , we find robust zero-energy states for (almost 2) all rational fillings 1 / 4 \u2272 \u03bd \u2272 0 . 292 within our finite-size simulations. This filling window extends beyond the homological predictions of Ref. [ 16 ].\n\nFootnote 2: For the two rational fillings accessible to us in \u03bd \u2208 (0 . 286 , 0 . 292), we suspect zero-energy states emerge for appropriate geometries but were unable to identify them in our study.\n\nIn contrast to the finite-energy Fermi-liquid and charge ordered phases discussed in Sec. II , due to the supersymmetry breaking, the ground state of the SUSY model on the honeycomb lattice from Eq. ( 2 ) is not a Fermi liquid, but a charge ordered phase. This is due to the fact that the fermion density is not conserved in the ground state, which is a consequence of the fact that the fermion number is not conserved in the ground state. The ground state of the SUSY model on the honeycomb lattice from Eq. ( 2 ) is therefore a non-Fermi liquid. The ground state of the SUS "
    ],
    "kosmos": [
      "that local interactions play a subordinate role and the hard-core constraint is largely inactive. As a result, the qualitative behavior of the system is dominated by the hopping processes, which naturally result in the formation of a Fermi surface similar to the one in graphene at high hole doping. The hard-core nature of the fermions and their density-density interactions on the enter as corrections to this pure Fermi-gas behavior. By measuring the lattice Green\u2019s function \\(G^{\\alpha,\\beta}_{i,j}\\equiv\\langle c^{\\dagger}_{\\beta,j}c_{\\alpha,i}\\rangle\\), we obtain the momentum-space occupation number \\(n({\\bf k})\\) of the ground state wave function as\n\n\\[n({\\bf k}) = 1\\over N\\sum_{\\alpha=A,B}\\sum_{i,j}e^{i{\\bf k}\\cdot({\\bf r}_{i}- {\\bf r}_{j})}G^{\\alpha,\\alpha}_{i,j}. \\tag{3}\\]\n\nFor non-interacting fermions on the honeycomb lattice, i.e. without density-density interactions or the hard-core constraint, \\(n({\\bf k})\\) should exhibit a clear step (whose magnitude is the quasiparticle residue, \\(Z\\)) as a function of the graphene dispersion \\(\\epsilon_{\\rm G}({\\bf k})\\). Fig. 3 (a) clearly displays a step-like behavior for the occupation at lower fillings \u03bd \u2272 0 . 2, consistent with the formation of a Fermi liquid. Increasing the fermion density leads to a gradual softening of the quasiparticle residue, so that the Fermi liquid appears to give way to a non-Fermi-liquid phase at \u03bd \u2273 0 . 25.\n\nInterestingly, we find that the Fermi liquid regime is interrupted by a charge ordered phase at filling \u03bd = 1 / 6, which triples the unit cell and spontaneously polarizes into either the A or B sublattice. The expected six-fold ground state degeneracy in the many-body spectrum as well as the clear energetic preference of simulation clusters supporting this form of order strongly points at spontaneous symmetry breaking as the root of this incompressible phase. Additionally, the static structure factor S C ( q ) obtained from Fourier transformation of the measured density-density correlation function\n\n\\[C^{\\alpha,\\beta}_{i}(a) \\equiv \\langle n^{\\alpha}_{i}n^{\\beta}_{i+a \u27e9, \\tag{4}\\]\n\nshown in Fig. 3 (b), is expected to exhibit pronounced peaks. Indeed, as can be seen in Fig. 3 (c), the peak at q = K extrapolates to finite values in the TDL while other signals vanish, indicative of long-range order.\n\nThese ED results are corroborated by our HF simulations, where the same type of symmetry breaking order prevails at \u03bd = 1 / 6 and ground state energies per site are relatively close to the ones obtained from ED (cf. Fig. 1 for \u03bd \u2272 1 / 6). HF also finds a charge density wave at \u03bd = 1 / 8 consisting of a fermion delocalized around a honeycomb for every enlarged 2 \u00d7 2 unit cell. This suggests a potentially more general instability towards charge order above some critical filling in the dilute fermion regime.\n\n## III Zero-energy state window\n\n1 / 4 \u2272 \u03bd \u2272 0 . 292\n\nStarting from \u03bd = 1 / 4, i.e. one fermion per two unit cells, the ground state of the SUSY model on the honeycomb lattice from Eq. ( 2 ) has exactly zero energy for a finite range of fillings. In fact, as illustrated in Fig. 4 , we find robust zero-energy states for (almost 2) all rational fillings 1 / 4 \u2272 \u03bd \u2272 0 . 292 within our finite-size simulations. This filling window extends beyond the homological predictions of Ref. [ 16 ].\n\nFootnote 2: For the two rational fillings accessible to us in \u03bd \u2208 (0 . 286 , 0 . 292), we suspect zero-energy states emerge for appropriate geometries but were unable to identify them in our study.\n\nIn contrast to the finite-energy Fermi-liquid and charge ordered phases discussed in Sec. II , due to the supersymmetry breaking, the ground state of the SUSY model on the honeycomb lattice from Eq. ( 2 ) is not a Fermi liquid, but a charge ordered phase. This is due to the fact that the fermion density is not conserved in the ground state, which is a consequence of the fact that the fermion number is not conserved in the ground state. The ground state of the SUSY model on the honeycomb lattice from Eq. ( 2 ) is therefore a non-Fermi liquid. The ground state of the SUS "
    ]
  },
  {
    "edit": [
      "University, Thailand. We acknowledge the supporting computing infrastructure provided by NSTDA, CU, CUAASC, NSRF via PMUB [B05F650021, B37G660013] (Thailand). URL:www.e-science.in.th. The Computational Materials Physics (CMP) Project, SLRI, Thailand, is acknowledged for providing computational resource.\n\n## References\n\n* Needs and Pickard [2016]R. J. Needs and C. J. Pickard, Perspective: Role of structure prediction in materials discovery and design, APL Materials **4**, 053210 (2016).\n* Kohn and Sham [1965]W. Kohn and L. J. Sham, Phys. Rev. **140**, A1133 (1965).\n* Oganov and Glass [2006]A. R. Oganov and C. W. Glass, Crystal structure prediction using ab initio evolutionary techniques: Principles and applications, The Journal of Chemical Physics **124**, 244704 (2006).\n* Wang _et al._ [2010]Y. Wang, J. Lv, L. Zhu, and Y. Ma, Crystal structure prediction via particle-swarm optimization, Phys. Rev. B **82**, 094116 (2010).\n* Pickard and Needs [2011]C. J. Pickard and R. J. Needs, Ab initio random structure searching, Journal of Physics: Condensed Matter **23**, 053201 (2011).\n* Oganov _et al._ [2019]A. R. Oganov, C. J. Pickard, Q. Zhu, and R. J. Needs, Structure prediction drives materials discovery, Nature Reviews Materials **4**, 331 (2019).\n* Schon _et al._ [2010]J. C. Schon, K. Doll, and M. Jansen, Predicting solid compounds via global exploration of the energy landscape of solids on the ab initio level without recourse to experimental information, physica status solidi (b) **247**, 23 (2010).\n* Xie _et al._ [2022]T. Xie, X. Fu, O.-E. Ganea, R. Barzilay, and T. S. Jaakkola, Crystal diffusion variational autoencoder for periodic material generation, in _International Conference on Learning Representations_ (2022).\n* Shi _et al._ [2021]C. Shi, S. Luo, M. Xu, and J. Tang, Learning gradient fields for molecular conformation generation, in _Proceedings of the 38th International Conference on Machine Learning_, Proceedings of Machine Learning Research, Vol. 139, edited by M. Meila and T. Zhang (PMLR, 2021) pp. 9558\u20139568.\n* Xu _et al._ [2022]M. Xu, L. Yu, Y. Song, C. Shi, S. Ermon, and J. Tang, Geodiff: A geometric diffusion model for molecular conformation generation, in _International Conference on Learning Representations_ (2022).\n* Guan _et al._ [2023]J. Guan, W. W. Qian, X. Peng, Y. Su, J. Peng, and J. Ma, 3d equivariant diffusion for target-aware molecule generation and affinity prediction, in _The Eleventh International Conference on Learning Representations_ (2023).\n* Kang and Cho [2019]S. Kang and K. Cho, Conditional molecular design with deep generative models, Journal of Chemical Information and Modeling **59**, 43 (2019), pMID: 30016587.\n* Lim _et al._ [2018]J. Lim, S. Ryu, J. W. Kim, and W. Y. Kim, Molecular generative model based on conditional variational autoencoder for de novo molecular design, Journal of Cheminformatics **10**, 31 (2018).\n* Song _et al._ [2022]Y. Song, L. Shen, L. Xing, and S. Ermon, Solving inverse problems in medical imaging with score-based generative models, in _International Conference on Learning Representations_ (2022).\n* Cui _et al._ [2019]A. Cui, K. Jiang, M. Jiang, L. Shang, L. Zhu, Z. Hu, G. Xu, and J. Chu, Decoding phases of matter by machine-learning raman spectroscopy, Phys. Rev. Appl. **12**, 054049 (2019).\n* Carbone _et al._ [2020]M. R. Carbone, M. Topsakal, D. Lu, and S. Q. (Thailand). URL:www.e-science.in.th. The Computational Materials Physics (CMP) Project, SLRI, "
    ],
    "kosmos": [
      "University, Thailand. We acknowledge the supporting computing infrastructure provided by NSTDA, CU, CUAASC, NSRF via PMUB [B05F650021, B37G660013] (Thailand). URL:www.e-science.in.th. The Computational Materials Physics (CMP) Project, SLRI, Thailand, is acknowledged for providing computational resource.\n\n## References\n\n* Needs and Pickard [2016]R. J. Needs and C. J. Pickard, Perspective: Role of structure prediction in materials discovery and design, APL Materials **4**, 053210 (2016).\n* Kohn and Sham [1965]W. Kohn and L. J. Sham, Phys. Rev. **140**, A1133 (1965).\n* Oganov and Glass [2006]A. R. Oganov and C. W. Glass, Crystal structure prediction using ab initio evolutionary techniques: Principles and applications, The Journal of Chemical Physics **124**, 244704 (2006).\n* Wang _et al._ [2010]Y. Wang, J. Lv, L. Zhu, and Y. Ma, Crystal structure prediction via particle-swarm optimization, Phys. Rev. B **82**, 094116 (2010).\n* Pickard and Needs [2011]C. J. Pickard and R. J. Needs, Ab initio random structure searching, Journal of Physics: Condensed Matter **23**, 053201 (2011).\n* Oganov _et al._ [2019]A. R. Oganov, C. J. Pickard, Q. Zhu, and R. J. Needs, Structure prediction drives materials discovery, Nature Reviews Materials **4**, 331 (2019).\n* Schon _et al._ [2010]J. C. Schon, K. Doll, and M. Jansen, Predicting solid compounds via global exploration of the energy landscape of solids on the ab initio level without recourse to experimental information, physica status solidi (b) **247**, 23 (2010).\n* Xie _et al._ [2022]T. Xie, X. Fu, O.-E. Ganea, R. Barzilay, and T. S. Jaakkola, Crystal diffusion variational autoencoder for periodic material generation, in _International Conference on Learning Representations_ (2022).\n* Shi _et al._ [2021]C. Shi, S. Luo, M. Xu, and J. Tang, Learning gradient fields for molecular conformation generation, in _Proceedings of the 38th International Conference on Machine Learning_, Proceedings of Machine Learning Research, Vol. 139, edited by M. Meila and T. Zhang (PMLR, 2021) pp. 9558\u20139568.\n* Xu _et al._ [2022]M. Xu, L. Yu, Y. Song, C. Shi, S. Ermon, and J. Tang, Geodiff: A geometric diffusion model for molecular conformation generation, in _International Conference on Learning Representations_ (2022).\n* Guan _et al._ [2023]J. Guan, W. W. Qian, X. Peng, Y. Su, J. Peng, and J. Ma, 3d equivariant diffusion for target-aware molecule generation and affinity prediction, in _The Eleventh International Conference on Learning Representations_ (2023).\n* Kang and Cho [2019]S. Kang and K. Cho, Conditional molecular design with deep generative models, Journal of Chemical Information and Modeling **59**, 43 (2019), pMID: 30016587.\n* Lim _et al._ [2018]J. Lim, S. Ryu, J. W. Kim, and W. Y. Kim, Molecular generative model based on conditional variational autoencoder for de novo molecular design, Journal of Cheminformatics **10**, 31 (2018).\n* Song _et al._ [2022]Y. Song, L. Shen, L. Xing, and S. Ermon, Solving inverse problems in medical imaging with score-based generative models, in _International Conference on Learning Representations_ (2022).\n* Cui _et al._ [2019]A. Cui, K. Jiang, M. Jiang, L. Shang, L. Zhu, Z. Hu, G. Xu, and J. Chu, Decoding phases of matter by machine-learning raman spectroscopy, Phys. Rev. Appl. **12**, 054049 (2019).\n* Carbone _et al._ [2020]M. R. Carbone, M. Topsakal, D. Lu, and S. Q. (Thailand). URL:www.e-science.in.th. The Computational Materials Physics (CMP) Project, SLRI, "
    ]
  },
  {
    "edit": [
      "which is precisely ( 1.3) for \\(p=1\\) due to ( 3.12); the non-normalized case follows in a standard way.\n\nSince the Sobolev inequality ( 1.3) for \\(p=1\\) is equivalent to the isoperimetric inequality\n\n\\[n(\\omega_{n}AVR_{g})^{\\frac{1}{n}}\\text{Vol}_{g}(\\Omega)^{\\frac{n-1}{n}}\\leq P _{g}(\\partial\\Omega) \\tag{3.15}\\]\n\nfor every bounded open domain \\(\\Omega\\subset M\\) with smooth boundary (\\(P_{g}\\) being the perimeter), and ( 3.15) is sharp, see Balogh and Kristaly [2] and Brendle [4], it turns out that ( 1.3) is also sharp. \\(\\square\\)\n\n## 4. Proof of the sharp \\(L^{p}\\)-logarithmic Sobolev inequality (Theorem 1.2)\n\n### The case \\(p&gt;1\\)\n\nLet \\(p&gt;1\\) and fix \\(f\\in C^{\\infty}_{0}(M)\\) arbitrarily; we may assume that \\(f\\) is nonnegative and\n\n\\[\\int_{M}f^{p}\\text{d}v_{g}=1.\\]\n\nAs before, let \\(\\Omega=\\{x\\in M:f(x)&gt;0\\}\\); since \\(f\\in C^{\\infty}_{0}(M)\\), then \\(\\Omega\\) is compact.\n\nLet \\(x_{0}\\in\\Omega.\\) For every \\(\\lambda&gt;0\\) and \\(k\\in\\mathbb{N}\\), we introduce the truncated Gaussian bubble \\(G_{\\lambda,k}:M\\to\\mathbb{R}\\) given by\n\n\\[G_{\\lambda,k}(x)=P_{k}(d_{g}(x_{0},x))e^{-\\lambda d_{g}^{p^{\\prime}}(x_{0},x)},\\]\n\nwhere \\(P_{k}\\) is defined in ( 3.2). We observe that the support of \\(G_{\\lambda,k}\\) is the ball \\(B_{x_{0}}(k+1).\\) Let\n\n\\[\\mathcal{J}_{\\lambda,k}=\\int_{M}G_{\\lambda,k}(y)\\text{d}v_{g}(y);\\]\n\nclearly, \\(0&lt;\\mathcal{J}_{\\lambda,k}&lt;\\infty\\) for every \\(\\lambda&gt;0\\) and \\(k\\in\\mathbb{N}\\).\n\nLet \\(\\text{d}\\mu(x)=f^{p}(x)\\text{d}v_{g}(x)\\) and \\(\\text{d}\\nu(y)=\\frac{G_{\\lambda,k}(y)}{\\mathcal{J}_{\\lambda,k}}\\text{d}v_{g}(y)\\) be two probability measures on \\((M,g)\\) with compact supports; by the theory of OMT one can find a unique map \\(T:\\Omega\\to B_{x_{0}}(k+1)\\subset M\\) pushing \\(\\mu\\) forward to \\(\\nu\\) having the form \\(T(x)=\\exp_{x}(-\\nabla_{g}u(x))\\) for a.e. \\(x\\in\\Omega,\\) for some \\(c=d_{g}^{2}/2\\)-concave function \\(u:\\Omega\\to\\mathbb{R}\\). The associated Monge-Ampere equation is\n\n\\[f^{p}(x)=\\frac{G_{\\lambda,k}(T(x))}{\\mathcal{J}_{\\lambda,k}}\\text{det}DT(x) \\text{ for a.e. }x\\in\\Omega. \\tag{4.1}\\]\n\nAccordingly, by ( 4.1), a change of variables, Jensen\u2019s inequality and Propositions 2.1 and 2.2, we have\n\n\\[\\int_{M}f^{p}\\log f^{p}\\text{d}v_{g} = \\int_{\\Omega}f^{p}(x)\\log\\left(\\frac{G_{\\lambda,k}(T(x))}{ \\mathcal{J}_{\\lambda,k}}\\text{det}DT(x)\\right)\\text{d}v_{g}(x)\\] \\[= \\int_{\\Omega}f^{p}(x)\\log\\left(G_{\\lambda,k}(T(x))\\right)\\text{d }v_{g}(x)+n\\int_{\\Omega}f^{p}(x)\\log\\left(\\frac{\\text{det}^{\\frac{1}{n}}DT(x)}{ \\mathcal{J}_{\\lambda "
    ],
    "kosmos": [
      "which is precisely ( 1.3) for \\(p=1\\) due to ( 3.12); the non-normalized case follows in a standard way.\n\nSince the Sobolev inequality ( 1.3) for \\(p=1\\) is equivalent to the isoperimetric inequality\n\n\\[n(\\omega_{n}AVR_{g})^{\\frac{1}{n}}\\text{Vol}_{g}(\\Omega)^{\\frac{n-1}{n}}\\leq P _{g}(\\partial\\Omega) \\tag{3.15}\\]\n\nfor every bounded open domain \\(\\Omega\\subset M\\) with smooth boundary (\\(P_{g}\\) being the perimeter), and ( 3.15) is sharp, see Balogh and Kristaly [2] and Brendle [4], it turns out that ( 1.3) is also sharp. \\(\\square\\)\n\n## 4. Proof of the sharp \\(L^{p}\\)-logarithmic Sobolev inequality (Theorem 1.2)\n\n### The case \\(p&gt;1\\)\n\nLet \\(p&gt;1\\) and fix \\(f\\in C^{\\infty}_{0}(M)\\) arbitrarily; we may assume that \\(f\\) is nonnegative and\n\n\\[\\int_{M}f^{p}\\text{d}v_{g}=1.\\]\n\nAs before, let \\(\\Omega=\\{x\\in M:f(x)&gt;0\\}\\); since \\(f\\in C^{\\infty}_{0}(M)\\), then \\(\\Omega\\) is compact.\n\nLet \\(x_{0}\\in\\Omega.\\) For every \\(\\lambda&gt;0\\) and \\(k\\in\\mathbb{N}\\), we introduce the truncated Gaussian bubble \\(G_{\\lambda,k}:M\\to\\mathbb{R}\\) given by\n\n\\[G_{\\lambda,k}(x)=P_{k}(d_{g}(x_{0},x))e^{-\\lambda d_{g}^{p^{\\prime}}(x_{0},x)},\\]\n\nwhere \\(P_{k}\\) is defined in ( 3.2). We observe that the support of \\(G_{\\lambda,k}\\) is the ball \\(B_{x_{0}}(k+1).\\) Let\n\n\\[\\mathcal{J}_{\\lambda,k}=\\int_{M}G_{\\lambda,k}(y)\\text{d}v_{g}(y);\\]\n\nclearly, \\(0&lt;\\mathcal{J}_{\\lambda,k}&lt;\\infty\\) for every \\(\\lambda&gt;0\\) and \\(k\\in\\mathbb{N}\\).\n\nLet \\(\\text{d}\\mu(x)=f^{p}(x)\\text{d}v_{g}(x)\\) and \\(\\text{d}\\nu(y)=\\frac{G_{\\lambda,k}(y)}{\\mathcal{J}_{\\lambda,k}}\\text{d}v_{g}(y)\\) be two probability measures on \\((M,g)\\) with compact supports; by the theory of OMT one can find a unique map \\(T:\\Omega\\to B_{x_{0}}(k+1)\\subset M\\) pushing \\(\\mu\\) forward to \\(\\nu\\) having the form \\(T(x)=\\exp_{x}(-\\nabla_{g}u(x))\\) for a.e. \\(x\\in\\Omega,\\) for some \\(c=d_{g}^{2}/2\\)-concave function \\(u:\\Omega\\to\\mathbb{R}\\). The associated Monge-Ampere equation is\n\n\\[f^{p}(x)=\\frac{G_{\\lambda,k}(T(x))}{\\mathcal{J}_{\\lambda,k}}\\text{det}DT(x) \\text{ for a.e. }x\\in\\Omega. \\tag{4.1}\\]\n\nAccordingly, by ( 4.1), a change of variables, Jensen\u2019s inequality and Propositions 2.1 and 2.2, we have\n\n\\[\\int_{M}f^{p}\\log f^{p}\\text{d}v_{g} = \\int_{\\Omega}f^{p}(x)\\log\\left(\\frac{G_{\\lambda,k}(T(x))}{ \\mathcal{J}_{\\lambda,k}}\\text{det}DT(x)\\right)\\text{d}v_{g}(x)\\] \\[= \\int_{\\Omega}f^{p}(x)\\log\\left(G_{\\lambda,k}(T(x))\\right)\\text{d }v_{g}(x)+n\\int_{\\Omega}f^{p}(x)\\log\\left(\\frac{\\text{det}^{\\frac{1}{n}}DT(x)}{ \\mathcal{J}_{\\lambda "
    ]
  },
  {
    "edit": [
      "<table>\n<colgroup>\n<col/>\n<col/>\n<col/>\n</colgroup>\n<thead>\n<tr>\n<th>\nParameter\n</th>\n<th>\nValue\n</th>\n<th>\nReference\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\nName\n</td>\n<td>\nHD 235088\n</td>\n<td>\nHD\n</td>\n</tr>\n<tr>\n<td></td>\n<td>\nTIC 293954617\n</td>\n<td>\n<em>\nTESS\n</em>\n</td>\n</tr>\n<tr>\n<td></td>\n<td>\nTOI-1430\n</td>\n<td>\nTOI\n</td>\n</tr>\n<tr>\n<td></td>\n<td>\nHIP 98668\n</td>\n<td>\nHIP\n</td>\n</tr>\n<tr>\n<td>\n<em>\nCoordinates and spectral types\n</em>\n</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>\n\u03b1 (J2000)\n</td>\n<td>\n20${}^{\\rm h}$ 02${}^{\\rm m}$ 27\u2032.4\n</td>\n<td>\n<em>\nGaia\n</em>\nEDR3\n</td>\n</tr>\n<tr>\n<td>\n\u03b4 (J2000)\n</td>\n<td>\n+53${}^{\\circ}$ 22\u2032 36\u2032.5\n</td>\n<td>\n<em>\nGaia\n</em>\nEDR3\n</td>\n</tr>\n<tr>\n<td>\nSpectral type\n</td>\n<td>\nK2 V\n</td>\n<td>\nSect. 3.1\n</td>\n</tr>\n<tr>\n<td>\n<em>\nParallax and kinematics\n</em>\n</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>\n\u03c0 [mas]\n</td>\n<td>\n24.25 \u00b1 0.01\n</td>\n<td>\n<em>\nGaia\n</em>\nEDR3\n</td>\n</tr>\n<tr>\n<td>\nd [pc]\n</td>\n<td>\n41.24 \u00b1 0.02\n</td>\n<td>\n<em>\nGaia\n</em>\nEDR3\n</td>\n</tr>\n<tr>\n<td>\n\u00b5 \u03b1 cos \u03b4 [mas yr${}^{-1}$]\n</td>\n<td>\n165.05 \u00b1 0.02\n</td>\n<td>\n<em>\nGaia\n</em>\nEDR3\n</td>\n</tr>\n<tr>\n<td>\n\u00b5\u03b4 [mas yr${}^{-1}$]\n</td>\n<td>\n145.17 \u00b1 0.02\n</td>\n<td>\n<em>\nGaia\n</em>\nEDR3\n</td>\n</tr>\n<tr>\n<td>\n\u03b3\n<sup>\n( o)\n</sup>\n[km s${}^{-1}$]\n</td>\n<td>\n\u221227.370\u00b1 0.002\n</td>\n<td>\n<em>\nGaia\n</em>\nDR2\n</td>\n</tr>\n<tr>\n<td>\nU [km s${}^{-1}$]\n</td>\n<td>\n\u221241.75 \u00b1 0.02\n</td>\n<td>\nThis work\n</td>\n</tr>\n<tr>\n<td>\nV [km s${}^{-1}$]\n</td>\n<td>\n\u221222.16 \u00b1 0.01\n</td>\n<td>\nThis work\n</td>\n</tr>\n<tr>\n<td>\nW [km s${}^{-1}$]\n</td>\n<td>\n\u221219.03 \u00b1 0.02\n</td>\n<td>\nThis work\n</td>\n</tr>\n<tr>\n<td>\nRUWE\n</td>\n<td>\n0.966\n</td>\n<td>\n<em>\nGaia\n</em>\nDR3\n</td>\n</tr>\n<tr>\n<td>\n<em>\nMagnitudes\n</em>\n</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>\nB [mag]\n</td>\n<td>\n10.129 \u00b1 0.038\n</td>\n<td>\nTYC\n</td>\n</tr>\n<tr>\n<td>\nV [mag]\n</td>\n<td>\n9.19 \u00b1 0.03\n</td>\n<td>\nHIP\n</td>\n</tr>\n<tr>\n<td>\nJ [mag]\n</td>\n<td>\n7.646 \u00b1 0.03\n</td>\n<td>\n2MASS\n</td>\n</tr>\n<tr>\n<td>\n<em>\nStellar parameters\n</em>\n</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>\nL${}_{X}$ [\u00d710${}^{28}$ erg s${}^{-1}$]\n</td>\n<td>\n1.89 \u00b1 0.07\n</td>\n<td>\nSect.\n\n "
    ],
    "kosmos": [
      "<table>\n<colgroup>\n<col/>\n<col/>\n<col/>\n</colgroup>\n<thead>\n<tr>\n<th>\nParameter\n</th>\n<th>\nValue\n</th>\n<th>\nReference\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\nName\n</td>\n<td>\nHD 235088\n</td>\n<td>\nHD\n</td>\n</tr>\n<tr>\n<td></td>\n<td>\nTIC 293954617\n</td>\n<td>\n<em>\nTESS\n</em>\n</td>\n</tr>\n<tr>\n<td></td>\n<td>\nTOI-1430\n</td>\n<td>\nTOI\n</td>\n</tr>\n<tr>\n<td></td>\n<td>\nHIP 98668\n</td>\n<td>\nHIP\n</td>\n</tr>\n<tr>\n<td>\n<em>\nCoordinates and spectral types\n</em>\n</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>\n\u03b1 (J2000)\n</td>\n<td>\n20${}^{\\rm h}$ 02${}^{\\rm m}$ 27\u2032.4\n</td>\n<td>\n<em>\nGaia\n</em>\nEDR3\n</td>\n</tr>\n<tr>\n<td>\n\u03b4 (J2000)\n</td>\n<td>\n+53${}^{\\circ}$ 22\u2032 36\u2032.5\n</td>\n<td>\n<em>\nGaia\n</em>\nEDR3\n</td>\n</tr>\n<tr>\n<td>\nSpectral type\n</td>\n<td>\nK2 V\n</td>\n<td>\nSect. 3.1\n</td>\n</tr>\n<tr>\n<td>\n<em>\nParallax and kinematics\n</em>\n</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>\n\u03c0 [mas]\n</td>\n<td>\n24.25 \u00b1 0.01\n</td>\n<td>\n<em>\nGaia\n</em>\nEDR3\n</td>\n</tr>\n<tr>\n<td>\nd [pc]\n</td>\n<td>\n41.24 \u00b1 0.02\n</td>\n<td>\n<em>\nGaia\n</em>\nEDR3\n</td>\n</tr>\n<tr>\n<td>\n\u00b5 \u03b1 cos \u03b4 [mas yr${}^{-1}$]\n</td>\n<td>\n165.05 \u00b1 0.02\n</td>\n<td>\n<em>\nGaia\n</em>\nEDR3\n</td>\n</tr>\n<tr>\n<td>\n\u00b5\u03b4 [mas yr${}^{-1}$]\n</td>\n<td>\n145.17 \u00b1 0.02\n</td>\n<td>\n<em>\nGaia\n</em>\nEDR3\n</td>\n</tr>\n<tr>\n<td>\n\u03b3\n<sup>\n( o)\n</sup>\n[km s${}^{-1}$]\n</td>\n<td>\n\u221227.370\u00b1 0.002\n</td>\n<td>\n<em>\nGaia\n</em>\nDR2\n</td>\n</tr>\n<tr>\n<td>\nU [km s${}^{-1}$]\n</td>\n<td>\n\u221241.75 \u00b1 0.02\n</td>\n<td>\nThis work\n</td>\n</tr>\n<tr>\n<td>\nV [km s${}^{-1}$]\n</td>\n<td>\n\u221222.16 \u00b1 0.01\n</td>\n<td>\nThis work\n</td>\n</tr>\n<tr>\n<td>\nW [km s${}^{-1}$]\n</td>\n<td>\n\u221219.03 \u00b1 0.02\n</td>\n<td>\nThis work\n</td>\n</tr>\n<tr>\n<td>\nRUWE\n</td>\n<td>\n0.966\n</td>\n<td>\n<em>\nGaia\n</em>\nDR3\n</td>\n</tr>\n<tr>\n<td>\n<em>\nMagnitudes\n</em>\n</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>\nB [mag]\n</td>\n<td>\n10.129 \u00b1 0.038\n</td>\n<td>\nTYC\n</td>\n</tr>\n<tr>\n<td>\nV [mag]\n</td>\n<td>\n9.19 \u00b1 0.03\n</td>\n<td>\nHIP\n</td>\n</tr>\n<tr>\n<td>\nJ [mag]\n</td>\n<td>\n7.646 \u00b1 0.03\n</td>\n<td>\n2MASS\n</td>\n</tr>\n<tr>\n<td>\n<em>\nStellar parameters\n</em>\n</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>\nL${}_{X}$ [\u00d710${}^{28}$ erg s${}^{-1}$]\n</td>\n<td>\n1.89 \u00b1 0.07\n</td>\n<td>\nSect.\n\n "
    ]
  },
  {
    "edit": [
      "\n\n## 9. Zeros\n\nIt is well known that the zeros of orthogonal polynomials with respect to a positive definite linear functional are real, simple, and located in the interior of the convex hull of the support [ 15 , 20 ]. With this in mind, let t x n,k p z qu n k \u201c 1 be the the zeros of P n p x q in an increasing order, i. e.,\n\n\\[P_{n}(x_{n,k}(z),z) =0 \\tag{9.1}\\]\n\nwith\n\n\\[x_{n,1}(z) <x_{n,2}(z)<\\cdots<x_{n,n}(z).\\] (5.6)=\"\" (5.4)=\"\" (5.6)=\"\" (5.7)=\"\" (5.8)=\"\" (5.9)=\"\" (5.9).=\"\" (the=\"\" 1.=\"\" 1.6,=\"\" 2.=\"\" 3.5]),=\"\" 5.4.=\"\" 5.5.=\"\" [22,=\"\" [23,=\"\" [24,=\"\" [25,=\"\" [26,=\"\" [27,=\"\" [28,=\"\" [29,=\"\" [30,=\"\" [31,=\"\" [32,=\"\" [33,=\"\" [34,=\"\" [35,=\"\" [36,=\"\" [37,=\"\" [38,=\"\" [39,=\"\" [40,=\"\" [41,=\"\" [42,=\"\" [43,=\"\" [44,=\"\" [45,=\"\" [46,=\"\" [47,=\"\" [48,=\"\" [49,=\"\" [50,=\"\" [51,=\"\" [52,=\"\" [53,=\"\" [54,=\"\" [55,=\"\" [56,=\"\" [57,=\"\" [58,=\"\" [59,=\"\" [60,=\"\" [61,=\"\" [62,=\"\" [63,=\"\" [64,=\"\" [65,=\"\" [66,=\"\" [67,=\"\" [68,=\"\" [69,=\"\" [70,=\"\" [71,=\"\" [72,=\"\" [73,=\"\" [74,=\"\" [75,=\"\" [76,=\"\" [77,=\"\" [78,=\"\" [79,=\"\" [80,=\"\" [81,=\"\" [82,=\"\" [83,=\"\" [84,=\"\" [85,=\"\" [86,=\"\" [87,=\"\" [88,=\"\" [89,=\"\" [90,=\"\" [91,=\"\" [92,=\"\" [93,=\"\" [94,=\"\" [95,=\"\" [96,=\"\" [97,=\"\" [98,=\"\" [99,=\"\" [100,=\"\" [101,=\"\" [102,=\"\" [103,=\"\" [104,=\"\" [105,=\"\" [105,1]=\"\" [106,=\"\" [106,1]=\"\" [107,=\"\" [107,1]=\"\" [108,=\"\" [108,1]=\"\" [108,1]1]=\"\" [108,1]2]=\"\" [108,1]3]=\"\" [108,1]4]=\"\" [108,1]5]=\"\" [108,1]6]=\"\" [108,1]7]=\"\" [108,1]8]=\"\" [108,1]9]=\"\" [108,1]10]=\"\" [108,1]11]=\"\" [108,1]12]=\"\" [108,1]13]=\"\" [108,1]14]=\"\" [108,1]15]=\"\" [108,1]16]=\"\" [108,1]17]=\"\" [108,1]18]=\"\" [108,1]19]=\"\" [108,1]20]]2]=\"\" [108,1]21]=\"\" [108,1]22]=\"\" [108,1]23]=\"\" [108,1]24]=\"\" [108,1]25]=\"\" [108,1]26]=\"\" [108,1]27]=\"\" [108,1]28]]2]=\"\" [108,1]29]=\"\" [108,1]30]=\"\" [108,1]31]=\"\" [108,1]32]=\"\" [108,1]33]=\"\" [108,1]34]=\"\" [108,1]35]=\"\" [108,1]36]=\"\" [108,1]37]=\"\" [108,1]38]=\"\" [108,1]39]=\"\" [108,1]40]=\"\" [108,1]41]=\"\" [108,1]42]=\"\" [108,1]43]=\"\" [108,1]44]=\"\" [108,1]45]=\"\" [108,1]46]=\"\" [108,1]47]=\"\" [108,1]48]=\"\" [108,1]49]=\"\" [108,1]50]=\"\" [108,1]51]=\"\" [108,1]52]=\"\" [108,1]53]=\"\" [108,"
    ],
    "kosmos": [
      "\n\n## 9. Zeros\n\nIt is well known that the zeros of orthogonal polynomials with respect to a positive definite linear functional are real, simple, and located in the interior of the convex hull of the support [15, 20]. With this in mind, let t x n,k p z qu n k \u201c 1 be the the zeros of P n p x q in an increasing order, i. e.,\n\n\\[P_{n}(x_{n,k}(z),z) =0 \\tag{9.1}\\]\n\nwith\n\n\\[x_{n,1}(z) <x_{n,2}(z)<\\cdots<x_{n,n}(z).\\] (5.6)=\"\" (5.4)=\"\" (5.6)=\"\" (5.7)=\"\" (5.8)=\"\" (5.9)=\"\" (5.9).=\"\" (the=\"\" 1.0pt\\hbox{$\\ell$}}\\)=\"\" 1.0pt\\hbox{$\\ell$}}\\]=\"\" 1.0pt\\hbox{$\\ell$}}\\leq\\widehat{x}_{n+1,2}<\\cdots<\\widehat{x}_{n+1,n+1}\\),=\"\" 1.0pt\\hbox{$\\ell$}}\\leq\\widehat{x}_{n+1,k}<\\cdots<\\widehat{x}_{n+1,k}\\]=\"\" 1.\n\n"
    ]
  },
  {
    "edit": [
      "for all \\(t\\in[0,\\delta)\\) almost surely. Furthermore, for \\(p\\geqslant 4\\), using Ito\u2019s formula for \\(\\|u\\|_{1 ,2}^{p}\\), we have\n\n\\[\\|w^{\\varepsilon}(t)\\|_{1 ,2}^{p}-\\|w_{0}\\|_{1 ,2}^{p}=p\\sum_{k}\\lambda_{k}\\int_{0}^{t}\\|w^{\\varepsilon}(s)\\|_{1 ,2}^{p-2}(\\partial_{x}(\\Psi_{k}w^{\\varepsilon}(s)),w^{\\varepsilon}(s))_{1 ,2}d\\beta^{k}(s)\\] \\[+ p\\sum_{k}\\gamma_{k}\\int_{0}^{t}\\|w^{\\varepsilon}(s)\\|_{1 ,2}^{p-2}(\\Psi_{k}f(w^{\\varepsilon}(s)),w^{\\varepsilon}(s))_{1 ,2}d\\beta_{1}^{k}(s)\\] \\[+ \\frac{p}{2}\\int_{0}^{t}\\|w^{\\varepsilon}(s)\\|_{1 ,2}^{p-2}(2\\langle A^{\\varepsilon}w^{\\varepsilon}(s),w^{\\varepsilon}(s) \\rangle)+\\|B(w^{\\varepsilon}(s))\\|_{L_{2}}^{2})\\,ds\\] \\[+ \\frac{p(p-2)}{2}\\sum_{k}\\lambda_{k}^{2}\\int_{0}^{t}\\|w^{\\varepsilon }(s)\\|_{1 ,2}^{p-4}(\\partial_{x}(\\Psi_{k}w^{\\varepsilon}(s)),w^{\\varepsilon}(s))_{1 ,2}^{2}\\,ds\\] \\[+ \\frac{p(p-2)}{2}\\sum_{k}\\gamma_{k}^{2}\\int_{0}^{t}\\|w^{\\varepsilon }(s)\\|_{1 ,2}^{p-4}(\\Psi_{k}f(w^{\\varepsilon}(s)),w^{\\varepsilon}(s))_{1 ,2}^{2}\\,ds \\tag{4.25}\\]\n\nwhere we used the definition of the norm \\(\\|B(u)\\|_{L_{2}}\\). Next, using Burkholder-Davis-Gundy inequality: E sup\n\nt' \\[\\int_{0}^{t}\\|w^{\\varepsilon}(s)\\|_{1 ,2}^{p-2}(\\partial_{x}(\\Psi_{k}w^{\\varepsilon}(s)),w^{\\varepsilon})_{1 ,2}d\\beta^{k}(s)\\] \\[\\leqslant 3\\mathbf{E}\\left(\\int_{0}^{t}\\|w^{\\varepsilon}(s)\\|_{1 ,2}^{2p-4}\\partial_{x}(\\Psi_{k}w^{\\varepsilon}(s),w^{\\varepsilon}(s))_{1 ,2}^{2}\\,ds\\right)^{\\frac{1}{2}}\\] \\[\\leqslant C\\mathbf{E}\\left(\\int_{0}^{t}\\|w^{\\varepsilon}(s)\\|_{1 ,2}^{2p}\\right)^{\\frac{1}{2}}\\leqslant C\\mathbf{E}\\sqrt{\\sup_{s\\in[0 ,t]}\\|w^{\\varepsilon}(s)\\|_{1 ,2}^{p}\\int_{0}^{t}\\|w^{\\varepsilon}(s)\\|_{1 ,2}^{p}\\,ds}\\] \\[\\leqslant\\nu\\mathbf{E}\\sup_{s\\in[0,t]}\\|w^{\\varepsilon}(s)\\|_{1 ,2}^{p}+\\frac{C}{\u03bd}\\mathbf{E}\\int_{0}^{t}\\|w^{\\varepsilon}(s)\\|_{1 ,2}^{p}\\,ds,\\] (4.26)\n\nwhere \\(\\nu&gt;0\\) can be chosen arbitrarily small, and \\(C\\) is independent of \\(\\nu\\). Note that the expected values in (4.26) are finite due to (4.8). In a similar way, we get E sup\n\nt' \\[\\int_{0}^{t}\\|w^{\\vare "
    ],
    "kosmos": [
      "for all \\(t\\in[0,\\delta)\\) almost surely. Furthermore, for \\(p\\geqslant 4\\), using Ito's formula for \\(\\|u\\|_{1 ,2}^{p}\\), we have\n\n\\[\\|w^{\\varepsilon}(t)\\|_{1 ,2}^{p}-\\|w_{0}\\|_{1 ,2}^{p}=p\\sum_{k}\\lambda_{k}\\int_{0}^{t}\\|w^{\\varepsilon}(s)\\|_{1 ,2}^{p-2}(\\partial_{x}(\\Psi_{k}w^{\\varepsilon}(s)),w^{\\varepsilon}(s))_{1 ,2}d\\beta^{k}(s)\\] \\[+ p\\sum_{k}\\gamma_{k}\\int_{0}^{t}\\|w^{\\varepsilon}(s)\\|_{1 ,2}^{p-2}(\\Psi_{k}f(w^{\\varepsilon}(s)),w^{\\varepsilon}(s))_{1 ,2}d\\beta_{1}^{k}(s)\\] \\[+ \\frac{p}{2}\\int_{0}^{t}\\|w^{\\varepsilon}(s)\\|_{1 ,2}^{p-2}(2\\langle A^{\\varepsilon}w^{\\varepsilon}(s),w^{\\varepsilon}(s) \\rangle)+\\|B(w^{\\varepsilon}(s))\\|_{L_{2}}^{2})\\,ds\\] \\[+ \\frac{p(p-2)}{2}\\sum_{k}\\lambda_{k}^{2}\\int_{0}^{t}\\|w^{\\varepsilon }(s)\\|_{1 ,2}^{p-4}(\\partial_{x}(\\Psi_{k}w^{\\varepsilon}(s)),w^{\\varepsilon}(s))_{1 ,2}^{2}\\,ds\\] \\[+ \\frac{p(p-2)}{2}\\sum_{k}\\gamma_{k}^{2}\\int_{0}^{t}\\|w^{\\varepsilon }(s)\\|_{1 ,2}^{p-4}(\\Psi_{k}f(w^{\\varepsilon}(s)),w^{\\varepsilon}(s))_{1 ,2}^{2}\\,ds \\tag{4.25}\\]\n\nwhere we used the definition of the norm \\(\\|B(u)\\|_{L_{2}}\\). Next, using Burkholder-Davis-Gundy inequality:\n\n\\[\\mathbf{E}\\sup_{t^{\\prime}\\in[0,t)}\\left|\\int_{0}^{t^{\\prime}}\\|w^ {\\varepsilon}(s)\\|_{1 ,2}^{p-2}(\\partial_{x}(\\Psi_{k}w^{\\varepsilon}(s)),w^{ \\varepsilon})_{1 ,2}d\\beta^{k}(s)\\right|\\] \\[\\leqslant 3\\mathbf{E}\\left(\\int_{0}^{t}\\|w^{\\varepsilon}(s)\\|_{1 ,2}^{2p-4}\\partial_{x}(\\Psi_{k}w^{\\varepsilon}(s),w^{\\varepsilon}(s))_{1 ,2}^{2}\\,ds\\right)^{\\frac{1}{2}}\\] \\[\\leqslant C\\mathbf{E}\\left(\\int_{0}^{t}\\|w^{\\varepsilon}(s)\\|_{1 ,2}^{2p}\\right)^{\\frac{1}{2}}\\leqslant C\\mathbf{E}\\sqrt{\\sup_{s\\in[0,t]}\\|w^{ \\varepsilon}(s)\\|_{1 ,2}^{p}}\\int_{0}^{t}\\|w^{\\varepsilon}(s)\\|_{1 ,2}^{p}\\,ds\\] \\[\\leqslant\\nu\\mathbf{E}\\sup_{s\\in[0,t]}\\|w^{\\varepsilon}(s)\\|_{1 ,2}^{p}+\\frac{C}{\u03bd}\\mathbf{E}\\int_{0}^{t}\\|w^{\\varepsilon}(s)\\|_{1 ,2}^{p}\\,ds, \\tag{4.26}\\]\n\nwhere \\(\\nu&gt;0\\) can be chosen arbitrarily small, and \\(C\\) is independent of \\(\\nu\\). Note that the expected values in (4.26) are finite due to (4.8).\n\nIn a similar way, we "
    ]
  },
  {
    "edit": [
      "To simplify the analysis, a narrow frequency band was selected between 24 and 61 Hz, with the fourth and fifth bending modes of the blades dominating the response in this band. Although other modes appear to have a small influence in this band, a 2DOF assumption was imposed. (This assumption results in smoothing of the FRF over the band, and might result in some loss of interpretability, but is acceptable for these preliminary analyses). The real part was modelled as a probabilistic FRF, using the FRF estimate from Eq. (13) as the mean of the likelihood function, as described in Case 1, presented in Section 6 of this paper. The real parts of the averaged FRFs for each blade, at the second accelerometer from the blade root (corresponding to the drive-point location), are shown in Figures 5a and 5b. Figure 5a shows the full measured bandwidth, and Figure 5b shows the FRF in the bandwidth of interest, between 24 and 61 Hz.\n\n Figure 4: Sensor locations on the helicopter blades.\n\nFigures 5a and 5b show increasing variability with respect to frequency, which is an expected result, given that higher-frequency modes are more sensitive to small physical changes than lower-frequency modes. For modes less than 80 Hz, the maximum frequency difference among the blades was approximately 2.5 Hz; for modes greater than 80 Hz, the maximum frequency difference was approximately 6.3 Hz. Note the grouping visible at several of the peaks, where Blades 1 and 2 appear closely aligned in frequency while Blades 3 and 4 appear closely aligned. These results are quite relevant for PBSHM. All of the helicopter blades are healthy, and represent a normal-condition state of the population. Consider a situation where only FRFs from one of the groupings are available for training a model (or FRFs from the other groups are missing data). The normal condition could be heavily biased towards the training set, and incoming FRFs could be flagged as damaged, even if they are healthy. Further details regarding the data collection and processing for these tests can be found in [27].\n\n "
    ],
    "kosmos": [
      "To simplify the analysis, a narrow frequency band was selected between 24 and 61 Hz, with the fourth and fifth bending modes of the blades dominating the response in this band. Although other modes appear to have a small influence in this band, a 2DOF assumption was imposed. (This assumption results in smoothing of the FRF over the band, and might result in some loss of interpretability, but is acceptable for these preliminary analyses). The real part was modelled as a probabilistic FRF, using the FRF estimate from Eq. (13) as the mean of the likelihood function, as described in Case 1, presented in Section 6 of this paper. The real parts of the averaged FRFs for each blade, at the second accelerometer from the blade root (corresponding to the drive-point location), are shown in Figures 5a and 5b. Figure 5a shows the full measured bandwidth, and Figure 5b shows the FRF in the bandwidth of interest, between 24 and 61 Hz.\n\nFigure 4: Sensor locations on the helicopter blades.\n\nFigures 5a and 5b show increasing variability with respect to frequency, which is an expected result, given that higher-frequency modes are more sensitive to small physical changes than lower-frequency modes. For modes less than 80 Hz, the maximum frequency difference among the blades was approximately 2.5 Hz; for modes greater than 80 Hz, the maximum frequency difference was approximately 6.3 Hz. Note the grouping visible at several of the peaks, where Blades 1 and 2 appear closely aligned in frequency while Blades 3 and 4 appear closely aligned. These results are quite relevant for PBSHM. All of the helicopter blades are healthy, and represent a normal-condition state of the population. Consider a situation where only FRFs from one of the groupings are available for training a model (or FRFs from the other groups are missing data). The normal condition could be heavily biased towards the training set, and incoming FRFs could be flagged as damaged, even if they are healthy. Further details regarding the data collection and processing for these tests can be found in [27].\n\n "
    ]
  },
  {
    "edit": [
      "\n\n# Adaptive multi-stage integration schemes for Hamiltonian Monte Carlo\n\nLorenzo Nagar\n\nlnagar@bcamath.org\n\nMario Fern\u00e1ndez-Pend\u00e1s\n\nJes\u00fas Mar\u00eda Sanz-Serna\n\nElena Akhmatskaya\n\n###### Abstract\n\nHamiltonian Monte Carlo (HMC) is a powerful tool for Bayesian statistical inference due to its potential to rapidly explore high dimensional state space, avoiding the random walk behavior typical of many Markov Chain Monte Carlo samplers. The proper choice of the integrator of the Hamiltonian dynamics is key to the efficiency of HMC. It is becoming increasingly clear that multi-stage splitting integrators are a good alternative to the Verlet method, traditionally used in HMC. Here we propose a principled way of finding optimal, problem-specific integration schemes (in terms of the best conservation of energy for harmonic forces/Gaussian targets) within the families of 2- and 3-stage splitting integrators. The method, which we call Adaptive Integration Approach for statistics, or s-AIA, uses a multivariate Gaussian model and simulation data obtained at the HMC burn-in stage to identify a system-specific dimensional stability interval and assigns the most appropriate 2-/3-stage integrator for any user-chosen simulation step size within that interval. s-AIA has been implemented in the in-house software package HaiCS without introducing computational overheads in the simulations. The efficiency of the s-AIA integrators and their impact on the HMC accuracy, sampling performance and convergence are discussed in compari"
    ],
    "kosmos": [
      "\n\n# Adaptive multi-stage integration schemes for Hamiltonian Monte Carlo\n\nLorenzo Nagar\n\nlnagar@bcamath.org\n\nMario Fern\u00e1ndez-Pendas\n\nJes\u00fas Mar\u00eda Sanz-Serna\n\nElena Akhmatskaya\n\n###### Abstract\n\nHamiltonian Monte Carlo (HMC) is a powerful tool for Bayesian statistical inference due to its potential to rapidly explore high dimensional state space, avoiding the random walk behavior typical of many Markov Chain Monte Carlo samplers. The proper choice of the integrator of the Hamiltonian dynamics is key to the efficiency of HMC. It is becoming increasingly clear that multi-stage splitting integrators are a good alternative to the Verlet method, traditionally used in HMC. Here we propose a principled way of finding optimal, problem-specific integration schemes (in terms of the best conservation of energy for harmonic forces/Gaussian targets) within the families of 2- and 3-stage splitting integrators. The method, which we call Adaptive Integration Approach for statistics, or s-AIA, uses a multivariate Gaussian model and simulation data obtained at the HMC burn-in stage to identify a system-specific dimensional stability interval and assigns the most appropriate 2-/3-stage integrator for any user-chosen simulation step size within that interval. s-AIA has been implemented in the in-house software package HaiCS without introducing computational overheads in the simulations. The efficiency of the s-AIA integrators and their impact on the HMC accuracy, sampling performance and convergence are discussed in compari"
    ]
  },
  {
    "edit": [
      "[5] and the references there for some of the algorithmic developments. A particular feature of numerical approximations of PDE solutions based on DNNs as approximation architectures that was observed in practice was the apparent insensitivity of the DNN approximation quality to the so-called \u201ccurse of dimensionality\u201d (CoD for short). This is particularly relevant for approximating maps\n\n\\[\\mathcal{G}:\\mathcal{X}\\to\\mathcal{Y} \\tag{1}\\]\n\nbetween (in general, infinite-dimensional) separable Hilbert spaces1\\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\). Operators \\(\\mathcal{G}\\) as in (1) emerge for example as parameter-to-solution mappings for parametric PDEs within the field of Uncertainty Quantification (see, e.g., [48] and the references there), or in so-called digital twins of complex, physical systems governed by partial differential equations (PDEs) (see [32] and the references there). Owing to the infinite dimension of \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\) in (1), efficient numerical approximations of maps \\(\\mathcal{G}\\) are to overcome the CoD.\n\nFootnote 1: More generally, separably-valued maps \\(\\mathcal{G}\\) into an otherwise nonseparable target space \\(\\mathcal{Y}\\) may be considered. In [35, Section 9, App. B] additional conditions on separable Banach spaces \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\) necessary to extend the present arguments to this more general setting are discussed.\n\nSeveral (intrinsically different) mechanisms for overcoming the CoD in DNN emulations have been identified and mathematically justified recently. This includes the seminal work of A. Barron [3], Monte-Carlo path simulation type arguments (e.g. [20, 29] and the references there), and the emulation of sparse (generalized) polynomial chaos expansions (e.g. [2, 17]) by DNNs (e.g. [56, 51, 57]).\n\nSpecifically, in [56, 51, 57], a parametric representation of inputs \\(x\\in\\mathcal{X}\\) of \\(\\mathcal{G}\\) was used to prove DNN emulation rates for approximating \\(\\mathcal{G}\\). The construction used DNNs whose depth scales polylogarithmic in the parameter dimension, and polynomially in the DNN expression accuracy (i.e., emulation fidelity). Key in the proofs of these results is the holomorphic dependence of \\(\\mathcal{G}(x)\\) on the input \\(x\\). The related DNN emulation results were obtained with sparsely connected, deep feedforward NNs with ReLU or smooth (e.g. sigmoidal or \\(\\tanh(\\cdot)\\)) activation. DNN emulation rate results that are free from the CoD for low regularity maps \\(\\mathcal{G}\\) between function spaces were obtained e.g. using the so-called Feynman-Kac representation of solutions of Kolmogorov PDEs within the field of Uncertainty Quantification (see, e.g., [48] and the references there). These results used ReLU DNNs of moderate depth [20, 29], but the error bounds hold in a mean-square sense or only with high probability.\n\nWhile quantified, parametric holomorphy of solution families of parametric PDEs has been verified in many settings (particularly in elliptic and parabolic PDEs, e.g. [27, 66, 31, 12, 23]), there are broad classes of applications where relevant maps are H\u00a8older or Lipschitz, but not holomorphic. One purpose of the present paper is to obtain mean-square DNN expression rate bounds for Operator Network (ONet) emulations with architecture (2) below, of Lipschitz (and, more generally, H\u00a8older smooth) maps \\(\\mathcal{G}\\) between separable Hilbert spaces.\n\n### Previous work for operator networks\n\nA rather recent line of research uses so-called Operator Networks to emulate the possibly nonlinear input-output map \\(\\mathcal{G}\\), such as for example the coefficient-to-solution map in linear, elliptic divergence form PDEs of second order. A variety of DNN architectures has been put forward recently with the aim of efficient operator emulation, with distinct architectures tailored to the emulation of particular operators. A number of acronyms labelling these DNN classes has been coined (\"deepONets\" [45], Fourier Neural Operators \"FNOs\" [35, 41], UNet architectures combined with FNOs \"U-FNOs\" [62], encoders based on transformers, etc.). We refer to [ "
    ],
    "kosmos": [
      "[5] and the references there for some of the algorithmic developments. A particular feature of numerical approximations of PDE solutions based on DNNs as approximation architectures that was observed in practice was the apparent insensitivity of the DNN approximation quality to the so-called \u201ccurse of dimensionality\u201d (CoD for short). This is particularly relevant for approximating maps\n\n\\[\\mathcal{G}:\\mathcal{X}\\to\\mathcal{Y} \\tag{1}\\]\n\nbetween (in general, infinite-dimensional) separable Hilbert spaces1\\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\). Operators \\(\\mathcal{G}\\) as in (1) emerge for example as parameter-to-solution mappings for parametric PDEs within the field of Uncertainty Quantification (see, e.g., [48] and the references there), or in so-called digital twins of complex, physical systems governed by partial differential equations (PDEs) (see [32] and the references there). Owing to the infinite dimension of \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\) in (1), efficient numerical approximations of maps \\(\\mathcal{G}\\) are to overcome the CoD.\n\nFootnote 1: More generally, separably-valued maps \\(\\mathcal{G}\\) into an otherwise nonseparable target space \\(\\mathcal{Y}\\) may be considered. In [35, Section 9, App. B] additional conditions on separable Banach spaces \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\) necessary to extend the present arguments to this more general setting are discussed.\n\nSeveral (intrinsically different) mechanisms for overcoming the CoD in DNN emulations have been identified and mathematically justified recently. This includes the seminal work of A. Barron [3], Monte-Carlo path simulation type arguments (e.g. [20, 29] and the references there), and the emulation of sparse (generalized) polynomial chaos expansions (e.g. [2, 17]) by DNNs (e.g. [56, 51, 57]).\n\nSpecifically, in [56, 51, 57], a parametric representation of inputs \\(x\\in\\mathcal{X}\\) of \\(\\mathcal{G}\\) was used to prove DNN emulation rates for approximating \\(\\mathcal{G}\\). The construction used DNNs whose depth scales polylogarithmic in the parameter dimension, and polynomially in the DNN expression accuracy (i.e., emulation fidelity). Key in the proofs of these results is the holomorphic dependence of \\(\\mathcal{G}(x)\\) on the input \\(x\\). The related DNN emulation results were obtained with sparsely connected, deep feedforward NNs with ReLU or smooth (e.g. sigmoidal or \\(\\tanh(\\cdot)\\)) activation. DNN emulation rate results that are free from the CoD for low regularity maps \\(\\mathcal{G}\\) between function spaces were obtained e.g. using the so-called Feynman-Kac representation of solutions of Kolmogorov PDEs in (jump-)diffusion models. These results used ReLU DNNs of moderate depth [20, 29], but the error bounds hold in a mean-square sense or only with high probability.\n\nWhile quantified, parametric holomorphy of solution families of parametric PDEs has been verified in many settings (particularly in elliptic and parabolic PDEs, e.g. [27, 66, 31, 12, 23]), there are broad classes of applications where relevant maps are H\u00a8older or Lipschitz, but not holomorphic. One purpose of the present paper is to obtain mean-square DNN expression rate bounds for Operator Network (ONet) emulations with architecture (2) below, of Lipschitz (and, more generally, H\u00a8older smooth) maps \\(\\mathcal{G}\\) between separable Hilbert spaces.\n\n### Previous work for operator networks\n\nA rather recent line of research uses so-called Operator Networks to emulate the possibly nonlinear input-output map \\(\\mathcal{G}\\), such as for example the coefficient-to-solution map in linear, elliptic divergence form PDEs of second order. A variety of DNN architectures has been put forward recently with the aim of efficient operator emulation, with distinct architectures tailored to the emulation of particular operators. A number of acronyms labelling these DNN classes has been coined (\u201cdeepONets\" [45], Fourier Neural Operators \u201cFNOs\" [35, 41], UNet architectures combined with FNOs \u201cU-FNOs\" [62], encoders based on transformers, etc.). We refer to [35, 38, 21, 49, 42, "
    ]
  },
  {
    "edit": [
      "Let \\(y\\in W_{4}\\). Then \\(y=[b_{k},\\,d_{0},\\,d_{1},\\,d_{2}]\\) for \\(k=0\\) or 1. The corresponding equation in ( 23) reads\n\n\\[t_{0}^{b_{k},d_{0},d_{1},d_{2}}a_{[b_{k},\\,d_{0}]}a_{[d_{1},\\,d_{2}]}+t_{1}^{b_ {k},d_{0},d_{1},d_{2}}a_{[b_{k},\\,d_{1}]}a_{[d_{0},\\,d_{2}]}+t_{2}^{b_{k},d_{0}, d_{1},d_{2}}a_{[b_{k},\\,d_{2}]}a_{[d_{0},\\,d_{1}]}=0\\]\n\n\\[t_{0}=\\begin{cases}-1&amp;\\text{if}\\quad d_{0}&lt;d_{1}&lt;b_{k}&lt;d_{2}\\\\ +1&amp;\\text{otherwise}\\end{cases}\\]\n\n\\[t_{1}=\\begin{cases}-1&amp;\\text{if}\\quad b_{k}&lt;d_{0}&lt;d_{1}&lt;d_{2}&amp;\\text{or}\\quad d_{ 0}&lt;d_{1}&lt;d_{2}&lt;b_{k}\\\\ +1&amp;\\text{otherwise}\\end{cases}\\]\n\n\\[t_{2}=\\begin{cases}-1&amp;\\text{if}\\quad d_{0}&lt;b_{k}&lt;d_{1}&lt;d_{2}\\\\ +1&amp;\\text{otherwise}\\end{cases}\\]\n\nEach of \\(a_{[d_{1},\\,d_{2}]},a_{[d_{0},\\,d_{2}]}\\&amp;a_{[d_{0},\\,d_{1}]}\\) are fixed by Equation ( 24). Substituting in, (multiplying out \\(a_{[b_{0},\\,b_{1}]}\\)), the equation now reads\n\n\\[t_{0}^{b_{k},d_{0},d_{1},d_{2}}a_{[b_{k},\\,d_{0}]}(s_{0}^{b_{0},b_{1},d_{1},d_{ 2}}a_{[b_{0},\\,d_{1}]}a_{[b_{1},\\,d_{2}]}+s_{1}^{b_{0},b_{1},d_{1},d_{2}}a_{[b_{ 0},\\,d_{2}]}a_{[b_{1},\\,d_{1}]})\\] \\[+t_{1}^{b_{k},d_{0},d_{1},d_{2}}a_{[b_{k},\\,d_{1}]}(s_{0}^{b_{0}, b_{1},d_{0},d_{2}}a_{[b_{0},\\,d_{0}]}a_{[b_{1},\\,d_{2}]}+s_{1}^{b_{0},b_{1},d_{ 0},d_{2}}a_{[b_{0},\\,d_{2}]}a_{[b_{1},\\,d_{0}]})\\] \\[+t_{2}^{b_{k},d_{0},d_{1},d_{2}}a_{[b_{k},\\,d_{0}]}(s_{0}^{b_{0}, b_{1},d_{0},d_{1}}a_{[b_{0},\\,d_{0}]}a_{[b_{1},\\,d_{1}]}+s_{1}^{b_{0},b_{1},d_{ 0},d_{1}}a_{[b_{0},\\,d_{1}]}a_{[b_{1},\\,d_{0}]}) =0 \\tag{25}\\]\n\nLabelling the terms of Equation ( 25 ) in order 1 to 6, we can pair them off as follows. If \\(k=0\\), pair \\((1,3),\\,(2,5)\\&amp;(4,6)\\). If \\(k=1\\), instead pair \\((1,6),\\,(2,4)\\&amp;(3,5)\\). Then it can be shown that each pair sums to 0. For example, for \\(k=0\\), we have:\n\n\\[(1)+(3) \\propto "
    ],
    "kosmos": [
      "Let \\(y\\in W_{4}\\). Then \\(y=[b_{k},\\,d_{0},\\,d_{1},\\,d_{2}]\\) for \\(k=0\\) or 1. The corresponding equation in ( 23) reads\n\n\\[t_{0}^{b_{k},d_{0},d_{1},d_{2}}a_{[b_{k},\\,d_{0}]}a_{[d_{1},\\,d_{2}]}+t_{1}^{b_ {k},d_{0},d_{1},d_{2}}a_{[b_{k},\\,d_{1}]}a_{[d_{0},\\,d_{2}]}+t_{2}^{b_{k},d_{0}, d_{1},d_{2}}a_{[b_{k},\\,d_{2}]}a_{[d_{0},\\,d_{1}]}=0\\]\n\n\\[t_{0}=\\begin{cases}-1&amp;\\text{if}\\quad d_{0}&lt;d_{1}&lt;b_{k}&lt;d_{2}\\\\ +1&amp;\\text{otherwise}\\end{cases}\\]\n\n\\[t_{1}=\\begin{cases}-1&amp;\\text{if}\\quad b_{k}&lt;d_{0}&lt;d_{1}&lt;d_{2}\\\\ +1&amp;\\text{otherwise}\\end{cases}\\]\n\n\\[t_{2}=\\begin{cases}-1&amp;\\text{if}\\quad d_{0}&lt;b_{k}&lt;d_{1}&lt;d_{2}\\\\ +1&amp;\\text{otherwise}\\end{cases}\\]\n\nEach of \\(a_{[d_{1},\\,d_{2}]},a_{[d_{0},\\,d_{2}]}\\&amp;a_{[d_{0},\\,d_{1}]}\\) are fixed by Equation ( 24). Substituting in, (multiplying out \\(a_{[b_{0},\\,b_{1}]}\\)), the equation now reads\n\n\\[t_{0}^{b_{k},d_{0},d_{1},d_{2}}a_{[b_{k},\\,d_{0}]}(s_{0}^{b_{0},b_{1},d_{1},d_{ 2}}a_{[b_{0},\\,d_{1}]}a_{[b_{1},\\,d_{2}]}+s_{1}^{b_{0},b_{1},d_{1},d_{2}}a_{[b_ {0},\\,d_{2}]}a_{[b_{1},\\,d_{1}]})\\] \\[+t_{1}^{b_{k},d_{0},d_{1},d_{2}}a_{[b_{k},\\,d_{1}]}(s_{0}^{b_{0}, b_{1},d_{0},d_{2}}a_{[b_{0},\\,d_{0}]}a_{[b_{1},\\,d_{2}]}+s_{1}^{b_{0},b_{1},d_{ 0},d_{2}}a_{[b_{0},\\,d_{2}]}a_{[b_{1},\\,d_{0}]})\\] \\[+t_{2}^{b_{k},d_{0},d_{1},d_{2}}a_{[b_{k},\\,d_{0}]}(s_{0}^{b_{0}, b_{1},d_{0},d_{1}}a_{[b_{0},\\,d_{0}]}a_{[b_{1},\\,d_{1}]}+s_{1}^{b_{0},b_{1},d_{ 0},d_{1}}a_{[b_{0},\\,d_{1}]}a_{[b_{1},\\,d_{0}]})=0 \\tag{25}\\]\n\nLabelling the terms of Equation ( 25) in order 1 to 6, we can pair them off as follows. If \\(k=0\\), pair \\((1,3),\\,(2,5)\\&amp;(4,6)\\). If \\(k=1\\), instead pair \\((1,6),\\,(2,4)\\&amp;(3,5)\\). Then it can be shown that each pair sums to 0. For example, for \\(k=0\\), we have:\n\n\\[(1)+(3) \\propto t_{0}^{b_{0},d_{0},d_{1},d_{2}}s_{0}^{b "
    ]
  },
  {
    "edit": [
      "one obtains from (3.75) (3.79) and due to (3.79)\n\n\\[y_{1}^{(2)}=-kz_{1}(v^{(2)})=-A(\\zeta_{0})\\frac{k^{1/2}}{2J_{0}}\\cdot\\Big{(}1 +O(k^{1/2})\\Big{)}+O(k^{-5/2}). \\tag{3.95}\\]\n\nThen, using the Taylor formula with respect to \\(\\mu\\), we shift initial conditions (3.92) to the point \\(y=y_{0}^{(2)}\\) as follows:\n\n\\[v_{0}(y_{0}^{(2)}) =v_{0}^{(2)},\\quad v_{1}(y_{0}^{(2)}) =-\\frac{dv_{0}}{dy}(y_{0}^{(2)})\\cdot y_{1}^{(2)},\\] \\[\\zeta_{1}(y_{0}^{(2)}) =\\zeta_{1}^{(2)},\\quad\\zeta_{2}(y_{0}^{(2)}) =\\zeta_{2}^{(2)}-\\frac{d\\zeta_{1}}{dy}(y_{0}^{(2)})\\cdot y_{1}^{( 2)},\\] (3.93) into (3.91) and expanding with respect to \\(\\mu\\), one obtains equations for the \\(i\\)-th approximations \\(\\zeta_{i}\\) and \\(J_{i}\\):\n\n\\[\\frac{d\\zeta_{i}}{dy} = \\mathcal{R}_{i}^{(\\zeta)}(y), \\tag{3.94}\\] \\[\\frac{dJ_{i}}{dy} = \\mathcal{R}_{i}^{(J)}(y). \\tag{3.95}\\]\n\nThe solution of this system is\n\n\\[\\zeta_{i}(y) = \\zeta_{i}(y_{0}^{(2)})+\\int_{y_{0}^{(2)}}^{y}\\mathcal{R}_{i}^{( \\zeta)}(s)\\,\\mathrm{d}s, \\tag{3.96}\\] \\[J_{i}(v) = J_{i}(y_{0}^{(2)})+\\int_{y_{0}^{(2)}}^{y}\\mathcal{R}_{i}^{(J)}(s )\\,\\mathrm{d}s. \\tag{3.97}\\]\n\nFor \\(i=1\\) one has\n\n\\[\\mathcal{R}_{1}^{(\\zeta)}(y) = k^{1/2}\\frac{f_{3}}{y^{2}-1-k(v_{0}(y)-1)},\\] (3.98) \\[\\mathcal{R}_{1}^{(J)}(y) = J_{0}k^{1/2}\\frac{f_{4}}{y^{2}-1-k(v_{0}(y)-1)}\\mathrm{e}^{-v_{0 }(y)},\\] (3.99) "
    ],
    "kosmos": [
      "one obtains from (3.75)\n\n\\[y_{0}^{(2)}=1-kz_{0}(v^{(2)})=1-1-1+ "
    ]
  },
  {
    "edit": [
      "Notice that at the optimal dual solution \\(\\lambda^{\\text{opt}}\\) and \\(\\{\\mu_{k}^{\\text{opt}}\\}\\), it must follow that \\(\\lambda^{\\text{opt}}&gt;0\\) and \\(\\boldsymbol{A}(\\lambda^{\\text{opt}},\\{\\mu_{k}^{\\text{opt}}\\})\\) is of full rank (i.e., \\(\\operatorname{rank}(\\boldsymbol{A}(\\lambda^{\\text{opt}},\\{\\mu_{k}^{\\text{opt}} \\}))=N_{t}\\)), since otherwise, the maximum transmit power constraint in (22b) cannot be satisfied. Then, Proposition 1 follows directly from Lemma 3. This completes the proof.\n\n## Appendix B Proof of Lemma 3\n\nFirst, we have \\(\\operatorname{tr}(\\boldsymbol{A}(\\lambda,\\{\\mu_{k}\\})\\boldsymbol{S}_{x})= \\operatorname{tr}(\\boldsymbol{\\Lambda}\\boldsymbol{U}^{H}\\boldsymbol{S}_{x} \\boldsymbol{U})\\). Let \\(\\tilde{\\boldsymbol{S}}_{x}=\\boldsymbol{U}^{H}\\boldsymbol{S}_{x}\\boldsymbol{U}\\). It is easy to figure out that \\(\\operatorname{tr}(\\boldsymbol{S}_{x}^{-1})=\\operatorname{tr}(\\tilde{\\boldsymbol {S}}_{x}^{-1})\\). Recall that \\(\\boldsymbol{S}_{x}\\) is positive semi-definite. We denote \\((\\tau_{1},\\ldots,\\tau_{N_{t}})\\) as the diagonal entries of \\(\\tilde{\\boldsymbol{S}}_{x}\\) to be determined. Note that \\(\\operatorname{tr}(\\boldsymbol{A}(\\lambda,\\{\\mu_{k}\\})\\boldsymbol{S}_{x})=\\sum_ {i=1}^{N}\\alpha_{i}\\tau_{i}\\). Here, we introduce the following lemma to find the minimum of \\(\\operatorname{tr}(\\tilde{\\boldsymbol{S}}_{x}^{-1})\\) w.r.t. \\((\\tau_{1},\\ldots,\\tau_{N_{t}})\\), for which the proof can be found in [43, Appendix A] and thus is omitted.\n\n**Lemma 5**.: [43] For a positive semi-definite matrix \\(\\boldsymbol{B}_{0}\\in\\mathbb{C}^{M\\times M}\\), with \\((m,n)\\)-th entry \\(a(m,n)\\), it holds that\n\n\\[\\operatorname{tr}(\\boldsymbol{B}_{0}^{-1})\\geq\\sum_{i=1}^{M}\\frac{1}{a(i,i)}, \\tag{60}\\]\n\nwhere the equality holds if and only if \\(\\boldsymbol{B}_{0}\\) is diagonal.\n\nHence, \\(\\tilde{\\boldsymbol{S}}_{x}\\) must be diagonal and we obtain\n\n\\[\\operatorname{tr}(\\boldsymbol{A}(\\lambda,\\{\\mu_{k}\\})\\boldsymbol{S}_{x})+ \\operatorname{tr}(\\boldsymbol{S}_{x}^{-1}) = \\operatorname{tr}(\\boldsymbol{\\Lambda}\\tilde{\\boldsymbol{S}}_{x}) +\\operatorname{tr}(\\tilde{\\boldsymbol{S}}_{x}^{-1})=\\sum_{i=1}^{N}\\alpha_{i} \\tau_{i}+\\sum_{i=1}^{N_{t}}\\frac{1}{\\tau_{i}}. \\tag{61}\\]\n\nIn this case, when \\(N<n_{t}\\), (62)=\"\" (63)=\"\" 0,=\"\" 1,=\"\" 2,=\"\" 2}}_{i}\\),=\"\" 3.=\"\" 4.=\"\" [ "
    ],
    "kosmos": [
      "Notice that at the optimal dual solution \\(\\lambda^{\\text{opt}}\\) and \\(\\{\\mu_{k}^{\\text{opt}}\\}\\), it must follow that \\(\\lambda^{\\text{opt}}&gt;0\\) and \\(\\boldsymbol{A}(\\lambda^{\\text{opt}},\\{\\mu_{k}^{\\text{opt}}\\})\\) is of full rank (i.e., \\(\\operatorname{rank}(\\boldsymbol{A}(\\lambda^{\\text{opt}},\\{\\mu_{k}^{\\text{opt}} \\}))=N_{t}\\)), since otherwise, the maximum transmit power constraint in (22b) cannot be satisfied. Then, Proposition 1 follows directly from Lemma 3. This completes the proof.\n\n## Appendix B Proof of Lemma 3\n\nFirst, we have \\(\\operatorname{tr}(\\boldsymbol{A}(\\lambda,\\{\\mu_{k}\\})\\boldsymbol{S}_{x})= \\operatorname{tr}(\\boldsymbol{\\Lambda}\\boldsymbol{U}^{H}\\boldsymbol{S}_{x} \\boldsymbol{U})\\). Let \\(\\tilde{\\boldsymbol{S}}_{x}=\\boldsymbol{U}^{H}\\boldsymbol{S}_{x}\\boldsymbol{U}\\). It is easy to figure out that \\(\\operatorname{tr}(\\boldsymbol{S}_{x}^{-1})=\\operatorname{tr}(\\tilde{\\boldsymbol {S}}_{x}^{-1})\\). Recall that \\(\\boldsymbol{S}_{x}\\) is positive semi-definite. We denote \\((\\tau_{1},\\ldots,\\tau_{N_{t}})\\) as the diagonal entries of \\(\\tilde{\\boldsymbol{S}}_{x}\\) to be determined. Note that \\(\\operatorname{tr}(\\boldsymbol{A}(\\lambda,\\{\\mu_{k}\\})\\boldsymbol{S}_{x})=\\sum_ {i=1}^{N}\\alpha_{i}\\tau_{i}\\). Here, we introduce the following lemma to find the minimum of \\(\\operatorname{tr}(\\tilde{\\boldsymbol{S}}_{x}^{-1})\\) w.r.t. \\((\\tau_{1},\\ldots,\\tau_{N_{t}})\\), for which the proof can be found in [43, Appendix A] and thus is omitted.\n\n**Lemma 5**.: [43] For a positive semi-definite matrix \\(\\boldsymbol{B}_{0}\\in\\mathbb{C}^{M\\times M}\\), with \\((m,n)\\)-th entry \\(a(m,n)\\), it holds that\n\n\\[\\operatorname{tr}(\\boldsymbol{B}_{0}^{-1})\\geq\\sum_{i=1}^{M}\\frac{1}{a(i,i)}, \\tag{60}\\]\n\nwhere the equality holds if and only if \\(\\boldsymbol{B}_{0}\\) is diagonal.\n\nHence, \\(\\tilde{\\boldsymbol{S}}_{x}\\) must be diagonal and we obtain\n\n\\[\\operatorname{tr}(\\boldsymbol{A}(\\lambda,\\{\\mu_{k}\\})\\boldsymbol{S}_{x})+ \\operatorname{tr}(\\boldsymbol{S}_{x}^{-1}) = \\operatorname{tr}(\\boldsymbol{\\Lambda}\\tilde{\\boldsymbol{S}}_{x}) +\\operatorname{tr}(\\tilde{\\boldsymbol{S}}_{x}^{-1})=\\sum_{i=1}^{N}\\alpha_{i} \\tau_{i}+\\sum_{i=1}^{N_{t}}\\frac{1}{\\tau_{i}}. \\tag{61}\\]\n\nIn this case, when \\(N<n_{t}\\), (62)=\"\" (63)=\"\" 0,=\"\" 1,=\"\" 2,=\"\" 2}}_{i}\\),=\"\" 3.=\"\" 4.=\"\" [ "
    ]
  },
  {
    "edit": [
      "Proof.: The proof is an adaptation of the proof of [ Kye08 , Lemma 4.6]. Since\n\n\\[\\omega_{H}^{g}(\\chi(u))=\\tau_{H}^{g}(u),\\,\\lambda_{H}^{g}\\circ\\chi\\]\n\nextends to an isometric embedding\n\n\\[L^{2}(Rep(G),\\tau_{H}^{g})\\to l_{g}^{2}(\\hat{H})\\cong L^{2}(O(G),\\omega_{H}^{g}).\\]\n\nThe image\n\n\\[\\lambda_{H}^{g}\\circ\\chi(\\mathcal{O}(G))\\]\n\nis a \\(\\ast\\)-algebra that maps\n\n\\[L^{2}(Rep(G),\\tau_{H}^{g})\\]\n\ninto itself and hence maps\n\n\\[L^{2}(Rep(G),\\tau_{H}^{g})^{\\perp}\\]\n\ninto itself. Therefore\n\n\\[\\lambda_{H}^{g}(\\chi(u))\\]\n\nhas the form\n\n\\[\\begin{bmatrix}\\lambda_{H}^{g}(\\chi(u))|_{L^{2}(Rep(G),\\tau_{H}^{g})}&amp;0\\\\ 0&amp;\\lambda_{H}^{g}(\\chi(u))|_{L^{2}(Rep(G),\\tau_{H}^{g})^{\\perp}}\\end{bmatrix}.\\]\n\nHence\n\n\\[||\\lambda_{H}^{g}(\\chi(u))|| =\\max\\{||\\lambda_{H}^{g}(\\chi(u))|_{L^{2}(Rep(G),\\tau_{H}^{g})} ||,||\\lambda_{H}^{g}(\\chi(u))|_{L^{2}(Rep(G),\\tau_{H}^{g})^{\\perp}}||\\}\\] \\[\\geq||\\lambda_{H}^{g}(\\chi(u))|_{L^{2}(Rep(G),\\tau_{H}^{g})}||\\] \\[=||\\pi_{\\tau_{H}^{g}}(u)||.\\]\n\nThis proves that the map\n\n\\[\\kappa:\\lambda_{H}^{g}\\circ\\chi(\\mathcal{O}(G))\\to\\pi_{\\tau_{H}^{g}}(C[Rep(G)] ),\\lambda_{H}^{g}(\\chi(u)))\\mapsto\\pi_{\\tau_{H}^{g}}(u)\\]\n\nis bounded and therefore extends to a contractive \\(\\ast\\)-homomorphism\n\n\\[\\kappa:\\lambda_{H}^{g}\\circ\\chi(\\mathcal{O}(G))\\to C_{\\tau_{H}^{g}}^{\\ast}(Rep (G)).\\]\n\nTo finish the proof, we claim\n\n\\[\\kappa\\]\n\nis injective. This easily follows from the observation\n\n\\[\\omega_{H}^{g}(\\chi(u)^{\\ast}\\chi(v))=\\omega_{H}^{g}(\\chi(u\\cdot v))=\\tau_{H}^ {g}(u\\cdot v).\\]\n\nFor\n\n\\[\\alpha\\in Irr(H)\\]\n\nlet\n\n\\[P_{\\alpha}\\in l^{\\infty}(\\hat{H})\\]\n\ndenote the orthogonal projection onto\n\n\\[H_{\\alpha}\\]\n\nwhich is nothing more than the identity operator in\n\n\\[M_{n_{\\alpha}}\\subset l^{\\infty}(\\hat{H})\\]\n\n. It is easily observed that the map\n\n\\[\\alpha\\mapsto P_{\\alpha}\\]\n\ninduces a unitary isomorphism\n\n\\[l^{2}(Irr(H))\\cong span\\{\\eta_{\\hat{H}}(P_{\\alpha}):\\alpha\\in Irr(H)\\}\\subset l _{g}^{2}(\\hat{H}).\\]\n\nFrom this and the duality it is also easy to check that\n\n\\[L^{2}(\\chi(\\mathcal{O}(G)),\\omega_{H}^{g})=span\\{\\eta_{H}(q_{H}(\\chi(u))):u\\in Rep (G)\\}\\subset l_{g}^{2}(Irr(H))\\] "
    ],
    "kosmos": [
      "Proof.: The proof is an adaptation of the proof of [ Kye08 , Lemma 4.6]. Since\n\n\\[\\omega^{g}_{H}(\\chi(u))=\\tau^{g}_{H}(u),\\,\\lambda^{g}_{H}\\circ\\chi\\]\n\nextends to an isometric embedding\n\n\\[L^{2}(Rep(G),\\tau^{g}_{H})\\to l^{2}_{g}(\\hat{H})\\cong L^{2}(O(G),\\omega^{g}_{H }).\\]\n\nThe image\n\n\\[\\lambda^{g}_{H}\\circ\\chi(\\mathcal{O}(G))\\]\n\nis a \\(\\ast\\)-algebra that maps\n\n\\[L^{2}(Rep(G),\\tau^{g}_{H})\\]\n\ninto itself and hence maps\n\n\\[L^{2}(Rep(G),\\tau^{g}_{H})^{\\perp}\\]\n\ninto itself. Therefore\n\n\\[\\lambda^{g}_{H}(\\chi(u))\\]\n\nhas the form\n\n\\[\\begin{bmatrix}\\lambda^{g}_{H}(\\chi(u))|_{L^{2}(Rep(G),\\tau^{g}_{H})}&amp;0\\\\ 0&amp;\\lambda^{g}_{H}(\\chi(u))|_{L^{2}(Rep(G),\\tau^{g}_{H})^{\\perp}}\\end{bmatrix}.\\]\n\nHence\n\n\\[||\\lambda^{g}_{H}(\\chi(u))|| =\\max\\{||\\lambda^{g}_{H}(\\chi(u))|_{L^{2}(Rep(G),\\tau^{g}_{H})} ||,||\\lambda^{g}_{H}(\\chi(u))|_{L^{2}(Rep(G),\\tau^{g}_{H})^{\\perp}}||\\}\\] \\[\\geq||\\lambda^{g}_{H}(\\chi(u))|_{L^{2}(Rep(G),\\tau^{g}_{H})}||\\] \\[=||\\pi_{\\tau^{g}_{H}}(u)||.\\]\n\nThis proves that the map\n\n\\[\\kappa:\\lambda^{g}_{H}\\circ\\chi(\\mathcal{O}(G))\\to\\pi_{\\tau^{g}_{H}}(C[Rep(G)] ),\\lambda^{g}_{H}(\\chi(u)))\\mapsto\\pi_{\\tau^{g}_{H}}(u)\\]\n\nis bounded and therefore extends to a contractive \\(\\ast\\)-homomorphism\n\n\\[\\kappa:\\lambda^{g}_{H}\\circ\\chi(\\mathcal{O}(G))\\to C_{\\tau^{g}_{H}}^{\\ast}(Rep (G)).\\]\n\nTo finish the proof, we claim\n\n\\[\\kappa\\]\n\nis injective. This easily follows from the observation\n\n\\[\\omega^{g}_{H}(\\chi(u)^{\\ast}\\chi(v))=\\omega^{g}_{H}(\\chi(u\\cdot v))=\\tau^{g}_ {H}(u\\cdot v).\\]\n\nFor\n\n\\[\\alpha\\in Irr(H)\\]\n\nlet\n\n\\[P_{\\alpha}\\in l^{\\infty}(\\hat{H})\\]\n\ndenote the orthogonal projection onto\n\n\\[H_{\\alpha}\\]\n\nwhich is nothing more than the identity operator in\n\n\\[M_{n_{\\alpha}}\\subset l^{\\infty}(\\hat{H})\\]\n\n. It is easily observed that the map\n\n\\[\\alpha\\mapsto P_{\\alpha}\\]\n\ninduces a unitary isomorphism\n\n\\[l^{2}(Irr(H))\\cong span\\{\\eta_{\\hat{H}}(P_{\\alpha}):\\alpha\\in Irr(H)\\}\\subset l ^{2}_{g}(\\hat{H}).\\]\n\nFrom this and the duality it is also easy to check that\n\n\\[L^{2}(\\chi(\\mathcal{O}(G)),\\omega^{g}_{H})=span\\{\\eta_{H}(q_{H}(\\chi(u))):u\\in Rep (G)\\}\\subset l^{2}_{g}(Irr(H))\\] "
    ]
  },
  {
    "edit": [
      "\n\n# Quantitative analysis of optimal Sobolev-Lorentz embeddings with \\(\\alpha\\)-homogeneous weights\n\nPetr Gurka\n\nDepartment of Mathematics, University of Zagreb, Bijenicka 30, 10000 Zagreb, Croatia petr.gurka@math.hr\n\nJan Lang\n\nDepartment of Mathematics, University of Zagreb, Bijenicka 30, 10000 Zagreb, Croatia jan.lang@unizg.hr\n\nZden\u011bk Mihula\n\nDepartment of Mathematics, University of Zagreb, Bijenicka 30, 10000 Zagreb, Croatia mihula@math.hr\n\nOctober 15, 2023\n\n###### Abstract.\n\nThis paper quantitatively investigates the structure of non-compactness of the optimal weighted Sobolev-Lorentz embedding with homogeneous weights in an open convex cone. We prove the optimal embedding in question and obtain the exact values of all injective strict \\(s\\)-numbers (in particular, the Bernstein numbers) of the embedding. Opposite to the earlier results in this direction, the non-compactness in this case does not occur uniformly over all sub-domains of the underlying domain. Despite that, we find an infinitely dimensional subspace restricted onto which the embedding is isomorphic, proving that the embedding is not strictly singular.\n\nKey words and phrases:Sobolev spaces, Sobolev-Lorentz embeddings, homogeneous weights, compactness, Bernstein numbers, measure of non-compactness 2020 Mathematics Subject Classification: 46E35, 47B06, 46B50 This research was supported by the grant GA23-04720S of the Czech Science Foundation\n\n"
    ],
    "kosmos": [
      "\n\n# Quantitative analysis of optimal Sobolev-Lorentz embeddings with \\(\\alpha\\)-homogeneous weights\n\nPetr Gurka\n\nDepartment of Mathematics, University of Zagreb, Bijenicka 30, 10000 Zagreb, Croatia petr.gurka@math.hr\n\nJan Lang\n\nDepartment of Mathematics, University of Zagreb, Bijenicka 30, 10000 Zagreb, Croatia jan.lang@unizg.hr\n\nZden\u011bk Mihula\n\nDepartment of Mathematics, University of Zagreb, Bijenicka 30, 10000 Zagreb, Croatia mihula@math.hr\n\nOctober 15, 2023\n\n###### Abstract.\n\nThis paper quantitatively investigates the structure of non-compactness of the optimal weighted Sobolev-Lorentz embedding with homogeneous weights in an open convex cone. We prove the optimal embedding in question and obtain the exact values of all injective strict \\(s\\)-numbers (in particular, the Bernstein numbers) of the embedding. Opposite to the earlier results in this direction, the non-compactness in this case does not occur uniformly over all sub-domains of the underlying domain. Despite that, we find an infinitely dimensional subspace restricted onto which the embedding is isomorphic, proving that the embedding is not strictly singular.\n\nKey words and phrases:Sobolev spaces, Sobolev-Lorentz embeddings, homogeneous weights, compactness, Bernstein numbers, measure of non-compactness 2020 Mathematics Subject Classification: 46E35, 47B06, 46B50 This research was supported by the grant GA23-04720S of the Czech Science Foundation\n\n"
    ]
  },
  {
    "edit": [
      "which we call the limit equations of ( 1.2 ). Here unknown functions are the tangenial velocity field v and the pressure q. Also, f is a given external force. We write P, \\(\\nabla_{\\Gamma}\\), div~ , D~ , and v for the orthogonal projection onto the tangent plane of , the tangential gradient, the surface divergence, the surface strain rate tensor, and the covariant derivative of along itself, respectively. Also, and are nonnegative constants which stands for the friction coefficients. For details of notations, see Section 2 . Note that, when g \u2261 1 on and , the limit equations ( 1.4 ) reduce to the surface Navier\u2013Stokes equations with Boussinesq\u2013Scriven surface stress tensor (see [ 4 , 52 , 2 ]) \\[\\left\\{-2vP\\mathrm{div}_{\\Gamma}[D_{\\Gamma}(v)]+\\nabla_{v}v+\\nabla_{\\Gamma}q=f \\quad\\text{on}\\quad\\Gamma,\\right.\\] (1.5 ) are equivalent to the Navier\u2013Stokes equations on an abstract Riemannian manifold (see [ 14 , 56 , 10 ]) \\[\\left\\{-\\nu\\{\\Delta_{B}v+\\mathrm{Ric}(v)\\}+\\nabla_{v}v+\\nabla_{\\Gamma}q=f\\quad \\text{on}\\quad\\Gamma,\\right.\\] (1.6) \\[\\mathrm{div}_{\\Gamma}v=0\\quad\\text{on}\\quad\\Gamma,\\]\n\nwhere \\(\\Delta_{B}\\) is the Bochner Laplacian on and is the Ricci curvature of (see e.g.  [ 34 , Lemma C.11] for the equivalence of the above equations). In the nonstationary setting, we rigorously derived the limit equations ( 1.4 ) from the bulk equations ( 1.2 ) by the thin-film limit in our previous work [ 34 ]. There we proved under suitable assumptions that, for an L2-strong solution to the nonstationary Navier\u2013Stokes equations in , its average\n\nconverges weakly to a tangential vector field v on in an appropriate function space as , and derived the nonstationary limit equations on by characterizing v as a unique L2-weak solution to the limit equations. We also obtained some estimates for the difference of and which show that v approximates in the L2 sense when is small. As in the nonstationary case [ 34 ], we can derive ( 1.4 ) from ( 1.2 ) by means of convergence of a solution and characterization of the limit, but the procedure is the same so we omit it here. In this paper, we focus on difference estimates for the solutions to ( 1.2 ) and to ( 1.4 ). Let us fix some notations and formally state our main results (see Section 2 for details). Let be the orthogonal projection from L2( )3 onto a function space given in ( 2.14 ), which is the standard solenoidal space on or its subspace. For a vector field u on , let be the tangential component of the average u on . Let\n\nand (see Theorems 2.5 and 2.6 for the precise statements).\n\n**Theorem 1.1**.: _Let be weak solutions to ( 1.2 ) and ( 1.4 ), respectively. Under suitable assumptions, suppose that there exist , and independent of such that_\n\n\\[\\|P_{\\varepsilon}f^{\\varepsilon}\\|_{L2(\\Omega_{\\varepsilon})}^{2} \\leqslant c_{1}\\varepsilon^{-1+\\alpha},\\quad\\|M_{\\varepsilon}P_{\\varepsilon}f^{ \\varepsilon}\\|_{H^{-1}(\\Gamma,TT)}^{2}\\leqslant c_{2} \\tag{1.7}\\]\n\n_for all sufficiently small. Then there exist , independent of such that_\n\n\\[\\|M_{\\varepsilon}u^{\\varepsilon}-v\\|_{H^{1}(\\Gamma)}\\leqslant c \\Big{(}\\delta(\\varepsilon)+\\|M_{\\varepsilon}P_{\\varepsilon}f^{\\varepsilon}-f\\|_{ H^{-1}(\\Gamma,TT)}\\Big{)} \\tag{1.8}\\]\n\n_for all sufficiently small provided that , where_\n\n_Moreover, we have the following difference estimate in :_\n\n(1.9) \\[\\varepsilon^{-1/2}\\|u^{\\varepsilon}-\\bar{v}\\|_{L2 "
    ],
    "kosmos": [
      "which we call the limit equations of (1.2). Here unknown functions are the tangential velocity field v and the pressure q. Also, f is a given external force. We write P, \\(\\nabla_{\\Gamma}\\), div~ , D~ , and v for the orthogonal projection onto the tangent plane of , the tangential gradient, the surface divergence, the surface strain rate tensor, and the covariant derivative of along itself, respectively. Also, and are nonnegative constants which stands for the friction coefficients. For details of notations, see Section 2. Note that, when g \u2261 1 on and , the limit equations (1.4) reduce to the surface Navier-Stokes equations with Boussinesq-Scriven surface stress tensor (see [ 4, 52, 2])\n\n\\[\\left\\{-2vP\\mathrm{div}_{\\Gamma}[D_{\\Gamma}(v)]+\\nabla_{v}v+\\nabla _{\\Gamma}q=f\\quad\\text{on}\\quad\\Gamma,\\right. \\tag{1.5}\\]\n\nMoreover, the equations (1.5) are equivalent to the Navier-Stokes equations on an abstract Riemannian manifold (see [ 14, 56, 10])\n\n\\[\\left\\{-\\nu\\{\\Delta_{B}v+\\mathrm{Ric}(v)\\}+\\nabla_{v}v+\\nabla_{ \\Gamma}q=f\\quad\\text{on}\\quad\\Gamma,\\right.\\] \\[\\mathrm{div}_{\\Gamma}v=0\\quad\\text{on}\\quad\\Gamma, \\tag{1.6}\\]\n\nwhere \\(\\Delta_{B}\\) is the Bochner Laplacian on and is the Ricci curvature of (see e.g. [ 34, Lemma C.11] for the equivalence of the above equations).\n\nIn the nonstationary setting, we rigorously derived the limit equations (1.4) from the bulk equations (1.2) by the thin-film limit in our previous work [ 34]. There we proved under suitable assumptions that, for an L^2-strong solution to the nonstationary Navier-Stokes equations in , its average\n\nconverges weakly to a tangential vector field v on in an appropriate function space as , and derived the nonstationary limit equations on by characterizing v as a unique L^2-weak solution to the limit equations. We also obtained some estimates for the difference of and which show that v approximates in the L^2 sense when is small.\n\nAs in the nonstationary case [ 34], we can derive (1.4) from (1.2) by means of convergence of a solution and characterization of the limit, but the procedure is the same so we omit it here. In this paper, we focus on difference estimates for the solutions to (1.2) and to (1.4). Let us fix some notations and formally state our main results (see Section 2 for details). Let be the orthogonal projection from L^2( )^3 onto a function space given in (2.14), which is the standard solenoidal space on or its subspace. For a vector field u on , let be the tangential component of the average u on . Let\n\nand (see Theorems 2.5 and 2.6 for the precise statements).\n\n**Theorem 1.1**.: _Let be weak solutions to (1.2) and (1.4), respectively. Under suitable assumptions, suppose that there exist , and independent of such that_\n\n\\[\\|P^{\\varepsilon}f^{\\varepsilon}\\|_{L^2( )^3} \\leqslant c_{1}\\varepsilon^{-1+\\alpha},\\quad\\|M_{\\varepsilon}P_{ \\varepsilon}f^{\\varepsilon}\\|_{H^{-1}(\\Gamma,TT)}^2 \\leqslant c_{2} \\tag{1.7}\\]\n\n_for all sufficiently small. Then there exist , independent of such that_\n\n\\[\\|M_{\\varepsilon}u^{\\varepsilon}-v\\|_{H^{1}(\\Gamma)} \\leqslant c\\Big{(}\\delta(\\varepsilon)+\\|M_{\\varepsilon}P_{\\varepsilon}f^{ \\varepsilon}-f\\|_{H^{-1}(\\Gamma,TT)}\\Big{)} \\tag{1.8}\\]\n\n_for all sufficiently small provided that , where_\n\n_Moreover, we have the following difference estimate in :_\n\n(1.9) \\[\\varepsilon^{-1/2}\\|u^{\\varepsilon}-\\bar{v}\\|_{L^2( )^3} \\le "
    ]
  },
  {
    "edit": [
      "for all \u03c4 \u2208 S 0 , where E [ \u00b7 ] denotes the expectation. In this context, the functions \u03c4 \u2208 S 0 are named weakly convex. Moreover, [TV97, Lemma 6] gives a weaker version of Theorem 3: Let \u03c4 \u2208 S 0 . For a, b \u2208 [0 , \u221e ) with a \u2265 b , it was shown that \u03c4 ( a + b ) + \u03c4 ( a \u2212 b ) \u2264 2 \u03c4 ( a ) + 2 \u03c4 ( b ). __\n\n#### 1.3.5 Statistics\n\nTheorem 1 can be applied to prove rates of convergence for certain kinds of means [Sch19]: We may want to calculate a mean value of some sample points in a metric spaces. One candidate for this is the Fr\u00b4echet mean [Fr\u00b4e48], also called barycenter. It is the (set of) minimizer(s) of the squared distance to the sample points. If Y is a random variable with values in a metric space ( Q , d ), the Fr\u00b4echet mean is arg min q \u2208Q E [ Y, q^2], where we assume E [ Y, q^2] < \u221e for all q \u2208 q. Similarly, one can define the Fr\u00b4echet median [FVJ09] as arg min q \u2208Q E [ Y, q], or a more general \u03c4-Fr\u00b4echet mean [Sch22] as arg min q \u2208Q E [ \u03c4 ( Y, q)] for functions \u03c4: [0 , \u221e ) \u2192 R. Given a sequence of independent random variables Y 1 , Y 2 , . . . with the same distribution as Y , a standard task in statistics is to bound the distance between the sample statistics and its corresponding population version. In our case, assume the \u03c4-Fr\u00b4echet mean is unique and define\n\n\\[m :=\\arg\\min_{q\\in Q}E [ \u03c4 ( Y, q)] , \\hat{m}_{n} :=\\arg\\min_{q\\in Q}\\frac{1}{n}\\sum_{i=1}^{n}\\tau (Y_{i},q)\\,.\\]\n\nWe want to bound m n , m depending on n. One can employ quadruple inequalities such as (3) to obtain a suitable upper bound [Sch19, Theorem 1]. This approach is particularly useful, if we do not want to make the assumption that the diameter of the metric space sup q,p \u2208Q q, p is finite. With Theorem 1, one can obtain such a bound for \u03c4-Fr\u00b4echet means with \u03c4 \u2208 S (under some conditions). We emphasize that this is only possible with (3) and not with (8). Noteworthy examples of \u03c4 \u2208 S in this context, aside from \u03c4 = \u03c4 \u03b1 , are the Huber loss \u03c4 H ,\u03b4 [Hub64] and the Pseudo-Huber loss \u03c4 pH ,\u03b4 [Cha+94] for \u03b4 \u2208 (0 , \u221e ),\n\n\\[\\tau_{\\text{H},\\delta}(x) :=\\begin{cases}\\frac{1}{2}x^{2}&amp;\\text{for }x\\leq\\delta\\,,\\\\ \\delta(x-\\frac{1}{2}\\delta)&amp;\\text{for }x&gt;\\delta\\,,\\end{cases}\\quad\\tau_{\\text{ pH},\\delta}(x) :=\\delta^{2}\\left(\\sqrt{1+\\frac{x^{2}}{\\delta^{2}}-1}\\right),\\]\n\nas well as x \ufffd\u2192 ln(cosh( x )) [Gre90]. These functions are of great interest in robust statistics and image processing as their respective minimizers combine properties of the classical mean ( \u03c4 2 -Fr\u00b4echet mean) and the median ( \u03c4 1 -Fr\u00b4echet mean). __\n\n### Outline\n\nIn the remaining sections, we first discuss the set T , i.e., the set of quadruple transformations, see section 2. We continue with a discussion of the set S , i.e., nondecresing, convex functions with concave derivative, in section 3. Thereafter, we prove our main result, i.e., S \u2286 T . The basic ideas of the proof and variations of the main result are presented in section 4. The technical details can be found in appendix B and C. The proof of Theorem 3 can be found in appendix A. In section 5 we discuss implications of the main results and open questions. __\n\n## 2 Quadruple Transformations\n\nWe explore some properties of quadruple functions \u03c4 \u2208 T and their quadruple constant L \u2217 \u03c4.\n\n### Properties\n\n**Lemma 4** (Constant functions).:\n1. For c \u2208 R , let \u03c4 c := ( x \ufffd\u2192 c ). Then \u03c4 c \u2208 T with L \u2217 \u03c4 = 0 "
    ],
    "kosmos": [
      "for all \u03c4 \u2208 S 0 , where E [ \u00b7 ] denotes the expectation. In this context, the functions \u03c4 \u2208 S 0 are named weakly convex. Moreover, [TV97, Lemma 6] gives a weaker version of Theorem 3: Let \u03c4 \u2208 S 0 . For a, b \u2208 [0 , \u221e ) with a \u2265 b , it was shown that \u03c4 ( a + b ) + \u03c4 ( a \u2212 b ) \u2264 2 \u03c4 ( a ) + 2 \u03c4 ( b ). __\n\n#### 1.3.5 Statistics\n\nTheorem 1 can be applied to prove rates of convergence for certain kinds of means [Sch19]: We may want to calculate a mean value of some sample points in a metric spaces. One candidate for this is the Fr\u00b4echet mean [Fr\u00b4e48], also called barycenter. It is the (set of) minimizer(s) of the squared distance to the sample points. If Y is a random variable with values in a metric space ( Q , d ), the Fr\u00b4echet mean is arg min q \u2208Q E [ Y, q^2], where we assume E [ Y, q^2] < \u221e for all q \u2208 q. Similarly, one can define the Fr\u00b4echet median [FVJ09] as arg min q \u2208Q E [ Y, q], or a more general \u03c4-Fr\u00b4echet mean [Sch22] as arg min q \u2208Q E [ \u03c4 ( Y, q)] for functions \u03c4: [0 , \u221e ) \u2192 R. Given a sequence of independent random variables Y 1 , Y 2 , . . . with the same distribution as Y , a standard task in statistics is to bound the distance between the sample statistics and its corresponding population version. In our case, assume the \u03c4-Fr\u00b4echet mean is unique and define\n\n\\[m :=\\arg\\min_{q\\in Q}E [ \u03c4 ( Y, q)] , \\hat{m}_{n} :=\\arg\\min_{q\\in Q}\\frac{1}{n}\\sum_{i=1}^{n}\\tau (Y_{i},q)\\,.\\]\n\nWe want to bound m n , m depending on n. One can employ quadruple inequalities such as (3) to obtain a suitable upper bound [Sch19, Theorem 1]. This approach is particularly useful, if we do not want to make the assumption that the diameter of the metric space sup q,p \u2208Q q, p is finite. With Theorem 1, one can obtain such a bound for \u03c4-Fr\u00b4echet means with \u03c4 \u2208 S (under some conditions). We emphasize that this is only possible with (3) and not with (8). Noteworthy examples of \u03c4 \u2208 S in this context, aside from \u03c4 = \u03c4 \u03b1 , are the Huber loss \u03c4 H ,\u03b4 [Hub64] and the Pseudo-Huber loss \u03c4 pH ,\u03b4 [Cha+94] for \u03b4 \u2208 (0 , \u221e ),\n\n\\[\\tau_{\\text{H},\\delta}(x) :=\\begin{cases}\\frac{1}{2}x^{2}&amp;\\text{for }x\\leq\\delta\\,,\\\\ \\delta(x-\\frac{1}{2}\\delta)&amp;\\text{for }x&gt;\\delta\\,,\\end{cases}\\quad\\tau_{\\text{ pH},\\delta}(x) :=\\delta^{2}\\left(\\sqrt{1+\\frac{x^{2}}{\\delta^{2}}-1}\\right),\\]\n\nas well as x \ufffd\u2192 ln(cosh( x )) [Gre90]. These functions are of great interest in robust statistics and image processing as their respective minimizers combine properties of the classical mean ( \u03c4 2 -Fr\u00b4echet mean) and the median ( \u03c4 1 -Fr\u00b4echet mean).\n\n### Outline\n\nIn the remaining sections, we first discuss the set T , i.e., the set of quadruple transformations, see section 2. We continue with a discussion of the set S , i.e., nondecesing, convex functions with concave derivative, in section 3. Thereafter, we prove our main result, i.e., S \u2286 T . The basic ideas of the proof and variations of the main result are presented in section 4. The technical details can be found in appendix B and C. The proof of Theorem 3 can be found in appendix A. In section 5 we discuss implications of the main results and open questions.\n\n## 2 Quadruple Transformations\n\nWe explore some properties of quadruple functions \u03c4 \u2208 T and their quadruple constant L \u2217 \u03c4.\n\n### Properties\n\n**Lemma 4** (Constant functions).:\n1. For c \u2208 R , let \u03c4 c := ( x \ufffd\u2192 c ). Then \u03c4 c \u2208 T with L \u2217 \u03c4 = 0.</q.\n\n "
    ]
  },
  {
    "edit": [
      "The above assumptions are fundamental to our approximate calculation, and a further mild technical assumption allows us to avoid tedious consideration of uninteresting cases: \u2022 The time scales of relaxation of flagellar force and orientation are comparable: \\[\\gamma_{F}/\\gamma_{\\Theta}\\sim\\text{ord}(1).\\] (25)\n\nIndeed, we find this condition satisfied by the parameters we inferred from experimental observations in Table 1. Under the assumptions described above, we obtain the following approximation for the translational diffusivity: lim \\[t\\to\\infty\\frac{\\langle\\mathbf{X}^{(c)}(t)\\odot\\mathbf{X}^{(c)}(t) \\rangle}{2t} =D_{\\text{t}}^{*}\\mathbf{l},\\] \\[D_{\\text{t}}^{*} \\equiv D_{\\text{t}} +\\frac{V^{\\ast 2}}{2}\\frac{D_{\\text{r}}^{*}}{D_{\\text{r}}^{\\ast 2} +\\Omega_{\\text{r}}^{\\ast 2}} +\\tilde{D}_{\\text{t}}\\] (26)\n\nwhere \\(V^{\\ast 2}\\) is the mean-square velocity coarse-grained over the flagellar time scale (19), and\n\n\\[\\tilde{D}_{\\text{t}} \\equiv\\frac{a\\Omega_{\\text{r}}^{\\ast}}{2\\gamma_{\\text{t}}^{2} \\gamma_{\\text{r}}(\\Omega_{\\text{r}}^{\\ast 2}+D_{\\text{r}}^{\\ast 2})}\\left[ \\frac{2\\sigma_{F}^{2}}{\\gamma_{F}}\\sum_{j,j^{\\prime}=1}^{N}F_{j}^{(0)}in\\Theta_ {j^{\\prime}}^{(o)}\\cos\\left(\\Theta_{j}^{(o)}-\\Theta_{j^{\\prime}}^{(o)}\\right) +\\frac{2\\pi}{N}\\left(j-j^{\\prime}+\\frac{S_{j}-S_{j^{\\prime}}}{l}\\right)\\right.\\] \\[\\left.+\\frac{\\sigma_{\\Theta}^{2}}{\\gamma_{\\Theta}}\\sum_{j,j^{\\prime }=1}^{N}F_{j}^{(0)}F_{j^{\\prime}}^{(0)}(F_{j^{\\prime}}^{(0)}\\cos(\\Theta_{j^{ \\prime}}^{(0)})-F_{j}^{(0)}\\cos(\\Theta_{j}^{(0)}))in\\left(\\Theta_{j}^{(o)}- \\Theta_{j^{\\prime}}^{(o)}\\right)+\\frac{2\\pi}{N}\\left(j-j^{\\prime}+\\frac{S_{j}- S_{j^{\\prime}}}{l}\\right)\\right)\\right]\\]\n\nHere \\(\\Omega_{\\text{r}}^{\\ast}\\) is the effective rotational drift coefficient and \\(D_{\\text{r}}^{\\ast}\\) is the effective rotational diffusion coefficient given in (8) and (10) respectively. As shown in Subsection 4.2, the error in our calculation is essentially fourth order in the small parameters \\(\\sigma_{\\Theta}\\) and \\(\\sigma_{F}\\). The first two terms of the expression in Eq. (26) are exactly the translational diffusion for a colony with thermal diffusivity \\(D_{\\text{t}}\\), moving at a constant speed \\(V^{\\ast}\\) along an orientation with constant rotational drift \\(\\Omega_{\\text{r}}^{\\ast}\\) and constant rotational diffusion \\(D_{\\text{r}}^{\\ast}\\). As we shall shortly explain, the complicated terms in \\(\\tilde{D}_{\\text{t}}\\) are included for completeness and consistency with the results presented for general geometries in Subsection 5.2, but should typically be considerably smaller than the simpler terms presented in Eq. (26) for disclike geometries. Due to the nonpolynomial dependence of the translational diffusion on the effective rotational statistics, a rigorous computation of the demographic mean is complicated. We therefore content ourselves with an estimate for the demographic mean by averaging separately the numerator and denominator, neglecting demographic correlations between the colony speed and rotational characteristics, and dropping the small contribution from \u02dcDt: \\[\\mathbb{E}[D_{\\text{t}}^{\\ast}]\\approx\\mathbb{E}[V^{\\ast 2}]\\frac{\\mathbb{E}[D _ "
    ],
    "kosmos": [
      "The above assumptions are fundamental to our approximate calculation, and a further mild technical assumption allows us to avoid tedious consideration of uninteresting cases:\n\n* The time scales of relaxation of flagellar force and orientation are comparable: \\[\\gamma_{F}/\\gamma_{\\Theta}\\sim\\text{ord}(1).\\] (25)\n\nIndeed, we find this condition satisfied by the parameters we inferred from experimental observations in Table 1.\n\nUnder the assumptions described above, we obtain the following approximation for the translational diffusivity:\n\n\\[\\lim_{t\\to\\infty}\\frac{\\langle\\mathbf{X}^{(c)}(t)\\odot\\mathbf{X}^{(c)}(t) \\rangle}{2t} =D_{\\text{t}}^{*}\\mathbf{l},\\] \\[D_{\\text{t}}^{*} \\equiv D_{\\text{t}} +\\frac{V^{\\ast 2}}{2}\\frac{D_{\\text{r}}^{*}}{D_{\\text{r}}^{\\ast 2} +\\Omega_{\\text{r}}^{\\ast 2}} +\\tilde{D}_{\\text{t}} \\tag{26}\\]\n\nwhere \\(V^{\\ast 2}\\) is the mean-square velocity coarse-grained over the flagellar time scale (19), and\n\n\\[\\tilde{D}_{\\text{t}} \\equiv\\frac{a\\Omega_{\\text{r}}^{\\ast}}{2\\gamma_{\\text{t}}^{2} \\gamma_{\\text{r}}(\\Omega_{\\text{r}}^{\\ast 2}+D_{\\text{r}}^{\\ast 2})}\\left[2 \\frac{\\sigma_{F}^{2}}{\\gamma_{F}}\\sum_{j,j^{\\prime}=1}^{N}F_{j}^{(0)}in\\Theta_{ j^{\\prime}}^{(o)}\\cos\\left(\\Theta_{j}^{(o)}-\\Theta_{j^{\\prime}}^{(o)}\\right)+ \\frac{2\\pi}{N}\\left(j-j^{\\prime}+\\frac{S_{j}-S_{j^{\\prime}}}{l}\\right)\\right.\\] \\[\\left.+\\frac{\\sigma_{\\Theta}^{2}}{\\gamma_{\\Theta}}\\sum_{j,j^{\\prime }=1}^{N}F_{j}^{(0)}F_{j^{\\prime}}^{(0)}(F_{j^{\\prime}}^{(0)}\\cos(\\Theta_{j^{ \\prime}}^{(0)})-F_{j}^{(0)}\\cos(\\Theta_{j}^{(0)}))in\\left(\\Theta_{j}^{(o)}- \\Theta_{j^{\\prime}}^{(o)}\\right)+\\frac{2\\pi}{N}\\left(j-j^{\\prime}+\\frac{S_{j}- S_{j^{\\prime}}}{l}\\right)\\right)\\right].\\]\n\nHere \\(\\Omega_{\\text{r}}^{\\ast}\\) is the effective rotational drift coefficient and \\(D_{\\text{r}}^{\\ast}\\) is the effective rotational diffusion coefficient given in (8) and (10) respectively. As shown in Subsection 4.2, the error in our calculation is essentially fourth order in the small parameters \\(\\sigma_{\\Theta}\\) and \\(\\sigma_{F}\\).\n\nThe first two terms of the expression in Eq. (26) are exactly the translational diffusion for a colony with thermal diffusivity \\(D_{\\text{t}}\\), moving at a constant speed \\(V^{\\ast}\\) along an orientation with constant rotational drift \\(\\Omega_{\\text{r}}^{\\ast}\\) and constant rotational diffusion \\(D_{\\text{r}}^{\\ast}\\). As we shall shortly explain, the complicated terms in \\(\\tilde{D}_{\\text{t}}\\) are included for completeness and consistency with the results presented for general geometries in Subsection 5.2, but should typically be considerably smaller than the simpler terms presented in Eq. (26) for disclike geometries.\n\nDue to the nonpolynomial dependence of the translational diffusion on the effective rotational statistics, a rigorous computation of the demographic mean is complicated. We therefore content ourselves with an estimate for the demographic mean by averaging separately the numerator and denominator, neglecting demographic correlations between the colony speed and rotational characteristics, and dropping the small contribution from \\(\\tilde{D}_{\\text{t}}\\):\n\n\\[\\mathbb{E}[D_{\\text{t}}^{\\ast}]\\approx\\mathbb{E}[V^{\\ast 2}]\\frac{\\ "
    ]
  },
  {
    "edit": [
      "\n\n# Quenched decay of correlations for random contracting Lorenz maps\n\nAndrew Larkin\n\nDepartment of Mathematics, University of Bristol, Bristol BS8 1TW, United Kingdom andrew.larkin@bristol.ac.uk\n\nMarks Ruziboev\n\nDepartment of Mathematics, University of Bristol, Bristol BS8 1TW, United Kingdom marks.ruziboev@bristol.ac.uk\n\n###### Abstract.\n\nIn this work we consider i.i.d. random perturbations of contracting Lorenz maps sufficiently close to a Rovella parameter. We prove that the quenched correlations of the random dynamical system decays exponentially.\n\n## 1. Introduction\n\nThe Lorenz system was introduced in [30] as a simplified model for atmospheric convection. Numerical simulations have shown that the Lorenz system admits a strange attractor, called the Lorenz attractor, which became one of the most iconic examples in the field.\n\nA rigorous mathematical approach was developed with the introduction of the so called geometric Lorenz flow by [1, 26], which mimicks simulation of the dynamics of the Lorenz flow, and which has a robust strange attractor under \\(C^{1}\\) perturbations. Later in [37, 38] it was shown that the actual Lorenz attractor is indeed a singular hyperbolic attractor, further showing that the geometric Lorenz attractor represents the Lorenz attractor well. Moreover, it admits the so called Sinai-Ruelle-Bowen (SRB) measure, which is ergodic [11]. Its statistical properties, such as mixing rates, limit theorems and their stability under various perturbations, were studied intensively (see for example, [31, 10, 9, 7, 14, 13, 24]).\n\nAnother class of systems with similar properties was introduced in [35] called the _contracting Lorenz flow_. A fundamental difference between these is that the attractor of the system introduced by Rovella is not robust under perturbations, but still abundant in a measure theoretic sense. The set of measures for which the system is chaotic is called Rovella parameters and satisfies strong chaotic properties [8]; moreover, restricted to this set the system is stochastically stable [32, 33]. In [34] the authors addressed thermodynamic formalism for it. Up to now, the contracting Lorenz flow and one dimensional maps with critical points remain a profound example of a truly nonuniformly hyperbolic systems, which is studied via construction of induced schemes. We refer to [3] for a comprehensive account of these constructions.\n\nRecently, there has been increased interest in studying statistical properties of random dynamical systems, especially quenched (path-wise) properties. When the system has good uniformly hyperbolic properties, spectral techniques are still applicable and imply strong statistical properties; we refer to [19, 20, 21, 23, 22] and references therein for results on quenched decay of correlations, limit theorems and stability results in this case.\n\nFor the non-uniformly expanding (or non-uniformly hyperbolic) case, spectral techniques are not applicable directly. In this regards, it is customary to employ inducing techniques, in particular randomised version of Young Tower [39] construction called random Young towers. This was first carried out in [15] for random"
    ],
    "kosmos": [
      "\n\n# Quenched decay of correlations for random contracting Lorenz maps\n\nAndrew Larkin\n\nDepartment of Mathematics, University of Bristol, Bristol BS8 1TW, United Kingdom andrew.larkin@bristol.ac.uk\n\nMarks Ruziboev\n\nDepartment of Mathematics, University of Bristol, Bristol BS8 1TW, United Kingdom marks.ruziboev@bristol.ac.uk\n\n###### Abstract.\n\nIn this work we consider i.i.d. random perturbations of contracting Lorenz maps sufficiently close to a Rovella parameter. We prove that the quenched correlations of the random dynamical system decays exponentially.\n\n## 1. Introduction\n\nThe Lorenz system was introduced in [30] as a simplified model for atmospheric convection. Numerical simulations have shown that the Lorenz system admits a strange attractor, called the Lorenz attractor, which became one of the most iconic examples in the field.\n\nA rigorous mathematical approach was developed with the introduction of the so called geometric Lorenz flow by [1, 26], which mimicks simulation of the dynamics of the Lorenz flow, and which has a robust strange attractor under \\(C^{1}\\) perturbations. Later in [37, 38] it was shown that the actual Lorenz attractor is indeed a singular hyperbolic attractor, further showing that the geometric Lorenz attractor represents the Lorenz attractor well. Moreover, it admits the so called Sinai-Ruelle-Bowen (SRB) measure, which is ergodic [11]. Its statistical properties, such as mixing rates, limit theorems and their stability under various perturbations, were studied intensively (see for example, [31, 10, 9, 7, 14, 13, 24]).\n\nAnother class of systems with similar properties was introduced in [35] called the _contracting Lorenz flow_. A fundamental difference between these is that the attractor of the system introduced by Rovella is not robust under perturbations, but still abundant in a measure theoretic sense. The set of measures for which the system is chaotic is called Rovella parameters and satisfies strong chaotic properties [8]; moreover, restricted to this set the system is stochastically stable [32, 33]. In [34] the authors addressed thermodynamic formalism for it. Up to now, the contracting Lorenz flow and one dimensional maps with critical points remain a profound example of a truly nonuniformly hyperbolic systems, which is studied via construction of induced schemes. We refer to [3] for a comprehensive account of these constructions.\n\nRecently, there has been increased interest in studying statistical properties of random dynamical systems, especially quenched (path-wise) properties. When the system has good uniformly hyperbolic properties, spectral techniques are still applicable and imply strong statistical properties; we refer to [19, 20, 21, 23, 22] and references therein for results on quenched decay of correlations, limit theorems and stability results in this case.\n\nFor the non-uniformly expanding (or non-uniformly hyperbolic) case, spectral techniques are not applicable directly. In this regards, it is customary to employ inducing techniques, in particular randomised version of Young Tower [39] construction called random Young towers. This was first carried out in [15] for random"
    ]
  },
  {
    "edit": [
      "though our model exhibits a larger variance in the estimated elasticity values compared to the BLP model.\n\nWe further estimate the average own-elasticity for high-priced, medium-priced, and low-priced cars and construct a confidence interval for each category using our inference procedure. We present our result in Table 14.\n\n## 6 Conclusion\n\nChoice models are fundamental in understanding consumer behavior and informing business decisions. Over the years, various methods, both parametric and non-parametric, have been developed to represent consumer behavior. While parametric methods, such as logit or probit-based models, are favored for their simplicity and interpretability, their restrictive assumptions can limit their ability to fully capture consumer preferences\u2019 intricacies. On the other hand, non-parametric methods offer a more flexible approach, but they often suffer from the \u201ccurse of dimensionality\u201d, where the complexity of estimating choice functions escalates exponentially with an increase in the number of products.\n\nIn this paper, we propose a fundamental characterization of choice models that combines the tractability of traditional choice models and the flexibility of non-parametric estimators. This characterization specifically tackles the challenge of high dimensionality in choice systems and facilitates flexible estimation of choice functions. Through extensive simulations, we validate the efficacy of our model, demonstrating its superior ability to capture a range of consumer behaviors that traditional choice models fail to capture. We also show how to\n\nFigure 4: Elasticity Estimation Comparison\n\nNote: Figure 4 illustrates the distributions of the estimated own- and cross-elasticities obtained from our model and the BLP model. The filled areas in the violin plots represent the complete range of the elasticities, while the text labels indicate the mean values.\n\n "
    ],
    "kosmos": [
      "though our model exhibits a larger variance in the estimated elasticity values compared to the BLP model.\n\nWe further estimate the average own-elasticity for high-priced, medium-priced, and low-priced cars and construct a confidence interval for each category using our inference procedure. We present our result in Table 14.\n\n## 6 Conclusion\n\nChoice models are fundamental in understanding consumer behavior and informing business decisions. Over the years, various methods, both parametric and non-parametric, have been developed to represent consumer behavior. While parametric methods, such as logit or probit-based models, are favored for their simplicity and interpretability, their restrictive assumptions can limit their ability to fully capture consumer preferences\u2019 intricacies. On the other hand, non-parametric methods offer a more flexible approach, but they often suffer from the \u201ccurse of dimensionality\u201d, where the complexity of estimating choice functions escalates exponentially with an increase in the number of products.\n\nIn this paper, we propose a fundamental characterization of choice models that combines the tractability of traditional choice models and the flexibility of non-parametric estimators. This characterization specifically tackles the challenge of high dimensionality in choice systems and facilitates flexible estimation of choice functions. Through extensive simulations, we validate the efficacy of our model, demonstrating its superior ability to capture a range of consumer behaviors that traditional choice models fail to capture. We also show how to\n\nFigure 4: Elasticity Estimation Comparison\n\nNote: Figure 4 illustrates the distributions of the estimated own- and cross-elasticities obtained from our model and the BLP model. The filled areas in the violin plots represent the complete range of the elasticities, while the text labels indicate the mean values.\n\n "
    ]
  },
  {
    "edit": [
      "Data Availability\n=================\n\nThe original data analysed in this work are part of the Guaranteed Time Observation (GTO) program 1282 (PI: Th. Henning) with number 66 and will become public on 2 August 2023 on the MAST database ( [https://mast.stsci.edu](https://mast.stsci.edu) ). The portion of the spectrum presented in Fig. 3 is available on Zenodo at [https://zenodo.org/record/7991022](https://zenodo.org/record/7991022) . The spectroscopic data for water can be downloaded from the HITRAN database ( [https://hitran.org](https://hitran.org) ). The Spitzer-IRS spectrum plotted in Fig. 1 is part of the Spitzer-IRS GTO program 40679 (PI: G. Rieke). The spectrum was extracted and calibrated using private codes , and is available on Zenodo at [https://zenodo.org/record/7991022](https://zenodo.org/record/7991022) . The optical constants of the dust species considered in the fitting procedure for the dust continuum can be downloaded from the HJPDOC database ( [https://www2.mpia-hd.mpg.de/HJPDOC](https://www2.mpia-hd.mpg.de/HJPDOC) ). **Code Availability**\n\nThe slab model used in this work is a private code developed by B.T. and collaborators. It can be obtained from B.T. upon request. The synthetic spectra presented in this work can be reproduced using the slabspec code, which can be found at [https://doi.org/10.5281/zenodo.4037306](https://doi.org/10.5281/zenodo.4037306) . The fitting procedure for the dust continuum uses the publicly available MultiNest Bayesian fitting algorithm ( [https://github.com/JohannesBuchner/MultiNest](https://github.com/JohannesBuchner/MultiNest) ) and the PyMultiNest package ( [https://github.com/JohannesBuchner/PyMultiNest](https://github.com/JohannesBuchner/PyMultiNest) ). Figures were made with Matplotlib version 3.5.1. under the Matplotlib license at [https://matplotlib.org/](https://matplotlib.org/) . **Acknowledgements**\n\nThe MINDS team would like to thank the entire MIRI European and US instrument team. Support from STScI is also appreciated. The following National and International Funding Agencies funded and supported the MIRI development: NASA; ESA; Belgian Science Policy Office (BELSPO); Centre Nationale d\u2019Etudes Spatiales (CNES); Danish National Space Centre; Deutsches Zentrum fur L\u00fcftund Raumfahrt (DLR); Enterprise Ireland; Ministerio De Economi\u00e1 y Competividad; Netherlands Research School for Astronomy (NOVA); Netherlands Organisation for Scientific Research (NWO); Science and Technology Facilities Council; Swiss Space Office; Swedish National Space Agency; and UK Space Agency. G.P. would like to thank B. Bitsch and E. Gaidos for fruitful discussions and P. Hausschildt for kindly providing the model atmosphere. V.C. and O.A. acknowledge funding from the Belgian F.R.S.FNRS. Th.H., R.F. and K.S. acknowledge support from the European Research Council under the Horizon 2020 Framework Program via the ERC Advanced Grant Origins 83 24 28. B.T. is a Laureate of the Paris Region fellowship program, which is supported by the Ile-de-France Region and has received funding under the Horizon 2020 innovation framework program and Marie Sklodowska-Curie grant agreement No. 945298. B.T. acknowledges support from the Programme National \u2018Physique et Chimie du Milieu Interstellaire\u2019 (PCMI) of CNRS/INSU with INC/INP cofunded by CNES. D.G. would like to thank the Research Foundation Flanders for co-financing the present research (grant number V435622N). D.G. and I.A. thank the European Space Agency (ESA) and the Belgian Federal Science Policy Office (BELSPO) for their support in the framework of the PRODEX Programme. I.K., A.M.A., and E.v.D. acknowledge support from grant TOP-1614.001.751 from the Dutch Research Council (NWO). I.K. and J.K. acknowledge funding from H2020-MSCA-ITN-2019, grant no. 860470 (CHAMELEON). E.F.v.D. acknowledges support from the ERC grant 101019751 MOLDISK and the Danish National Research Foundation through the Center of Excellence \u201cInterCat\u201d (DNRF150). T.P.R acknowledges support from ERC grant 743029 EASY. D.B. has been funded by Spanish MCIN/AEI/10.13039/501100011033 grants PID2019- 104058GB-C21.\n\n "
    ],
    "kosmos": [
      "Data Availability\n=================\n\nThe original data analysed in this work are part of the Guaranteed Time Observation (GTO) program 1282 (PI: Th. Henning) with number 66 and will become public on 2 August 2023 on the MAST database ([https://mast.stsci.edu](https://mast.stsci.edu)). The portion of the spectrum presented in Fig. 3 is available on Zenodo at [https://zenodo.org/record/7991022](https://zenodo.org/record/7991022). The spectroscopic data for water can be downloaded from the HITRAN database ([https://hitran.org](https://hitran.org)). The Spitzer-IRS spectrum plotted in Fig. 1 is part of the Spitzer-IRS GTO program 40679 (PI: G. Rieke). The spectrum was extracted and calibrated using private codes [56, 64] and is available on Zenodo at [https://zenodo.org/record/7991022](https://zenodo.org/record/7991022). The optical constants of the dust species considered in the fitting procedure for the dust continuum can be downloaded from the HJPDOC database ([https://www2.mpia-hd.mpg.de/HJPDOC](https://www2.mpia-hd.mpg.de/HJPDOC)).\n\nCode Availability\n=================\n\nThe slab model used in this work is a private code developed by B.T. and collaborators. It can be obtained from B.T. upon request. The synthetic spectra presented in this work can be reproduced using the slabspec code, which can be found at [https://doi.org/10.5281/zenodo.4037306](https://doi.org/10.5281/zenodo.4037306). The fitting procedure for the dust continuum uses the publicly available MultiNest Bayesian fitting algorithm ([https://github.com/JohannesBuchner/MultiNest](https://github.com/JohannesBuchner/MultiNest)) and the PyMultiNest package ([https://github.com/JohannesBuchner/PyMultiNest](https://github.com/JohannesBuchner/PyMultiNest)). Figures were made with Matplotlib version 3.5.1. under the Matplotlib license at [https://matplotlib.org/](https://matplotlib.org/).\n\nAcknowledgements\n================\n\nThe MINDS team would like to thank the entire MIRI European and US instrument team. Support from STScI is also appreciated. The following National and International Funding Agencies funded and supported the MIRI development: NASA; ESA; Belgian Science Policy Office (BELSPO); Centre Nationale d\u2019Etudes Spatiales (CNES); Danish National Space Centre; Deutsches Zentrum fur Luft- und Raumfahrt (DLR); Enterprise Ireland; Ministerio De Econom\u00eda y Competividad; Netherlands Research School for Astronomy (NOVA); Netherlands Organisation for Scientific Research (NWO); Science and Technology Facilities Council; Swiss Space Office; Swedish National Space Agency; and UK Space Agency. G.P. would like to thank B. Bitsch and E. Gaidos for fruitful discussions and P. Hausschildt for kindly providing the model atmosphere. V.C. and O.A. acknowledge funding from the Belgian F.R.S.-FNRS. Th.H., R.F. and K.S. acknowledge support from the European Research Council under the Horizon 2020 Framework Program via the ERC Advanced Grant Origins 83 24 28. B.T. is a Laureate of the Paris Region fellowship program, which is supported by the Ile-de-France Region and has received funding under the Horizon 2020 innovation framework program and Marie Sklodowska-Curie grant agreement No. 945298. B.T. acknowledges support from the Programme National \u2018Physique et Chimie du Milieu Interstellaire\u2019 (PCMI) of CNRS/INSU with INC/INP cofunded by CNES. D.G. would like to thank the Research Foundation Flanders for co-financing the present research (grant number V435622N). D.G. and I.A. thank the European Space Agency (ESA) and the Belgian Federal Science Policy Office (BELSPO) for their support in the framework of the PRODEX Programme. I.K., A.M.A., and E.v.D. acknowledge support from grant TOP-1614.001.751 from the Dutch Research Council (NWO). I.K. and J.K. acknowledge funding from H2020-MSCA-ITN-2019, grant no. 860470 (CHAMELEON). E.F.v.D. acknowledges support from the ERC grant 101019751 MOLDISK and the Danish National Research Foundation through the Center of Excellence \u201cInterCat\u201d (DNRF150). T.P.R acknowledges support from ERC grant 743029 EASY. D.B. has been funded by Spanish MCIN/AEI/10.13039/501100011033 grants PID2019 "
    ]
  },
  {
    "edit": [
      "coupled WGM resonators to achieve multi-band UR of photons through modulation of the intermode backscatterings of resonators[ 41 ]. In the reciprocal system that the equivalent transmission in both directions was exhibited. Obviously, these systems previously mentioned only investigated the transmission or reflection characteristics, without considering achieving complete nonreciprocity in both channels, whereas directional transport in both channels is vital to enhance the controllability of photons. To this end, we propose a non-reciprocal system consisting of two WGM resonators that are individually embedded with a Zeeman split quantum dot (QD)[ 42 \u2013 44 ] and indirectly coupled through an optical fiber. By optimizing some sys- tem parameters, we demonstrate the simultaneous realization of UR and unidirectional transmissionlessness (UT). Moreover, the conversion between UR and UT can be achieved by adjusting the coupling strength between WGM resonators and optical fiber. Additionally, a one-to-one correspondence is established between the resonant frequencies of QDs energy levels and the positions of UR and UT peaks. Model and calculations The schematic of system is shown in Fig. 1 (a) and energy levels of QD is shown in Fig. 1 (b). Assuming that the WGM resonators and QDs have the same loss rates \\(\\gamma\\), then the Hamiltonian of the system can be written as (assuming \u210f = 1)\n\n\\[H=\\int dx{[-iv_{g}C^{\\dagger}_{R}(x)\\frac{\\partial}{\\partial x}C_{R}(x)+iv_{g} C^{\\dagger}_{L}(x)\\frac{\\partial}{\\partial x}C_{L}(x)]}\\]\n\n\\[+\\sum_{j=1,2}G_{j}\\delta[x-(j-1)d](LINK_SOLT)\\]\n\n\\[+\\sum_{j=1,2}{\\{[g_{j}(C_{a_{j}}\\sigma^{\\dagger}_{Rj}+C_{b_{j}}\\sigma^{\\dagger} _{Lj})+h_{j}C^{\\dagger}_{a_{j}}C_{b_{j}}+\\text{H.c.}]}\\]\n\n\\[+(\u03c9_{0}-\\omega_{j}-i\\gamma)\\sigma^{\\dagger}_{Rj}\\sigma_{Rj}+(\\omega_{0}+\\omega _{j}-i\\gamma)\\sigma^{\\dagger}_{Lj}\\sigma_{Lj}\\]\n\n\\[+(\u03c9_{a_{j}}-i\\gamma)C^{\\dagger}_{a_{j}}C_{a_{j}}+(\\omega_{b_{j}}-i\\gamma)C^{ \\dagger}_{b_{j}}C_{b_{j}}\\}, \\tag{1}\\]\n\nwhere \\(C^{\\dagger}_{R}(x)\\) (\\(C_{R}(x)\\)) and \\(C^{\\dagger}_{L}(x)\\) (\\(C_{L}(x)\\)) are creation (annihilation) operators at \\(x\\) for forward and backward propagating photon along fiber, respectively. \\(C^{\\dagger}_{b_{j}}\\) (\\(C_{b_{j}}\\)) and \\(C^{\\dagger}_{a_{j}}\\) (\\(C_{a_{j}}\\)) are creation (annihilation) operators of CW mode \\(b_{j}\\) and CCW mode \\(a_{j}\\) with resonance frequencies \\(\\omega_{b_{j}}\\) and \\(\\omega_{a_{j}}\\), respectively. \\(\\sigma^{\\dagger}_{Lj}\\) (\\(\\sigma_{Lj}\\)) and \\(\\sigma^{\\dagger}_{Rj}\\) (\\(\\sigma_{Rj}\\)) are transition operators which mean the transitions from states |g j \u27e9 (|e Lj \u27e9) to |e Lj \u27e9 (|g j \u27e9) and |g j \u27e9 (|e Rj \u27e9) to |e Rj \u27e9 (|g j \u27e9), respectively. \\(v_{g}\\) is group velocity of photon. \\(G_{j}\\) (\\(g_{j}\\)) is coupling strength between the \\(j\\)th WGM resonator and fiber ( \\(j\\)th QD), and \\(h_{j}\\) is transition rate between \\(b_{j}\\) and \\(a_{j}\\). We set up \\(\\omega_{a1}=\\omega_{a2}=\\omega_{a}\\), \\(\\omega_{b1}=\\omega_{b2}=\\omega_{b}\\), \\(G_{1}=G_{2}=G\\), \\(g_{1}=g_{2}=g\\) and \\(h_{1}=h_{2}=h\\) for the sake of simplicity in the following discussions. Assuming that photon with energy \\(E_{k}=\\omega=v_{g}\n\n "
    ],
    "kosmos": [
      "coupled WGM resonators to achieve multi-band UR of photons through modulation of the intermode backscattering of resonators[41]. In the reciprocal system that the equivalent transmission in both directions was exhibited. Obviously, these systems previously mentioned only investigated the transmission or reflection characteristics, without considering achieving complete nonreciprocity in both channels, whereas directional transport in both channels is vital to enhance the controllability of photons.\n\nTo this end, we propose a non-reciprocal system consisting of two WGM resonators that are individually embedded with a Zeeman split quantum dot (QD)[42, 43, 44] and indirectly coupled through an optical fiber. By optimizing some system parameters, we demonstrate the simultaneous realization of UR and unidirectional transmissionlessness (UT). Moreover, the conversion between UR and UT can be achieved by adjusting the coupling strength between WGM resonators and optical fiber. Additionally, a one-to-one correspondence is established between the resonant frequencies of QDs energy levels and the positions of UR and UT peaks.\n\n## 1 Model and calculations\n\nThe schematic of system is shown in Fig.1 (a) and energy levels of QD is shown in Fig.1 (b). Assuming that the WGM resonators and QDs have the same loss rates \\(\\gamma\\), then the Hamiltonian of the system can be written as (assuming \u210f = 1)\n\n\\[H=\\int dx{[-iv_{g}C^{\\dagger}_{R}(x)\\frac{\\partial}{\\partial x}C_{R}(x)+iv_{g}C ^{\\dagger}_{L}(x)\\frac{\\partial}{\\partial x}C_{L}(x)]}\\]\n\n\\[+\\sum_{j=1,2}G_{j}\\delta[x-(j-1)d](LINK_SOLT)\\]\n\n\\[+\\sum_{j=1,2}\\{[g_{j}(C_{a_{j}}\\sigma^{\\dagger}_{Rj}+C_{b_{j}}\\sigma^{\\dagger}_{ Lj})+h_{j}C^{\\dagger}_{a_{j}}C_{b_{j}}+\\text{H.c.}]\\]\n\n\\[+(\\omega_{0}-\\omega_{j}-i\\gamma)\\sigma^{\\dagger}_{Rj}\\sigma_{Rj}+(\\omega_{0}+ \\omega_{j}-i\\gamma)\\sigma^{\\dagger}_{Lj}\\sigma_{Lj}\\]\n\n\\[+(\\omega_{a_{j}}-i\\gamma)C^{\\dagger}_{a_{j}}C_{a_{j}}+(\\omega_{b_{j}}-i\\gamma)C ^{\\dagger}_{b_{j}}C_{b_{j}}\\}, \\tag{1}\\]\n\nwhere \\(C^{\\dagger}_{R}(x)\\) (\\(C_{R}(x)\\)) and \\(C^{\\dagger}_{L}(x)\\) (\\(C_{L}(x)\\)) are creation (annihilation) operators at \\(x\\) for forward and backward propagating photon along fiber, respectively. \\(C^{\\dagger}_{b_{j}}\\) (\\(C_{b_{j}}\\)) and \\(C^{\\dagger}_{a_{j}}\\) (\\(C_{a_{j}}\\)) are creation (annihilation) operators of CW mode \\(b_{j}\\) and CCW mode \\(a_{j}\\) with resonance frequencies \\(\\omega_{b_{j}}\\) and \\(\\omega_{a_{j}}\\), respectively. \\(\\sigma^{\\dagger}_{Lj}\\) (\\(\\sigma_{Lj}\\)) and \\(\\sigma^{\\dagger}_{Rj}\\) (\\(\\sigma_{Rj}\\)) are transition operators which mean the transitions from states |g j \u27e9 (|e Lj \u27e9) to |e Lj \u27e9 (|g j \u27e9) and |g j \u27e9 (|e Rj \u27e9) to |e Rj \u27e9 (|g j \u27e9), respectively. \\(v_{g}\\) is group velocity of photon. \\(G_{j}\\) (\\(g_{j}\\)) is coupling strength between the \\(j\\)th WGM resonator and fiber (\\(j\\)th QD), and \\(h_{j}\\) is transition rate between \\(b_{j}\\) and \\(a_{j}\\). We set up \\(\\omega_{a1}=\\omega_{a2}=\\omega_{a}\\), \\(\\omega_{b1}=\\omega_{b2}=\\omega_{b}\\), \\(G_{1}=G_{2}=G\\), \\(g_{1}=g_{2}=g\\) and \\(h_{1}=h_{2}=h\\) for the sake of simplicity in the following discussions.\n\nAssuming that photon with energy \\(E_{k}=\\omega=v_{g}k "
    ]
  },
  {
    "edit": [
      "with a step size of 0.01. The final weights are used in our OWAF technique. Table 3 illustrates the effects of various fusion strategies. For fare comparison, we use both MRI and DTI data in all cases. The experimental results in the table 3 clearly reveal that the proposed OWAF outperforms other fusion strategies. Table 4: Comparisons of the proposed method with State-of-the-art Approaches\n\n<table>\n<colgroup>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n</colgroup>\n<thead>\n<tr>\n<th>\nApproach\n</th>\n<th>\nML/DL\n</th>\n<th>\nMODALITY\n</th>\n<th>\nPD vs HC\n</th>\n<th>\nPD vs. SWEDD\n</th>\n<th>\nHC vs. SWEDD\n</th>\n<th>\nPD vs. HC vs SWEDD\n</th>\n<th>\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td></td>\n<td></td>\n<td>\nAc\n</td>\n<td>\nPr\n</td>\n<td>\nRe\n</td>\n<td>\nAc\n</td>\n<td>\nAc\n</td>\n<td>\nAc\n</td>\n</tr>\n<tr>\n<td>\nAdeli 2016 [1]\n</td>\n<td>\nML\n</td>\n<td>\nMRI\n</td>\n<td>\n81.9\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n</tr>\n<tr>\n<td>\nCigdem 2018 [6]\n</td>\n<td>\nML\n</td>\n<td>\nMRI\n</td>\n<td>\n93.7\n</td>\n<td>\n-\n</td>\n<td>\n95\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n</tr>\n<tr>\n<td>\nPrashanth 2018 [20]\n</td>\n<td>\nML\n</td>\n<td>\nSPECT\n</td>\n<td>\n95\n</td>\n<td>\n-\n</td>\n<td>\n96.7\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n</tr>\n<tr>\n<td>\nSingh 2018 [2]\n</td>\n<td>\nML\n</td>\n<td>\nMRI\n</td>\n<td>\n95.37\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n<td>\n96.04\n</td>\n<td>\n93.03\n</td>\n<td>\n-\n</td>\n</tr>\n<tr>\n<td>\nGabriel 2021 [21]\n</td>\n<td>\nML\n</td>\n<td>\nMRI\n</td>\n<td>\n99.01(M)\n</td>\n<td>\n100(M)\n</td>\n<td>\n99.3(M)\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n<td></td>\n<td>\n87.10(F)\n</td>\n<td>\n97.2(F)\n</td>\n<td>\n100(F)\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n<td></td>\n<td>\n93.05 (A)\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n</tr>\n<tr>\n<td>\nLi 2019 [10]\n</td>\n<td>\nDL (AE)\n</td>\n<td>\nMRI + DTI\n</td>\n<td>\n85.24\n</td>\n<td>\n95.8\n</td>\n<td>\n68.1\n</td>\n<td>\n-\n</td>\n<td>\n89.67\n</td>\n<td>\n-\n</td>\n</tr>\n<tr>\n<td>\nTremblay 2020 [22]\n</td>\n<td>\nDL\n</td>\n<td>\nMRI\n</td>\n<td>\n88.3\n</td>\n<td>\n88.2\n</td>\n<td>\n88.4\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n</tr>\n<tr>\n<td>\nChakraborty 2020 [7]\n</td>\n<td>\nDL\n</td>\n<td>\nMRI\n</td>\n<td>\n95.3\n</td>\n<td>\n92.7\n</td>\n<td>\n91.4\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n</tr>\n<tr>\n<td>\nSivaranjini 2020 [23]\n</ "
    ],
    "kosmos": [
      "with a step size of 0.01. The final weights are used in our OWAF technique. Table 3 illustrates the effects of various fusion strategies. For fare comparison, we use both MRI and DTI data in all cases. The experimental results in the table 3 clearly reveal that the proposed OWAF outperforms other fusion strategies.\n\nTable 4: Comparisons of the proposed method with State-of-the-art Approaches\n\n<table>\n<colgroup>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n</colgroup>\n<thead>\n<tr>\n<th>\nApproach\n</th>\n<th>\nML/DL\n</th>\n<th>\nMODALITY\n</th>\n<th>\nPD vs HC\n</th>\n<th>\nPD vs. SWEDD\n</th>\n<th>\nHC vs. SWEDD\n</th>\n<th>\nPD vs. HC vs SWEDD\n</th>\n<th>\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td></td>\n<td></td>\n<td>\nAc\n</td>\n<td>\nPr\n</td>\n<td>\nRe\n</td>\n<td>\nAc\n</td>\n<td>\nAc\n</td>\n<td>\nAc\n</td>\n</tr>\n<tr>\n<td>\nAdeli 2016 [1]\n</td>\n<td>\nML\n</td>\n<td>\nMRI\n</td>\n<td>\n81.9\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n</tr>\n<tr>\n<td>\nCigdem 2018 [6]\n</td>\n<td>\nML\n</td>\n<td>\nMRI\n</td>\n<td>\n93.7\n</td>\n<td>\n-\n</td>\n<td>\n95\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n</tr>\n<tr>\n<td>\nPrashanth 2018 [20]\n</td>\n<td>\nML\n</td>\n<td>\nSPECT\n</td>\n<td>\n95\n</td>\n<td>\n-\n</td>\n<td>\n96.7\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n</tr>\n<tr>\n<td>\nSingh 2018 [2]\n</td>\n<td>\nML\n</td>\n<td>\nMRI\n</td>\n<td>\n95.37\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n<td>\n96.04\n</td>\n<td>\n93.03\n</td>\n<td>\n-\n</td>\n</tr>\n<tr>\n<td>\nGabriel 2021 [21]\n</td>\n<td>\nML\n</td>\n<td>\nMRI\n</td>\n<td>\n99.01(M)\n</td>\n<td>\n100(M)\n</td>\n<td>\n99.3(M)\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n<td></td>\n<td>\n87.10(F)\n</td>\n<td>\n97.2(F)\n</td>\n<td>\n100(F)\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n<td></td>\n<td>\n93.05 (A)\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n</tr>\n<tr>\n<td>\nLi 2019 [10]\n</td>\n<td>\nDL (AE)\n</td>\n<td>\nMRI + DTI\n</td>\n<td>\n85.24\n</td>\n<td>\n95.8\n</td>\n<td>\n68.1\n</td>\n<td>\n-\n</td>\n<td>\n89.67\n</td>\n<td>\n-\n</td>\n</tr>\n<tr>\n<td>\nTremblay 2020 [22]\n</td>\n<td>\nDL\n</td>\n<td>\nMRI\n</td>\n<td>\n88.3\n</td>\n<td>\n88.2\n</td>\n<td>\n88.4\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n</tr>\n<tr>\n<td>\nChakraborty 2020 [7]\n</td>\n<td>\nDL\n</td>\n<td>\nMRI\n</td>\n<td>\n95.3\n</td>\n<td>\n92.7\n</td>\n<td>\n91.4\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n<td>\n-\n</td>\n</tr>\n<tr>\n<td>\nSivaranjini 2020 [23]\n</ "
    ]
  },
  {
    "edit": [
      "We then define the damped mode: (1.3) can be rewritten as (1.4)\n\nAs the first equation of (1.3) can be rewritten as\n\n(1.5) "
    ],
    "kosmos": [
      "We then define the damped mode:\n\n\\[\\tilde{W}^{\\varepsilon}:=\\frac{\\nabla\\left(P(\\tilde{g}^{\\varepsilon})\\right)}{ \\tilde{g}^{\\varepsilon}}+\\tilde{v}^{\\varepsilon}+\\nabla(-\u2206)^{-1}(\\tilde{g}^{ \\varepsilon}-\\varrho). \\tag{1.4}\\]\n\nAs the first equation of (1.3) can be rewritten as\n\n\\[\\partial_{t}\\tilde{g}^{\\varepsilon}-\\Delta\\left(P(\\tilde{g}^{\\varepsilon}) \\right)-\\text{div}\\left(\\tilde{g}^{\\varepsilon}\\nabla(-\u2206)^{-1}(\\tilde{g}^{ \\varepsilon}-\\varrho)\\right)=\\text{div}(\\tilde{g}^{\\varepsilon}\\tilde{W}^{ \\varepsilon}),\\]\n\nwe expect the limit density \\(N\\) to satisfy the following _parabolic-elliptic Keller-Segel system_ :\n\n\\[\\left\\{\\begin{array}{l}\\partial_{t}N-\\Delta\\left(P(N)\\right)=\\text{div}\\left( N\\nabla V\\right)\\\\ -\\Delta V=N-\\varrho\\end{array}\\right. \\tag{1.5}\\]\n\nsupplemented with the initial data \\(\\lim\\limits_{\\varepsilon\\to 0}\\tilde{g}^{\\varepsilon}_{0}\\).\n\nOur second aim is to justify the passage to the limit when \\(\\varepsilon\\to 0\\) of the Euler-Poisson system towards the parabolic-elliptic Keller-Segel system.\n\nRecall that (1.5) is a model for describing the evolution of density \\(N=N(t,x)\\in\\mathbb{R}_{+}\\) of a biological population under the influence of a chemical agent with concentration \\(V=V(t,x)\\in\\mathbb{R}^{d}\\). Chemotaxis are an important means of cell communication. How cells are arranged and organized is determined by communication by chemical signals. Studying such a biological process is important because it has repercussions in many branches of medicine such as cancer [0], [0], embryonic development [0] or vascular networks [0], [0]. The previous system is famous in biology and comes from E.F Keller and L.A Segel in [0]. This basic model was used to describe the collective movement of bacteria possibly leading to cell aggregation by chemotactic effect. We refer to the articles [0] and [0] for more details and information about the different Keller-Segel models studied since the 1970s.\n\nOur aim here is to demonstrate that (1.5) may be obtained from the Euler-Poisson system with damping when the parameter \\(\\varepsilon\\) tends to 0. This question has been addressed in [0] on the torus case and Sobolev spaces in a situation where the potential satisfies a less singular equation : the author justifies the passage to the limit for regular periodic solutions. A lot of articles justify another limit: the passage from the parabolic-parabolic Keller-Segel system to the parabolic-elliptic Keller-Segel system (see e.g. the paper [0] by P-G. Lemari\u00e9-Rieusset for the case of Morrey spaces).\n\nIn the same spirit as this article, T. Crin-Barat, Q. He and L. Shou in [0] justified the high relaxation asymptotics for the (less singular) parabolic-parabolic Keller-Segel system (the potential satisfies the equation \\(-\\Delta V+bV=aN\\) with \\(a,b&gt;0\\)) : this other system comes from the system (HPC) (hyperbolic-parabolicchemotaxis) which is a damped isentropic compressible Euler system with a potential satisfying an elliptical equation. In comparison with what is done here, T. Crin-Barat _et al_ used a parabolic approach to justify their passage to the limit. Here, we have to handle the more singular case where the limit system is parabolic-elliptic.\n\n## 2. Main results and sketch of the proof\n\nIn this section, we will first present and motivate the functional spaces used. Secondly we will state the results and the sketch of the proofs about the well-posedness behavior of Euler-Poisson system and the justification of the passage to the limit to parabolic-elliptic Keller-Segel system.\n\n### Functional spaces.\n\nBefore describing the main results of this article, we introduce the different notations and definitions used throughout this document. We will "
    ]
  },
  {
    "edit": [
      "\n\n**4.5. Corollary.** _Let P be a polyhedron of dimension n with abelian \\(\\pi_{1}(P)\\) and finitely generated \\(H_{i}(\\tilde{P})\\), for \\(i\\geq 2\\). Then \\(D(P)\\leq\\sum_{i=1}^{n}n_{i}\\), where \\(n_{1}\\) and \\(n_{i}\\) (\\(i\\geq 2\\)) are the number of nonzero direct summands in the canonical form of \\(\\pi_{1}(P)\\) and \\(H_{i}(\\tilde{P})\\), respectively._\n\n**4.6. Corollary.** _Let P be a polyhedron of dimension n with free \\(\\pi_{1}(P)\\) and finitely generated \\(H_{i}(\\tilde{P})\\), for \\(i\\geq 2\\). Then \\(D(P)\\leq rank(\\pi_{1}(P))+\\sum_{i=2}^{n}n_{i}\\), where \\(n_{i}\\) is the number of nonzero direct summands in the canonical form of \\(H_{i}(\\tilde{P})\\)._\n\n**4.7. Corollary.** _Let P be a polyhedron of dimension n with elementary amenable \\(\\pi_{1}(P)\\) of finite cohomological dimension and finitely generated \\(H_{i}(\\tilde{P})\\), for \\(i\\geq 2\\). Then \\(D(P)\\leq h(\\pi_{1}(P))+\\sum_{i=2}^{n}n_{i}\\), where \\(h(\\pi_{1}(P))\\) is the Hirsch length of \\(\\pi_{1}(P)\\) and \\(n_{i}\\) is the number of nonzero direct summands in the canonical form of \\(H_{i}(\\tilde{P})\\)._\n\nRecently, in [14], Ko lodziejczyk proved that 2-dimensional polyhedra whose fundamental groups are elementary amenable with finite cohomological dimension have finite depth. In the sequel, we are going to present upper bounds for such polyhedra. Consider if P is a polyhedron of dimension n, then \\(H_{n}(P)\\) is free abelian (see, for example, [18, Theorem 7.24]). Now we state our second main result. If P is a 2-dimensional polyhedron and sl ( \u03c0 1 ( P )) < \u221e, then (by=\"\" 2-dimensional=\"\" 4.29]).=\"\" [14,=\"\" [18,=\"\" [18,=\"\" [14,=\"\" [18,=\"\" [18,=\"\" [18,=\"\" [18,theorem=\"\" [18,theorem=\"\" [18,theorem,=\"\" [18,"
    ],
    "kosmos": [
      "\n\n**4.5. Corollary**.: _Let P be a polyhedron of dimension n with abelian \\(\\pi_{1}(P)\\) and finitely generated \\(H_{i}(\\tilde{P})\\), for \\(i\\geq 2\\). Then \\(D(P)\\leq\\sum_{i=1}^{n}n_{i}\\), where \\(n_{1}\\) and \\(n_{i}\\) (\\(i\\geq 2\\)) are the number of nonzero direct summands in the canonical form of \\(\\pi_{1}(P)\\) and \\(H_{i}(\\tilde{P})\\), respectively._\n\n**4.6. Corollary**.: _Let P be a polyhedron of dimension n with free \\(\\pi_{1}(P)\\) and finitely generated \\(H_{i}(\\tilde{P})\\), for \\(i\\geq 2\\). Then \\(D(P)\\leq rank(\\pi_{1}(P))+\\sum_{i=2}^{n}n_{i}\\), where \\(n_{i}\\) is the number of nonzero direct summands in the canonical form of \\(H_{i}(\\tilde{P})\\)._\n\n**4.7. Corollary**.: _Let P be a polyhedron of dimension n with elementary amenable \\(\\pi_{1}(P)\\) of finite cohomological dimension and finitely generated \\(H_{i}(\\tilde{P})\\), for \\(i\\geq 2\\). Then \\(D(P)\\leq h(\\pi_{1}(P))+\\sum_{i=2}^{n}n_{i}\\), where \\(h(\\pi_{1}(P))\\) is the Hirsch length of \\(\\pi_{1}(P)\\) and \\(n_{i}\\) is the number of nonzero direct summands in the canonical form of \\(H_{i}(\\tilde{P})\\)._\n\nRecently, in [14], Kolodziejczyk proved that 2-dimensional polyhedra whose fundamental groups are elementary amenable with finite cohomological dimension have finite depth. In the sequel, we are going to present upper bounds for such polyhedra.\n\nRecall that if P is a polyhedron of dimension n, then \\(H_{n}(P)\\) is free abelian (see, for example, [18, Theorem 7.24]). Now we state our second main result.\n\n**4.8. Theorem**.: _If P is a 2-dimensional polyhedron and \\(sl(\\pi_{1}(P))&lt;\\infty\\), then \\(D(P)\\leq sl\\big{(}\\pi_{1}(P)\\big{)}+rank\\big{(}H_{2}(P)\\big{)}\\)._\n\nProof.: Consider the following chain of CW-complexes:\n\n\\[\\cdots&lt;X_{i+1}&lt;X_{i}&lt;\\cdots&lt;X_{3}&lt;X_{2}&lt;X_{1}&lt;X_{0}=P.\\]\n\nLet \\(d_{X_{i+1}}:X_{i}\\to X_{i+1}\\) and \\(u_{X_{i+1}}:X_{i+1}\\to X_{i}\\) be the domination of X\\({}_{i}\\) over X\\({}_{i+1}\\) and the converse map, i.e., \\(d_{X_{i+1}}u_{X_{i+1}}\\simeq id_{X_{i+1}}\\). Then \\(\\pi_{1}(d_{X_{i+1}})\\pi_{1}(u_{X_{i+1}})=id_{\\pi_{1}(X_{i+1})}\\) and \\(H_{2}(d_{X_{i+1}})H_{2}(u_{X_{i+1}})=id_{H_{2}(X_{i+1})}\\). As a result, im\\(\\pi_{1}(u_{X_{i+1}})\\) and im\\(H_{2}(u_{X_{i+1}})\\) are retracts of \\(\\pi_{1}(X_{i})\\) and \\(H_{2}(X_{i})\\).\n\nAssume that im\\(\\big{(}\\pi_{1}(u_{X_{i+1}})\\big{)}=\\pi_{1}(X_{i})\\) and im\\(\\big{(}H_{2}(u_{X_{i+1}})\\big{)}=H_{2}(X_{i})\\). Then \\(\\pi_{1}(u_{X_{i+1}})\\) and \\(H_{2}(u_{X_{"
    ]
  },
  {
    "edit": [
      "\n\n**Outline.** We use Fenichel\u2019s reduction to regularize the singular perturbation in existence and eigenvalue problem in Section 2. In Section 4, we study the resulting regularized traveling wave problem using functional-analytic methods, using methods developed in [2, 4, 3] to find pulled and pushed front profiles as well as the transition curve. Section 5 establishes marginal spectral stability of these fronts thus justifying the pushed and pulled terminology. In Section 6, we briefly compare the expansions obtained in Theorem 1.2 to those obtained using numerical continuation. The appendix contains the construction and properties of traveling fronts at \\(\\delta=0\\).\n\n## 2 Regularization via geometric singular perturbation theory\n\n### Reduction of existence problem\n\nWe express (1.5) as a dynamical system in the variable x by choosing coordinates U, W = U^{\\prime},H = \\frac{V-U}{\u03b4^{2}},Z = \u03b4H^{\\prime}\\), obtaining\n\n\\[U^{\\prime} =W\\] \\[W^{\\prime} =- \\frac{1}{d_{1}}\\Gamma(U,W,H,Z)\\] \\[\\delta H^{\\prime} =Z\\] \\[\\delta Z^{\\prime} =H +\\frac{1}{d_{1}}\\Gamma(U,W,H,Z),\\] (2.1) reduces to two algebraic equations coupled to two differential equations. One identifies the following reduced slow manifold comprised of solutions of the algebraic of equations in the singular limit \\(\\delta=0\\),\n\n\\[\\mathcal{M}_{0}=\\left\\{(U,W,H,Z)\\ |\\ Z=0,\\ H=-\\frac{cW+W^{2}+U-U^{2}}{d_{1}+U }\\right\\}. \\tag{2.2}\\]\n\nThe linearization of (2.1) at any such fixed point has two zero eigenvalues and two hyperbolic eigenvalues \\(\\pm\\sqrt{1+\\frac{U}{d_{1}}}\\) for U \u2265 0. The eigenspaces of the non-zero eigenvalues are traverse to \\(\\mathcal{M}_{0}\\) and therefore the reduced manifold is normally hyperbolic. Fenichel\u2019s Persistence Theorem [5] implies that \\(\\mathcal{M}_{0}\\) persists as an invariant manifold \\(\\mathcal{M}_{\\delta}\\) with the following properties.\n\n**Proposition 2.1** (Reduction for existence problem) .: _Fix 0 < d < d, (2.1)=\"\" (2.2)=\"\" (2.3)=\"\" (2.4)=\"\" (2.5)=\"\" (2.6)=\"\" (2.7)=\"\" (2.8)=\"\" (2.9)=\"\" (2.9),=\"\" (2.9),=\"\" (2.\n\n"
    ],
    "kosmos": [
      "\n\n**Outline.** We use Fenichel\u2019s reduction to regularize the singular perturbation in existence and eigenvalue problem in Section 2. In Section 4, we study the resulting regularized traveling wave problem using functional-analytic methods, using methods developed in [2, 4, 3] to find pulled and pushed front profiles as well as the transition curve. Section 5 establishes marginal spectral stability of these fronts thus justifying the pushed and pulled terminology. In Section 6, we briefly compare the expansions obtained in Theorem 1.2 to those obtained using numerical continuation. The appendix contains the construction and properties of traveling fronts at \\(\\delta=0\\).\n\n## 2 Regularization via geometric singular perturbation theory\n\n### Reduction of existence problem\n\nWe express (1.5) as a dynamical system in the variable x by choosing coordinates U, W = U^{\\prime},H = \\frac{V-U}{\u03b4^{2}},Z = \u03b4H^{\\prime}\\), obtaining\n\n\\[U^{\\prime} =W\\] \\[W^{\\prime} =- \\frac{1}{d_{1}}\\Gamma(U,W,H,Z)\\] \\[\\delta H^{\\prime} =Z\\] \\[\\delta Z^{\\prime} =H +\\frac{1}{d_{1}}\\Gamma(U,W,H,Z), \\tag{2.1}\\]\n\nwhere\n\n\\[\\Gamma(U,W,H,Z)=cW+W^{2}+\\delta WZ+UH+U(1-U). \\tag{2.2}\\]\n\nWhen \\(\\delta=0\\) the system (2.1) reduces to two algebraic equations coupled to two differential equations. One identifies the following reduced slow manifold comprised of solutions of the algebraic of equations in the singular limit \\(\\delta=0\\),\n\n\\[\\mathcal{M}_{0}=\\left\\{(U,W,H,Z)~{}|~{}Z=0,~{}H=-\\frac{cW+W^{2}+U-U^{2}}{d_{1} +U}\\right\\}. \\tag{2.3}\\]\n\nThe linearization of (2.1) at any such fixed point has two zero eigenvalues and two hyperbolic eigenvalues \\(\\pm\\sqrt{1+\\frac{U}{d_{1}}}\\) for U \u2265 0. The eigenspaces of the non-zero eigenvalues are traverse to \\(\\mathcal{M}_{0}\\) and therefore the reduced manifold is normally hyperbolic. Fenichel\u2019s Persistence Theorem [5] implies that \\(\\mathcal{M}_{0}\\) persists as an invariant manifold \\(\\mathcal{M}_{\\delta}\\) with the following properties.\n\n**Proposition 2.1** (Reduction for existence problem).: _Fix 0 < d < d, (2.1)=\"\" (2.1)=\"\" (2.1)_,=\"\" (2.4)=\"\" (2.5)=\"\" (2.6)=\"\" (2.7)=\"\" (2.8)=\"\" (2.9)=\"\" (2.9),=\"\" (2.9),=\"\" (2.9).=\"\" (2.\n\n"
    ]
  },
  {
    "edit": [
      "the input data dimension. Figure 4a shows the memory size (in MB) of the 4 shallow classifiers considered in our experiments, based on the input size in terms of the percentage of PCA explained variance. As expected, the size of most of the classifiers increases according to the input dimension, except for Random Forest (RF), whose size remains nearly constant at around 1 MB (between 0.98 and 1.11). Logistic Regression (LR), the simplest classifier, is also the one with the lowest memory footprint in all the experiments, starting from less than 1 KB (i.e., 922 Bytes), up to 3.64 KB with 99% of PCA explained variance. On the other hand, AdaBoost (AB) results to be the most demanding model in terms of memory, with an overall size that ranges from just 5.74 MB with 60% of PCA, up to 185.05 MB with the full dimension of the input. Finally, SVM has an intermediate memory footprint among the other classifiers, ranging from 42.6 KB up to 1.88 MB. On the other hand, when the deep audio models are fine-tuned, the size of the additional fully-connected layers should be considered to estimate the overall memory footprint. Figure 4b shows the average size of the fine-tuned models, highlighting both the size of the original pre-trained models, and the size of the additional layers for classification. We can note that, in general, "
    ],
    "kosmos": [
      "the input data dimension. Figure 4a shows the memory size (in MB) of the 4 shallow classifiers considered in our experiments, based on the input size in terms of the percentage of PCA explained variance. As expected, the size of most of the classifiers increases according to the input dimension, except for Random Forest (RF), whose size remains nearly constant at around 1 MB (between 0.98 and 1.11). Logistic Regression (LR), the simplest classifier, is also the one with the lowest memory footprint in all the experiments, starting from less than 1 KB (i.e., 922 Bytes), up to 3.64 KB with 99% of PCA explained variance. On the other hand, AdaBoost (AB) results to be the most demanding model in terms of memory, with an overall size that ranges from just 5.74 MB with 60% of PCA, up to 185.05 MB with the full dimension of the input. Finally, SVM has an intermediate memory footprint among the other classifiers, ranging from 42.6 KB up to 1.88 MB.\n\nOn the other hand, when the deep audio models are fine-tuned, the size of the additional fully-connected layers should be considered to estimate the overall memory footprint. Figure 4b shows the average size of the fine-tuned models, highlighting both the size of the original pre-trained models, and the size of the additional layers for classification. We can note that, in general, the size of the additional layers is not a good indicator of the overall memory footprint, as the number of layers is not constant, and the number of layers is not constant in the number of classes. In fact, the number of layers is not constant in the number of classes, and the number of layers is not constant in the number of classes.\n\n "
    ]
  },
  {
    "edit": [
      "be effective for support vector machines (SVM) [ 79 ] and large language models [ 25 ]. Another line of work called transductive learning uses test data to add constraints to the margin of SVMs [ 31 , 11 , 66 ]. The principle of transduction, as stated by Vapnik, also emphasizes locality [ 18 , 67 ]: \"Try to get the answer that you really need but not a more general one.\" In computer vision, the idea of training at test time has been well explored for specific applications [ 30 , 57 , 46 , 73 ], especially depth estimation [ 62 , 63 , 82 , 84 , 43 ]. Our paper extends TTT-MAE [ 19 ], detailed in Section 3 . TTT-MAE, in turn, is inspired by the work Sun et al. [ 61 ], which proposed the general framework for test-time training with self-supervision, regardless of application. The particular self-supervised task used in [ 61 ] is rotation prediction [ 21 ]. Many other papers have followed this framework since then [ 24 , 60 , 40 , 77 ], including [ 69 ] on videos discussed in Section 1 , and [ 5 ] which we discuss next. In [ 5 ] experiment on videos with artificial corruptions. These corruptions are also i.i.d. across frames. In [ 5 ], each video is treated as a dataset of unordered frames instead of a teacher model. Rather than focusing on computational e ffi ciency as in [ 45 ], the main goal of our paper is to improve inference quality. Behind their particular algorithm, however, we see the shared idea of locality, regardless of the form of supervision. Our paper is very much inspired by [ 45 ]. To make video segmentation more efficient, [ 45 ] makes predictions frame-by-frame using a small student model. If the student is not confident, it queries an expensive teacher model, and then trains the student to fit the prediction from the teacher online. Thanks to temporal smoothness, the student can generalize confidently across many frames without querying the teacher, so learning and predicting combined is still faster than naively using the teacher at every frame. Our method only consists of one model, which learns from a self-supervised task instead of a teacher model. Rather than focusing on computational e ffi ciency as in [ 45 ], we see the shared idea of locality, regardless of the form of supervision. Our paper extends the work of _Test-Time Training with Masked Autoencoders_ (TTT-MAE) [ 19 ], and uses TTT-MAE as the inner loop when updating the model for each frame. This section briefly describes TTT-MAE, as background for our extension. Figure 3 illustrates the process of TTT-MAE. The architecture for TTT with self-supervision [ 61 ] is Y-shaped with a stem and two heads: a prediction head \\(g\\) for the self-supervised task, a prediction head \\(h\\) for the main task, and a feature extractor \\(f\\) as the stem. The output features of \\(f\\) are shared between \\(g\\) and \\(h\\) as input. For TTT-MAE, the self-supervised task is masked image reconstruction [ 27 ]. Following standard terminology for autoencoders, \\(f\\) is also called the encoder, and \\(g\\) the decoder. Each input image \\(x\\) is first split into many non-overlapping patches. To produce the autoencoder input \\(\\tilde{x}\\), we mask out majority, e.g. 80%, of the patches in \\(x\\) at random. The self-supervised objective \\(\\ell_{s}(g\\circ f(\\tilde{x}),x)\\) compares the reconstructed patches from \\(g\\circ f(\\tilde{x})\\) to the masked patches in \\(x\\), and computes the pixel-wise mean squared error. For the main task, e.g. segmentation, all patches in the original \\(x\\) are given as input to \\(h\\circ f\\), during both training and testing. There are three widely accepted ways to optimize the model components \\((f,g,h)\\) at training time: joint training, probing, and fine-tuning. Fine-tuning is unsuitable for TTT, because it makes \\(h\\) rely too much on features that are used by the main task. Our paper uses joint training, described in Section 4. In contrast, [ 19 ] uses probing, which we describe next for completeness. To prepare for probing, the common practice is to first train \\(f\\) and \\(g\\) with \\(\\ell_{s}\\) on the training set without ground truth. This preparation stage is also called self-supervised pre-training. TTT- "
    ],
    "kosmos": [
      "be effective for support vector machines (SVM) [79] and large language models [25]. Another line of work called transductive learning uses test data to add constraints to the margin of SVMs [31, 11, 66]. The principle of transduction, as stated by Vapnik, also emphasizes locality [18, 67]: \"Try to get the answer that you really need but not a more general one.\"\n\nIn computer vision, the idea of training at test time has been well explored for specific applications [30, 57, 46, 73], especially depth estimation [62, 63, 82, 84, 43]. Our paper extends TTT-MAE [19], detailed in Section 3. TTT-MAE, in turn, is inspired by the work Sun et al. [61], which proposed the general framework for test-time training with self-supervision, regardless of application. The particular self-supervised task used in [61] is rotation prediction [21]. Many other papers have followed this framework since then [24, 60, 40, 77], including [69] on videos discussed in Section 1, and [5] which we discuss next.\n\nIn [5], each video is treated as a dataset of unordered frames instead of a stream. In particular, there is no concept of past vs. future frames. The same model is used on the entire video. In contrast, our paper emphasizes locality. We have access to only the current and past frames, and our model keeps learning over time. In addition, all of our results are on real world videos, while [5] experiment on videos with artificial corruptions. These corruptions are also i.i.d. across frames.\n\nOur paper is very much inspired by [45]. To make video segmentation more efficient, [45] makes predictions frame-by-frame using a small student model. If the student is not confident, it queries an expensive teacher model, and then trains the student to fit the prediction from the teacher online. Thanks to temporal smoothness, the student can generalize confidently across many frames without querying the teacher, so learning and predicting combined is still faster than naively using the teacher at every frame. Our method only consists of one model, which learns from a self-supervised task instead of a teacher model. Rather than focusing on computational efficiency as in [45], the main goal of our paper is to improve inference quality. Behind their particular algorithm, however, we see the shared idea of locality, regardless of the form of supervision.\n\n## 3 Background: TTT-MAE\n\nOur paper extends the work of *Test-Time Training with Masked Autoencoders* (TTT-MAE) [19], and uses TTT-MAE as the inner loop when updating the model for each frame. This section briefly describes TTT-MAE, as background for our extension. Figure 3 illustrates the process of TTT-MAE.\n\nThe architecture for TTT with self-supervision [61] is Y-shaped with a stem and two heads: a prediction head *g* for the self-supervised task, a prediction head *h* for the main task, and a feature extractor *f* as the stem. The output features of *f* are shared between *g* and *h* as input. For TTT-MAE, the self-supervised task is masked image reconstruction [27]. Following standard terminology for autoencoders, *f* is also called the encoder, and *g* the decoder.\n\nEach input image *x* is first split into many non-overlapping patches. To produce the autoencoder input \\(\\tilde{x}\\), we mask out majority, e.g. 80%, of the patches in *x* at random. The self-supervised objective \\(\\ell_{s}(g\\circ f(\\tilde{x}),x)\\) compares the reconstructed patches from *g* \u25cb *f*(\\(\\tilde{x}\\)) to the masked patches in *x*, and computes the pixel-wise mean squared error. For the main task, e.g. segmentation, all patches in the original *x* are given as input to *h* \u25cb *f*, during both training and testing.\n\n### Training-Time Training\n\nThere are three widely accepted ways to optimize the model components (*f*, *g*, *h*) at training time: joint training, probing, and fine-tuning. Fine-tuning is unsuitable for TTT, because it makes *h* rely too much on features that are used by the main task. Our paper uses joint training, described in Section 4. In contrast, [19] uses probing, which we describe next for completeness.\n\nTo prepare for probing, the common practice is to first train *f* and *g* with \\(\\ell_{s}\\) on the training set without ground truth. This preparation stage is also called self-supervised pre-training. TTT "
    ]
  },
  {
    "edit": [
      "the temperature feature to evaluate its impact on the clustering algorithm\u2019s efficiency in identifying locations of interest.\n\n#### 3.3.1 Obtaining Historical Temperature Data.\n\nHowever, such temperature analysis is only feasible when temperature data is available. Certain studies concerning elephant movement (Tsalyuk et al. [23]; Wall et al. [24]) lack a temperature feature. Therefore, we explored methods to approximate temperature data from other data sources. Using the meteostat python package and API, we identified weather stations proximate to the study site. The historical data was queried and appended to the study data, enabling calculation of Temperature-influenced centroids that would have been impossible to calculate otherwise.\n\nThe procedure entailed three key steps: (1) Identifying a nearby weather station, (2) Matching timestamps with the queried data, and (3) Evaluating the capability of the appended histcal temperature data in calculating temperature-influenced centroids.\n\nFor the first step, the median latitude and longitude of the given elephant\u2019s movement data was computed, which was then used to query a nearby station. The second step involved normalizing and interpolating the time series data from the station, provided by meteostat, to ensure a higher temporal granularity that matches the given data. In the third step, the correlation between the historical station data and Kruger temperature data was evaluated using the coefficient of determination, R-squared.\n\nOur results indicate a moderate correlation between the study data and the station data. This correlation, combined with the performance of the Temperature-influenced centroids with the weather data, gives us confidence to extend this technique to datasets that lack temperature data. Based on our experiment with elephant AM306 (See Figure 1) from the Kruger dataset, we found that the Temperature-influenced feature space aided in revealing more nuanced locations of interest within the larger clusters identified by the Without Temperature influence feature space.\n\n#### 3.3.2 Fuzzy Timestamp Matching.\n\nFuzzy timestamp matching is an advanced data processing technique that matches timestamps not based on exact equality but within a certain tolerance level. This tolerance level, or fuzzy threshold, is usually calculated by taking half of the median of the difference of timestamps in the dataset. The mathematical representation of the fuzzy timestamp matching process could be described as follows:\n\nGiven two timestamps, \\(t1\\) and \\(t2\\), and a tolerance level \\(\\delta\\), the timestamps \\(t1\\) and \\(t2\\) are said to match if:\n\n\\[|t1-t2|&lt;\\delta \\tag{2}\\]\n\nwhere \\(|t1-t2|\\) denotes the absolute difference between the timestamps \\(t1\\) and \\(t2\\). In this case, \\(\\delta\\) is calculated as:\n\n\\[\\delta=0.5*median(|t[i+1]-t[i]|),\\forall i\\ 1\\ to\\ N-1 \\tag{3}\\]\n\nwhere N is the total number of timestamps, and t[i] represents the ith timestamp in the ordered sequence. This fuzzy matching approach increases the likelihood of matches and can help to mitigate data loss when aligning data from different sources or with different temporal resolutions. However, it is important to note that this technique may also introduce some uncertainty into the analysis due to the mismatched timestamps. Hence, an appropriate balance between data retention and accuracy should be maintained while deciding the value of \\(\\delta\\).\n\nThe integration of weather station temperature data with animal movement datasets presented a significant challenge due to the relatively low percentage of matching timestamps. For instance, in the case of AM189 from Etosha, a mere 19.662% of timestamps corresponded. This limited overlap signifies a considerable loss of data, which undermines the analysis. To address this issue, we utilized \"fuzzy\" timestamp matching. This method extends the criteria of a match beyond exact timestamp equality, incorporating a pre-defined threshold for the discrepancy between two timestamps that still qualifies them as a match. The mathematical formulation of this concept is as follows: Given two timestamps t1 and t2, and\n\n<table>\n<thead>\n<tr>\n<th>\nMetric\n</th>\n<th>\nValue\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\nR-squared (zero-centered)\n</td>\n<td>\n0.6871044690549571\n</td>\n</tr>\n<tr>\n<td>\nOffset (study \u2212 station)\n</td>\n<td>\n9.840106696689293\n</td>\n</tr>\n<tr>\n<td>\n% of timestamps found\n</td>\n<td>\n61.6%\n</td>\n</tr>\n</tbody>\n</table>\n\nTable 1: Statistics for Figure 2\n\na tolerance level (or fuzzy threshold) \\(\\delta\\), the timestamps t1 and t2 are said to match if the absolute difference between them, denoted as |t1 - t2|, does not exceed \\(\\delta\\). The fuzzy threshold \\(\\delta\\) is calculated as half the median of the differences between all sequential pairs of timestamps in "
    ],
    "kosmos": [
      "the temperature feature to evaluate its impact on the clustering algorithm\u2019s efficiency in identifying locations of interest.\n\n#### 3.3.1 Obtaining Historical Temperature Data.\n\nHowever, such temperature analysis is only feasible when temperature data is available. Certain studies concerning elephant movement (Tsalyuk et al. [23]; Wall et al. [24]) lack a temperature feature. Therefore, we explored methods to approximate temperature data from other data sources. Using the meteostat python package and API, we identified weather stations proximate to the study site. The historical data was queried and appended to the study data, enabling calculation of Temperature-influenced centroids that would have been impossible to calculate otherwise.\n\nThe procedure entailed three key steps: (1) Identifying a nearby weather station, (2) Matching timestamps with the queried data, and (3) Evaluating the capability of the appended histcal temperature data in calculating temperature-influenced centroids.\n\nFor the first step, the median latitude and longitude of the given elephant\u2019s movement data was computed, which was then used to query a nearby station. The second step involved normalizing and interpolating the time series data from the station, provided by meteostat, to ensure a higher temporal granularity that matches the given data. In the third step, the correlation between the historical station data and Kruger temperature data was evaluated using the coefficient of determination, R-squared.\n\nOur results indicate a moderate correlation between the study data and the station data. This correlation, combined with the performance of the Temperature-influenced centroids with the weather data, gives us confidence to extend this technique to datasets that lack temperature data. Based on our experiment with elephant AM306 (See Figure 1) from the Kruger dataset, we found that the Temperature-influenced feature space aided in revealing more nuanced locations of interest within the larger clusters identified by the Without Temperature influence feature space.\n\n#### 3.3.2 Fuzzy Timestamp Matching.\n\nFuzzy timestamp matching is an advanced data processing technique that matches timestamps not based on exact equality but within a certain tolerance level. This tolerance level, or fuzzy threshold, is usually calculated by taking half of the median of the difference of timestamps in the dataset. The mathematical representation of the fuzzy timestamp matching process could be described as follows:\n\nGiven two timestamps, \\(t1\\) and \\(t2\\), and a tolerance level \\(\\delta\\), the timestamps \\(t1\\) and \\(t2\\) are said to match if:\n\n\\[|t1-t2|&lt;\\delta \\tag{2}\\]\n\nwhere \\(|t1-t2|\\) denotes the absolute difference between the timestamps \\(t1\\) and \\(t2\\). In this case, \\(\\delta\\) is calculated as:\n\n\\[\\delta=0.5*median(|t[i+1]-t[i]|),\\forall i\\ 1\\ to\\ N-1 \\tag{3}\\]\n\nwhere N is the total number of timestamps, and t[i] represents the ith timestamp in the ordered sequence. This fuzzy matching approach increases the likelihood of matches and can help to mitigate data loss when aligning data from different sources or with different temporal resolutions. However, it is important to note that this technique may also introduce some uncertainty into the analysis due to the mismatched timestamps. Hence, an appropriate balance between data retention and accuracy should be maintained while deciding the value of \\(\\delta\\).\n\nThe integration of weather station temperature data with animal movement datasets presented a significant challenge due to the relatively low percentage of matching timestamps. For instance, in the case of AM189 from Etosha, a mere 19.662% of timestamps corresponded. This limited overlap signifies a considerable loss of data, which undermines the analysis. To address this issue, we utilized \"fuzzy\" timestamp matching. This method extends the criteria of a match beyond exact timestamp equality, incorporating a pre-defined threshold for the discrepancy between two timestamps that still qualifies them as a match. The mathematical formulation of this concept is as follows: Given two timestamps t1 and t2, and\n\n<table>\n<thead>\n<tr>\n<th>\nMetric\n</th>\n<th>\nValue\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\nR-squared (zero-centered)\n</td>\n<td>\n0.6871044690549571\n</td>\n</tr>\n<tr>\n<td>\nOffset (study \u2212 station)\n</td>\n<td>\n9.840106696689293\n</td>\n</tr>\n<tr>\n<td>\n% of timestamps found\n</td>\n<td>\n61.6%\n</td>\n</tr>\n</tbody>\n</table>\n\nTable 1: Statistics for Figure 2\n\na tolerance level (or fuzzy threshold) \\(\\delta\\), the timestamps t1 and t2 are said to match if the absolute difference between them, denoted as |t1 - t2|, does not exceed \\(\\delta\\). The fuzzy threshold \\(\\delta\\) is calculated as half the median of the differences between all sequential pairs of timestamps in "
    ]
  },
  {
    "edit": [
      "marking a quantity evaluated at the stationary expansion point (U \u25e6, \u2212\u2192r ) as X . We will use this notation throughout this work. c Equation 7 yields an alternative formulation of Equation 4 using the exchanged second mixed derivative (Eq. 1).\n\nGrand canonical energy of a stationary pointWe can now insert the just derived potential-dependent geometric shift of a stationary point \u2212\u2192\u2206 r*(\u2206 U) (Eq. 7) into the second-order expansion of the gcPES (Eq. 6) around a known stationary point (U \u25e6, \u2212\u2192r ), eliminating the spatial dependence and returning the potential-dependent grand canonical energy E*(U) of a stationary point accurate to second order:\n\n\\[\\begin{split} E*(U)&amp;=E(U\u25e6+\u2206 U, \u2212\u2192r+\u2206 r*(\u2206 U))\\\\ &amp;=E(U\u25e6, \u2212\u2192r)\u2212 q\u2206 U\\\\ &amp;+\\frac{1}{2}\\sum_{i,j,k,l}(H\u22121_{j,k}(\\frac{\\partial q}{\\partial r _{k}})\u2206 U)H\u22121_{i,j}(H\u22121_{i,l}(\\frac{\\partial q}{\\partial r_{l}})\u2206 U)\\\\ &amp;-2\\sum_{i,j}(\\frac{\\partial q}{\\partial r_{i}})\u2206(H\u22121_{i,j}(\\frac {\\partial q}{\\partial r_{j}})\u2206 U)\\Delta U-C^{\\Delta}_{\\text{el}}\\Delta U^{2} \\\\ \\end{split} \\tag{7}\\]\n\nwhere we dropped the force-contribution, since F = 0. Rearranging and using \\(\\sum_{i}H\u22121_{k,i}H = \\delta_{k,j}\\) then yields the energy E* of the stationary point at potential U = U\u25e6 +\u2206 U:\n\n\\[\\begin{split} E*(U)&amp;=E*\u2212 q*\u2206 U-\\frac{1}{2}(C^{\\Delta}_{\\text{el}}+\\sum_{i,j}H \u22121_{i,j}(\\frac{\\partial q}{\\partial r_{i}})\u2206(H \u22121_{i,j}(\\frac{\\partial q}{\\partial r_{j}})\u2206 U^{2})\\Delta U^{3})\\\\ &amp;"
    ],
    "kosmos": [
      "marking a quantity evaluated at the stationary expansion point (U \u25e6, \u2212\u2192r ) as X . We will use this notation throughout this work. c Equation 7 yields an alternative formulation of Equation 4 using the exchanged second mixed derivative (Eq. 1).\n\nGrand canonical energy of a stationary pointWe can now insert the just derived potential-dependent geometric shift of a stationary point \u2212\u2192\u2206 r*(\u2206 U) (Eq. 7) into the second-order expansion of the gcPES (Eq. 6) around a known stationary point (U \u25e6, \u2212\u2192r ), eliminating the spatial dependence and returning the potential-dependent grand canonical energy E*(U) of a stationary point accurate to second order:\n\n\\[\\begin{split} E*(U)&amp;=E(U\u25e6+\u2206 U, \u2212\u2192r+\u2206 r*(\u2206 U))\\\\ &amp;=E(U\u25e6, \u2212\u2192r)\u2212 q\u2206 U\\\\ &amp;+\\frac{1}{2}\\sum_{i,j,k,l}(H\u22121_{j,k}(\\frac{\\partial q}{\\partial r _{k}})\u2206 U)H\u22121_{i,j}(H\u22121_{i,l}(\\frac{\\partial q}{\\partial r_{l}})\u2206 U)\\\\ &amp;-2\\sum_{i,j}(\\frac{\\partial q}{\\partial r_{i}})\u2206(H\u22121_{i,j}(\\frac {\\partial q}{\\partial r_{j}})\u2206 U)\\Delta U-C^{\\Delta}_{\\text{el}}\\Delta U^{2} \\\\ \\end{split} \\tag{7}\\]\n\nwhere we dropped the force-contribution, since F = 0. Rearranging and using \\(\\sum_{i}H\u22121_{k,i}H = \\delta_{k,j}\\) then yields the energy E* of the stationary point at potential U = U\u25e6 +\u2206 U:\n\n\\[\\begin{split} E*(U)&amp;=E*\u2212 q*\u2206 U-\\frac{1}{2}(C^{\\Delta}_{\\text{el}}+\\sum_{i,j}H \u22121_{i,j}(\\frac{\\partial q}{\\partial r_{i}})\u2206(H \u22121_{i,j}(\\frac{\\partial q}{\\partial r_{j}})\u2206 U^{2})\\Delta U^{3})\\\\ &amp;\\ "
    ]
  },
  {
    "edit": [
      "nd scales like m 4 \u03bd such that the dynamical friction effect is dominated by the most massive neutrino eigenstate. We may then assume a single neutrino species for simplicity, or explicitly write the total dynamical friction force as a sum of individual contribution from different eigenstates. In order to connect Eq. ( 3.19 ) with the results of future sections, it is convenient to rewrite it in terms of a quantity with dimension of inverse time. Since F = Mdv H /dt , we can define:\n\n\\[\\tau^{-1}=-\\frac{\\vec{F}\\cdot\\vec{v}_{\\rm H}}{Mv 2H 0}=2 \\tag{3.20}\\]\n\nwhich is the characteristic time scale for an order one fractional decrease in the halo velocity due to the dynamical friction effect. Note that 1 /\u03c4H 0 = \u2206 v/v is the overall relative decrease in the halo velocity over the age of the Universe t \u223c 1 /H 0 . We obtain a numerical value of \u2206 v/v = 3 . 4 \u00d7 10 \u2212 5 for a halo mass M = 10 13 M and individual neutrino mass m \u03bd = 0 . 1eV, when also assuming \u039b = 100. This already suggests that the dynamical friction effect is quite small, although it can pick up some significant contributions from the clustering of nearby halos as we will see in Sec. 5 . **Limitations to the 1-halo approach**\n\nThus far we have determined the anisotropic clustering of massive neutrinos behind moving point mass halos and the corresponding dynamical friction force. A more realistic calculation would have to account for both the finite extent of the halo and the presence of large-scale structure. Indeed, the Eq. ( 3.20 ) involves an unknown Coulomb logarithm, log \u039b, where in typical applications of the dynamical friction formula the cutoff \u039b can be estimated as the ratio of maximum and minimum impact parameters, \u039b \u223c b max /b min [ 51 ]. Here b min \u223c R halo is the halo radius, and b max \u223c \u03bb coh \u223c 0 . 1 Mpc \u2212 1 is the CDM velocity coherence scale. The CDM bulk flow is only coherent over sufficiently small scales and hence our analysis based on a single moving halo is expected to break down at scales \u03bb \u2273 \u03bb coh . 3 This point will be made more clear in the next section, where we also provide a precise definition for the velocity coherence scale.\n\nFootnote 3: We should also impose a cutoff corresponding to the distance traveled by free-streaming neutrinos, which sets the scale where neutrino inhomogeneities are coherent with CDM. As we shall see the neutrino free-streaming scale is much smaller than the scale of the halo, and hence the dynamical friction force is expected to be suppressed by the same factor as the CDM velocity coherence scale. This is consistent with the fact that the dynamical friction force is expected to be suppressed by the same factor as the CDM velocity coherence scale, and hence we can safely ignore the e\ufb00ect of the free-streaming neutrinos. In this case the dynamical friction force is dominated by the most massive neutrino eigenstate, and scales like m 4 \u03bd . In this case the dynamical friction force is dominated by the most massive neutrino eigenstate.\n\n "
    ],
    "kosmos": [
      "nd scales like m 4 \u03bd such that the dynamical friction e\ufb00ect is dominated by the most massive neutrino eigenstate. We may then assume a single neutrino species for simplicity, or explicitly write the total dynamical friction force as a sum of individual contribution from di\ufb00erent eigenstates.\n\nIn order to connect Eq. ( 3.19 ) with the results of future sections, it is convenient to rewrite it in terms of a quantity with dimension of inverse time. Since F = Mdv H /dt , we can de\ufb01ne:\n\n\\[\\tau \u2212 1 = \u2212\\vec{F}\\cdot\\vec{v}_{\\rm H} = 2\\over 3\\pi\\log\\Lambda G 2 Mm 4 \u03bd = 3 . 4\\times 10^{-5}\\log\\Lambda\\over\\log 100\\frac{M}{10^{13}M}_{ \\bigodot}\\left(\\frac{m_{\\nu}}{0 .1{\\rm eV}}\\right)^{4}H_{0}\\,, \\tag{3.20}\\]\n\nwhich is the characteristic time scale for an order one fractional decrease in the halo velocity due to the dynamical friction e\ufb00ect. Note that 1 /\u03c4H 0 = \u2206 v/v is the overall relative decrease in the halo velocity over the age of the Universe t \u223c 1 /H 0 . We obtain a numerical value of \u2206 v/v = 3 . 4 \u00d7 10 \u2212 5 for a halo mass M = 10 13 M and individual neutrino mass m \u03bd = 0 . 1eV, when also assuming \u039b = 100. This already suggests that the dynamical friction e\ufb00ect is quite small, although it can pick up some signi\ufb01cant contributions from the clustering of nearby halos as we will see in Sec. 5 .\n\n### Limitations to the 1-halo approach\n\nThus far we have determined the anisotropic clustering of massive neutrinos behind moving point mass halos and the corresponding dynamical friction force. A more realistic calculation would have to account for both the \ufb01nite extent of the halo and the presence of large-scale structure. Indeed, the Eq. ( 3.20 ) involves an unknown Coulomb logarithm, log \u039b, where in typical applications of the dynamical friction formula the cuto\ufb00 \u039b can be estimated as the ratio of maximum and minimum impact parameters, \u039b \u223c b max /b min [ 51 ]. Here b min \u223c R halo is the halo radius, and b max \u223c \u03bb coh \u223c 0 . 1 Mpc \u2212 1 is the CDM velocity coherence scale. The CDM bulk \ufb02ow is only coherent over su\ufb03ciently small scales and hence our analysis based on a single moving halo is expected to break down at scales \u03bb \u2273 \u03bb coh . 3 This point will be made more clear in the next section, where we also provide a precise de\ufb01nition for the velocity coherence scale.\n\nFootnote 3: We should also impose a cuto\ufb00 corresponding to the distance traveled by free-streaming neutrinos, which sets the scale where neutrino inhomogeneities are coherent with CDM. As we shall see the neutrino free-streaming scale is much smaller than the scale of the CDM velocity coherence scale, and hence we can safely ignore the e\ufb00ect of the free-streaming neutrinos. In this case the dynamical friction force is dominated by the most massive neutrino eigenstate, and scales like m 4 \u03bd . In this case the dynamical friction force is dominated by the most massive neutrino eigenstate.\n\n "
    ]
  },
  {
    "edit": [
      "\n\n# Branches Mutual Promotion for End-to-End Weakly Supervised Semantic Segmentation\n\nLei Zhu, Hangzhou He, Xinliang Zhang, Qian Chen, Shuang Zeng, Qiushi Ren, Yanye Lu*,\n\n###### Abstract\n\nEnd-to-end weakly supervised semantic segmen- **A. Multi-Stage WSSS B. E2E-WSSS with Unidirectional Supervision** tation aims at optimizing a segmentation model in a single- stage training process based on only image annotations. Existing methods adopt an online-trained classification branch to provide pseudo annotations for supervising the segmentation branch. However, this strategy makes the classification branch dominate the whole concurrent training process, hindering these two branches from assisting each other. In our work, we treat these two branches equally by viewing them as diverse ways to generate the segmentation map, and add interactions on both their supervision and operation to achieve mutual promotion. For this purpose, a bidirectional supervision mechanism is elaborated to force the consistency between the outputs of these two branches. Thus, the segmentation branch can also give feedback to the classification branch to enhance the quality of localization seeds. Moreover, our method also designs interaction operations between these two branches to exchange their knowledge to assist each other. Experiments indicate our work outperforms existing end-to-end weakly supervised segmentation methods. **Index Terms\u2014 Weakly Supervised Learning, Image Segmenta-**\n\n**tion, Object Localization**\n\n## I Introduction\n\nSemantic segmentation is a primary vision task, aiming to annotate pixels in an image as target objects or backgrounds. However, training a segmentation model in a fullysupervised manner requires annotating all pixels in training images, costing extensive human resources. To solve this problem, weakly supervised semantic segmentation (WSSS) appears and attracts extensive attention, which adopts only image-level annotation for the training process. However, as shown in Fig. 1 **A**, WSSS methods usually require multiple training stages, _e.g._, tuning a classification network with image annotations to produce localization seeds [1]\u2013[3], deriving pseudo annotations after refining the seeds [4]\u2013[6], and finally training the segmentation network with the pseudo annotations [7], [8]. Recently, some end-to-end weakly supervised semantic segmentation (E2E-WSSS) methods arose to simplify the heavy multi-stage training process into a single stage [9]\u2013[12]. As shown in Fig. 1 **B**, these methods train a two- branch network in only a single stage, where the classification branch supervised by image-level annotation can online provide pseudo annotations for the segmentation branch. Compared with multi-stage WSSS strategies: **A. Multi-Stage WSSS contains multiple training stages. B. Existing E2E-WSSS unidirectionally supervises the segmentation branch with pseudo annotations online provided by the classification branch. C. Our proposed E2E-WSSS) methods arose to simplify the heavy multi-stage training process into a single stage [9]\u2013 [12]. As shown in Fig. 1 **B**, these methods train a twobranch network in only a single stage, where the classification branch supervised by image-level annotation can online provide pseudo annotations for the segmentation branch. Compared with multi-stage WSSS, the concurrently-trained classification branch cannot stably provide seed to derive accurate pseudo annotations for supervising the segmentation branch. So, existing E2E-WSSS methods focus on improving the classification branch to provide better supervision by refining the localization seed with online spatial propagation [11]\u2013[13] or determining reliable regions on the pseudo annotations [9], [10]. In our work, we argue that current E2E-WSSS methods may fall into a trap, following the multi-stage WSSS to unidirectionally supervise the segmentation branch based on the prediction of the classification branch, without considering the feedback of the segmentation branch. In this way, the classification branch will dominate the whole training process, even if it may perform worse than the segmentation branch, as visualized in Fig. 2. Thus, the classification branch will converge to a similar optimum as the offline trained classification network but cannot stably provide pseudo annotations for the segmentation branch, which causes the large performance gap between current E2E-WSSS and multi-stage WSSS methods. Actually, in the E2E-WSSS setting, these two branches are basically at equal status because they are concurrently optimized during training. From another perspective, the segmentation branch can also assist the concurrently-trained classification branch in generating better localization seeds, which is a crucial trait of E2E-WSSS and yet to be explored by existing methods. Based on this perspective, our work treats these two branches equally by viewing them as diverse ways to achieve the same goal, generating the segmentation map of input images. Thus, as shown in Fig. 1 **C**, interactions are\n\nFig. 1: Comparison of WSSS strategies: **A.** Multi-stage WSSS contains multiple training stages. **B"
    ],
    "kosmos": [
      "\n\n# Branches Mutual Promotion for End-to-End Weakly Supervised Semantic Segmentation\n\nLei Zhu, Hangzhou He, Xinliang Zhang, Qian Chen, Shuang Zeng, Qiushi Ren, Yanye Lu*\n\n###### Abstract\n\nEnd-to-end weakly supervised semantic segmentation aims at optimizing a segmentation model in a single-stage training process based on only image annotations. Existing methods adopt an online-trained classification branch to provide pseudo annotations for supervising the segmentation branch. However, this strategy makes the classification branch dominate the whole concurrent training process, hindering these two branches from assisting each other. In our work, we treat these two branches equally by viewing them as diverse ways to generate the segmentation map, and add interactions on both their supervision and operation to achieve mutual promotion. For this purpose, a bidirectional supervision mechanism is elaborated to force the consistency between the outputs of these two branches. Thus, the segmentation branch can also give feedback to the classification branch to enhance the quality of localization seeds. Moreover, our method also designs interaction operations between these two branches to exchange their knowledge to assist each other. Experiments indicate our work outperforms existing end-to-end weakly supervised segmentation methods.\n\nWeakly Supervised Learning, Image Segmentation, Object Localization\n\n## I Introduction\n\nSemantic segmentation is a primary vision task, aiming to annotate pixels in an image as target objects or backgrounds. However, training a segmentation model in a fully-supervised manner requires annotating all pixels in training images, costing extensive human resources. To solve this problem, weakly supervised semantic segmentation (WSSS) appears and attracts extensive attention, which adopts only image-level annotation for the training process. However, as shown in Fig. 1 A, WSSS methods usually require multiple training stages, _e.g._, tuning a classification network with image annotations to produce localization seeds [1, 2, 3], deriving pseudo annotations after refining the seeds [4, 5, 6], and finally training the segmentation network with the pseudo annotations [7, 8].\n\nRecently, some end-to-end weakly supervised semantic segmentation (E2E-WSSS) methods arose to simplify the heavy multi-stage training process into a single stage [9, 10, 11, 12]. As shown in Fig. 1 B, these methods train a two-branch network in only a single stage, where the classification branch supervised by image-level annotation can online provide pseudo annotations for the segmentation branch. Compared with multi-stage WSSS, the concurrently-trained classification branch cannot stably provide seed to derive accurate pseudo annotations for supervising the segmentation branch. So, existing E2E-WSSS methods focus on improving the classification branch to provide better supervision by refining the localization seed with online spatial propagation [11, 12, 13] or determining reliable regions on the pseudo annotations [9, 10].\n\nIn our work, we argue that current E2E-WSSS methods may fall into a trap, following the multi-stage WSSS to unidirectionally supervise the segmentation branch based on the prediction of the classification branch, without considering the feedback of the segmentation branch. In this way, the classification branch will dominate the whole training process, even if it may perform worse than the segmentation branch, as visualized in Fig. 2. Thus, the classification branch will converge to a similar optimum as the offline trained classification network but cannot stably provide pseudo annotations for the segmentation branch, which causes the large performance gap between current E2E-WSSS and multi-stage WSSS methods.\n\nActually, in the E2E-WSSS setting, these two branches are basically at equal status because they are concurrently optimized during training. From another perspective, the segmentation branch can also assist the concurrently-trained classification branch in generating better localization seeds, which is a crucial trait of E2E-WSSS and yet to be explored by existing methods. Based on this perspective, our work treats these two branches equally by viewing them as diverse ways to achieve the same goal, generating the segmentation map of input images. Thus, as shown in Fig. 1 C, interactions are\n\nFig. 1: Comparison of WSSS strategies: A. Multi-stage WSSS contains multiple training stages. B. Existing E2E-WSSS unidirectionally supervises the segmentation branch with pseudo annotations online provided by the classification branch. C. Our proposed E2E-WSSS strategy interacts both the supervision and operation between these two branches to achieve mutual promotion.\n\n"
    ]
  },
  {
    "edit": [
      "shown that the dependence of mining rewards on propagation latency is more intricate than this [35]. Specifically, an honest miner that is well connected with other miners inadvertently creates efficient, low latency paths for other miners by acting as a centrally located bridge between the miners. However, to maximize the marginal gains in reward due to the network, it is important for a miner to have paths to other miners that are, on average, of a lower delay _relative_ to the delays of paths between other miners. For example, if miners are arranged as a star topology with links of unit delay and uniform compute power across nodes, the central node receives a higher reward compared to the leaf nodes by including more blocks on the blockchain. On the other hand, on a complete graph topology with unit delay links and uniform compute power as before, all nodes receive the same reward. A node identically connected to other nodes in the two cases (i.e., the central node in the star topology and any arbitrary node in the complete graph topology: both have direct links to all other nodes) receives different rewards, as rewards depend not only on the node\u2019s own connections but also on how other nodes\u2019 connections. Thus, there is an inherent tension for a miner in increasing her own connectivity to the rest of the network while simultaneously ensuring that the connectivity between other miners do not significantly increase. A systematic research of this tension, and efficient connection policies to maximize marginal mining reward gain due to the network, have not been done to our best knowledge. In this work, we formalize the p2p topology construction problem as a game between miners and present Cobalt, a decentralized policy for optimizing reward. We consider a simplified setting where only a single node chooses its connections, while the rest of the network\u2019s topology is fixed. We assume that the global topology of the p2p network is unknown to miners. We thus model the problem of optimizing rewards by the connections-deciding miner node as a Markov decision process (MDP) with no state and an action set with a combinatorial number of actions. We derive the optimal neighbor selection policy using a combinatorial multi-armed bandit (MAB) approach [14]. In the MAB algorithm, the agent (miner) explores various candidate connection configurations, and gradually adapts its connections based on past experience to gain the most mining rewards. A key contribution of our work is a network coordinates based model for efficiently learning the MAB environment [19]. In this model, miners are assigned realvalued vectors from an Euclidean space, which capture the relative location of miners with respect to each other in the network. The coodinates are continuously updated based upon the reward feedback the agent receives from the environment. Thus, despite not having global knowledge of the network initially, we show that it is possible for an agent to learn about the network by just using the observed reward information. To enable the deployment of MAB algorithm, we have built a simulator. To simplify the reward computation in the simulator, rather than simulating the actual mining process at each step of the MDP, we consider a computationally easier function that only depends on the pairwise shortest path lengths between miners. Importantly, our MDP reward function captures the property that a miner\u2019s mining gains depends on how small the shortest path lengths between the agent and other miners are relative to the shortest path lengths between other miners. Experimentally we show Cobalt outperforms or matches heuristics on diverse network settings.\n\n ## II Related Work\n\nP2P network design for optimizing mining rewards has remained a relatively under-explored topic in the community. The work that is closest to our is Perigee [34] which proposes an adaptive peer-selection algorithm for minimizing block propagation latency in the network. However, Perigee does not model the game-theoretic competition between miners. Subsequent works [11, 43] consider optimizing the network to maximize extractable value (MEV) from transactions. A number of prior works have exposed the impact of the network on mining [12, 26, 28, 37, 40, 47, 48]. While these works generally suggest that better network connectivity translates to higher mining rewards earned, the competitive effects of network connectivity and methods to optimize them have not been discussed. Other related works include KadCast [38] which proposes a Kadmila-based structured overlay for efficient block broadcast, and relay networks such as BloXroute [29] for transports blocks quickly across vast geographic distances.\n\nThe idea of network coordinates for p2p networks has been prominently explored in the network systems literature since the turn of the millenium, including distributed approaches to learn them [19, 32, 36]. More recently, a number of theoretical works have studied using low-distortion embeddings in finite metrics (i.e., over finite graphs) for various applications, e.g., sparse spanner construction [10, 13, 16, 21].\n\nGame theory of blockchains, especially at the consensus layer, has received considerable attention. For "
    ],
    "kosmos": [
      "shown that the dependence of mining rewards on propagation latency is more intricate than this [35]. Specifically, an honest miner that is well connected with other miners inadvertently creates efficient, low latency paths for other miners by acting as a centrally located bridge between the miners. However, to maximize the marginal gains in reward due to the network, it is important for a miner to have paths to other miners that are, on average, of a lower delay _relative_ to the delays of paths between other miners. For example, if miners are arranged as a star topology with links of unit delay and uniform compute power across nodes, the central node receives a higher reward compared to the leaf nodes by including more blocks on the blockchain. On the other hand, on a complete graph topology with unit delay links and uniform compute power as before, all nodes receive the same reward. A node identically connected to other nodes in the two cases (i.e., the central node in the star topology and any arbitrary node in the complete graph topology: both have direct links to all other nodes) receives different rewards, as rewards depend not only on the node\u2019s own connections but also on how other nodes\u2019 connections. Thus, there is an inherent tension for a miner in increasing her own connectivity to the rest of the network while simultaneously ensuring that the connectivity between other miners do not significantly increase. A systematic research of this tension, and efficient connection policies to maximize marginal mining reward gain due to the network, have not been done to our best knowledge.\n\nIn this work, we formalize the p2p topology construction problem as a game between miners and present Cobalt, a decentralized policy for optimizing reward. We consider a simplified setting where only a single node chooses its connections, while the rest of the network\u2019s topology is fixed. We assume that the global topology of the p2p network is unknown to miners. We thus model the problem of optimizing rewards by the connections-deciding miner node as a Markov decision process (MDP) with no state and an action set with a combinatorial number of actions.\n\nWe derive the optimal neighbor selection policy using a combinatorial multi-armed bandit (MAB) approach [14]. In the MAB algorithm, the agent (miner) explores various candidate connection configurations, and gradually adapts its connections based on past experience to gain the most mining rewards. A key contribution of our work is a network coordinates based model for efficiently learning the MAB environment [19]. In this model, miners are assigned real-valued vectors from an Euclidean space, which capture the relative location of miners with respect to each other in the network. The coodinates are continuously updated based upon the reward feedback the agent receives from the environment. Thus, despite not having global knowledge of the network initially, we show that it is possible for an agent to learn about the network by just using the observed reward information.\n\nTo enable the deployment of MAB algorithm, we have built a simulator. To simplify the reward computation in the simulator, rather than simulating the actual mining process at each step of the MDP, we consider a computationally easier function that only depends on the pairwise shortest path lengths between miners. Importantly, our MDP reward function captures the property that a miner\u2019s mining gains depends on how small the shortest path lengths between the agent and other miners are relative to the shortest path lengths between other miners. Experimentally we show Cobalt outperforms or matches heuristics on diverse network settings.\n\n## II Related Work\n\nP2P network design for optimizing mining rewards has remained a relatively under-explored topic in the community. The work that is closest to our is Perigee [34] which proposes an adaptive peer-selection algorithm for minimizing block propagation latency in the network. However, Perigee does not model the game-theoretic competition between miners. Subsequent works [11, 43] consider optimizing the network to maximize extractable value (MEV) from transactions. A number of prior works have exposed the impact of the network on mining [12, 26, 28, 37, 40, 47, 48]. While these works generally suggest that better network connectivity translates to higher mining rewards earned, the competitive effects of network connectivity and methods to optimize them have not been discussed. Other related works include KadCast [38] which proposes a Kadmila-based structured overlay for efficient block broadcast, and relay networks such as BloXroute [29] for transports blocks quickly across vast geographic distances.\n\nThe idea of network coordinates for p2p networks has been prominently explored in the network systems literature since the turn of the millenium, including distributed approaches to learn them [19, 32, 36]. More recently, a number of theoretical works have studied using low-distortion embeddings in finite metrics (i.e., over finite graphs) for various applications, e.g., sparse spanner construction [10, 13, 16, 21].\n\nGame theory of blockchains, especially at the consensus layer, has received considerable attention. For "
    ]
  },
  {
    "edit": [
      "<table>\n<colgroup>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n</colgroup>\n<thead>\n<tr>\n<th>\nEpoches\n</th>\n<th>\nOptimizer\n</th>\n<th>\nBatch size\n</th>\n<th>\nLearning rate\n</th>\n<th>\nLR decay\n</th>\n<th>\nWeight decay\n</th>\n<th>\nDrop path\n</th>\n<th>\nEmbedding dim\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\n300\n</td>\n<td>\nAdam[ 17 ]\n</td>\n<td>\n8\n</td>\n<td>\n4e-4\n</td>\n<td>\ncosine\n</td>\n<td>\n1e-7\n</td>\n<td>\n0.1\n</td>\n<td>\n15\n</td>\n</tr>\n</tbody>\n</table>\n\nTable 1: Default training and network hyper-parameters used in our method, unless stated otherwise.\n\n**COMO-ViT.** Given the input feature F l \u2212 1 \u2208 R H \u00d7 W \u00d7 c of COMO-ViT, we conduct two branches of operations. In the first branch, we uniformly split it into n non-overlapping windows P = [P 1 , P 2 , \u00b7 \u00b7 \u00b7 , P n ] \u2208 R n \u00d7 w \u00d7 w \u00d7 c , where ( w, w ) is the window resolution. SNR [ 43 ] and STAR [ 52 ] downsample images, losing local structures and some important pixel-level information. Instead, the proposed COMO-ViT completely models the dependencies among all pixels of an image via a local-to-global hierarchical self-attention. Locally, each pixel in a window P i is regarded as an individual, we thus reshape P i as follows:\n\nP i \u2192 [ p i, 1 , p i, 2 , \u00b7 \u00b7 \u00b7 , p i,m ] , (10)\n\nwhere p i,j \u2208 R 1 \u00d7 1 \u00d7 c , m = w 2 is the number of pixels in P i. With a linear projection, we then transform the pixels into a sequence of pixel embeddings X i = [ x i, 1 , x i, 2 , \u00b7 \u00b7 \u00b7 , x i,m ] , where x i,j \u2208 R c 1 is the j-th pixel embedding, c 1 is the embedding dimension. For X i , we utilize a local Transformer module to extract deep features as follows:\n\nY i = X i + MSA(LN( X i )) , (11)\n\nY i = Y i + MLP(LN( Y i )) , (12)\n\nwhere Y i is the feature learned by the local Transformer module, MSA( \u00b7 ) is the Multi-head Self-Attention [ 35 ], LN( \u00b7 ) is layer normalization [ 1 ] for stable training and faster convergence, MLP( \u00b7 ) is multi-layer perceptron for feature transformation at channel dimension and non-linearity. In such a process, we adopt 1D learnable location embedding to encode the spatial information of pixels.\n\nTo complement the non-overlapping window attention, in the second branch which is parallel with local attention, we use a CNN module to model local pixel dependencies in F l \u2212 1 via an overlapped sliding kernel to recover image details, in which a SE block [ 11 ] is used to explore channel relationship to boost representative power:\n\nF \u2032 = Conv(LN( F l \u2212 1 )) , F conv = F \u2032 \u2299 SE( F \u2032 ) . (13)\n\nF conv is then split into n non-overlapping windows Q = [ Q 1 , Q 2 , \u00b7 \u00b7 \u00b7 , Q n ] \u2208 R n \u00d7 w \u00d7 w \u00d7 c , and each Q i is reshaped:\n\nQ i \u2192 [ q i, 1 , q i, 2 , \u00b7 \u00b7 \u00b7 , q i,m ] . (14)\n\nWe combine the features from both branches as:\n\nC = [ C 1 , C 2 , \u00b7 \u00b7 \u00b7 , C n ] , C i = Q i + Y i (15)\n\nGlobal pixel dependencies are explored by calculating window attention via a global attention module. Firstly, C is transformed into a sequence of window embedding:\n\nU = [ u 1 , u 2 , \u00b7 \u00b7 \u00b7 , u n ] , u i = FC(Vec( C i )) , (16)\n\nwhere Vec( \u00b7 ) is vectorization operation. Then, we utilize a global Transformer module to explore inter-window dependencies, obtaining the feature F l \u2208 R H \u00d7 W \u00d7 c , where l \u2208 {1 , 2 , \u00b7 \u00b7 \u00b7 , L} , and L is the COMO-ViT number. When l = 1, F l \u2212 1 is the fused feature F f in Eq. (9).\n\nThe result of the second stage is obtained by decoding F L with a convolutional layer ( D ( \u00b7 )):\n\nR s "
    ],
    "kosmos": [
      "<table>\n<colgroup>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n</colgroup>\n<thead>\n<tr>\n<th>\nEpoches\n</th>\n<th>\nOptimizer\n</th>\n<th>\nBatch size\n</th>\n<th>\nLearning rate\n</th>\n<th>\nLR decay\n</th>\n<th>\nWeight decay\n</th>\n<th>\nDrop path\n</th>\n<th>\nEmbedding dim\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\n300\n</td>\n<td>\nAdam[ 17 ]\n</td>\n<td>\n8\n</td>\n<td>\n4e-4\n</td>\n<td>\ncosine\n</td>\n<td>\n1e-7\n</td>\n<td>\n0.1\n</td>\n<td>\n15\n</td>\n</tr>\n</tbody>\n</table>\n\nTable 1: Default training and network hyper-parameters used in our method, unless stated otherwise.\n\n**COMO-ViT.** Given the input feature F l \u2212 1 \u2208 R H \u00d7 W \u00d7 c of COMO-ViT, we conduct two branches of operations. In the first branch, we uniformly split it into n non-overlapping windows P = [P 1 , P 2 , \u00b7 \u00b7 \u00b7 , P n ] \u2208 R n \u00d7 w \u00d7 w \u00d7 c , where ( w, w ) is the window resolution. SNR [ 43 ] and STAR [ 52 ] downsample images, losing local structures and some important pixel-level information. Instead, the proposed COMO-ViT completely models the dependencies among all pixels of an image via a local-to-global hierarchical self-attention. Locally, each pixel in a window P i is regarded as an individual, we thus reshape P i as follows:\n\nP i \u2192 [ p i, 1 , p i, 2 , \u00b7 \u00b7 \u00b7 , p i,m ] , (10)\n\nwhere p i,j \u2208 R 1 \u00d7 1 \u00d7 c , m = w 2 is the number of pixels in P i. With a linear projection, we then transform the pixels into a sequence of pixel embeddings X i = [ x i, 1 , x i, 2 , \u00b7 \u00b7 \u00b7 , x i,m ] , where x i,j \u2208 R c 1 is the j-th pixel embedding, c 1 is the embedding dimension. For X i , we utilize a local Transformer module to extract deep features as follows:\n\nY i = X i + MSA(LN( X i )) , (11)\n\nY i = Y i + MLP(LN( Y i )) , (12)\n\nwhere Y i is the feature learned by the local Transformer module, MSA( \u00b7 ) is the Multi-head Self-Attention [ 35 ], LN( \u00b7 ) is layer normalization [ 1 ] for stable training and faster convergence, MLP( \u00b7 ) is multi-layer perceptron for feature transformation at channel dimension and non-linearity. In such a process, we adopt 1D learnable location embedding to encode the spatial information of pixels.\n\nTo complement the non-overlapping window attention, in the second branch which is parallel with local attention, we use a CNN module to model local pixel dependencies in F l \u2212 1 via an overlapped sliding kernel to recover image details, in which a SE block [ 11 ] is used to explore channel relationship to boost representative power:\n\nF \u2032 = Conv(LN( F l \u2212 1 )) , F conv = F \u2032 \u2299 SE( F \u2032 ) . (13)\n\nF conv is then split into n non-overlapping windows Q = [ Q 1 , Q 2 , \u00b7 \u00b7 \u00b7 , Q n ] \u2208 R n \u00d7 w \u00d7 w \u00d7 c , and each Q i is reshaped:\n\nQ i \u2192 [ q i, 1 , q i, 2 , \u00b7 \u00b7 \u00b7 , q i,m ] . (14)\n\nWe combine the features from both branches as:\n\nC = [ C 1 , C 2 , \u00b7 \u00b7 \u00b7 , C n ] , C i = Q i + Y i (15)\n\nGlobal pixel dependencies are explored by calculating window attention via a global attention module. Firstly, C is transformed into a sequence of window embedding:\n\nU = [ u 1 , u 2 , \u00b7 \u00b7 \u00b7 , u n ] , u i = FC(Vec( C i )) , (16)\n\nwhere Vec( \u00b7 ) is vectorization operation. Then, we utilize a global Transformer module to explore inter-window dependencies, obtaining the feature F l \u2208 R H \u00d7 W \u00d7 c , where l \u2208 {1 , 2 , \u00b7 \u00b7 \u00b7 , L} , and L is the COMO-ViT number. When l = 1, F l \u2212 1 is the fused feature F f in Eq. (9).\n\nThe result of the second stage is obtained by decoding F L with a convolutional layer ( D ( \u00b7 )):\n\nR s "
    ]
  },
  {
    "edit": [
      "of 10^3^ G (Reiners & Christensen 2010), we get a magnetic field strength of 4.5\u00d710^1^ G at the eclipse edge (\u03c6~b~ = 0.31). A magnetosphere field strength of 10 G is sufficient to trap and dominate plasma (assuming protons and electrons) of number density &lt; 3 \u00d7 10^13^ cm^\u22123^ and velocity \\(\\sim V_{\\rm orb}\\).\n\nThompson et al. (1994) suggests that, at the edge of the magnetosphere of the brown dwarf, the magnetic pressure should balance the pulsar wind pressure, while the pulsar wind energy density is U~E~ = ~E~4~xc~a~^2^ and the magnetic pressure is ~B~E~^2^~. The ~E~ is the spin-down luminosity, c is the speed of light, a is the orbital separation, and B~E~ is the magnetic field of the eclipse medium. From this, the magnetic field strength of B~E~ should be \u2248 8 G (Wang et al. 2021) . Interestingly, the derived theoretical magnetic field strength (45 G) is more than sufficient for the required field strength (8 G) at the eclipsing edge. However, these field strengths are more than three orders of magnitude higher than the value observed in our egress (10 mG). **Pulsar wind**\n\nThe third scenario (Fig. 7 c) supplements the second one with pulsar wind and a shock boundary, and fixes the inconsistency mentioned above. Such a picture was proposed by Phinney et al. (1988) as one of the early models. In this picture, a shock boundary exists between the magnetosphere and the pulsar wind. Outside of the shock boundary are high-speed, low-density pulsar wind particles traveling with a low magnetic field, and inside, the slow-moving, high-density plasma trapped by the companion\u2019s magnetic fields. This is similar to the boundary shock observed from the Solar wind and the Earth magnetosphere (Sckopke et al. 1983) where both the electron density and magnetic field rose suddenly as the ISEE-1 8 probe traveled downstream of the Solar wind into the Earth magnetosphere. The majority of energy in the pulsar wind is carried by relativistic particles. The magnetic fields in the pulsar wind could be much smaller than the magnetic field of the companion at the orbital distance. After all, the pulsar\u2019s magnetic field is only 1.6\u00d710^8^ G at its 10 km radius surface (Tab. 1). The pulsar wind is almost transparent to the pulsar emission. This is because of the low density and the high Lorenz factor of the wind particles. The wind particles have motion masses far exceeding their rest masses, causing their Faraday rotation effect to be negligible (Quataert & Gruzinov 2000; Wang et al. 2011). When a moderate amount of slow-moving ionized materials from the companion\u2019s magnetosphere flow out of the boundary and come to the pulsar wind side, the combination of the extra slow electrons and a reasonably low magnetic field (10 mG) environment leads to the incomplete depolarization and the Faraday rotation. As we mentioned in the previous section, such a condition is rarely met (only be observed in MJD 59214). In most of the ingresses and egresses of this pulsar, the out-flowing electrons are either too dense or too variable and often completely depolarize the pulsar signal. Thompson et al. (1994) predicted that the pulsar wind could contain an oscillating part around the eclipsing edge with an oscillation length of cP/2 \u2243 500 km, where c is the speed of light and P is the spin period of the pulsar. It should be noted that such reciprocating magnetic fields in the pulsar wind was already illustrated in the model of Phinney et al. (1988). But such field was never observed until now. We "
    ],
    "kosmos": [
      "of 10^3^ G (Reiners &amp; Christensen 2010), we get a magnetic field strength of 4.5\u00d710^1^ G at the eclipse edge (\u03c6~b~ = 0.31). A magnetosphere field strength of 10 G is sufficient to trap and dominate plasma (assuming protons and electrons) of number density &lt; 3 \u00d7 10^13^ cm^\u22123^ and velocity \\(\\sim V_{\\rm orb}\\).\n\nThompson et al. (1994) suggests that, at the edge of the magnetosphere of the brown dwarf, the magnetic pressure should balance the pulsar wind pressure, while the pulsar wind energy density is U~E~ = ~E~4~xc~a~^2^ and the magnetic pressure is ~B~E~^2^~. The ~E~ is the spin-down luminosity, c is the speed of light, a is the orbital separation, and B~E~ is the magnetic field of the eclipse medium. From this, the magnetic field strength of B~E~ should be \u2248 8 G (Wang et al. 2021).\n\nInterestingly, the derived theoretical magnetic field strength (45 G) is more than sufficient for the required field strength (8 G) at the eclipsing edge. However, these field strengths are more than three orders of magnitude higher than the value observed in our egress (10 mG).\n\n**Pulsar wind**\n\nThe third scenario (Fig. 7 c) supplements the second one with pulsar wind and a shock boundary, and fixes the inconsistency mentioned above. Such a picture was proposed by Phinney et al. (1988) as one of the early models. In this picture, a shock boundary exists between the magnetosphere and the pulsar wind. Outside of the shock boundary are high-speed, low-density pulsar wind particles traveling with a low magnetic field, and inside, the slow-moving, high-density plasma trapped by the companion\u2019s magnetic fields. This is similar to the boundary shock observed from the Solar wind and the Earth magnetosphere (Sckopke et al. 1983) where both the electron density and magnetic field rose suddenly as the ISEE-1^8^ probe traveled downstream of the Solar wind into the Earth magnetosphere.\n\nThe majority of energy in the pulsar wind is carried by relativistic particles. The magnetic fields in the pulsar wind could be much smaller than the magnetic field of the companion at the orbital distance. After all, the pulsar\u2019s magnetic field is only 1.6\u00d710^8^ G at its 10 km radius surface (Tab. 1). The pulsar wind is almost transparent to the pulsar emission. This is because of the low density and the high Lorenz factor of the wind particles. The wind particles have motion masses far exceeding their rest masses, causing their Faraday rotation effect to be negligible (Quataert &amp; Gruzinov 2000; Wang et al. 2011). When a moderate amount of slow-moving ionized materials from the companion\u2019s magnetosphere flow out of the boundary and come to the pulsar wind side, the combination of the extra slow electrons and a reasonably low magnetic field (10 mG) environment leads to the incomplete depolarization and the Faraday rotation. As we mentioned in the previous section, such a condition is rarely met (only be observed in MJD 59214). In most of the ingresses and egresses of this pulsar, the out-flowing electrons are either too dense or too variable and often completely depolarize the pulsar signal.\n\nThompson et al. (1994) predicted that the pulsar wind could contain an oscillating part around the eclipsing edge with an oscillation length of cP/2 \u2243 500 km, where c is the speed of light and P is the spin period of the pulsar. It should be noted that such reciprocating magnetic fields in the pulsar wind was already illustrated in the model of Phinney et al. (1988). But such field was never observed until now. We "
    ]
  },
  {
    "edit": [
      "between these works and ours is that they assume the buyer is _fully strategic_ and processes fully how their actions today affect the seller\u2019s decisions tomorrow (whereas we instead model buyers as no-regret learners).\n\nThe most related work to ours is in the [BMSW18] model itself. Here we provide a brief summary of the main results in [BMSW18] and their connection to our main results. [BMSW18] studies the one seller one buyer scenario, where the buyer employs a mean-based no-regret algorithm. The authors present three results, each obtained under different assumptions regarding the behavior of the buyers. Firstly (as we have already mentioned earlier in the introduction), [BMSW18] shows that for vanilla mean-based no-regret buyers, [BMSW18] can extract revenue that is an arbitrarily large fraction of the bidder\u2019s expected value. Our Theorem 4 extends this result to the multiple buyer setting, overcoming novel technical and conceptual challenges. Second, [BMSW18] designs a novel (not mean-based) learning algorithm against which the optimal mechanism for the seller is simply Myerson\u2019s auction in each round. Their proof of this result naturally accommodates multiple buyers. Finally, [BMSW18] shows that if the buyer is clever and mean-based no regret (where they do not overbid their value), then the optimal auction has a clean tractable format (pay-your-bid with declining reserve over time). As we have discussed in the \u201cNo Overbidding\u201d section of the introduction, our work shows several formal barriers in extending these results to multiple buyers. In summary, our main result extends their first main result to multiple bidders. Their second result already holds for multiple bidders (so there is nothing for us to extend). Our secondary results establish formal barriers to extending their final main result to multiple bidders.\n\nTwo recent follow-ups have extended the setting in [BMSW18] in a different direction. First, [DSS19b] considers the problem of playing a two-player game against a no-regret learner. While technically not an auctions problem, there is thematic overlap with our main result. [DSS19a] extends the single-buyer results in [BMSW18] to be _prior-free_. Specifically, they show how to design auctions achieving the same guarantees as those in [BMSW18] but where the buyer\u2019s values are chosen adversarially. In comparison to these works, ours is the first to extend the model to consider multiple buyers.\n\nFinally, recent work of [CHJ20] considers interaction between a learning buyer and a _learning_ seller. Their seller does not have a prior against which to optimize, and instead itself targets a no-regret guarantee. In comparison, our seller (like the seller in all previously cited works) optimizes expected revenue with respect to a prior.\n\n## 2 Preliminaries\n\nWe consider the same setting as [BMSW18], extended to multiple buyers. Specifically, there are \\(n\\) buyers and \\(T\\) rounds. In each round, there is a single item for sale. Each buyer \\(i\\) has value \\(v_{i,t}\\) for the item during round \\(t\\), and each \\(v_{i,t}\\) is drawn from \\(\\mathcal{D}\\) independently (that is, the buyers are i.i.d., and the rounds are i.i.d. as well). For simplicity of exposition (and to match prior work), we assume \\(\\mathcal{D}\\) has finite support \\(0\\leq w_{1}&lt;w_{2}&lt;\\ldots&lt;w_{m}\\leq 1\\) and we define \\(q_{j}\\) to be the probability \\(w_{j}\\) is drawn from \\(\\mathcal{D}\\).\n\nEach round, the seller presents \\(K\\) arms for the buyers. Each arm is labeled with a bid, and we assume that one of the arms is labeled with 0 (to represent a bid of \u201cdon\u2019t participate\u201d). Note that the same set of arms is presented to all buyers, and the same set of arms is presented in each round.\n\nIn each round \\(t\\), the seller defines an anonymous auction. Specifically, for all \\(i,t\\), the seller defines "
    ],
    "kosmos": [
      "between these works and ours is that they assume the buyer is _fully strategic_ and processes fully how their actions today affect the seller\u2019s decisions tomorrow (whereas we instead model buyers as no-regret learners).\n\nThe most related work to ours is in the [BMSW18] model itself. Here we provide a brief summary of the main results in [BMSW18] and their connection to our main results. [BMSW18] studies the one seller one buyer scenario, where the buyer employs a mean-based no-regret algorithm. The authors present three results, each obtained under different assumptions regarding the behavior of the buyers. Firstly (as we have already mentioned earlier in the introduction), [BMSW18] shows that for vanilla mean-based no-regret buyers, [BMSW18] can extract revenue that is an arbitrarily large fraction of the bidder\u2019s expected value. Our Theorem 4 extends this result to the multiple buyer setting, overcoming novel technical and conceptual challenges. Second, [BMSW18] designs a novel (not mean-based) learning algorithm against which the optimal mechanism for the seller is simply Myerson\u2019s auction in each round. Their proof of this result naturally accommodates multiple buyers. Finally, [BMSW18] shows that if the buyer is clever and mean-based no regret (where they do not overbid their value), then the optimal auction has a clean tractable format (pay-your-bid with declining reserve over time). As we have discussed in the \u201cNo Overbidding\u201d section of the introduction, our work shows several formal barriers in extending these results to multiple buyers. In summary, our main result extends their first main result to multiple bidders. Their second result already holds for multiple bidders (so there is nothing for us to extend). Our secondary results establish formal barriers to extending their final main result to multiple bidders.\n\nTwo recent follow-ups have extended the setting in [BMSW18] in a different direction. First, [DSS19b] considers the problem of playing a two-player game against a no-regret learner. While technically not an auctions problem, there is thematic overlap with our main result. [DSS19a] extends the single-buyer results in [BMSW18] to be _prior-free_. Specifically, they show how to design auctions achieving the same guarantees as those in [BMSW18] but where the buyer\u2019s values are chosen adversarially. In comparison to these works, ours is the first to extend the model to consider multiple buyers.\n\nFinally, recent work of [CHJ20] considers interaction between a learning buyer and a _learning_ seller. Their seller does not have a prior against which to optimize, and instead itself targets a no-regret guarantee. In comparison, our seller (like the seller in all previously cited works) optimizes expected revenue with respect to a prior.\n\n## 2 Preliminaries\n\nWe consider the same setting as [BMSW18], extended to multiple buyers. Specifically, there are \\(n\\) buyers and \\(T\\) rounds. In each round, there is a single item for sale. Each buyer \\(i\\) has value \\(v_{i,t}\\) for the item during round \\(t\\), and each \\(v_{i,t}\\) is drawn from \\(\\mathcal{D}\\) independently (that is, the buyers are i.i.d., and the rounds are i.i.d. as well). For simplicity of exposition (and to match prior work), we assume \\(\\mathcal{D}\\) has finite support \\(0\\leq w_{1}&lt;w_{2}&lt;\\ldots&lt;w_{m}\\leq 1\\) and we define \\(q_{j}\\) to be the probability \\(w_{j}\\) is drawn from \\(\\mathcal{D}\\).\n\nEach round, the seller presents \\(K\\) arms for the buyers. Each arm is labeled with a bid, and we assume that one of the arms is labeled with 0 (to represent a bid of \u201cdon\u2019t participate\u201d). Note that the same set of arms is presented to all buyers, and the same set of arms is presented in each round.\n\nIn each round \\(t\\), the seller defines an anonymous auction. Specifically, for all \\(i,t\\), the seller defines "
    ]
  },
  {
    "edit": [
      "but consistent with a model where only the gravitational potential of the gas is considered. To investigate the impact of the SMBH on the kinematics of J0109\u20133047 further, we construct a simple \u201cdispersion\u2013dominated + SMBH\u201d model in QUBEFit . In this model the velocity dispersion is the sum of the SMBH component of Eq. 1 and a constant dispersion value throughout the quasar host (\u03c3\\({}_{\\rm CII,tot}^{2}=\\sigma_{\\rm CII,SMBH}(r)^{2}+\\sigma_{\\rm const}^{2}\\)). The intensity profile is assumed to be exponentially declining as in the constant dispersion model (see Section 3 ). We note that any additional contribution (besides the central SMBH) to the kinematics as a constant is in agreement with the inferred gas mass profile (see Fig. 4 ). In Fig. 5 we show the best-fit dispersion fields for different SMBH masses from 10\\({}^{8}\\)\\(M_{\\odot}\\) to 10\\({}^{9}\\)\\(M_{\\odot}\\). We find that the [C II ] kinematics of J0109\u20133047 are clearly incompatible with a \u223c 10\\({}^{9}\\)\\(M_{\\odot}\\) SMBH. A full MCMC fit of the model to the data yields an upper limit of \\(M_{\\rm SMBH}&lt;6.5\\times 10^{8}\\)\\(M_{\\odot}\\)(2\u03c3) (see Appendix A for the full posterior distribution of the model parameters). However, even for the maximum-likelihood model ( M\\({}_{\\rm BH}=2.4\\times 10^{8}\\)\\(M_{\\odot}\\)), the BIC is slightly higher than for a model without a black hole (\u2206BIC = 5.2), showing that any SMBH contribution to the dispersion velocity field is disfavored by the observations. One way to alleviate the tension with the rest-frame UV mass measurement could be to change the radial [C II ]\u2013emitting gas profile. By definition, the observed [C II ] kinematics are a luminosity\u2013weighted, beam\u2013 convolved realisation of the intrinsic kinematics. Following Eq. 1 , the velocity dispersion increases exponentially close to the black hole, and due to the exponential intensity profile, these inner regions will contribute more to the beam\u2013convolved velocity dispersion measurement in the center. As a result, the observed velocity dispersion could be reduced, if the [C II ] intensity profile is not increasing close to the black hole (for example due to feedback). To address this further, we have used a toy model where the gas density profile follows an exponentially declining profile with a central gap where the [C II ] emission is null. As in the fiducial model, the velocity dispersion is composed of the SMBH component and a constant. We use this simple model to calculate the size of the central gap necessary to \u201chide\u201d the SMBH impact on the [C II ] kinematics tracer. We find that, for a SMBH with a fixed mass M\\({}_{\\rm BH}=1.1\\times 10^{9}M_{\\odot}\\), the best-fit central gap is constrained to be r &lt; 22 pc (2\u03c3) to reproduce the [C II ] profile and kinematics. The best-fit model has r\\({}_{\\rm pc}=0.015^{+0.015}_{-0.010}\\) pc (see Appendix A ), and is formally ruled out with an increased \u2206BIC = 10.42 compared to the model without a gap. Moreover, a central gap in the gas distribution would be at odds with simulations and observations where the central region contains up to \u223c 10 times the mass of the BH in gas (e.g., Lupi et al. 2022 ; Walter et al. 2022 ). In summary, the flat velocity dispersion profile implies a flat radial mass density profile. The constant dispersion implies that the underlying mass distribution is not centrally peaked, consistent with the expectations of the gas mass distribution derived from the far-infrared continuum emission under standard assumptions. This leaves only few alternatives to explain the absence of a central peak in the velocity dispersion. One possibility is that the gas mass decreases in the central 200 pc in order to compensate the presence of a 0.6 \u2212 1.1 \u00d7 10\\({}^{9}\\)\\(M_{\\odot}\\) black hole and produce a flat mass profile. However, we have previously excluded the presence of a central gap in the [C II ]\u2013emitting gas, and the FIR continuum shows no sign of a central gap either. A decrease in the central gas mass would imply fine-tuning of the physical properties of the ISM at the "
    ],
    "kosmos": [
      "but consistent with a model where only the gravitational potential of the gas is considered.\n\nTo investigate the impact of the SMBH on the kinematics of J0109-3047 further, we construct a simple \u201cdispersion-dominated + SMBH\u201d model in *QUBEFit*. In this model the velocity dispersion is the sum of the SMBH component of Eq. 1 and a constant dispersion value throughout the quasar host (\u03c3\\({}_{\\rm CII,tot}^{2}=\\sigma_{\\rm CII,SMBH}(r)^{2}+\\sigma_{\\rm const}^{2}\\)). The intensity profile is assumed to be exponentially declining as in the constant dispersion model (see Section 3). We note that any additional contribution (besides the central SMBH) to the kinematics as a constant is in agreement with the inferred gas mass profile (see Fig. 4).\n\nIn Fig. 5 we show the best-fit dispersion fields for different SMBH masses from 10\\({}^{8}\\)\\(M_{\\odot}\\) to 10\\({}^{9}\\)\\(M_{\\odot}\\). We find that the [C II] kinematics of J0109-3047 are clearly incompatible with a \u223c 10\\({}^{9}\\)\\(M_{\\odot}\\) SMBH. A full MCMC fit of the model to the data yields an upper limit of \\(M_{\\rm SMBH}&lt;6.5\\times 10^{8}\\)\\(M_{\\odot}\\)(2\\(\\sigma\\)) (see Appendix A for the full posterior distribution of the model parameters). However, even for the maximum-likelihood model (\\(M_{\\rm BH}=2.4\\times 10^{8}\\)\\(M_{\\odot}\\)), the BIC is slightly higher than for a model without a black hole (\u2206BIC = 5.2), showing that any SMBH contribution to the dispersion velocity field is disfavored by the observations.\n\nOne way to alleviate the tension with the rest-frame UV mass measurement could be to change the radial [C II]\u2013emitting gas profile. By definition, the observed [C II] kinematics are a luminosity-weighted, beam-convolved realisation of the intrinsic kinematics. Following Eq. 1, the velocity dispersion increases exponen- tially close to the black hole, and due to the exponential intensity profile, these inner regions will contribute more to the beam-convolved velocity dispersion measurement in the center. As a result, the observed velocity dispersion could be reduced, if the [C II] intensity profile is not increasing close to the black hole (for example due to feedback).\n\nTo address this further, we have used a toy model where the gas density profile follows an exponentially declining profile with a central gap where the [C II] emission is null. As in the fiducial model, the velocity dispersion is composed of the SMBH component and a constant. We use this simple model to calculate the size of the central gap necessary to \u201chide\u201d the SMBH impact on the [C II] kinematics tracer. We find that, for a SMBH with a fixed mass \\(M_{\\rm BH}=1.1\\times 10^{9}M_{\\odot}\\), the best-fit central gap is constrained to be \\(r&lt;22\\) pc (2\\(\\sigma\\)) to reproduce the [C II] profile and kinematics. The best-fit model has \\(r_{\\rm pc}=0.015^{+0.015}_{-0.010}\\) pc (see Appendix A), and is formally ruled out with an increased \u2206BIC = 10.42 compared to the model without a gap. Moreover, a central gap in the gas distribution would be at odds with simulations and observations where the central \u223c 400 \u2212 500 pc region contains up to \u223c 10 times the mass of the BH in gas (e.g., Lupi et al., 2022; Walter et al., 2022).\n\nIn summary, the flat velocity dispersion profile implies a flat radial mass density profile. The constant dispersion implies that the underlying mass distribution is not centrally peaked, consistent with the expectations of the gas mass distribution derived from the far-infrared continuum emission under standard assumptions. This leaves only few alternatives to explain the absence of a central peak in the velocity dispersion. One possibility is that the gas mass decreases in the central 200 pc in order to compensate the presence of a \\(0.6-1.1\\times 10^{9}\\)\\(M_{\\odot}\\) black hole and produce a flat mass profile. However, we have previously excluded the presence of a central gap in the [C II]\u2013emitting gas, and the FIR continuum shows no sign of a central gap either. A decrease in the central gas mass would imply fine-t "
    ]
  },
  {
    "edit": [
      "the particular case of a Lie group. In what follows, we will show the previous reduction process in the particular case when the initial manifold \\(Q\\) is a Lie group \\(G\\). In such a case, one may use the left trivialization of the cotangent bundle \\(T^{*}G\\) in order to identify \\(T^{*}G\\) with the product manifold \\(G\\times\\mathfrak{g}^{*}\\), where \\((\\mathfrak{g},[\\cdot,\\cdot]_{\\mathfrak{g}})\\) is the Lie algebra of \\(G\\), in such a way that the canonical projection \\(\\tau_{G}^{*}:T^{*}G\\to G\\) is just the first projection \\(p_{1}:G\\times\\mathfrak{g}^{*}\\to G\\). The left action \\(\\Phi:G\\times G\\to G\\) on \\(G\\) is the one defined by the group operation of \\(G\\). We take the left invariant vector field \\(Y=\\xi\\) on \\(G\\) induced by an element \\(\\xi\\) of \\(\\mathfrak{g}\\). In the first reduction with the cotangent lift of \\(\\Phi\\), the reduced space is \\((T^{*}G-0_{G})/G\\cong\\mathfrak{g}^{*}-\\{0\\}\\) and the reduced function induced by \\(Y\\) is the restriction to \\(\\mathfrak{g}^{*}-\\{0\\}\\) of the linear map \\(\\xi^{\\ell}\\) associated with \\(\\xi\\in\\mathfrak{g}\\), i.e.\n\n\\[\\xi^{\\ell}:\\mathfrak{g}^{*}-\\{0\\}\\to\\mathbb{R},\\quad\\xi^{\\ell}(\\alpha)=\\alpha( \\xi).\\]\n\nOn the other hand, the Lie-Poisson bracket \\(\\{\\cdot,\\cdot\\}_{\\mathfrak{g}^{*}}\\) on \\((T^{*}G-0_{G})/G\\cong\\mathfrak{g}^{*}-\\{0\\}\\) is characterized by\n\n\\[\\{\\xi_{1}^{\\ell},\\xi_{2}^{\\ell}\\}_{\\mathfrak{g}^{*}}=-[\\xi_{1},\\xi_{2}]_{ \\mathfrak{g}^{*}}^{\\ell},\\quad\\text{for all }\\xi_{1},\\xi_{2}\\in\\mathfrak{g}.\\]\n\nThe scaling symmetry on \\(\\mathfrak{g}^{*}-\\{0\\}\\) is just\n\n\\[\\phi^{G}:(\\mathbb{R}-\\{0\\})\\times(\\mathfrak{g}^{*}-\\{0\\})\\to(\\mathfrak{g}^{*}- \\{0\\}),\\qquad(s,\\alpha)\\to s\\alpha. \\tag{40}\\]\n\nNow, we apply the second reduction step to the (Lie)-Poisson Hamiltonian system \\((\\mathfrak{g}^{*}-\\{0\\},\\{\\cdot,\\cdot\\}_{\\mathfrak{g}^{*}},\\xi^{\\ell})\\), with respect to the scaling symmetry \\(\\phi^{G}\\). In this case, the reduced space is the projective space \\(\\mathbb{P}\\mathfrak{g}^{*}\\). The corresponding line bundle \\(\\pi_{L}:L:=(\\mathfrak{g}^{*}-\\{0\\}\\times\\mathbb{R})/(\\mathbb{R}-\\{0\\})\\to \\mathbb{P}\\mathfrak{g}^{*}\\) is defined by the action\n\n\\[\\widetilde{\\phi}^{G}:(\\mathbb{R}-\\{0\\})\\times((\\mathfrak{g}^{*}-\\{0\\})\\times \\mathbb{R})\\to(\\mathfrak{g}^{*}-\\{0\\})\\times\\mathbb{R},\\qquad\\widetilde{\\phi}^ {G}_{s}(\\alpha,t)=(s\\alpha,\\frac{t}{s}).\\]\n\nThe section of the dual line bundle \\(\\pi_{L^{*}}:L^{*}\\to\\mathbb{P}\\mathfrak{g}^{*}\\) associated with the linear map \\(\\xi^{\\ell}:\\mathfrak{g}^{*}-\\{0\\}\\to\\mathbb{R}\\) is\n\n\\[h_{\\xi}(p(\\alpha))([(\u03b1,t)])=t\\alpha(\\xi),\\ "
    ],
    "kosmos": [
      "the particular case of a Lie group. In what follows, we will show the previous reduction process in the particular case when the initial manifold \\(Q\\) is a Lie group \\(G\\). In such a case, one may use the left trivialization of the cotangent bundle \\(T^{*}G\\) in order to identify \\(T^{*}G\\) with the product manifold \\(G\\times\\mathfrak{g}^{*}\\), where \\((\\mathfrak{g},[\\cdot,\\cdot]_{\\mathfrak{g}})\\) is the Lie algebra of \\(G\\), in such a way that the canonical projection \\(\\tau_{G}^{*}:T^{*}G\\to G\\) is just the first projection \\(p_{1}:G\\times\\mathfrak{g}^{*}\\to G\\). The left action \\(\\Phi:G\\times G\\to G\\) on \\(G\\) is the one defined by the group operation of \\(G\\). We take the left invariant vector field \\(Y=\\xi\\) on \\(G\\) induced by an element \\(\\xi\\) of \\(\\mathfrak{g}\\). In the first reduction with the cotangent lift of \\(\\Phi\\), the reduced space is \\((T^{*}G-0_{G})/G\\cong\\mathfrak{g}^{*}-\\{0\\}\\) and the reduced function induced by \\(Y\\) is the restriction to \\(\\mathfrak{g}^{*}-\\{0\\}\\) of the linear map \\(\\xi^{\\ell}\\) associated with \\(\\xi\\in\\mathfrak{g}\\), i.e.\n\n\\[\\xi^{\\ell}:\\mathfrak{g}^{*}-\\{0\\}\\to\\mathbb{R},\\quad\\xi^{\\ell}(\\alpha)=\\alpha( \\xi).\\]\n\nOn the other hand, the Lie-Poisson bracket \\(\\{\\cdot,\\cdot\\}_{\\mathfrak{g}^{*}}\\) on \\((T^{*}G-0_{G})/G\\cong\\mathfrak{g}^{*}-\\{0\\}\\) is characterized by\n\n\\[\\{\\xi_{1}^{\\ell},\\xi_{2}^{\\ell}\\}_{\\mathfrak{g}^{*}}=-[\\xi_{1},\\xi_{2}]_{ \\mathfrak{g}^{*}}^{\\ell},\\quad\\text{for all }\\xi_{1},\\xi_{2}\\in\\mathfrak{g}.\\]\n\nThe scaling symmetry on \\(\\mathfrak{g}^{*}-\\{0\\}\\) is just\n\n\\[\\phi^{G}:(\\mathbb{R}-\\{0\\})\\times(\\mathfrak{g}^{*}-\\{0\\})\\to(\\mathfrak{g}^{*}-\\{ 0\\}),\\qquad(s,\\alpha)\\to s\\alpha. \\tag{40}\\]\n\nNow, we apply the second reduction step to the (Lie)-Poisson Hamiltonian system \\((\\mathfrak{g}^{*}-\\{0\\},\\{\\cdot,\\cdot\\}_{\\mathfrak{g}^{*}},\\xi^{\\ell})\\), with respect to the scaling symmetry \\(\\phi^{G}\\). In this case, the reduced space is the projective space \\(\\mathbb{P}\\mathfrak{g}^{*}\\). The corresponding line bundle \\(\\pi_{L}:L:=(\\mathfrak{g}^{*}-\\{0\\}\\times\\mathbb{R})/(\\mathbb{R}-\\{0\\})\\to \\mathbb{P}\\mathfrak{g}^{*}\\) is defined by the action\n\n\\[\\widetilde{\\phi}^{G}:(\\mathbb{R}-\\{0\\})\\times((\\mathfrak{g}^{*}-\\{0\\})\\times \\mathbb{R})\\to(\\mathfrak{g}^{*}-\\{0\\})\\times\\mathbb{R},\\qquad\\widetilde{\\phi}^ {G}_{s}(\\alpha,t)=(s\\alpha,\\frac{t}{s}).\\]\n\nThe section of the dual line bundle \\(\\pi_{L^{*}}:L^{*}\\to\\mathbb{P}\\mathfrak{g}^{*}\\) associated with the linear map \\(\\xi^{\\ell}:\\mathfrak{g}^{*}-\\{0\\}\\to\\mathbb{R}\\) is\n\n\\[h_{\\xi}(p(\\alpha))([(\u03b1,t)])=t\\alpha(\\xi), "
    ]
  },
  {
    "edit": [
      "<table>\n<thead>\n<tr>\n<th>\nModels\n</th>\n<th>\nData\n</th>\n<th>\nSuccess\n</th>\n<th>\nDiff.\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\nCC-Net [28]\n</td>\n<td>\n2.4M\n</td>\n<td>\n32.0%\n</td>\n<td>\n\u2013\n</td>\n</tr>\n<tr>\n<td>\nWebN-T5-XL [24]\n</td>\n<td>\n12K\n</td>\n<td>\n48.4%\n</td>\n<td>\n\u2013\n</td>\n</tr>\n<tr>\n<td>\nLongT5-Base\n</td>\n<td></td>\n<td>\n53.8%\n</td>\n<td>\n0.0\n</td>\n</tr>\n<tr>\n<td>\nLongT5-Large\n</td>\n<td>\n12K\n</td>\n<td>\n56.3%\n</td>\n<td>\n0.0\n</td>\n</tr>\n<tr>\n<td>\nLongT5-XL\n</td>\n<td></td>\n<td>\n60.4%\n</td>\n<td>\n0.0\n</td>\n</tr>\n<tr>\n<td>\nFlan-LongT5-Base\n</td>\n<td></td>\n<td>\n54.1%\n</td>\n<td>\n+0.3\n</td>\n</tr>\n<tr>\n<td>\nFlan-LongT5-Large\n</td>\n<td>\n12K\n</td>\n<td>\n56.1%\n</td>\n<td>\n-0.2\n</td>\n</tr>\n<tr>\n<td>\nFlan-LongT5-XL\n</td>\n<td></td>\n<td>\n61.1%\n</td>\n<td>\n+0.7\n</td>\n</tr>\n<tr>\n<td>\nHTML-T5-Base (ours)\n</td>\n<td></td>\n<td>\n57.0%\n</td>\n<td>\n+3.2\n</td>\n</tr>\n<tr>\n<td>\nHTML-T5-Large (ours)\n</td>\n<td>\n12K\n</td>\n<td>\n60.8%\n</td>\n<td>\n+4.5\n</td>\n</tr>\n<tr>\n<td>\nHTML-T5-XL (ours)\n</td>\n<td></td>\n<td>\n63.3%\n</td>\n<td>\n+2.9\n</td>\n</tr>\n<tr>\n<td>\nFlan-T5-XL [19]\n</td>\n<td>\n347K\n</td>\n<td>\n75.5%\n</td>\n<td>\n\u2013\n</td>\n</tr>\n<tr>\n<td>\nFlan-T5-XXL [19]\n</td>\n<td></td>\n<td>\n79.0%\n</td>\n<td>\n\u2013\n</td>\n</tr>\n<tr>\n<td>\nHTML-T5-XL (ours)\n</td>\n<td>\n347K\n</td>\n<td>\n79.4%\n</td>\n<td>\n\u2013\n</td>\n</tr>\n</tbody>\n</table>\n\nTable 4: Average success rate of MiniWoB++ with 56 tasks. We use 12K demonstrations [42], and compare HTML-T5 among supervised-finetuned baselines [24, 28]. HTML-T5-XL remarkably outperforms WebN-T5-XL, the prior best method, by 14.9%, and HTML-denoising improves the success rate better than instruction tuning. We also finetune HTML-T5 with 347K expert traces [19], which performs better than Flan-T5-XXL (11B parameters) even with 3B parameters. See Appendix H for the detailed results.\n\ncommunity filtered by Tutorial tag on social media web-*\n\nsite?*), and acts via planning, summarizing by HTML-T5, and then programming by Flan-U-PaLM. See Appendix C for the example workflow. We finetune HTML-T5 with traces that are collected using scripted agents by procedurally generating instructions from human curated templates. This results in 260 episodes on real estate website and 230 episodes on social media website (about 20/10 steps per episode respectively). We prepare 20 different natural language instructions, and measure the success rate and score for the evaluation. The score represents the percentage of required attributes covered during the episode [ 81 ]; for instance (1) apartments for (2) corporate housing with (3) studio bedroom and (4) 1+ bathroom located in (5) oroville, ca . When the agents could search the housing satisfying (1), (2), (5) and not (3), (4), the score would be 60 ( = 100 \u00d7 3 / 5). When the agents could achieve 100 score, that episode would mark as success. For comparison, we prepare three baselines, consisting of partial plug-in language models and a single LLM prompting different examplers per role: "
    ],
    "kosmos": [
      "<table>\n<thead>\n<tr>\n<th>\nModels\n</th>\n<th>\nData\n</th>\n<th>\nSuccess\n</th>\n<th>\nDiff.\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\nCC-Net [28]\n</td>\n<td>\n2.4M\n</td>\n<td>\n32.0%\n</td>\n<td>\n\u2013\n</td>\n</tr>\n<tr>\n<td>\nWebN-T5-XL [24]\n</td>\n<td>\n12K\n</td>\n<td>\n48.4%\n</td>\n<td>\n\u2013\n</td>\n</tr>\n<tr>\n<td>\nLongT5-Base\n</td>\n<td></td>\n<td>\n53.8%\n</td>\n<td>\n0.0\n</td>\n</tr>\n<tr>\n<td>\nLongT5-Large\n</td>\n<td>\n12K\n</td>\n<td>\n56.3%\n</td>\n<td>\n0.0\n</td>\n</tr>\n<tr>\n<td>\nLongT5-XL\n</td>\n<td></td>\n<td>\n60.4%\n</td>\n<td>\n0.0\n</td>\n</tr>\n<tr>\n<td>\nFlan-LongT5-Base\n</td>\n<td></td>\n<td>\n54.1%\n</td>\n<td>\n+0.3\n</td>\n</tr>\n<tr>\n<td>\nFlan-LongT5-Large\n</td>\n<td>\n12K\n</td>\n<td>\n56.1%\n</td>\n<td>\n-0.2\n</td>\n</tr>\n<tr>\n<td>\nFlan-LongT5-XL\n</td>\n<td></td>\n<td>\n61.1%\n</td>\n<td>\n+0.7\n</td>\n</tr>\n<tr>\n<td>\nHTML-T5-Base (ours)\n</td>\n<td></td>\n<td>\n57.0%\n</td>\n<td>\n+3.2\n</td>\n</tr>\n<tr>\n<td>\nHTML-T5-Large (ours)\n</td>\n<td>\n12K\n</td>\n<td>\n60.8%\n</td>\n<td>\n+4.5\n</td>\n</tr>\n<tr>\n<td>\nHTML-T5-XL (ours)\n</td>\n<td></td>\n<td>\n63.3%\n</td>\n<td>\n+2.9\n</td>\n</tr>\n<tr>\n<td>\nFlan-T5-XL [19]\n</td>\n<td>\n347K\n</td>\n<td>\n75.5%\n</td>\n<td>\n\u2013\n</td>\n</tr>\n<tr>\n<td>\nFlan-T5-XXL [19]\n</td>\n<td></td>\n<td>\n79.0%\n</td>\n<td>\n\u2013\n</td>\n</tr>\n<tr>\n<td>\nHTML-T5-XL (ours)\n</td>\n<td>\n347K\n</td>\n<td>\n79.4%\n</td>\n<td>\n\u2013\n</td>\n</tr>\n</tbody>\n</table>\n\nTable 4: Average success rate of MiniWoB++ with 56 tasks. We use 12K demonstrations [42], and compare HTML-T5 among supervised-finetuned baselines [24, 28]. HTML-T5-XL remarkably outperforms WebN-T5-XL, the prior best method, by 14.9%, and HTML-denoising improves the success rate better than instruction tuning. We also finetune HTML-T5 with 347K expert traces [19], which performs better than Flan-T5-XXL (11B parameters) even with 3B parameters. See Appendix H for the detailed results.\n\ncommunity filtered by Tutorial tag on social media website?*), and acts via planning, summarizing by HTML-T5, and then programming by Flan-U-PaLM. See Appendix C for the example workflow. We finetune HTML-T5 with traces that are collected using scripted agents by procedurally generating instructions from human curated templates. This results in 260 episodes on real estate website and 230 episodes on social media website (about 20/10 steps per episode respectively).\n\nWe prepare 20 different natural language instructions, and measure the success rate and score for the evaluation. The score represents the percentage of required attributes covered during the episode [81]; for instance (1) apartments for (2) corporate housing with (3) studio bedroom and (4) 1+ bathroom located in (5) oroville, ca. When the agents could search the housing satisfying (1), (2), (5) and not (3), (4), the score would be 60 ( = 100 \u00d7 3 / 5). When the agents could achieve 100 score, that episode would mark as success.\n\n**Results** For comparison, we prepare three baselines, consisting of partial plug-in language models and a single LLM prompting different examplers per role: "
    ]
  },
  {
    "edit": [
      "* [JX23b] Zhuchao Ji and Junyi Xie. Homoclinic orbits, multiplier spectrum and rigidity theorems in complex dynamics. _Forum Math. Pi_, 11:Paper No. e11, 37, 2023.\n* [Lev14] A. Levy. Aim workshop postcritically finite maps in complex and arithmetic dynamics, 2014.\n* [McM87] Curt McMullen. Families of rational maps and iterative root-finding algorithms. _Ann. of Math. (2)_, 125(3):467-493, 1987.\n* [Mil06] John Milnor. On Lattes maps. _Dynamics on the Riemann Sphere: A Bodil Branner Festschrift_, page 9, 2006.\n* [MS14] Alice Medvedev and Thomas Scanlon. Invariant varieties for polynomial dynamical systems. _Ann. of Math. (2)_, 179(1):81-177, 2014.\n* [Nar04] Wladys\ufffdl aw Narkiewicz. _Elementary and analytic theory of algebraic numbers_. Springer Monographs in Mathematics. Springer-Verlag, Berlin, third edition, 2004.\n* [Pak23] Fedor Pakovich. Invariant curves for endomorphisms of \\(\\mathbb{P}^{1}\\times\\mathbb{P}^{1}\\). _Math. Ann._, 385(1-2):259-307, 2023.\n* [Poo17] Bjorn Poonen. _Rational points on varieties_, volume 186 of _Graduate Studies in Mathematics_. American Mathematical Society, Providence, RI, 2017.\n* [Sil98] Joseph H. Silverman. The space of rational maps on \\(\\mathbb{P}^{1}\\). _Duke Math. J._, 94(1):41-77, 1998.\n* [Sil07] Joseph H. Silverman. _The arithmetic of dynamical systems_, volume 241 of _Graduate Texts in Mathematics_. Springer-Verlag, New York, 2007.\n* [Sil12] Joseph H. Silverman. _Moduli spaces and arithmetic dynamics_, volume 30 of _CRM Monograph Series_. American Mathematical Society, Providence, RI, 2012.\n* [Tuc14] T. Tucker. Problem 6 in the problem list of the aim workshop postcritically finite maps in complex and arithmetic dynamics, 2014.\n* [Xie17] Junyi Xie. The existence of Zariski dense orbits for polynomial endomorphisms of the affine plane. _Compos. Math._, 153(8):1658-1672, 2017.\n* [Xie22] Junyi Xie. The existence of Zariski dense orbits for endomorphisms of projective surfaces (with an appendix in collaboration with T. Tucker). _J. Amer. Math. Soc._, 2022. published online.\n* [Xie23] Junyi Xie. Remarks on algebraic dynamics in positive characteristic. _J. Reine Angew. Math._, 797:117-153, 2023.\n* [XY23] Junyi Xie and Xinyi Yuan. Partial heights and the geometric Bombieri-Lang conjecture. arXiv:2305.14789, 2023.\n* [Yua08] Xinyi Yuan. Big line bundles over arithmetic varieties. _Invent. Math._, 173(3):603-649, 2008.\n* [Zdu14] Anna Zdunik. Characteristic exponents of rational functions. _Bulletin of the Polish Academy of Sciences. Mathematics_, 62(3), 2014.\n* [Zha95] Shou-Wu Zhang. Small points and adelic metrics. _J. Algebraic Geom._, 4(2):281-300, 1995.\n* [Zha98] Shou-Wu Zhang. Equidistribution of small points on abelian varieties. _Ann.of Math. (2), 147(1998)_, 147:159-165, 1998.\n\nInstitute for Theoretical Sciences, Westlake University, Hangzhou 310030, China\n\n_Email address_: jizhuchao@westlake.edu.cn\n\nBeijing International Center for Mathematical Research, Peking University, Beijing 100871, China\n\n_Email address_: xiejunyi@bicmr.pku.edu.cn\n\nSchool of Mathematical Sciences, Peking University, Beijing 100871, China\n\n_Email address_: grzhang@stu.pku.edu.cn "
    ],
    "kosmos": [
      "* [JX23b] Zhuchao Ji and Junyi Xie. Homoclinic orbits, multiplier spectrum and rigidity theorems in complex dynamics. _Forum Math. Pi_, 11:Paper No. e11, 37, 2023.\n* [Lev14] A. Levy. Aim workshop postcritically finite maps in complex and arithmetic dynamics, 2014.\n* [McM87] Curt McMullen. Families of rational maps and iterative root-finding algorithms. _Ann. of Math. (2)_, 125(3):467-493, 1987.\n* [Mil06] John Milnor. On Lattes maps. _Dynamics on the Riemann Sphere: A Bodil Branner Festschrift_, page 9, 2006.\n* [MS14] Alice Medvedev and Thomas Scanlon. Invariant varieties for polynomial dynamical systems. _Ann. of Math. (2)_, 179(1):81-177, 2014.\n* [Nar04] Wladys\ufffdl aw Narkiewicz. _Elementary and analytic theory of algebraic numbers_. Springer Monographs in Mathematics. Springer-Verlag, Berlin, third edition, 2004.\n* [Pak23] Fedor Pakovich. Invariant curves for endomorphisms of \\(\\mathbb{P}^{1}\\times\\mathbb{P}^{1}\\). _Math. Ann._, 385(1-2):259-307, 2023.\n* [Poo17] Bjorn Poonen. _Rational points on varieties_, volume 186 of _Graduate Studies in Mathematics_. American Mathematical Society, Providence, RI, 2017.\n* [Sil98] Joseph H. Silverman. The space of rational maps on \\(\\mathbb{P}^{1}\\). _Duke Math. J._, 94(1):41-77, 1998.\n* [Sil07] Joseph H. Silverman. _The arithmetic of dynamical systems_, volume 241 of _Graduate Texts in Mathematics_. Springer-Verlag, New York, 2007.\n* [Sil12] Joseph H. Silverman. _Moduli spaces and arithmetic dynamics_, volume 30 of _CRM Monograph Series_. American Mathematical Society, Providence, RI, 2012.\n* [Tuc14] T. Tucker. Problem 6 in the problem list of the aim workshop postcritically finite maps in complex and arithmetic dynamics, 2014.\n* [Xie17] Junyi Xie. The existence of Zariski dense orbits for polynomial endomorphisms of the affine plane. _Compos. Math._, 153(8):1658-1672, 2017.\n* [Xie22] Junyi Xie. The existence of Zariski dense orbits for endomorphisms of projective surfaces (with an appendix in collaboration with T. Tucker). _J. Amer. Math. Soc._, 2022. published online.\n* [Xie23] Junyi Xie. Remarks on algebraic dynamics in positive characteristic. _J. Reine Angew. Math._, 797:117-153, 2023.\n* [XY23] Junyi Xie and Xinyi Yuan. Partial heights and the geometric Bombieri-Lang conjecture. arXiv:2305.14789, 2023.\n* [Yua08] Xinyi Yuan. Big line bundles over arithmetic varieties. _Invent. Math._, 173(3):603-649, 2008.\n* [Zdu14] Anna Zdunik. Characteristic exponents of rational functions. _Bulletin of the Polish Academy of Sciences. Mathematics_, 62(3), 2014.\n* [Zha95] Shou-Wu Zhang. Small points and adelic metrics. _J. Algebraic Geom._, 4(2):281-300, 1995.\n* [Zha98] Shou-Wu Zhang. Equidistribution of small points on abelian varieties. _Ann.of Math. (2), 147(1998)_, 147:159-165, 1998.\n\nInstitute for Theoretical Sciences, Westlake University, Hangzhou 310030, China\n\n_Email address_: jizhuchao@westlake.edu.cn\n\nBeijing International Center for Mathematical Research, Peking University, Beijing 100871, China\n\n_Email address_: xiejunyi@bicmr.pku.edu.cn\n\nSchool of Mathematical Sciences, Peking University, Beijing 100871, China\n\n_Email address_: grzhang@stu.pku.edu.cn "
    ]
  },
  {
    "edit": [
      "some additional components are added to the matter distribution due to which the number of unknowns grow that make more challenging to solve the Einstein field equations analytically. In this regard, such analytical solutions developed via gravitational decoupling (GD) with minimal geometric deformation (MGD) method in both cosmology and astrophysics [21, 22]. Multiple methods studied to investigate important properties of self-gravitating objects, including the phenomenon of stability and hydrodynamic equilibrium, the upper limit of the mass-to-radius ratio, the upper limit of superficial redshift, and dynamics of matter content under energy conditions, etc. [23]. One of these techniques is MGD approach, which was initially intended as an optional means of deforming Schwarzschild space-time in framework of the Randall-Sundrum braneworld [24, 25]. Recently, there is a lot of interest in developing novel analytic and an anisotropic solutions for Einstein field equations, which is a difficult task as Einstein field equations are non-linear and difficult to handle. In this way, the method of MGD to gain new models representing relativistic objects with well-determined characteristics have been proposed [26]. For a compact spherical distribution, the analytical solution of an anisotropic fluid as well as the braneworld model of Tolman IV solution have been found [27]. Two essential components are required in which first one is dimensionless coupling constant \u201c**\u03b1**\u201d to incorporate an extra source into the stress-energy tensor of seed solution. Second one is MGD method on the metric potentials (often on the radial component of metric) in the context of braneworld model. If the seed solution is assumed to be anisotropic, the inclusion of this additional component combined with a static and spherically-symmetric system gives rise to a complex simultaneous equations. The MGD technique separates Einstein field equations into two systems, namely the \u201cEinstein system\u201d and \u201cquasi-Einstein system\u201d, which in comparison to the original system are easy to solve. At this point, a few observations are appropriate, firstly, the decoupled systems satisfy Bianchi Identities and secondly, the extra source may be a scalar, vector, or tensor field [28\u201332]. Moreover, a number of interesting results on the solutions of black hole with 2+1 and 3+1 decomposition obtained in [33\u201336]. Additionally, the solutions of new hairy black hole have just been explored [37], and a mechanism is created as well to turn any non-rotational black hole into a rotational one [38, 39]. When weak gravitational forces are at work, the hypothesis in GR has effectively aligned with many tests carried out within the solar system, demonstrating its success in cosmology. To get more accurate and dependable results, this theory may need to be modified when dealing with high gravitational fields or while being observed on a big scale. These changes may be very important in explaining the phenomena of accelerated expansion. These modifications are termed as modified theories (see, for instance [40\u201347]) Many modified theories of gravity [48\u201352] are taken into account by changing the Einstein-Hilbert action that is frequently used to study both the existence of dark energy and dark matter as well as the mystery of universe rapid expansion. Geometrical representation and scalar tensor representation of \\(f(G,T)\\) gravity has been presented to establish novel junction condition [53]. Several researchers are interested to explore the gravitational collapse phenomena because it is a prominent case in a strong-field regime [54\u201356]. Jordan [57] developed a full gravitational theory which gave the title of a gravitational scalar field to gravitational constant. Brans and Dicke [58] developed a scalar-tensor field theory named as the BransDicke (BD) theory obtained by substituting a time modifying constant \\(G(t)\\) and with the help of a scalar field (\\(\\Phi\\)) having interaction along with the geometry. Additionally, the well-known scalar field coupling constant or parameter (\u03c9\\({}_{BD}\\)) of the BD theory is a constant that can be adjusted to get the desired outcomes in Jordan frame. It is assumed that the \u03a6 is reciprocal of the dynamical gravitational constant, i.e., \\(G(t)=\\frac{1}{\\Phi(t)}\\). The test particles travel along geodesics according to the BD theory, they consequently obey the weak equivalence principle, which states that the gravitational mass and inertial mass are equivalent. Mach principle, agreement with the weak equivalence principle, and Dirac\u2019s large number hypothesis are the main ingredients of the BD theory. This theory includes a metric tensor and a scalar field that describes gravity. The large value of \u03a6 describes the fast expansion of the universe, is found by recent study in cosmology, including the redshift and distance-luminosity connection of type Ia Supernovae [59]. The evidence for various cosmic concerns, including the late behavior of the universe, cosmic acceleration, and the inflation issue, etc are also supported by the BD theory [60]. This theory has "
    ],
    "kosmos": [
      "some additional components are added to the matter distribution due to which the number of unknowns grow that make more challenging to solve the Einstein field equations analytically. In this regard, such analytical solutions developed via gravitational decoupling (GD) with minimal geometric deformation (MGD) method in both cosmology and astrophysics [21, 22].\n\nMultiple methods studied to investigate important properties of self-gravitating objects, including the phenomenon of stability and hydrodynamic equilibrium, the upper limit of the mass-to-radius ratio, the upper limit of super\ufb01cial redshift, and dynamics of matter content under energy conditions, etc. [23]. One of these techniques is MGD approach, which was initially intended as an optional means of deforming Schwarzschild space-time in framework of the Randall-Sundrum braneworld [24, 25]. Recently, there is a lot of interest in developing novel analytic and an anisotropic solutions for Einstein field equations, which is a difficult task as Einstein field equations are non-linear and difficult to handle. In this way, the method of MGD to gain new models representing relativistic objects with well-determined characteristics have been proposed [26]. For a compact spherical distribution, the analytical solution of an anisotropic fluid as well as the braneworld model of Tolman IV solution have been found [27]. Two essential components are required in which first one is dimensionless coupling constant \u201c**\u03b1**\u201d to incorporate an extra source into the stress-energy tensor of seed solution. Second one is MGD method on the metric potentials (often on the radial component of metric) in the context of braneworld model. If the seed solution is assumed to be anisotropic, the inclusion of this additional component combined with a static and spherically-symmetric system gives rise to a complex simultaneous equations. The MGD technique separates Einstein field equations into two systems, namely the \u201cEinstein system\u201d and \u201cquasi-Einstein system\u201d, which in comparison to the original system are easy to solve. At this point, a few observations are appropriate, firstly, the decoupled systems satisfy Bianchi Identities and secondly, the extra source may be a scalar, vector, or tensor field [28, 29, 30, 31, 32]. Moreover, a number of interesting results on the solutions of black hole with 2+1 and 3+1 decomposition obtained in [33, 34, 35, 36]. Additionally, the solutions of new hairy black hole have just been explored [37], and a mechanism is created as well to turn any non-rotational black hole into a rotational one [38, 39].\n\nWhen weak gravitational forces are at work, the hypothesis in GR has effectively aligned with many tests carried out within the solar system, demonstrating its success in cosmology. To get more accurate and dependable results, this theory may need to be modified when dealing with high gravitational fields or while being observed on a big scale. These changes may be very important in explaining the phenomena of accelerated expansion. These modifications are termed as modified theories (see, for instance [40, 41, 42, 43, 44, 45, 46, 47])\n\nMany modified theories of gravity [48, 49, 50, 51, 52] are taken into account by changing the Einstein-Hilbert action that is frequently used to study both the existence of dark energy and dark matter as well as the mystery of universe rapid expansion. Geometrical representation and scalar tensor representation of \\(f(G,T)\\) gravity has been presented to establish novel junction condition [53].\n\nSeveral researchers are interested to explore the gravitational collapse phenomena because it is a prominent case in a strong-field regime [54, 55, 56]. Jordan [57] developed a full gravitational theory which gave the title of a gravitational scalar field to gravitational constant. Brans and Dicke [58] developed a scalar-tensor field theory named as the BransDicke (BD) theory obtained by substituting a time modifying constant \\(G(t)\\) and with the help of a scalar field (\\(\\Phi\\)) having interaction along with the geometry. Additionally, the well-known scalar field coupling constant or parameter (\\(\\omega_{BD}\\)) of the BD theory is a constant that can be adjusted to get the desired outcomes in Jordan frame. It is assumed that the \\(\\Phi\\) is reciprocal of the dynamical gravitational constant, i.e., \\(G(t)=\\frac{1}{\\Phi(t)}\\). The test particles travel along geodesics according to the BD theory, they consequently obey the weak equivalence principle, which states that the gravitational mass and inertial mass are equivalent. Mach principle, agreement with the weak equivalence principle, and Dirac\u2019s large number hypothesis are the main ingredients of the BD theory. This theory includes a metric tensor and a scalar field that describes gravity.\n\nThe large value of \\(\\Phi\\) describes the fast expansion of the universe, is found by recent study in cosmology, "
    ]
  },
  {
    "edit": [
      "\n\n# Adaptive Low Rank Adaptation of Segment Anything to Salient Object Detection\n\nRuikai Cui, Siyuan He, and Shi Qiu\n\nAustralian National University\n\n{ruikai.cui, siyuan.he, shi.qiu}@anu.edu.au\n\n###### Abstract\n\nFoundation models, such as OpenAI\u2019s GPT-3 and GPT-4, Meta\u2019s LLaMA, and Google\u2019s PaLM2, have revolutionized the field of artificial intelligence. A notable paradigm shift has been the advent of the Segment Anything Model (SAM), which has exhibited a remarkable capability to segment real-world objects, trained on 1 billion masks and 11 million images. Although SAM excels in general object segmentation, it lacks the intrinsic ability to detect salient objects, resulting in suboptimal performance in this domain. To address this challenge, we present the Segment Salient Object Model (SSOM), an innovative approach that adaptively fine-tunes SAM for salient object detection \u00b7 large-scale pre-trained models \u00b7 parameter-efficient fine-tuning.\n\nKeywords:salient object detection large-scale pre-trained models parameter-efficient fine-tuning.\n\n## 1 Introduction\n\nFoundation models [3, 14, 23] have received significant interests in recent years, owing to their exceptional performance across a multitude of diverse tasks These models typically consume billions of parameters, trained on expansive web-scaled datasets for fundamental tasks such as next token prediction [ 6 ] or masked region completion [ 7 ]. A particularly compelling instance of these models is the Segment-Anything Model (SAM) [ 14 ], which has been trained on an unprecedentedly vast dataset comprising 11 million images and 1 billion masks. Despite the Segment-Anything Model\u2019s (SAM) noteworthy proficiency in generating masks to segment real-world objects, it is deficient in the detection of salient objects. This shortcoming leads to suboptimal performance in isolating a single salient object from a given RGB image, a crucial aspect of computer vision that emphasizes the identification of the most visually striking or attention-demanding object within an image. Traditional approaches for harnessing the capabilities of foundation models for downstream tasks generally include fine-tuning the entire model [ 11 ] or integrating additional adapter layers [ 9 ]. However, most foundation models possess"
    ],
    "kosmos": [
      "\n\n# Adaptive Low Rank Adaptation of Segment Anything to Salient Object Detection\n\nRuikai Cui\n\nAustralian National University\n\n1\n\nSiyuan He\n\nAustralian National University\n\n1\n\nShi Qiu\n\nAustralian National University\n\n1\n\nFootnote 1: [https://github.com/keras-team/keras](https://github.com/keras-team/keras)\n\n###### Abstract\n\nFoundation models, such as OpenAI\u2019s GPT-3 and GPT-4, Meta\u2019s LLaMA, and Google\u2019s PaLM2, have revolutionized the field of artificial intelligence. A notable paradigm shift has been the advent of the Segment Anything Model (SAM), which has exhibited a remarkable capability to segment real-world objects, trained on 1 billion masks and 11 million images. Although SAM excels in general object segmentation, it lacks the intrinsic ability to detect salient objects, resulting in suboptimal performance in this domain. To address this challenge, we present the Segment Salient Object Model (SSOM), an innovative approach that adaptively fine-tunes SAM for salient object detection by harnessing the low-rank structure inherent in deep learning. Comprehensive qualitative and quantitative evaluations across five challenging RGB benchmark datasets demonstrate the superior performance of our approach, surpassing state-of-the-art methods.\n\nKeywords:salient object detection large-scale pre-trained models parameter-efficient fine-tuning.\n\n## 1 Introduction\n\nFoundation models [3, 14, 23] have received significant interests in recent years, owing to their exceptional performance across a multitude of diverse tasks These models typically consume billions of parameters, trained on expansive web-scaled datasets for fundamental tasks such as next token prediction [6] or masked region completion [7]. A particularly compelling instance of these models is the Segment-Anything Model (SAM) [14], which has been trained on an unprece-\n\ndentedly vast dataset comprising 11 million images and 1 billion masks.\n\nDespite the Segment-Anything Model\u2019s (SAM) noteworthy proficiency in generating masks to segment real-world objects, it is deficient in the detection of salient objects. This shortcoming leads to suboptimal performance in isolating a single salient object from a given RGB image, a crucial aspect of computer vision that emphasizes the identification of the most visually striking or attention-demanding object within an image.\n\nTraditional approaches for harnessing the capabilities of foundation models for downstream tasks generally include fine-tuning the entire model [11] or integrating additional adapter layers [9]. However, most foundation models possess"
    ]
  },
  {
    "edit": [
      "TABLE III\n\nTRAINING DETAILS FOR TASK-AGNOSTIC LANGUAGE MODELS. FOR EACH MODEL, WE LIST HARDWARE DETAILS, TRAINING TIME IN HOURS AND ESTIMATED ENERGY CONSUMPTION IN kWh, IF THE INFORMATION IS AVAILABLE.\n\n<table>\n<colgroup>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n</colgroup>\n<thead>\n<tr>\n<th>\nApproach\n</th>\n<th>\nYear\n</th>\n<th>\nLanguage\n</th>\n<th>\nHardware\n</th>\n<th>\nTime in hours\n</th>\n<th>\nkWh\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\nBLOOM [13]\n</td>\n<td>\n2022\n</td>\n<td>\nJava, PHP, C++, Python, JavaScript, C#, Ruby, Lua, TypeScript, GO, C, Scala, Rust\n</td>\n<td>\nserver: 384 NVIDIA A100 GPUs, 80 GB\n</td>\n<td>\n1,082,990\n</td>\n<td>\n433,196\n</td>\n</tr>\n<tr>\n<td>\nProphetnet-x [91]\n</td>\n<td>\n2021\n</td>\n<td>\nGo, Java, JS, Php, Python, Ruby\n</td>\n<td>\nNVIDIA Tesla V100 GPUs\n</td>\n<td>\n30,000\n</td>\n<td>\n15,330\n</td>\n</tr>\n<tr>\n<td>\nCodeBERT [92]\n</td>\n<td>\n2020\n</td>\n<td>\nPython, Java, JavaScript, PHP, Ruby, Go\n</td>\n<td>\nserver: 16 NVIDIA Tesla V100 GPUs, 32 GB\n</td>\n<td>\n1,320\n</td>\n<td>\n10,618\n</td>\n</tr>\n<tr>\n<td>\nDobf [93]\n</td>\n<td>\n2021\n</td>\n<td>\nJava, Python\n</td>\n<td>\n32 NVIDIA V100 GPUs\n</td>\n<td>\n192\n</td>\n<td>\n3,080\n</td>\n</tr>\n<tr>\n<td>\nCodet5 [94]\n</td>\n<td>\n2021\n</td>\n<td>\nRuby, JavaScript, Go, Python, Java, Php, C, C#\n</td>\n<td>\nserver/cluster: 16 NVIDIA A100 GPUs, 40 GB\n</td>\n<td>\n288\n</td>\n<td>\n1,930\n</td>\n</tr>\n<tr>\n<td>\nFLBART [95]\n</td>\n<td>\n2021\n</td>\n<td>\nJava, Python\n</td>\n<td>\n8 NVIDIA RTX 2080 Ti GPUs\n</td>\n<td>\n276\n</td>\n<td>\n925\n</td>\n</tr>\n<tr>\n<td>\nMastropaolo et al. [96]\n</td>\n<td>\n2021\n</td>\n<td>\nJava\n</td>\n<td>\nGoogle Cloud, Colab: 8 TPUs, 35.5 GB memory\n</td>\n<td>\n343\n</td>\n<td>\n766\n</td>\n</tr>\n<tr>\n<td>\nGraphcodebert [97]\n</td>\n<td>\n2021\n</td>\n<td>\nRuby, JS, Go, Python, Java, PHP\n</td>\n<td>\nserver: 32 NVIDIA Tesla V100 GPUs, 32 GB\n</td>\n<td>\n83\n</td>\n<td>\n667\n</td>\n</tr>\n<tr>\n<td>\nCodeTrans [98]\n</td>\n<td>\n2021\n</td>\n<td>\nPython, Java, JavaScript, PHP, Ruby, Go, C#, SQL, LISP\n</td>\n<td>\n1 TPU v3.8\n</td>\n<td>\n2,088\n</td>\n<td>\n582\n</td>\n</tr>\n<tr>\n<td>\nGREAT [99]\n</td>\n<td>\n2025\n</td>\n<td>\nPython\n</td>\n<td>\n1 Tesla P100 GPU\n</td>\n<td>\n120\n</td>\n<td>\n51\n</td>\n</tr>\n<tr>\n<td>\nJavabert [100]\n</td>\n<td>\n2021\n</td>\n<td>\nJava\n</td>\n<td>\n3 NVIDIA Titan X GPUs, 12 GB\n</td>\n<td>\n24\n</td>\n<td>\n30\n</td>\n</tr>\n<tr>\n<td>\ncode2vec [101]\n</td>\n<td>\n2019\n</td>\n<td>\nJava\n</td>\n<td>\n1 NVIDIA Tesla K80 GPU\n</td>\n<td>\n36\n</td>\n<td>\n18\n</td>\n</tr>\n<tr>\n<td>\nOpenVocabCodeNLM [102]\n</td>\n<td>\n2020\n</td>\n<td>\nJava, Python, C\n</td>\n<td>\nGPUs\n</td>\n<td>\n336\n</ "
    ],
    "kosmos": [
      "TABLE III\n\nTRAINING DETAILS FOR TASK-AGNOSTIC LANGUAGE MODELS. FOR EACH MODEL, WE LIST HARDWARE DETAILS, TRAINING TIME IN HOURS AND ESTIMATED ENERGY CONSUMPTION IN kWh, IF THE INFORMATION IS AVAILABLE.\n\n<table>\n<colgroup>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n</colgroup>\n<thead>\n<tr>\n<th>\nApproach\n</th>\n<th>\nYear\n</th>\n<th>\nLanguage\n</th>\n<th>\nHardware\n</th>\n<th>\nTime in hours\n</th>\n<th>\nkWh\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\nBLOOM [13]\n</td>\n<td>\n2022\n</td>\n<td>\nJava, PHP, C++, Python, JavaScript, C#, Ruby, Lua, TypeScript, GO, C, Scala, Rust\n</td>\n<td>\nserver: 384 NVIDIA A100 GPUs, 80 GB\n</td>\n<td>\n1,082,990\n</td>\n<td>\n433,196\n</td>\n</tr>\n<tr>\n<td>\nProphetnet-x [91]\n</td>\n<td>\n2021\n</td>\n<td>\nGo, Java, JS, Php, Python, Ruby\n</td>\n<td>\nNVIDIA Tesla V100 GPUs\n</td>\n<td>\n30,000\n</td>\n<td>\n15,330\n</td>\n</tr>\n<tr>\n<td>\nCodeBERT [92]\n</td>\n<td>\n2020\n</td>\n<td>\nPython, Java, JavaScript, PHP, Ruby, Go\n</td>\n<td>\nserver: 16 NVIDIA Tesla V100 GPUs, 32 GB\n</td>\n<td>\n1,320\n</td>\n<td>\n10,618\n</td>\n</tr>\n<tr>\n<td>\nDobf [93]\n</td>\n<td>\n2021\n</td>\n<td>\nJava, Python\n</td>\n<td>\n32 NVIDIA V100 GPUs\n</td>\n<td>\n192\n</td>\n<td>\n3,080\n</td>\n</tr>\n<tr>\n<td>\nCodet5 [94]\n</td>\n<td>\n2021\n</td>\n<td>\nRuby, JavaScript, Go, Python, Java, Php, C, C#\n</td>\n<td>\nserver/cluster: 16 NVIDIA A100 GPUs, 40 GB\n</td>\n<td>\n288\n</td>\n<td>\n1,930\n</td>\n</tr>\n<tr>\n<td>\nFLBART [95]\n</td>\n<td>\n2021\n</td>\n<td>\nJava, Python\n</td>\n<td>\n8 NVIDIA RTX 2080 Ti GPUs\n</td>\n<td>\n276\n</td>\n<td>\n925\n</td>\n</tr>\n<tr>\n<td>\nMastropaolo et al. [96]\n</td>\n<td>\n2021\n</td>\n<td>\nJava\n</td>\n<td>\nGoogle Cloud, Colab: 8 TPUs, 35.5 GB memory\n</td>\n<td>\n343\n</td>\n<td>\n766\n</td>\n</tr>\n<tr>\n<td>\nGraphcodebert [97]\n</td>\n<td>\n2021\n</td>\n<td>\nRuby, JS, Go, Python, Java, PHP\n</td>\n<td>\nserver: 32 NVIDIA Tesla V100 GPUs, 32 GB\n</td>\n<td>\n83\n</td>\n<td>\n667\n</td>\n</tr>\n<tr>\n<td>\nCodeTrans [98]\n</td>\n<td>\n2021\n</td>\n<td>\nPython, Java, JavaScript, PHP, Ruby, Go, C#, SQL, LISP\n</td>\n<td>\n1 TPU v3.8\n</td>\n<td>\n2,088\n</td>\n<td>\n582\n</td>\n</tr>\n<tr>\n<td>\nGREAT [99]\n</td>\n<td>\n2025\n</td>\n<td>\nPython\n</td>\n<td>\n1 Tesla P100 GPU\n</td>\n<td>\n120\n</td>\n<td>\n51\n</td>\n</tr>\n<tr>\n<td>\nJavabert [100]\n</td>\n<td>\n2021\n</td>\n<td>\nJava\n</td>\n<td>\n3 NVIDIA Titan X GPUs, 12 GB\n</td>\n<td>\n24\n</td>\n<td>\n30\n</td>\n</tr>\n<tr>\n<td>\ncode2vec [101]\n</td>\n<td>\n2019\n</td>\n<td>\nJava\n</td>\n<td>\n1 NVIDIA Tesla K80 GPU\n</td>\n<td>\n36\n</td>\n<td>\n18\n</td>\n</tr>\n<tr>\n<td>\nOpenVocabCodeNLM [102]\n</td>\n<td>\n2020\n</td>\n<td>\nJava, Python, C\n</td>\n<td>\nGPUs\n</td>\n<td>\n336\n</ "
    ]
  },
  {
    "edit": [
      "For the fourth property we need to find a value for \u03b2 such that \\(|Opt ( G ) \u2212 | I F ||\\leq\\beta|Opt ( T G , T^{\\prime}_{G})-| F ||\\). Let \\(\\ell=| I F |\\) be the number of \\(A_{v}\\) in \\(F^{\\prime\\prime}\\) that are covered by 6 components. We know that \\(| F|\\geq|F^{\\prime}|\\geq|F^{\\prime\\prime}|=12n-\\ell\\). Observe:\n\n\\[|Opt ( T G , T^{\\prime}_{G})-| F || =| F|-Opt ( T G , T^{\\prime}_{G})\\] \\[\\geq|F^{\\prime\\prime}|-Opt ( T G ,T^{\\prime}_{G})\\] \\[=12n-\\ell-Opt ( T G ,T^{\\prime}_{G})\\] \\[=12n-\\ell-(12n-k)\\] \\[=k-\\ell\\] \\[=|Opt ( G )-| I F ||\\]\n\nSo we pick \u03b2 = 1 and we are done.\n\n## 5 A tight 7k kernel\n\nRecall the definitions of common subtrees and common chains from the preliminaries. It is well-known that the following two polynomial-time reduction rules do not alter the size of the uMAF [2]:\n\n**Subtree reduction.** If \\(T\\) and \\(T^{\\prime}\\) have a maximal common pendant subtree \\(S\\) with at least two leaves, then reduce \\(T\\) and \\(T^{\\prime}\\) to \\(T_{r}\\) and \\(T^{\\prime}_{r}\\), respectively, by replacing \\(S\\) with a single leaf with a new label.\n\n**Chain reduction.** If \\(T\\) and \\(T^{\\prime}\\) have a maximal common \\(n\\)-chain \\(C=(\\ell_{1},\\ell_{2},\\ldots,\\ell_{n})\\) with \\(n\\geq 4\\), then reduce \\(T\\) and \\(T^{\\prime}\\) to \\(T_{r}=T|X\\setminus\\{\\ell_{4},\\ell_{5},\\ldots,\\ell_{n}\\}\\) and \\(T^{\\prime}_{r}=T^{\\prime}|X\\setminus\\{\\ell_{4},\\ell_{5},\\ldots,\\ell_{n}\\}\\), respectively.\n\nWhen applied to exhaustion on two unrooted binary trees, at which point we say the trees are _fully reduced_, these rules yield an instance with (ignoring additive terms) at most 15 k taxa [10], where k is the size of the uMAF3, and the analysis is tight.\n\nFootnote 3: The kernel bound given in [10] is in terms of TBR distance, rather than uMAF, but as noted earlier these quantities only differ by 1, so only additive terms are affected.\n\nNote that applying the subtree or chain reduction to a caterpillar produces a new caterpillar. In this section we will show that, when applied to exhaustion on two caterpillars, a much smaller kernel is obtained than on general unrooted binary trees.\n\n**Theorem 4**.: _There is a 7k kernel for uMAF on caterpillars using only the common chain and subtree reductions, and this is tight up to a constant additive term._ "
    ],
    "kosmos": [
      "For the fourth property we need to find a value for \u03b2 such that \\(|Opt ( G ) \u2212 | I F ||\\leq\\beta|Opt ( T G , T^{\\prime}_{G})-| F ||\\). Let \\(\\ell=| I F |\\) be the number of \\(A_{v}\\) in \\(F^{\\prime\\prime}\\) that are covered by 6 components. We know that \\(| F|\\geq|F^{\\prime}|\\geq|F^{\\prime\\prime}|=12n-\\ell\\). Observe:\n\n\\[|Opt ( T G , T^{\\prime}_{G})-| F || =| F|-Opt ( T G , T^{\\prime}_{G})\\] \\[\\geq|F^{\\prime\\prime}|-Opt ( T G ,T^{\\prime}_{G})\\] \\[=12n-\\ell-Opt ( T G ,T^{\\prime}_{G})\\] \\[=12n-\\ell-(12n-k)\\] \\[=k-\\ell\\] \\[=|Opt ( G )-| I F ||\\]\n\nSo we pick \u03b2 = 1 and we are done.\n\n## 5 A tight 7k kernel\n\nRecall the definitions of common subtrees and common chains from the preliminaries. It is well-known that the following two polynomial-time reduction rules do not alter the size of the uMAF [2]:\n\n**Subtree reduction.** If \\(T\\) and \\(T^{\\prime}\\) have a maximal common pendant subtree \\(S\\) with at least two leaves, then reduce \\(T\\) and \\(T^{\\prime}\\) to \\(T_{r}\\) and \\(T^{\\prime}_{r}\\), respectively, by replacing \\(S\\) with a single leaf with a new label.\n\n**Chain reduction.** If \\(T\\) and \\(T^{\\prime}\\) have a maximal common \\(n\\)-chain \\(C=(\\ell_{1},\\ell_{2},\\ldots,\\ell_{n})\\) with \\(n\\geq 4\\), then reduce \\(T\\) and \\(T^{\\prime}\\) to \\(T_{r}=T|X\\setminus\\{\\ell_{4},\\ell_{5},\\ldots,\\ell_{n}\\}\\) and \\(T^{\\prime}_{r}=T^{\\prime}|X\\setminus\\{\\ell_{4},\\ell_{5},\\ldots,\\ell_{n}\\}\\), respectively.\n\nWhen applied to exhaustion on two unrooted binary trees, at which point we say the trees are _fully reduced_, these rules yield an instance with (ignoring additive terms) at most 15 k taxa [10], where k is the size of the uMAF3, and the analysis is tight.\n\nFootnote 3: The kernel bound given in [10] is in terms of TBR distance, rather than uMAF, but as noted earlier these quantities only differ by 1, so only additive terms are affected.\n\nNote that applying the subtree or chain reduction to a caterpillar produces a new caterpillar. In this section we will show that, when applied to exhaustion on two caterpillars, a much smaller kernel is obtained than on general unrooted binary trees.\n\n**Theorem 4**.: _There is a 7k kernel for uMAF on caterpillars using only the common chain and subtree reductions, and this is tight up to a constant additive term._ "
    ]
  },
  {
    "edit": [
      "all p i 's are integers and B = \\(\\frac{4ac^{*}}{\\varepsilon}\\). We can compute min( h I 0 1 ( x ) , B ) exactly using standard dynamic programming in O ( |I 0 1 | \u00b7 B ) = \\(\\widetilde{O}(\\frac{1}{\\varepsilon^{2}})\\) time.\n\nThe complexity of the resulting function is O ( |I 0 1 | ) = \\(\\widetilde{O}(\\frac{1}{\\varepsilon})\\).\n\nIn \\(\\widetilde{O}(n+\\frac{1}{\\varepsilon^{2}})\\) time, we can compute a function \\(\\widetilde{f}_{\\mathcal{I}0 3}\\) that approximates min( f I 0 3 , \\(\\frac{4c^{*}}{\\varepsilon^{3/2}})\\) with a factor of 1 + \\(\\widetilde{O}(\\varepsilon)\\) via Lemma 7. The additive error caused by \\(\\widetilde{f}_{\\mathcal{I}0 3}\\) is at most \\(\\widetilde{O}(\\varepsilon\\cdot\\frac{4c^{*}}{\\varepsilon^{3/2}})\\leqslant \\widetilde{O}(\\frac{1}{ac})\\) as a \\(\\leqslant\\frac{1}{\\varepsilon^{1/2}}\\), and the complexity of \\(\\widetilde{f}_{\\mathcal{I}0 3}\\) is \\(\\widetilde{O}(\\frac{1}{\\varepsilon})\\).\n\n**Lemma 13**.: _Let \\(\\widetilde{f}_{\\mathcal{I}0 1}\\) and \\(\\widetilde{f}_{\\mathcal{I}0 3}\\) be two functions that approximate max( f \\(\\mathcal{I}0 1\\), p( I 0 1) - \\(\\frac{4c^{*}}{\\varepsilon^{3/2}})\\) and min( f \\(\\mathcal{I}0 3\\), \\(\\frac{4c^{*}}{\\varepsilon^{3/2}})\\) with additive error \\(\\widetilde{O}(\\frac{1}{ac})\\) and complexity \\(\\widetilde{O}(\\frac{1}{\\varepsilon})\\), respectively. We can approximate \\(\\widetilde{f}_{\\mathcal{I}0 1}\\oplus\\widetilde{f}_{\\mathcal{I}0 3}\\) with additive error \\(\\widetilde{O}(\\frac{1}{ac})\\) and complexity \\(\\widetilde{O}(\\frac{1}{\\varepsilon})\\) in \\(\\widetilde{O}(\\frac{a^{2}}{\\varepsilon})\\) time._\n\nProof.: Let u = max(0, p( I 0 1) - \\(\\frac{4c^{*}}{\\varepsilon^{3/2}})\\). Note that \\(\\widetilde{f}_{\\mathcal{I}0 1}\\) have a range contained in [ u, p( I 0 1)], so \\(\\widetilde{f}_{\\mathcal{I}0 1}-u+1\\) have a range contained in [1, \\(\\frac{4c^{*}}{\\varepsilon^{3/2}}+1\\)]. Since\n\n\\[\\widetilde{f}_{\\mathcal{I}0 1}\\oplus\\widetilde{f}_{\\mathcal{I}0 3}=\\Big{(}(\\widetilde{f}_{\\mathcal{I}0 1}-u+1)\\oplus\\widetilde{f}_{\\mathcal{I}0 3}\\Big{)}+u-1.\\]\n\nTo approximate \\(\\widetilde{f}_{\\mathcal{I}0 1}\\oplus\\widetilde{f}_{\\mathcal{I}0 3}\\), it suffices to approximate \\((\\widetilde{f}_{\\mathcal{I}0 1}-u+1)\\oplus\\widetilde{f}_{\\mathcal{I}0 3}\\) with an additive error of \\(\\widetilde{O}(\\frac{1}{ac})\\). Both \\((\\widetilde{f}_{\\mathcal{I}0 1}-u+1)\\) and \\(\\widetilde{f}_{\\mathcal "
    ],
    "kosmos": [
      "all p i 's are integers and B = \\(\\frac{4ac^{*}}{\\varepsilon}\\). We can compute min( h I 0 1 ( x ) , B ) exactly using standard dynamic programming in O ( |I 0 1 | \u00b7 B ) = \\(\\widetilde{O}(\\frac{1}{\\varepsilon^{2}})\\) time.\n\nThe complexity of the resulting function is O ( |I 0 1 | ) = \\(\\widetilde{O}(\\frac{1}{\\varepsilon})\\).\n\nIn \\(\\widetilde{O}(n+\\frac{1}{\\varepsilon^{2}})\\) time, we can compute a function \\(\\widetilde{f}_{\\mathcal{I}0 3}\\) that approximates min( f I 0 3 , \\(\\frac{4c^{*}}{\\varepsilon^{3/2}})\\) with a factor of 1 + \\(\\widetilde{O}(\\varepsilon)\\) via Lemma 7. The additive error caused by \\(\\widetilde{f}_{\\mathcal{I}0 3}\\) is at most \\(\\widetilde{O}(\\varepsilon\\cdot\\frac{4c^{*}}{\\varepsilon^{3/2}})\\leqslant \\widetilde{O}(\\frac{1}{ac})\\) as a \\(\\leqslant\\frac{1}{\\varepsilon^{1/2}}\\), and the complexity of \\(\\widetilde{f}_{\\mathcal{I}0 3}\\) is \\(\\widetilde{O}(\\frac{1}{\\varepsilon})\\).\n\n**Lemma 13**.: _Let \\(\\widetilde{f}_{\\mathcal{I}0 1}\\) and \\(\\widetilde{f}_{\\mathcal{I}0 3}\\) be two functions that approximate max( f \\(\\mathcal{I}0 1\\), p( I 0 1) - \\(\\frac{4c^{*}}{\\varepsilon^{3/2}})\\) and min( f \\(\\mathcal{I}0 3\\), \\(\\frac{4c^{*}}{\\varepsilon^{3/2}})\\) with additive error \\(\\widetilde{O}(\\frac{1}{ac})\\) and complexity \\(\\widetilde{O}(\\frac{1}{\\varepsilon})\\), respectively. We can approximate \\(\\widetilde{f}_{\\mathcal{I}0 1}\\oplus\\widetilde{f}_{\\mathcal{I}0 3}\\) with additive error \\(\\widetilde{O}(\\frac{1}{ac})\\) and complexity \\(\\widetilde{O}(\\frac{1}{\\varepsilon})\\) in \\(\\widetilde{O}(\\frac{a^{2}}{\\varepsilon})\\) time._\n\nProof.: Let u = max(0, p( I 0 1) - \\(\\frac{4c^{*}}{\\varepsilon^{3/2}})\\). Note that \\(\\widetilde{f}_{\\mathcal{I}0 1}\\) have a range contained in [ u, p( I 0 1)], so \\(\\widetilde{f}_{\\mathcal{I}0 1}-u+1\\) have a range contained in [1, \\(\\frac{4c^{*}}{\\varepsilon^{3/2}}+1\\)]. Since\n\n\\[\\widetilde{f}_{\\mathcal{I}0 1}\\oplus\\widetilde{f}_{\\mathcal{I}0 3}=\\Big{(}(\\widetilde{f}_{\\mathcal{I}0 1}-u+1)\\oplus\\widetilde{f}_{\\mathcal{I}0 3}\\Big{)}+u-1.\\]\n\nTo approximate \\(\\widetilde{f}_{\\mathcal{I}0 1}\\oplus\\widetilde{f}_{\\mathcal{I}0 3}\\), it suffices to approximate \\((\\widetilde{f}_{\\mathcal{I}0 1}-u+1)\\oplus\\widetilde{f}_{\\mathcal{I}0 3}\\) with an additive error of \\(\\widetilde{O}(\\frac{1}{ac})\\). Both \\((\\widetilde{f}_{\\mathcal{I}0 1}-u+1)\\) and \\(\\widetilde{f}_{\\mathcal "
    ]
  },
  {
    "edit": [
      "Many of the earlier mathematical works on rcsps focused on determining their satisfiability thresholds and verifying the sharpness of sat-unsat transitions. For models that are known not to exhibit rsb, such goals were established. These models include random 2sat [ CR92 , BBC + 01 ], random 1in - k -sat [ ACIM01 ], k-xor-sat [ DM02 , DGM + 10 , PS16 ], and random linear equations [ ACOGM20 ]. On the other hand, for the models which are predicted to belong to 1 rsb class, intensive studies have been conducted to estimate their satisfiability threshold, as shown in [ KKKS98 , AP04 , COP16 ] (random k-sat), [ AM06 , COZ12 , COP12 ] (random graph coloring). More recently, the satisfiability thresholds for r csp s that exhibits rsb have been rigorously determined for several models, namely the random regular k-nae-sat [ DSS16b ], maximum independent set on d -regular graphs [ DSS16a ], random regular k-sat [ COP16 ] and random k-sat [ DSS22 ] for large k and d. Although determining the location of q -colorability threshold for the sparse Erdos Renyi graph is left open, the condensation threshold \u03b1 cond for random graph coloring, where the free energy becomes non-analytic, was settled in [ BCOH + 16 ]. They carried out a technically challenging analysis based on a clever \u201cplanting\u201d technique, where the results were further generalized to other models in [ COKPZ18 ]. Similarly, [ BCO16 ] identified the condensation threshold for random regular k-sat, where each variable appears d/ 2-times positive and d/ 2-times negative. Further, in the condensation regime \u03b1 \u2208 ( \u03b1 cond , \u03b1 sat ), many quantities of interest was established for random regular k-nae-sat with large enough k, matching the statistical physics prediction. Namely, the number of solutions at exponential scale (free energy) [ SSZ22 ], the concentration of the overlap [ NSS20 , NSS21 ], and the local weak limit [ SS23 ] were established. Establishing the same quantities for other models in the condensation regime is left open. The closest result to ours in the literature is by Ayre, Coja-Oghlan, and Greenhill [ ACOG22 ], where they lower bound the chromatic number (or equivalently, upper bound the colorability threshold) of the random regular graph of any degree, which is conjectured to be tight. [ ACOG22 ] also considers the sparse Erdos Renyi graph, which is more complicated since the conjectured chromatic number is defined in terms of a distributional (rather than real-valued) optimization due to the randomness of the local neighborhoods. In this work, we do not consider Erdos Renyi type problems, but we additionally address the question of the uniqueness of the bp fixed point for any k \u2265 3 (unique solution to the equation ( 1.1 )). As in [ ACOG22 ], we use an interpolation bound, which gives an upper bound of the satisfiability threshold also for the (nonregular) random k-nae-sat model. It would be interesting to address the uniqueness of the bp fixed point for random k-nae-sat and random k-sat for small k \u2265 3. We refer to [ ST03 , MRSY19 , YP22 , GP23 ] which addresses the uniqueness of bp fixed point for various models. We aim to rigorously establish the upper bound the satisfiability threshold predicted by the so-called \u20181rsb cavity method\u2019 from statistical physics [ DRZ08 ]. To do so, instead of using moment methods, we use a technique called \u2018interpolation method\u2019 from the theory of spin glasses developed by [ FL03 , Gue03 , PT04 ]. The interpolation method has been successful in upperbounding the satisfiability threshold for random k-sat [ DSS15 ] for large k, the free energy for random regular k-nae-sat [ SSZ16 ], and the colorability threshold for random graphs [ ACOG22 ]. We first introduce the notations and mathematical framework that we use throughout the paper. For both the d -regular k-uniform hypergraphs and the k-nae-sat formula, we can represent them as (labelled) ( d, k )-regular bipartite graph. Let V = { v 1 , . . . , v n } be the set of variables or nodes and F = { a 1 , . . . , a m } be the set of clauses or hyperedges. An edge is formed if the variable or node v i is included in the clause or hyperedge a j . For an edge e, we denote v ( e ) (resp. a ( e )) by the variable (resp. clause) adjacent to it "
    ],
    "kosmos": [
      "Many of the earlier mathematical works on rcsp s focused on determining their satis\ufb01ability thresholds and verifying the sharpness of sat-unsat transitions. For models that are known not to exhibit rsb , such goals were established. These models include random 2-sat [CR92, BBC + 01], random 1-in- k -sat [ACIM01], k-xor-sat [DM02, DGM + 10, PS16], and random linear equations [ACOGM20]. On the other hand, for the models which are predicted to belong to irsb class, intensive studies have been conducted to estimate their satis\ufb01ability threshold, as shown in [KKKS98, AP04, COP16] (random k-sat), [AM06, COZ12, COP12] (random k-nae-sat), and [AN05, CO13, COV13, COEH16] (random graph coloring).\n\nMore recently, the satis\ufb01ability thresholds for rcsp s that exhibits rsb have been rigorously determined for several models, namely the random regular k-nae-sat [DSS16b], maximum independent set on d -regular graphs [DSS16a], random regular k-sat [COP16] and random k-sat [DSS22] for large k and d . Although determining the location of q -colorability threshold for the sparse Erdos Renyi graph is left open, the _condensation threshold_ \u03b1 cond for random graph coloring, where the _free energy_ becomes non-analytic, was settled in [BCOH + 16]. They carried out a technically challenging analysis based on a clever \u201cplanting\u201d technique, where the results were further generalized to other models in [COKPZ18]. Similarly, [BCO16] identi\ufb01ed the condensation threshold for random regular k-sat, where each variable appears d/ 2-times positive and d/ 2-times negative. Further, in the condensation regime \u03b1 \u2208 ( \u03b1 cond , \u03b1 sat ), many quantities of interest was established for random regular k-nae-sat with large enough k, matching the statistical physics prediction. Namely, the number of solutions at exponential scale (free energy) [SSZ22], the concentration of the _overlap_ [NSS20, NSS21], and the local weak limit [SS23] were established. Establishing the same quantities for other models in the condensation regime is left open.\n\nThe closest result to ours in the literature is by Ayre, Coja-Oghlan, and Greenhill [ACOG22], where they lower bound the chromatic number (or equivalently, upper bound the colorability threshold) of the random regular graph of any degree, which is conjectured to be tight. [ACOG22] also considers the sparse Erdos Renyi graph, which is more complicated since the conjectured chromatic number is de\ufb01ned in terms of a distributional (rather than real-valued) optimization due to the randomness of the local neighborhoods. In this work, we do not consider Erdos Renyi type problems, but we additionally address the question of the uniqueness of the bp \ufb01xed point for any k \u2265 3 (unique solution to the equation ( 1.1 )). As in [ACOG22], we use an interpolation bound, which gives an upper bound of the satis\ufb01ability threshold also for the (non-regular) random k-nae-sat model. It would be interesting to address the uniqueness of the bp \ufb01xed point for random k-nae-sat and random k-sat for small k \u2265 3. We refer to [ST03, MRSY19, YP22, GP23] which addresses the uniqueness of bp \ufb01xed point for various models.\n\n### Proof methods\n\nWe aim to rigorously establish the upper bound the satis\ufb01ability threshold predicted by the so-called \u20181rsb cavity method\u2019 from statistical physics [DRZ08]. To do so, instead of using moment methods, we use a technique called \u2018interpolation method\u2019 from the theory of spin glasses developed by [FL03, Gue03, PT04]. The interpolation method has been successful in upperbounding the satis\ufb01ability threshold for random k-sat [DSS15] for large k, the free energy for random regular k-nae-sat [SSZ16], and the colorability threshold for random graphs [ACOG22].\n\nWe \ufb01rst introduce the notations and mathematical framework that we use throughout the paper. For both the d -regular k-uniform hypergraphs and the k-nae-sat formula, we can represent them as (labelled) ( d, k )-regular bipartite graph. Let V = { v 1 , . . . , v n } be the set of variables or nodes and F = { a 1 , . . . , "
    ]
  },
  {
    "edit": [
      "expanding box like model valid for both radially and temporally varying spherical flows. The generalisation of the expanding box model to radially varying flows was done in Tenerani &amp; Velli (2017). The generalisation to background flows which are also time dependant, motivated by the stellar formation problem, results in a model close to the accelerated expanding box (although our treatment of pressure will be closer to the distorted shearing box models of Ogilvie &amp; Latter (2013); Ogilvie &amp; Barker (2014)). We shall focus, in this paper, on the hydrodynamic case as it posses a number of important features that are worth understanding before generalising to MHD.\n\nIn Section 2 we present the derivation of our local model. Sections 2.4 and 2.5 derives symmetries and conservation laws of the local model. Section 3 presents some nonlinear solutions to the local model - and discuss how these relate to the global problem. In Section 4 we derive the linear theory of our local model. We discuss possible extension of our model in Section 5. We present our conclusions in Section 6 and additional mathematical details (including alternative formulations which maybe more convenient for implementation in hydrocodes) are presented in the appendices.\n\n## 2 Derivation\n\n### Global geometry\n\nTo derive a local model for spherical collapse/expansion consider a local neighbourhood of a point, \ud835\udf0c, located on the equator of a sphere of radius \ud835\udc45. The line element of the usual spherical polar coordinate system is\n\n\\[\\mathrm{d}s^{2}=\\mathrm{d}R^{2}+\ud835\udc45^{2}(\\mathrm{d}\\theta^{2}+\\sin^{2}\\theta\\, \\mathrm{d}\\phi^{2}). \\tag{1}\\]\n\nWe are interested in describing the local dynamics near to \ud835\udf0c occurring on a horizontal lengthscale \ud835\udc3f H \u226a \ud835\udc45 (See Figure 1, which show the relationship between the global and local geometries). Without loss of generality, we can locate our local model on the equator of the sphere (\\(\\theta=\\pi/2\\)) meaning we can approximate the line element by\n\n\\[\\mathrm{d}s^{2}=\\mathrm{d}R^{2}+\ud835\udc45^{2}(\\mathrm{d}\\theta^{2}+\\mathrm{d}\\phi^{2})+ O((\ud835\udc3f H/\ud835\udc45)^{2}d\ud835\udf03^{2}), \\tag{2}\\]\n\nwhich results in metric tensor components,\n\n\\[g_{RR}=1,\\quad g_{\u03b8\u03b8}=g_{\\phi\\phi}=R^{2}, \\tag{3}\\]\n\nand inverse metric tensor components\n\n\\[g^{RR}=1,\\quad g^{\\theta\\theta}=g^{\\phi\\phi}=R^{-2}, \\tag{4}\\]\n\nwith all other components zero. The Christoffel Symbols components, for this coordinate system, are\n\n\\[\\Gamma^{R}_{\u03b8\u03b8}=\\Gamma^{R}_{\\phi\\phi}=-R, \\tag{5}\\]\n\n\\[\\Gamma^{\\theta}_{\u03b8R}=\\Gamma^{\\theta}_{\\phi\\theta}=\\Gamma^{\\phi}_{\\phi R}= \\Gamma^{\\phi}_{R\\phi}=R^{-1}, \\tag{6}\\]\n\nwith all others vanishing. The fluid equations in this coordinate system are\n\n\\[Du^{\\theta}+2\\overline{R}u^{R}u^{\\theta}=-R^{-2}\\left(\\partial_{\\theta}\\Phi+1 \\overline{\\rho}\\partial_{\\theta}\\rho\\right), \\tag{7}\\]\n\n\\[Du^{\\phi}+2\\overline{R}u^{R}u^{\\phi}=-R^{-2}\\left(\\partial_{\\phi}\\Phi+1\\overline {\\rho}\\partial_{\\phi}\\rho\\right), \\tag{8}\\]\n\n\\[Du^{R}-Ru^{\\phi}u^{\\phi}-Ru^{\\theta}u^{\\theta}=-\\left(\\partial_{R}\\Phi+1\\overline {\\rho}\\partial_{R}\\rho\\right), \\tag{9}\\]\n\n\\[Dp=-\\rho R^{-2}\\partial_{l}(R^{2}u^{l}), \\tag{10}\\]\n\nwhere the Lagrangian derivative is\n\n\\[D=\\partial_{t}+u^{l}\\partial_{l}. \\tag{11}\\]\n\nNote, we have listed the \ud835\udc45 component of the momentum equation last as it will become the \ud835\udc67 momentum equation in the local coordinate system. To close this system of equations we must supplement them with an equation of state determining \ud835\udc5d, which we assume is barotropic,\n\n\\[p=p(\\rho). \\tag{12}\\]\n\n### Spherical Collapse/Expansion\n\nFor the background fluid flow we wish to "
    ],
    "kosmos": [
      "expanding box like model valid for both radially and temporally varying spherical flows. The generalisation of the expanding box model to radially varying flows was done in Tenerani &amp; Velli (2017). The generalisation to background flows which are also time dependant, motivated by the stellar formation problem, results in a model close to the accelerated expanding box (although our treatment of pressure will be closer to the distorted shearing box models of Ogilvie &amp; Latter (2013); Ogilvie &amp; Barker (2014)). We shall focus, in this paper, on the hydrodynamic case as it posses a number of important features that are worth understanding before generalising to MHD.\n\nIn Section 2 we present the derivation of our local model. Sections 2.4 and 2.5 derives symmetries and conservation laws of the local model. Section 3 presents some nonlinear solutions to the local model - and discuss how these relate to the global problem. In Section 4 we derive the linear theory of our local model. We discuss possible extension of our model in Section 5. We present our conclusions in Section 6 and additional mathematical details (including alternative formulations which maybe more convenient for implementation in hydrocodes) are presented in the appendices.\n\n## 2 Derivation\n\n### Global geometry\n\nTo derive a local model for spherical collapse/expansion consider a local neighbourhood of a point, \ud835\udf0c, located on the equator of a sphere of radius \ud835\udc45. The line element of the usual spherical polar coordinate system is\n\n\\[\\mathrm{d}s^{2}=\\mathrm{d}R^{2}+\ud835\udc45^{2}(\\mathrm{d}\\theta^{2}+\\sin^{2}\\theta\\, \\mathrm{d}\\phi^{2}). \\tag{1}\\]\n\nWe are interested in describing the local dynamics near to \ud835\udf0c occurring on a horizontal lengthscale \ud835\udc3f H \u226a \ud835\udc45 (See Figure 1, which show the relationship between the global and local geometries). Without loss of generality, we can locate our local model on the equator of the sphere (\\(\\theta=\\pi/2\\)) meaning we can approximate the line element by\n\n\\[\\mathrm{d}s^{2}=\\mathrm{d}R^{2}+\ud835\udc45^{2}(\\mathrm{d}\\theta^{2}+\\mathrm{d}\\phi^{2})+ O((\ud835\udc3f H/\ud835\udc45)^{2}d\ud835\udf03^{2}), \\tag{2}\\]\n\nwhich results in metric tensor components,\n\n\\[g_{RR}=1,\\quad g_{\u03b8\u03b8}=g_{\\phi\\phi}=R^{2}, \\tag{3}\\]\n\nand inverse metric tensor components\n\n\\[g^{RR}=1,\\quad g^{\\theta\\theta}=g^{\\phi\\phi}=R^{-2}, \\tag{4}\\]\n\nwith all other components zero. The Christoffel Symbols components, for this coordinate system, are\n\n\\[\\Gamma^{R}_{\u03b8\u03b8}=\\Gamma^{R}_{\\phi\\phi}=-R, \\tag{5}\\]\n\n\\[\\Gamma^{\\theta}_{\u03b8R}=\\Gamma^{\\theta}_{\\phi\\theta}=\\Gamma^{\\phi}_{\\phi R}= \\Gamma^{\\phi}_{R\\phi}=R^{-1}, \\tag{6}\\]\n\nwith all others vanishing. The fluid equations in this coordinate system are\n\n\\[Du^{\\theta}+2\\overline{R}u^{R}u^{\\theta}=-R^{-2}\\left(\\partial_{\\theta}\\Phi+1 \\overline{\\rho}\\partial_{\\theta}\\rho\\right), \\tag{7}\\]\n\n\\[Du^{\\phi}+2\\overline{R}u^{R}u^{\\phi}=-R^{-2}\\left(\\partial_{\\phi}\\Phi+1\\overline {\\rho}\\partial_{\\phi}\\rho\\right), \\tag{8}\\]\n\n\\[Du^{R}-Ru^{\\phi}u^{\\phi}-Ru^{\\theta}u^{\\theta}=-\\left(\\partial_{R}\\Phi+1\\overline {\\rho}\\partial_{R}\\rho\\right), \\tag{9}\\]\n\n\\[Dp=-\\rho R^{-2}\\partial_{l}(R^{2}u^{l}), \\tag{10}\\]\n\nwhere the Lagrangian derivative is\n\n\\[D=\\partial_{t}+u^{l}\\partial_{l}. \\tag{11}\\]\n\nNote, we have listed the \ud835\udc45 component of the momentum equation last as it will become the \ud835\udc67 momentum equation in the local coordinate system. To close this system of equations we must supplement them with an equation of state determining \ud835\udc5d, which we assume is barotropic,\n\n\\[p=p(\\rho). \\tag{12}\\]\n\n### Spherical Collapse/Expansion\n\nFor the background fluid flow we wish to "
    ]
  },
  {
    "edit": [
      "<table>\n<colgroup>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n</colgroup>\n<thead>\n<tr>\n<th>\nModel\n</th>\n<th>\nDifficult\n</th>\n<th>\n</th>\n<th>\n</th>\n<th>\nNormal\n</th>\n<th>\n</th>\n<th>\n</th>\n<th>\nEasy\n</th>\n<th>\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\n2-11\n</td>\n<td>\nIoU\n</td>\n<td>\nMCC\n</td>\n<td>\nNMM\n</td>\n<td>\nIoU\n</td>\n<td>\nMCC\n</td>\n<td>\nNMM\n</td>\n<td>\nIoU\n</td>\n<td>\nMCC\n</td>\n<td>\nNMM\n</td>\n</tr>\n<tr>\n<td>\nTAF-separate\n</td>\n<td>\n0.6239\n</td>\n<td>\n0.7167\n</td>\n<td>\n0.2963\n</td>\n<td>\n0.8734\n</td>\n<td>\n0.9143\n</td>\n<td>\n0.7744\n</td>\n<td>\n0.9490\n</td>\n<td>\n0.9604\n</td>\n<td>\n0.9164\n</td>\n</tr>\n<tr>\n<td>\nMSTAF-separate\n</td>\n<td>\n0.7712\n</td>\n<td>\n0.8432\n</td>\n<td>\n0.5670\n</td>\n<td>\n0.9210\n</td>\n<td>\n0.9486\n</td>\n<td>\n0.8583\n</td>\n<td>\n0.9693\n</td>\n<td>\n0.9764\n</td>\n<td>\n0.9488\n</td>\n</tr>\n<tr>\n<td>\nTAF\n</td>\n<td>\n0.8001\n</td>\n<td>\n0.8586\n</td>\n<td>\n0.6312\n</td>\n<td>\n0.9379\n</td>\n<td>\n0.9605\n</td>\n<td>\n0.8937\n</td>\n<td>\n0.9753\n</td>\n<td>\n0.9811\n</td>\n<td>\n0.9617\n</td>\n</tr>\n<tr>\n<td>\nMSTAF\n</td>\n<td>\n**0.8394**\n</td>\n<td>\n**0.8918**\n</td>\n<td>\n**0.7064**\n</td>\n<td>\n**0.9510**\n</td>\n<td>\n**0.9700**\n</td>\n<td>\n**0.9151**\n</td>\n<td>\n**0.9788**\n</td>\n<td>\n**0.9838**\n</td>\n<td>\n**0.9646**\n</td>\n</tr>\n</tbody>\n</table>\n\nTable 4: Ablation study on the Synthetic set\n\n<table>\n<colgroup>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n</colgroup>\n<thead>\n<tr>\n<th>\nModel\n</th>\n<th>\nDifficult\n</th>\n<th>\n</th>\n<th>\n</th>\n<th>\nNormal\n</th>\n<th>\n</th>\n<th>\n</th>\n<th>\nEasy\n</th>\n<th>\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\n2-11\n</td>\n<td>\nIoU\n</td>\n<td>\nMCC\n</td>\n<td>\nNMM\n</td>\n<td>\nIoU\n</td>\n<td>\nMCC\n</td>\n<td>\nNMM\n</td>\n<td>\nIoU\n</td>\n<td>\nMCC\n</td>\n<td>\nNMM\n</td>\n</tr>\n<tr>\n<td>\nTAF\n</td>\n<td>\n0.7009\n</td>\n<td>\n0.7644\n</td>\n<td>\n0.4400\n</td>\n<td>\n0.8789\n</td>\n<td>\n0.9160\n</td>\n<td>\n0.7807\n</td>\n<td>\n0.9704\n</td>\n<td>\n0.9769\n</td>\n<td>\n0.9550\n</td>\n</tr>\n<tr>\n<td>\nMSTAF\n</td>\n<td>\n**0.7427**\n</td>\n<td>\n**0.8022**\n</td>\n<td>\n**0.5206**\n</td>\n<td>\n**0.9105**\n</td>\n<td>\n**0.9410**\n</td>\n<td>\n**0.8406**\n</td>\n<td>\n**0.9752**\n</td>\n<td>\n**0.9808**\n</ "
    ],
    "kosmos": [
      "<table>\n<colgroup>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n</colgroup>\n<thead>\n<tr>\n<th>\nModel\n</th>\n<th>\nDifficult\n</th>\n<th>\n</th>\n<th>\n</th>\n<th>\nNormal\n</th>\n<th>\n</th>\n<th>\n</th>\n<th>\nEasy\n</th>\n<th>\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\n2-11\n</td>\n<td>\nIoU\n</td>\n<td>\nMCC\n</td>\n<td>\nNMM\n</td>\n<td>\nIoU\n</td>\n<td>\nMCC\n</td>\n<td>\nNMM\n</td>\n<td>\nIoU\n</td>\n<td>\nMCC\n</td>\n<td>\nNMM\n</td>\n</tr>\n<tr>\n<td>\nTAF-separate\n</td>\n<td>\n0.6239\n</td>\n<td>\n0.7167\n</td>\n<td>\n0.2963\n</td>\n<td>\n0.8734\n</td>\n<td>\n0.9143\n</td>\n<td>\n0.7744\n</td>\n<td>\n0.9490\n</td>\n<td>\n0.9604\n</td>\n<td>\n0.9164\n</td>\n</tr>\n<tr>\n<td>\nMSTAF-separate\n</td>\n<td>\n0.7712\n</td>\n<td>\n0.8432\n</td>\n<td>\n0.5670\n</td>\n<td>\n0.9210\n</td>\n<td>\n0.9486\n</td>\n<td>\n0.8583\n</td>\n<td>\n0.9693\n</td>\n<td>\n0.9764\n</td>\n<td>\n0.9488\n</td>\n</tr>\n<tr>\n<td>\nTAF\n</td>\n<td>\n0.8001\n</td>\n<td>\n0.8586\n</td>\n<td>\n0.6312\n</td>\n<td>\n0.9379\n</td>\n<td>\n0.9605\n</td>\n<td>\n0.8937\n</td>\n<td>\n0.9753\n</td>\n<td>\n0.9811\n</td>\n<td>\n0.9617\n</td>\n</tr>\n<tr>\n<td>\nMSTAF\n</td>\n<td>\n**0.8394**\n</td>\n<td>\n**0.8918**\n</td>\n<td>\n**0.7064**\n</td>\n<td>\n**0.9510**\n</td>\n<td>\n**0.9700**\n</td>\n<td>\n**0.9151**\n</td>\n<td>\n**0.9788**\n</td>\n<td>\n**0.9838**\n</td>\n<td>\n**0.9646**\n</td>\n</tr>\n</tbody>\n</table>\n\nTable 4: Ablation study on the Synthetic set\n\n<table>\n<colgroup>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n<col/>\n</colgroup>\n<thead>\n<tr>\n<th>\nModel\n</th>\n<th>\nDifficult\n</th>\n<th>\n</th>\n<th>\n</th>\n<th>\nNormal\n</th>\n<th>\n</th>\n<th>\n</th>\n<th>\nEasy\n</th>\n<th>\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\n2-11\n</td>\n<td>\nIoU\n</td>\n<td>\nMCC\n</td>\n<td>\nNMM\n</td>\n<td>\nIoU\n</td>\n<td>\nMCC\n</td>\n<td>\nNMM\n</td>\n<td>\nIoU\n</td>\n<td>\nMCC\n</td>\n<td>\nNMM\n</td>\n</tr>\n<tr>\n<td>\nTAF\n</td>\n<td>\n0.7009\n</td>\n<td>\n0.7644\n</td>\n<td>\n0.4400\n</td>\n<td>\n0.8789\n</td>\n<td>\n0.9160\n</td>\n<td>\n0.7807\n</td>\n<td>\n0.9704\n</td>\n<td>\n0.9769\n</td>\n<td>\n0.9550\n</td>\n</tr>\n<tr>\n<td>\nMSTAF\n</td>\n<td>\n**0.7427**\n</td>\n<td>\n**0.8022**\n</td>\n<td>\n**0.5206**\n</td>\n<td>\n**0.9105**\n</td>\n<td>\n**0.9410**\n</td>\n<td>\n**0.8406**\n</td>\n<td>\n**0.9752**\n</td>\n<td>\n**0.9808**\n</ "
    ]
  },
  {
    "edit": [
      "the geometry of these regions is identical to the Einstein-Rosen bridge in a two-sided BTZ geometry with the same mass as the corresponding operator. We will take the interpretation that the details of what appears outside the domain of dependence of W is part of the definition of the operator associated to that region. So in sum, the action of the Lorentzian cap is\n\n\\[I_{\\rm cap}=\\frac{1}{4\\pi G_{\\rm N}}V_{\\mathcal{W}}+\\sum_{i={\\rm BH}}I_{i}= \\frac{\\pi}{4G_{\\rm N}}\\sum_{i}\\left({\\rm Re}(\\eta_{i})-\\frac{1}{3}\\right)+ \\sum_{i={\\rm BH}}I_{i}\\,. \\tag{6.8}\\]\n\nHence we see that it is the sum of three contributions that each depend only on one of the three operators. Thus, we can absorb the corresponding phase generated by the action of the Lorentzian geometry into the definition of the operators, and we will arrive at a real result for the three-point function. Hence we arrive at the total action for the single-sided geometry, I single-sided = 1/2 I wormhole + i\\sum_{i}f(i)\\,.\\] (6.9)\n\nAgain, it is a nontrivial result that the imaginary term is given by the sum shown above since this allows us to absorb the corresponding phases into the definition of the operators. After eliminating these phases, we recover the three-point function from the single-sided bulk geometry, i.e.,\n\n\\[e^{-I}\\approx G_{L}(z_{1},z_{2},z_{3})\\,, \\tag{6.10}\\]\n\nwhere \\(G_{L}(z_{1},z_{2},z_{3})\\) is the semiclassical Liouville three-point correlator in [ 17].\n\n## 7 Discussion\n\nIn this paper we discussed three-dimensional asymptotically AdS\\({}_{3}\\) geometries that are sourced by the insertion of boundary operators whose scaling dimensions is heavy as the central charge of the holographic CFT 2 . The presence of any such operators deforms the AdS geometry by inducing a non vanishing expectation value for the holographic stress tensor, close to the boundary. This is true perturbatively in general dimensions, but in three-dimensions there is an exact solution, due to Ba\u02dcnados [ 4 ], that describes such deformation. However, this metric does not describe the full bulk spacetime. When only two black hole operators are inserted, we showed that the full geometry is simply an infinite covering of the Euclidean BTZ black hole [ 1 ], but when three or more operators are inserted, we found that the completion of the Ba\u02dcnados metric into the bulk is a wormhole geometry involving multiple asymptotic boundaries. To understand this rather non trivial fact we rephrased the construction of the bulk geometry as a quotient of AdS\\({}_{3}\\) realized by domes and doors. The dome construction is a well know characterization of hyperbolic geometries with an asymptotically AdS\\({}_{3}\\) metric, and more familiar from the study of black hole thermodynamics, see e.g., [ 38 ], but the addition of the doors is new as far as we can tell. As in the description of a Euclidean two-point function geometry in section 2 , i.e., as empty AdS\\({}_{3}\\) with identifications, the doors are needed to describe the insertion of boundary\n\n29 "
    ],
    "kosmos": [
      "the geometry of these regions is identical to the Einstein-Rosen bridge in a two-sided BTZ geometry with the same mass as the corresponding operator. We will take the interpretation that the details of what appears outside the domain of dependence of W is part of the definition of the operator associated to that region. So in sum, the action of the Lorentzian cap is\n\n\\[I_{\\rm cap}=\\frac{1}{4\\pi G_{\\rm N}}V_{\\mathcal{W}}+\\sum_{i={\\rm BH}}I_{i}= \\frac{\\pi}{4G_{\\rm N}}\\sum_{i}\\left({\\rm Re}(\\eta_{i})-\\frac{1}{3}\\right)+ \\sum_{i={\\rm BH}}I_{i}\\,. \\tag{6.8}\\]\n\nHence we see that it is the sum of three contributions that each depend only on one of the three operators. Thus, we can absorb the corresponding phase generated by the action of the Lorentzian geometry into the definition of the operators, and we will arrive at a real result for the three-point function.\n\nHence we arrive at the total action for the single-sided geometry,\n\n\\[I_{\\rm single-sided}=\\frac{1}{2}I_{\\rm wormhole}+i\\sum_{i}f(i)\\,. \\tag{6.9}\\]\n\nAgain, it is a nontrivial result that the imaginary term is given by the sum shown above since this allows us to absorb the corresponding phases into the definition of the operators. After eliminating these phases, we recover the three-point function from the single-sided bulk geometry, i.e.,\n\n\\[e^{-I}\\approx G_{L}(z_{1},z_{2},z_{3})\\,, \\tag{6.10}\\]\n\nwhere \\(G_{L}(z_{1},z_{2},z_{3})\\) is the semiclassical Liouville three-point correlator in [ 17].\n\n## 7 Discussion\n\nIn this paper we discussed three-dimensional asymptotically AdS\\({}_{3}\\) geometries that are sourced by the insertion of boundary operators whose scaling dimensions is heavy as the central charge of the holographic CFT\\({}_{2}\\). The presence of any such operators deforms the AdS geometry by inducing a non vanishing expectation value for the holographic stress tensor, close to the boundary. This is true perturbatively in general dimensions, but in three-dimensions there is an exact solution, due to Ba\u02dcnados [ 4 ], that describes such deformation. However, this metric does not describe the full bulk spacetime. When only two black hole operators are inserted, we showed that the full geometry is simply an infinite covering of the Euclidean BTZ black hole [ 1 ], but when three or more operators are inserted, we found that the completion of the Ba\u02dcnados metric into the bulk is a wormhole geometry involving multiple asymptotic boundaries. To understand this rather non trivial fact we rephrased the construction of the bulk geometry as a quotient of AdS\\({}_{3}\\) realized by domes and doors. The dome construction is a well know characterization of hyperbolic geometries with an asymptotically AdS\\({}_{3}\\) metric, and more familiar from the study of black hole thermodynamics, see e.g., [ 38], but the addition of the doors is new as far as we can tell.\n\nAs in the description of a Euclidean two-point function geometry in section 2, i.e., as empty AdS\\({}_{3}\\) with identifications, the doors are needed to describe the insertion of boundary "
    ]
  },
  {
    "edit": [
      "3.1.3. The case of a general periodic force. Finally, we remark that in the general case of a \\(\\theta\\)-periodic force of the form\n\n\\[\\mathcal{F}(t/\\theta)=\\sum_{\\ell=1}^{+\\infty}F_{\\ell}\\cos(\\omega( \\ell)t),\\quad\\text{where }\\omega(\\ell):=\\frac{2\\pi\\ell}{\\theta} \\tag{3.20}\\]\n\nwhose real valued Fourier coefficients satisfy \\(\\sum_{\\ell=1}^{+\\infty}(\\ell F_{\\ell})^{2}&lt;+\\infty\\), the work performed by the force can be determined from the formula:\n\n\\[W(n)=\\sum_{\\ell=1}^{+\\infty}(\\omega(\\ell)F_{\\ell})\\frac{2N( \\omega(\\ell),n)}{D(\\omega(\\ell),n)}. \\tag{3.21}\\]\n\nTherefore its behavior, as \\(n\\) gets large, can be determined from the term by term analysis of the series appearing on the right hand side of ( 3.21 ).\n\n### Energy\n\nAs in Section 3.1 we assume that the periodic force \\(\\mathcal{F}(t)\\) is given by ( 3.3 ). The time average of the expectation of the total energy energy of the chain \\(E(\\omega,n)\\) breaks up into the sum of thermal component \\(E_{\\text{th}}(\\omega,n)=\\sum_{x \u2208 I_{n}}\u27e8\u27e8 e_{x}^{\\text{th}} \u27e9\u27e9\\) and the mechanical one \\(E_{\\text{mech}}(\\omega,n)=\\sum_{x \u2208 I_{n}}\u27e8\u27e8 e_{x}^{\\text{mech}} \u27e9\u27e9\\), with \\(e_{x}^{\\text{th}}\\) and \\(e_{x}^{\\text{mech}}\\) defined in ( 2.17 ) and ( 2.16 ), respectively.\n\nConsidering the behavior of the thermal energy functional, defined in ( 2.15 ), it has been shown in [ 0 ], that in the case \\(\\omega_{0}=0\\) and \u03b3 \u2212 = \u03b3 + we have \u27e8\u27e8 e_{x}^{\\text{th}} \u27e9\u27e9 = 1 2 (T \u2212 + T + ) for all x = 1 ,...,n \u2212 1. If \u03c9 0 > 0 and \u03b3 \u2212 = \u03b3 + , then [ 0 , formulas (38) and (42)] give\n\n\u27e8\u27e8 e_{x}^{\\text{th}} \u27e9\u27e9 = 1 2 (T \u2212 + T + )(1 + o x ), where \u2223 o x \u2223 \u2a7d g x (n+1\u2212x)\n\nfor some constants C > 0, g > 1 independent of n. As a result we have E th (\u03c9,n) \u223c n, as n \u2192 +\u221e.\n\n#### 3.2.1. Formula for the total mechanical energy functional for a single mode oscillating force\n\nIn what follows we consider the behavior of the mechanical component of the energy. Again, assume that the force is given by ( 3.3 ). It turns out, see Section C of the Appendix, that the time average over the period of the microscopic mechanical energy density equals\n\n\u27e8\u27e8 e_{x}^{\\text{mech}} \u27e9\u27e9 = \\frac{F^{2}}{2}\\cdot\\frac{M_{x}(\\omega,n)}{D(\\omega,n)},\\] (3.22)\n\nwhere \\(D(\\omega,n)\\) is given by ( 3.5 ) and\n\n\\[M_{x}(\\omega,n)=G_{x}^{1}(\\omega,n)^{2}(\\omega^{2}+\\omega_{0}^{2 })+(\\nabla^{ \u2217}G_{x}^{1})(\\omega,n)^{2}+(2\u03c9\u03b3 \u2212 )^{2}[\\mathcal{G}_{x}(\\omega,n)^{2}+(\\nabla^{ \u2217}\\mathcal{G}_{x})(\\omega,n)^{2}],\\] "
    ],
    "kosmos": [
      "3.1.3. The case of a general periodic force. Finally, we remark that in the general case of a \\(\\theta\\)-periodic force of the form\n\n\\[\\mathcal{F}(t/\\theta)=\\sum_{\\ell=1}^{+\\infty}F_{\\ell}\\cos(\\omega( \\ell)t),\\quad\\text{where }\\omega(\\ell):=\\frac{2\\pi\\ell}{\\theta} \\tag{3.20}\\]\n\nwhose real valued Fourier coefficients satisfy \\(\\sum_{\\ell=1}^{+\\infty}(\\ell F_{\\ell})^{2}&lt;+\\infty\\), the work performed by the force can be determined from the formula:\n\n\\[W(n)=\\sum_{\\ell=1}^{+\\infty}(\\omega(\\ell)F_{\\ell})\\frac{2N( \\omega(\\ell),n)}{D(\\omega(\\ell),n)}. \\tag{3.21}\\]\n\nTherefore its behavior, as \\(n\\) gets large, can be determined from the term by term analysis of the series appearing on the right hand side of ( 3.21 ).\n\n### Energy\n\nAs in Section 3.1 we assume that the periodic force \\(\\mathcal{F}(t)\\) is given by ( 3.3 ). The time average of the expectation of the total energy energy of the chain \\(E(\\omega,n)\\) breaks up into the sum of thermal component \\(E_{\\text{th}}(\\omega,n)=\\sum_{x \u2208 I_{n}}\u27e8\u27e8 e_{x}^{\\text{th}} \u27e9\u27e9\\) and the mechanical one \\(E_{\\text{mech}}(\\omega,n)=\\sum_{x \u2208 I_{n}}\u27e8\u27e8 e_{x}^{\\text{mech}} \u27e9\u27e9\\), with \\(e_{x}^{\\text{th}}\\) and \\(e_{x}^{\\text{mech}}\\) defined in ( 2.17 ) and ( 2.16 ), respectively.\n\nConsidering the behavior of the thermal energy functional, defined in ( 2.15 ), it has been shown in [ 0 ], that in the case \\(\\omega_{0}=0\\) and \u03b3 \u2212 = \u03b3 + we have \u27e8\u27e8 e_{x}^{\\text{th}} \u27e9\u27e9 = 1 2 (T \u2212 + T + ) for all x = 1 ,...,n \u2212 1. If \u03c9 0 > 0 and \u03b3 \u2212 = \u03b3 + , then [ 0 , formulas (38) and (42)] give\n\n\u27e8\u27e8 e_{x}^{\\text{th}} \u27e9\u27e9 = 1 2 (T \u2212 + T + )(1 + o x ), where \u2223 o x \u2223 \u2a7d g x (n+1\u2212x)\n\nfor some constants C > 0, g > 1 independent of n. As a result we have E th (\u03c9,n) \u223c n, as n \u2192 +\u221e.\n\n#### 3.2.1. Formula for the total mechanical energy functional for a single mode oscillating force\n\nIn what follows we consider the behavior of the mechanical component of the energy. Again, assume that the force is given by ( 3.3 ). It turns out, see Section C of the Appendix, that the time average over the period of the microscopic mechanical energy density equals\n\n\u27e8\u27e8 e_{x}^{\\text{mech}} \u27e9\u27e9 = \\frac{F^{2}}{2}\\cdot\\frac{M_{x}(\\omega,n)}{D(\\omega,n)},\\] (3.22)\n\nwhere \\(D(\\omega,n)\\) is given by ( 3.5 ) and\n\n\\[M_{x}(\\omega,n)=G_{x}^{1}(\\omega,n)^{2}(\\omega^{2}+\\omega_{0}^{2 })+(\\nabla^{ \u2217}G_{x}^{1})(\\omega,n)^{2}+(2\u03c9\u03b3 \u2212 )^{2}[\\mathcal{G}_{x}(\\omega,n)^{2}+(\\nabla^{ \u2217}\\mathcal{G}_{x})(\\omega,n)^{2}],\\] "
    ]
  },
  {
    "edit": [
      "G\u00f6ldi and Rietsche\n\nFigure 2: Study 1 procedure.\n\n### Study 1\n\nBased on the pilot, we decided to proceed by fixing the reported bugs and replacing the usability scale with potential mediators to better assess which users would prefer the different bots. We conducted this study with n=71 participants (see Section 3).\n\n#### 4.2.1 Measures\n\nIn Study 1, we added 7-level bipolar rating scales for direct comparison (See Section 3.2). We were interested in how much control participants experienced; how natural they felt the chats to be; how well their intent was fulfilled in the chats; and how satisfied they were with them. See Table 2 for items and reliabilities of the constructs.\n\nTo account for potential mediation, we assessed interindividual difference variables, namely the most prominent personality measure, big 5, using a 15-item scale (BFI-2-XS) \\[39\\]. As insert expansions exist to smooth interaction by shaping expectations, we also assessed our participants need for cognitive closure (NFC-15) \\[40\\].\n\n<table>\n<colgroup>\n<col/>\n<col/>\n<col/>\n</colgroup>\n<thead>\n<tr>\n<th>\n<strong>\nScale\n</strong>\n</th>\n<th>\n<strong>\nItems\n</strong>\n</th>\n<th>\n<strong>\nReliability\n</strong>\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>\nWhich of the two chats...\n</td>\n<td></td>\n</tr>\n<tr>\n<td>\nControl\n</td>\n<td>\nenabled more personal direction? offered you more autonomy? let you steer the conversation more?\n</td>\n<td>\n<em>\nStudy 1:\n</em>\n\u03b1 =\n<em>\n.77\n</em>\n, \u03bb =\n<em>\n.79\n</em>\n<em>\n(overall), for scenarios .77/.79, .84/.85, .91/.91 each; Study 2:\n</em>\n\u03b1 =\n<em>\n.81\n</em>\n, \u03bb =\n<em>\n.84\n</em>\n</td>\n</tr>\n</tbody>\n</table>\n\n| Naturalness | seemed more authentic? had a more genuine feel? was more natural? | *Study 1:* \u03b1 = *.93*, \u03bb = *.93* *(overall), for scenarios .93/.93, .92/.91, .97/.97 each; Study 2:* \u03b1 = *.93*, \u03bb = *.95* |\n| Intent-Effectiveness | had more suitable responses? lived up to your expectations better? | *Study 1:* \u03b1 = *.93*, \u03bb = *.93* *(overall), for scenarios .93/.91, .95/.93, .96/.95 each; Study 2:* \u03b1 = *.95*, \u03bb = *.96* |\n| Satisfaction | was more to your liking? was more satisfactory? | *Study 1:* \u03b1 = *.95*, \u03bb = *.95* *(overall), for scenarios .95/.95, .92/.92, .97/.97 each; Study 2:* \u03b1 = *.98*, \u03bb = *.97* |\n\nTable 2: Direct comparison bipolar rating scales with Crohnbach\u2019s \u03b1 and Guttman\u2019s \u03bb 6.\n\n#### 4.2.2 Procedure\n\nAfter a short demographic questionnaire, n=71 participants were given 3 scenarios (see Figure 2), rating each before turning to the next (see Figure 3.2). Having completed all scenarios and evaluations, feedback was elicited and interindividual variables were assessed.\n\n#### 4.2.3 Feedback\n\nTo quantify the feedback, we repeated the procedure described for the pilot. Feedback on the study was 69.01% positive (neutral opinions tend to be rated as negative as well, e.g. \"I have no opinion about the study.\"), on the interface only 52.11% (negatives include no feedback at all, neutral statements such as \"It was fast and responsive, just feel like the loading is too big and the lettering also\", but also some on bugs like \"It was fine, although sometimes my prompt would trigger a loading animation that the bot would never reply to, so I had to prompt again, which left the loading anim on the screen for one of the bots but not the other, not a big deal.\" or \"It was trusting when it could not listen or answer all "
    ],
    "kosmos": [
      "G\u00f6ldi and Rietsche\n\nFigure 2: Study 1 procedure.\n\n### Study 1\n\nBased on the pilot, we decided to proceed by fixing the reported bugs and replacing the usability scale with potential mediators to better assess which users would prefer the different bots. We conducted this study with n=71 participants (see Section 3).\n\n#### 4.2.1 Measures\n\nIn Study 1, we added 7-level bipolar rating scales for direct comparison (See Section 3.2). We were interested in how much control participants experienced; how natural they felt the chats to be; how well their intent was fulfilled in the chats; and how satisfied they were with them. See Table 2 for items and reliabilities of the constructs.\n\nTo account for potential mediation, we assessed interindividual difference variables, namely the most prominent personality measure, big 5, using a 15-item scale (BFI-2-XS) \\[39\\]. As insert expansions exist to smooth interaction by shaping expectations, we also assessed our participants need for cognitive closure (NFC-15) \\[40\\].\n\n<table>\n<colgroup>\n<col/>\n<col/>\n<col/>\n</colgroup>\n<thead>\n<tr>\n<th>\n<strong>\nScale\n</strong>\n</th>\n<th>\n<strong>\nItems\n</strong>\n</th>\n<th>\n<strong>\nReliability\n</strong>\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>\nWhich of the two chats...\n</td>\n<td></td>\n</tr>\n<tr>\n<td>\nControl\n</td>\n<td>\nenabled more personal direction? offered you more autonomy? let you steer the conversation more?\n</td>\n<td>\n<em>\nStudy 1:\n</em>\n\u03b1 =\n<em>\n.77\n</em>\n, \u03bb =\n<em>\n.79\n</em>\n<em>\n(overall), for scenarios .77/.79, .84/.85, .91/.91 each; Study 2:\n</em>\n\u03b1 =\n<em>\n.81\n</em>\n, \u03bb =\n<em>\n.84\n</em>\n</td>\n</tr>\n</tbody>\n</table>\n\n| Naturalness | seemed more authentic? had a more genuine feel? was more natural? | *Study 1:* \u03b1 = *.93*, \u03bb = *.93* *(overall), for scenarios .93/.93, .92/.91, .97/.97 each; Study 2:* \u03b1 = *.93*, \u03bb = *.95* |\n| Intent-Effectiveness | had more suitable responses? lived up to your expectations better? | *Study 1:* \u03b1 = *.93*, \u03bb = *.93* *(overall), for scenarios .93/.91, .95/.93, .96/.95 each; Study 2:* \u03b1 = *.95*, \u03bb = *.96* |\n| Satisfaction | was more to your liking? was more satisfactory? | *Study 1:* \u03b1 = *.95*, \u03bb = *.95* *(overall), for scenarios .95/.95, .92/.92, .97/.97 each; Study 2:* \u03b1 = *.98*, \u03bb = *.97* |\n\nTable 2: Direct comparison bipolar rating scales with Crohnbach\u2019s \u03b1 and Guttman\u2019s \u03bb 6.\n\n#### 4.2.2 Procedure\n\nAfter a short demographic questionnaire, n=71 participants were given 3 scenarios (see Figure 2), rating each before turning to the next (see Figure 3.2). Having completed all scenarios and evaluations, feedback was elicited and interindividual variables were assessed.\n\n#### 4.2.3 Feedback\n\nTo quantify the feedback, we repeated the procedure described for the pilot. Feedback on the study was 69.01% positive (neutral opinions tend to be rated as negative as well, e.g. \"I have no opinion about the study.\"), on the interface only 52.11% (negatives include no feedback at all, neutral statements such as \"It was fast and responsive, just feel like the loading is too big and the lettering also\", but also some on bugs like \"It was fine, although sometimes my prompt would trigger a loading animation that the bot would never reply to, so I had to prompt again, which left the loading anim on the screen for one of the bots but not the other, not a big deal.\" or \"It was trusting when it could not listen or answer all "
    ]
  },
  {
    "edit": [
      "\n\n## 1. Introduction\n\nThe (logarithmic) Mahler measure of a non-zero rational function \\(P\\in\\mathbb{C}\\left(x_{1},\\ldots,x_{n}\\right)^{*}\\) is defined by\n\n\\[\\mathrm{m}\\left(P\\right)=\\mathrm{m}(P(x_{1},\\ldots,x_{n})) :=\\frac{1}{(2\\pi i)^{n}}\\int_{\\mathbb{T}^{n}}\\log|P\\left(x_{1}, \\ldots,x_{n}\\right)|\\frac{dx_{1}}{x_{1}}\\cdots\\frac{dx_{n}}{x_{n}}, \\tag{1}\\]\n\nwhere \\(\\mathbb{T}^{n}=\\left\\{(x_{1},\\ldots,x_{n})\\in\\mathbb{C}^{*}\\times\\mathbb{C}^{ *}\\times\\cdots\\times\\mathbb{C}^{*}:\\left|x_{1}\\right|=\\cdots=\\left|x_{n} \\right|=1\\right\\}.\\)\n\nThe first appearance of this quantity (for one variable polynomials) can be traced back to Lehmer\u2019s work [1] on Mersenne numbers, and its several variable form first appeared in the work of Mahler [2] regarding a simpler proof of the Gel\u2019fond-Mahler inequality, and it was later named after him. In the early 80 \u2019s, Smyth [3] discovered the following remarkable identities: m( x + y + 1) =3 4\\pi L( \\chi_{-3}, 2), m(1 + x + y + z) =7 2\\pi^{2}\\zeta(3), where L( \\chi_{-3}, 2) is the Dirichlet L-function of the quadratic character \\(\\chi_{-3}\\) of conductor 3, and \\(\\zeta(s)\\) is the Riemann zeta function (for more details see [4]). These are two of the initial formulas for several variable cases. Later the work of Boyd [5], Deninger [6], Rodriguez-Villeags [7] and others provided us with interesting connections among Mahler measure, higher regulators, and Be\u02d8\u0131linson\u2019s conjectures. The conjectural formulas to support their work, such as\n\n\\[\\mathrm{m}(P_{k}(x,y))\\stackrel{{?}}{{=}}r_{k}L^{\\prime}(E_{N(k) },0),\\qquad r_{k}\\in\\mathbb{Q},\\]\n\nwere eventually proved for certain polynomials, due to Rodriguez-Villegas [7], Rogers and Zudilin [8, 9] et al. Here \\(E_{N(k)}\\) is an elliptic curve of conductor \\(N(k)\\) associated to \\(P_{k},\\) and the question mark stands for a numerical formula that is true for at least 20 decimal places. (See the book of Brunault and Zudilin [10] for more details.) In a different direction, Cassaigne and Maillot [11] generalized the formula found by Smyth to m( a + bx + cy) for arbitrary complex constants a, b, and c : m( ax + by + c) = \\begin{cases}\\alpha\\log|a|+\\beta\\log|b|+\\gamma\\log|c|+D\\left(\\frac{|a|}{|b|}e^ {i\\gamma}\\right)&amp;\\text{if $\\Delta$ holds,}\\\\ \\log\\max\\{|a|,|b|,|c|\\}&amp;\\text{if $\\Delta$ does not hold,}\\end{cases}\\]\n\nwhere \\(\\Delta\\) stands for the statement that |a| , |b| , and |c| are the lengths of the sides of a planar triangle, and in that case, \\(\\alpha,\\beta,\\) and \\(\\gamma\\) are the angles opposite to the sides of the lengths |a| , |b| and |c| respectively (see Figure 1).\n\nWe also remark that the constant coefficient can be replaced by a variable without changing the Mahler measure, in the sense that m( ax + by + c) = m( ax + by + cz). Additionally, it is"
    ],
    "kosmos": [
      "\n\n## 1. Introduction\n\nThe (logarithmic) Mahler measure of a non-zero rational function \\(P\\in\\mathbb{C}\\left(x_{1},\\ldots,x_{n}\\right)^{*}\\) is defined by\n\n\\[\\mathrm{m}\\left(P\\right)=\\mathrm{m}(P(x_{1},\\ldots,x_{n})) :=\\frac{1}{(2\\pi i)^{n}}\\int_{\\mathbb{T}^{n}}\\log|P\\left(x_{1}, \\ldots,x_{n}\\right)|\\frac{dx_{1}}{x_{1}}\\cdots\\frac{dx_{n}}{x_{n}}, \\tag{1}\\]\n\nwhere \\(\\mathbb{T}^{n}=\\left\\{\\left(x_{1},\\ldots,x_{n}\\right)\\in\\mathbb{C}^{*}\\times \\mathbb{C}^{*}\\times\\cdots\\times\\mathbb{C}^{*}:\\left|x_{1}\\right|=\\cdots= \\left|x_{n}\\right|=1\\right\\}.\\)\n\nThe first appearance of this quantity (for one variable polynomials) can be traced back to Lehmer's work [1] on Mersenne numbers, and its several variable form first appeared in the work of Mahler [2] regarding a simpler proof of the Gel'fond-Mahler inequality, and it was later named after him.\n\nIn the early 80's, Smyth [3] discovered the following remarkable identities:\n\n\\[\\mathrm{m}(x+y+1) =\\frac{3}{4\\pi}L(\\chi_{-3},2),\\] \\[\\mathrm{m}(1+x+y+z) =\\frac{7}{2\\pi^{2}}\\zeta(3),\\]\n\nwhere \\(L(\\chi_{-3},2)\\) is the Dirichlet \\(L\\)-function of the quadratic character \\(\\chi_{-3}\\) of conductor 3, and \\(\\zeta(s)\\) is the Riemann zeta function (for more details see [4]). These are two of the initial formulas for several variable cases.\n\nLater the work of Boyd [5], Deninger [6], Rodriguez-Villeags [7] and others provided us with interesting connections among Mahler measure, higher regulators, and Beilinson's conjectures. The conjectural formulas to support their work, such as\n\n\\[\\mathrm{m}(P_{k}(x,y))\\stackrel{{?}}{{=}}r_{k}L^{\\prime}(E_{N(k) },0),\\qquad r_{k}\\in\\mathbb{Q},\\]\n\nwere eventually proved for certain polynomials, due to Rodriguez-Villegas [7], Rogers and Zudilin [8, 9] et al. Here \\(E_{N(k)}\\) is an elliptic curve of conductor \\(N(k)\\) associated to \\(P_{k},\\) and the question mark stands for a numerical formula that is true for at least 20 decimal places. (See the book of Brunault and Zudilin [10] for more details.)\n\nIn a different direction, Cassaigne and Maillot [11] generalized the formula found by Smyth to \\(\\mathrm{m}(a+bx+cy)\\) for arbitrary complex constants \\(a,b,\\) and \\(c:\\)\n\n(2) \\[\\pi\\mathrm{m}(ax+by+c)=\\left\\{\\begin{array}{ll}\\alpha\\log|a|+ \\beta\\log|b|+\\gamma\\log|c|+D\\left(\\frac{|a|}{|b|}e^{i\\gamma}\\right)&amp;\\text{if\n$\\Delta$ holds,}\\\\ \\log\\max\\{|a| , |b| , |c|\\}&amp;\\text{if $\\Delta$ does not hold,}\\end{array}\\right.\\]\n\nwhere \\(\\Delta\\) stands for the statement that \\(|a|,\\)\\(|b|,\\) and \\(|c|\\) are the lengths of the sides of a planar triangle, and in that case, \\(\\alpha,\\beta,\\) and \\(\\gamma\\) are the angles opposite to the sides of the lengths \\(|a|,\\)\\(|b|\\) and \\(|c|\\) respectively (see Figure 1).\n\nWe also remark that the constant coefficient can be replaced by a variable without changing the Mahler measure, in the sense that \\(\\mathrm{m}(ax+by+c)=\\mathrm{m}(ax+by+cz).\\) Additionally, it is"
    ]
  },
  {
    "edit": [
      "where the initial uAD is set as u. By running the iteration (16) N times, we obtain uAD \\(\\doteq u^{(N)}_{AD}\\) as the Nth order van Cittert operator applied to u. However, to make this iteration practical, we need to recall that each application of the filter G requires the inversion of a differential operator. Therefore, denoting by \\(\\widetilde{\\boldsymbol{u}}^{(n+1)}\\) the quantity \\(\\widetilde{\\boldsymbol{u}}^{(n+1)}\\doteq G\\boldsymbol{u}^{(n)}_{AD}\\), and substituting in our filter from (12), we observe that \\(\\widetilde{\\boldsymbol{u}}^{(n+1)}\\) is found by solving\n\n\\[(I-\\delta^{2}\\Delta)^{-1}\\boldsymbol{u}^{(n)}_{AD}=\\widetilde{\\boldsymbol{u}} ^{(n+1)}\\Longleftrightarrow(I-\\delta^{2}\\Delta)\\widetilde{\\boldsymbol{u}}^{(n +1)}=\\boldsymbol{u}^{(n)}_{AD}. \\tag{17}\\]\n\nEquation (17) requires to solve a linear system at each iteration n = 0, ..., N\u2212 1. As discussed above, in order to employ the van Cittert AD in a ROM setting, by multiplying (17) by each test function in our ROM space and expanding our prospective solution as a linear combination of ROM basis functions, we obtain the linear system\n\n\\[\\left(\\boldsymbol{M}+\\delta^{2}\\boldsymbol{S}\\right)\\widetilde{\\boldsymbol{c} }^{(n+1)}=\\boldsymbol{M}\\boldsymbol{c}^{n}_{AD}. \\tag{18}\\]\n\nThus, the iterative process (16) amounts to setting \\(\\boldsymbol{c}^{(0)}_{AD}=\\boldsymbol{c}\\), updating the coefficients of the ROM AD velocity as\n\n\\[\\boldsymbol{c}^{(n+1)}_{AD}=\\boldsymbol{c}^{(n)}_{AD}+\\{\\boldsymbol{c}- \\widetilde{\\boldsymbol{c}}^{(n+1)}\\}\\quad\\quad n=0,\\ldots,N-1,\\]\n\nand finally defining \\(\\boldsymbol{c}_{AD}\\doteq\\boldsymbol{c}^{(N)}_{AD}\\).\n\n### The Tikhonov AD\n\nThe Tikhonov method of approximate deconvolution is defined as\n\n\\[\\boldsymbol{u}_{AD}=D_{\\mu}^{T}\\boldsymbol{u}=(G^{*}G+\\mu I)^{-1}G^{*} \\boldsymbol{u}, \\tag{19}\\]\n\nwhere G\\({}^{*}\\) denotes the adjoint of the operator G, and \u00b5 \u2208 R\\({}^{+}\\) is a positive constant; we refer, e.g., to [39, section 3.3.1] for further details on the Tikhonov AD. When plugging in the specific filter from (12) and proceeding formally, we can write\n\n\\[[G^{*}G+\\mu I]\\boldsymbol{u}_{AD}=G^{*}\\boldsymbol{u},\\] \\[[(I-\\delta^{2}\\Delta)^{-*}(I-\\delta^{2}\\Delta)^{-1}+\\mu I] \\boldsymbol{u}_{AD}=(I-\\delta^{2}\\Delta)^{-*}\\boldsymbol{u}],\\] \\[[(I-\\delta^{2}\\Delta)^{-1}+\\mu(I-\\delta^{2}\\Delta)^{*}] \\boldsymbol{u}_{AD}=\\boldsymbol{u},\\] \\[[I+\\mu(I-\\delta^{2}\\Delta)(I-\\delta^{2}\\Delta)^{*}] \\boldsymbol{u}_{AD}=(I-\\delta^{2}\\Delta)\\boldsymbol{u},\\] \\[[I+\\mu(I-\\delta^{2}\\Delta^{*}-\\delta^{2}\\Delta+\\delta^{2}\\Delta^ {*})] \\boldsymbol{u}_{AD}=(I-\\delta^{2}\\Delta)\\boldsymbol{u},\\] \\[[I+\\mu(I-2\\delta^{2}\\Delta+\\delta^{4}\\Delta\\Delta)] \\boldsymbol{u}_{AD}=(I-\\delta^{2}\\Delta)\\boldsymbol{u}.\\] (20) "
    ],
    "kosmos": [
      "where the initial uAD is set as u. By running the iteration (16) N times, we obtain uAD \\(\\doteq u^{(N)}_{AD}\\) as the Nth order van Cittert operator applied to u. However, to make this iteration practical, we need to recall that each application of the filter G requires the inversion of a differential operator. Therefore, denoting by \\(\\widetilde{\\boldsymbol{u}}^{(n+1)}\\) the quantity \\(\\widetilde{\\boldsymbol{u}}^{(n+1)}\\doteq G\\boldsymbol{u}^{(n)}_{AD}\\), and substituting in our filter from (12), we observe that \\(\\widetilde{\\boldsymbol{u}}^{(n+1)}\\) is found by solving\n\n\\[(I-\\delta^{2}\\Delta)^{-1}\\boldsymbol{u}^{(n)}_{AD}=\\widetilde{\\boldsymbol{u}} ^{(n+1)}\\Longleftrightarrow(I-\\delta^{2}\\Delta)\\widetilde{\\boldsymbol{u}}^{(n +1)}=\\boldsymbol{u}^{(n)}_{AD}. \\tag{17}\\]\n\nEquation (17) requires to solve a linear system at each iteration n = 0, ..., N\u2212 1. As discussed above, in order to employ the van Cittert AD in a ROM setting, by multiplying (17) by each test function in our ROM space and expanding our prospective solution as a linear combination of ROM basis functions, we obtain the linear system\n\n\\[\\left(\\boldsymbol{M}+\\delta^{2}\\boldsymbol{S}\\right)\\widetilde{\\boldsymbol{c} }^{(n+1)}=\\boldsymbol{M}\\boldsymbol{c}^{n}_{AD}. \\tag{18}\\]\n\nThus, the iterative process (16) amounts to setting \\(\\boldsymbol{c}^{(0)}_{AD}=\\boldsymbol{c}\\), updating the coefficients of the ROM AD velocity as\n\n\\[\\boldsymbol{c}^{(n+1)}_{AD}=\\boldsymbol{c}^{(n)}_{AD}+\\{\\boldsymbol{c}- \\widetilde{\\boldsymbol{c}}^{(n+1)}\\}\\quad\\quad n=0,\\ldots,N-1,\\]\n\nand finally defining \\(\\boldsymbol{c}_{AD}\\doteq\\boldsymbol{c}^{(N)}_{AD}\\).\n\n### The Tikhonov AD\n\nThe Tikhonov method of approximate deconvolution is defined as\n\n\\[\\boldsymbol{u}_{AD}=D_{\\mu}^{T}\\boldsymbol{u}=(G^{*}G+\\mu I)^{-1}G^{*} \\boldsymbol{u}, \\tag{19}\\]\n\nwhere G\\({}^{*}\\) denotes the adjoint of the operator G, and \u00b5 \u2208 R\\({}^{+}\\) is a positive constant; we refer, e.g., to [39, section 3.3.1] for further details on the Tikhonov AD. When plugging in the specific filter from (12) and proceeding formally, we can write\n\n\\[[G^{*}G+\\mu I]\\boldsymbol{u}_{AD}=G^{*}\\boldsymbol{u},\\] \\[[(I-\\delta^{2}\\Delta)^{-*}(I-\\delta^{2}\\Delta)^{-1}+\\mu I] \\boldsymbol{u}_{AD}=(I-\\delta^{2}\\Delta)^{-*}\\boldsymbol{u}],\\] \\[[(I-\\delta^{2}\\Delta)^{-1}+\\mu(I-\\delta^{2}\\Delta)^{*}] \\boldsymbol{u}_{AD}=\\boldsymbol{u},\\] \\[[I+\\mu(I-\\delta^{2}\\Delta)(I-\\delta^{2}\\Delta)^{*}] \\boldsymbol{u}_{AD}=(I-\\delta^{2}\\Delta)\\boldsymbol{u},\\] \\[[I+\\mu(I-\\delta^{2}\\Delta^{*}-\\delta^{2}\\Delta+\\delta^{2}\\Delta^ {*})] \\boldsymbol{u}_{AD}=(I-\\delta^{2}\\Delta)\\boldsymbol{u},\\] \\[[I+\\mu(I-2\\delta^{2}\\Delta+\\delta^{4}\\Delta\\Delta)] \\boldsymbol{u}_{AD}=(I-\\delta^{2}\\Delta)\\boldsymbol{u}.\\] (20) "
    ]
  },
  {
    "edit": [
      "o accept a trigger of 40 ns and an integration time of 325 ns to collect photons on the sensor tile. The overvoltage of the silicon photomultipliers (SiPMs) is set to 3 V. As this is a digital tile it is possible to disable the SPADs which produce a high number of dark counts. This inhibit fraction is set to 10 %. The surface of each sensor tile is covered with a glass plate of 1.1 mm. The sensor tiles are connected to a singles processing unit (SPU) which manages their voltage supply and feeds their data to the data acquisition and processing server (DAPS). During the measurement, the tiles are cooled by a 15 \u25e6C liquid cooling system.\n\n#### 2.1.3 Masks\n\nWe perform measurements in one and two dimensions, i.e., we reconstruct an image along one axis or on a plane. For these two tasks, we use a one- and a two-dimensional versions of a MURA mask of rank 476, clipped to 31 \\(\\times\\) 31 central pixels (see Fig. 2). The mask rank as well as the setup geometry have been optimised via Monte Carlo simulations before the experiment. To construct the physical masks we use tungsten rods of (2.26 \\(\\times\\) 2.26 \\(\\times\\) 20) mm3 which are inserted into 3D printed rasters made from Pro Grey Resin. The rod manufacturing reaches a precision of 0.1 mm. The resulting masks have a dimension of (73.6 \\(\\times\\) 73.6) mm2. The rasters have a total thickness of 13 mm and the holes to insert the rods are 10 mm deep. To prevent the rods from falling out, the assembled masks are wrapped in cling film.\n\n### Radioactive sources\n\nFor image reconstruction, the experimental data were obtained with a radioactive 22Na source with an activity of 2.89 MBq. The active material in that source covers an area of 1 mm \\(\\times\\) 1 mm. As a \u03b2 + -emitter, 22Na provides two photons of 511 keV emitted back-to-back, which can be used for electronic collimation. For calibration of the detectors we additionally used the 1275 keV gamma line of 22Na and two more radioactive sources: a 137Cs source with a gamma line at 662 keV with an activity of 1.73 MBq and a 133Ba source with\n\nFigure 2: Coded masks for 1D measurement with the small-scale prototype (a) and for 2D measurement with the three-layered PET array (b).\n\n "
    ],
    "kosmos": [
      "o accept a trigger of 40 ns and an integration time of 325 ns to collect photons on the sensor tile. The overvoltage of the silicon photomultipliers (SiPMs) is set to 3 V. As this is a digital tile it is possible to disable the SPADs which produce a high number of dark counts. This inhibit fraction is set to 10 %. The surface of each sensor tile is covered with a glass plate of 1.1 mm. The sensor tiles are connected to a singles processing unit (SPU) which manages their voltage supply and feeds their data to the data acquisition and processing server (DAPS). During the measurement, the tiles are cooled by a 15 \u25e6C liquid cooling system.\n\n#### 2.1.3 Masks\n\nWe perform measurements in one and two dimensions, i.e., we reconstruct an image along one axis or on a plane. For these two tasks, we use a one- and a two-dimensional versions of a MURA mask of rank 476, clipped to 31 \\(\\times\\) 31 central pixels (see Fig. 2). The mask rank as well as the setup geometry have been optimised via Monte Carlo simulations before the experiment. To construct the physical masks we use tungsten rods of (2.26 \\(\\times\\) 2.26 \\(\\times\\) 20) mm3 which are inserted into 3D printed rasters made from Pro Grey Resin. The rod manufacturing reaches a precision of 0.1 mm. The resulting masks have a dimension of (73.6 \\(\\times\\) 73.6) mm2. The rasters have a total thickness of 13 mm and the holes to insert the rods are 10 mm deep. To prevent the rods from falling out, the assembled masks are wrapped in cling film.\n\n### Radioactive sources\n\nFor image reconstruction, the experimental data were obtained with a radioactive 22Na source with an activity of 2.89 MBq. The active material in that source covers an area of 1 mm \\(\\times\\) 1 mm. As a \u03b2 + -emitter, 22Na provides two photons of 511 keV emitted back-to-back, which can be used for electronic collimation. For calibration of the detectors we additionally used the 1275 keV gamma line of 22Na and two more radioactive sources: a 137Cs source with a gamma line at 662 keV with an activity of 1.73 MBq and a 133Ba source with\n\nFigure 2: Coded masks for 1D measurement with the small-scale prototype (a) and for 2D measurement with the three-layered PET array (b).\n\n "
    ]
  },
  {
    "edit": [
      "zz0 = cell(1, N); for i = 1 : N-1\n\nzz0(i) = {[initial\\_position(:, i); initial\\_position(:, i + 1)]}; end\n\nzz0(N) = {[initial\\_position(:, N); initial\\_position(:, 1)]}; end\n\nSuch that the overall problem can be set up with the following function: function \\[sProb \\] = setupSolver(N, sigma) n = 4; d = 2; y = sym(\u2019y%d%d\u2019, \\[N n\\], \u2019real\u2019); y = y\u2019;\n\n\\[eta, eta\\_bar\\] = getEta(N, d, sigma);\n\nF = getObjective(N, y, eta, eta\\_bar, sigma);\n\nH = getInequalityConstr(N, y, eta\\_bar); AA = getCouplingMatrix(N, n); zz0 = getStartValue(N, sigma); sProb.llbx = cell(1, N); sProb.uubx = cell(1, N); for i = 1 : N\n\nsProb.llbx(i) = mat2cell(\\[-inf; -inf; -inf; -inf\\], 4, 1); sProb.uubx(i) = mat2cell(\\[ inf; inf; inf; inf\\], 4, 1); end\n\nsProb.locFuns.ffi = cell(1, N); sProb.locFuns.hhi = cell(1, N); for i = 1 : N\n\nsProb.locFuns.ffi(i) = {matlabFunction(F(i), \u2019Vars\u2019, {y(:, i)})} ; sProb.locFuns.hhi(i) = {matlabFunction(H(i), \u2019Vars\u2019, {y(:, i)})} ; end sProb.AA = AA; sProb.zz0 = zz0;\n\n12.2.4 Runtime Analysis\n======================\n\nFor the runtime analysis, the idea is to \u00b4run the sensor network localization problem with varying number of sensors both with a decentral and a central optimization step. To do so, firstly a vector with a number of sensors is needed and secondly a vector with variances. Then, the time needed for the decentral and the central optimization is measured and can be plotted. N = \\[5, 10, 15 , 20, 25, 30, 35, 40, 50, 60, 70, 80, 90, 100]; sigma = \\[0.5, 1, 1.5, 2, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5]; time = zeros(2, length(N)); for i = 1 : length(N)\n\nsProb = setupSolver(N(i), sigma(i));\n\n22 "
    ],
    "kosmos": [
      "zz0 = cell(1, N); for i = 1 : N-1\n\nzz0(i) = {[initial\\_position(:, i); initial\\_position(:, i + 1)]}; end\n\nzz0(N) = {[initial\\_position(:, N); initial\\_position(:, 1)]}; end\n\nSuch that the overall problem can be set up with the following function:\n\nfunction \\[sProb \\] = setupSolver(N, sigma) n = 4; d = 2; y = sym(\u2019y%d%d\u2019, \\[N n\\], \u2019real\u2019); y = y\u2019;\n\n\\[eta, eta\\_bar\\] = getEta(N, d, sigma);\n\nF = getObjective(N, y, eta, eta\\_bar, sigma);\n\nH = getInequalityConstr(N, y, eta\\_bar); AA = getCouplingMatrix(N, n); zz0 = getStartValue(N, sigma); sProb.llbx = cell(1, N); sProb.uubx = cell(1, N); for i = 1 : N\n\nsProb.llbx(i) = mat2cell(\\[-inf; -inf; -inf; -inf\\], 4, 1); sProb.uubx(i) = mat2cell(\\[ inf; inf; inf; inf\\], 4, 1); end\n\nsProb.locFuns.ffi = cell(1, N); sProb.locFuns.hhi = cell(1, N); for i = 1 : N\n\nsProb.locFuns.ffi(i) = {matlabFunction(F(i), \u2019Vars\u2019, {y(:, i)})} ; sProb.locFuns.hhi(i) = {matlabFunction(H(i), \u2019Vars\u2019, {y(:, i)})} ; end sProb.AA = AA; sProb.zz0 = zz0;\n\n#### 12.2.4 Runtime Analysis\n\nFor the runtime analysis, the idea is to \u02d9run the sensor network localization problem with varying number of sensors both with a decentral and a central optimization step. To do so, \ufb01rstly a vector with a number of sensors is needed and secondly a vector with variances. Then, the time needed for the decentral and the central optimization is measured and can be plotted.\n\nN = \\[5, 10, 15 , 20, 25, 30, 35, 40, 50, 60, 70, 80, 90, 100\\]; sigma = \\[0.5, 1, 1.5, 2, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5\\]; time = zeros(2, length(N)); for i = 1 : length(N)\n\nsProb = setupSolver(N(i), sigma(i));\n\n22 "
    ]
  },
  {
    "edit": [
      "\n\n**Definition 4.11**.: We say a spin system \\(\\mu\\) on \\(\\mathbb{Z}^{d}\\) satisfies the _strong spatial mixing (SSM)_ condition if there exist constants \\(\\alpha,\\gamma,L&gt;0\\) such that for every \\(d\\)-dimensional rectangle \\(\\Lambda\\subset\\mathbb{Z}^{d}\\) of side length between \\(L\\) and \\(2L\\) and every subset \\(B\\subset\\Lambda\\), with any pair \\((\\tau,\\tau^{\\prime})\\) of boundary configurations on \\(\\partial\\Lambda\\) that only difer at a vertex \ud835\udc62\\), we have\n\n\\[\\|\\mu_{B}^{\\tau}(\\cdot)-\\mu_{B}^{\\tau^{\\prime}}(\\cdot)\\|_{TV}\\leq\\gamma\\cdot \\exp(-\\alpha\\cdot dist(u,B)),\\]\n\nwhere \\(dist(\\cdot,\\cdot)\\) denotes graph distance.\n\nThe definition above differs from other variants of SSM in the literature (e.g., [ 12, 3, 34]) in that \\(\\Lambda\\) has been restricted to \"regular enough\" rectangles. In particular, our variant of SSM is easier to satisfy than those in [12, 34] but more restricting than the one in [34] (that only considers squares). Nevertheless, it follows from [11, 34, 1, 7] that for the ferromagnetic Ising model, this form of SSM holds up to a critical threshold temperature \\(\\beta&lt;\\beta_{c}(2)=\\ln(1+\\sqrt{2})\\) on \\(\\mathbb{Z}^{2}\\).\n\nCorollary 1.9 from the introduction states that for \\(b\\)-marginally bounded monotone spin system on \\(d\\)-dimensional cubes \\(V\\subseteq\\mathbb{Z}^{d}\\), SSM implies that the mixing time of any systematic scan \\(P_{\\phi}\\) is \\(O(\\log n)\\). As mentioned there, this result in turn implies that any systematic scan dynamics for the ferromagnetic Ising model is mixing in \\(O(\\log n)\\) steps on boxes of \\(\\mathbb{Z}^{2}\\) when \\(\\beta&lt;\\beta_{c}(2)\\). Another interesting consequence of Corollary 1.9 is that we obtain \\(O(\\log n)\\) mixing time for any systematic scan dynamics \\(P_{\\phi}\\) for the hardcore model on \\(\\mathbb{Z}^{2}\\) when \\(\\lambda&lt;2.538\\), which is the best known condition for ensuring SSM [34, 34].\n\nOur proof of Corollary 1.9 relies on Lemma 4.12 that is restated below. Remarkably, Lemma 4.12 generalizes beyond monotone systems and may be of independent interests.\n\n**Lemma 4.12**.: _For a spin system on a \\(d\\)-dimensional cube \\(V\\subseteq\\mathbb{Z}^{d}\\), SSM implies \\(\\eta\\)-spectral independence, where \\(\\eta=O(1)\\)._\n\n\\(\\star\\)\n\nProof of Corollary 1.9.: Assume a monotone spin system satisfies SSM condition. Then the spin system satisfies \\(\\eta\\)-spectral independence, where \\(\\eta=O(1)\\) by Lemma 4.12. By noting that \\(\\Delta=2^{d}\\) the corollary follows from Theorem 4.3.\n\nLastly, we give a proof of Lemma 4.12. For this, we recall the notion of a \\(\\kappa\\)-contractive coupling which is known to imply spectral independence. We say a distribution \\(\\mu\\) is _\\(\\kappa\\)-contractive_ with respect to a Markov chain \\(P\\) if for all \\(X_{0},Y_{0}\\in\\Omega\\), there exists a coupling of step of \\(P\\) so that\n\n\\[\\mathbb{E}[d(X_{1},Y_{1})\\mid X_{0},Y_{0}]\\leq\\kappa d(X_{0},Y_{0}),\\]\n\nwhere \\(d(\\cdot,\\cdot)\\) denotes the Hamming distance of two configurations. The following lemma from [3] shows that spectral independence follows from the existence of a contractive coupling with respect to a heat-bath block dynamics.\n\n**Lemma 4.13** ([3]).: _If \\(\\mu\\) is \\(\\kappa\\)-contractive with respect to a block dynamics, then \\(\\mu\\) is \\((2DM\\over 1-\\kappa)\\)-spectrally independent, where \\(M\\) is the maximum block size and \\(D\\) is the maximum probability of a vertex being selected as part"
    ],
    "kosmos": [
      "\n\n**Definition 4.11**.: We say a spin system \\(\\mu\\) on \\(\\mathbb{Z}^{d}\\) satisfies the _strong spatial mixing (SSM)_ condition if there exist constants \\(\\alpha,\\gamma,L&gt;0\\) such that for every \\(d\\)-dimensional rectangle \\(\\Lambda\\subset\\mathbb{Z}^{d}\\) of side length between \\(L\\) and \\(2L\\) and every subset \\(B\\subset\\Lambda\\), with any pair \\((\\tau,\\tau^{\\prime})\\) of boundary configurations on \\(\\partial\\Lambda\\) that only difer at a vertex \ud835\udc62\\), we have\n\n\\[\\|\\mu_{B}^{\\tau}(\\cdot)-\\mu_{B}^{\\tau^{\\prime}}(\\cdot)\\|_{TV}\\leq\\gamma\\cdot \\exp(-\\alpha\\cdot dist(u,B)),\\]\n\nwhere \\(dist(\\cdot,\\cdot)\\) denotes graph distance.\n\nThe definition above differs from other variants of SSM in the literature (e.g., [ 12, 3, 34]) in that \\(\\Lambda\\) has been restricted to \"regular enough\" rectangles. In particular, our variant of SSM is easier to satisfy than those in [12, 34] but more restricting than the one in [34] (that only considers squares). Nevertheless, it follows from [11, 34, 1, 7] that for the ferromagnetic Ising model, this form of SSM holds up to a critical threshold temperature \\(\\beta&lt;\\beta_{c}(2)=\\ln(1+\\sqrt{2})\\) on \\(\\mathbb{Z}^{2}\\).\n\nCorollary 1.9 from the introduction states that for \\(b\\)-marginally bounded monotone spin system on \\(d\\)-dimensional cubes \\(V\\subseteq\\mathbb{Z}^{d}\\), SSM implies that the mixing time of any systematic scan \\(P_{\\phi}\\) is \\(O(\\log n)\\). As mentioned there, this result in turn implies that any systematic scan dynamics for the ferromagnetic Ising model is mixing in \\(O(\\log n)\\) steps on boxes of \\(\\mathbb{Z}^{2}\\) when \\(\\beta&lt;\\beta_{c}(2)\\). Another interesting consequence of Corollary 1.9 is that we obtain \\(O(\\log n)\\) mixing time for any systematic scan dynamics \\(P_{\\phi}\\) for the hardcore model on \\(\\mathbb{Z}^{2}\\) when \\(\\lambda&lt;2.538\\), which is the best known condition for ensuring SSM [34, 34].\n\nOur proof of Corollary 1.9 relies on Lemma 4.12 that is restated below. Remarkably, Lemma 4.12 generalizes beyond monotone systems and may be of independent interests.\n\n**Lemma 4.12**.: _For a spin system on a \\(d\\)-dimensional cube \\(V\\subseteq\\mathbb{Z}^{d}\\), SSM implies \\(\\eta\\)-spectral independence, where \\(\\eta=O(1)\\)._\n\n\\(\\star\\)\n\nProof of Corollary 1.9.: Assume a monotone spin system satisfies SSM condition. Then the spin system satisfies \\(\\eta\\)-spectral independence, where \\(\\eta=O(1)\\) by Lemma 4.12. By noting that \\(\\Delta=2^{d}\\) the corollary follows from Theorem 4.3.\n\nLastly, we give a proof of Lemma 4.12. For this, we recall the notion of a \\(\\kappa\\)-contractive coupling which is known to imply spectral independence. We say a distribution \\(\\mu\\) is _\\(\\kappa\\)-contractive_ with respect to a Markov chain \\(P\\) if for all \\(X_{0},Y_{0}\\in\\Omega\\), there exists a coupling of step of \\(P\\) so that\n\n\\[\\mathbb{E}[d(X_{1},Y_{1})\\mid X_{0},Y_{0}]\\leq\\kappa d(X_{0},Y_{0}),\\]\n\nwhere \\(d(\\cdot,\\cdot)\\) denotes the Hamming distance of two configurations. The following lemma from [3] shows that spectral independence follows from the existence of a contractive coupling with respect to a heat-bath block dynamics.\n\n**Lemma 4.13** ([3]).: _If \\(\\mu\\) is \\(\\kappa\\)-contractive with respect to a block dynamics, then \\(\\mu\\) is \\((2DM\\over 1-\\kappa)\\)-spectrally independent, where \\(M\\) is the maximum block size and \\(D\\) is the maximum probability of a vertex being selected as part"
    ]
  },
  {
    "edit": [
      "The initial conditions for the Taylor-Green vortex are\n\n\\[\\rho(x,y,z) =1\\] \\[u(x,y,z) =\\sin(x)\\cos(y)\\cos(z)\\] \\[v(x,y,z) =-\\cos(x)\\sin(y)\\cos(z)\\] \\[w(x,y,z) =0\\] \\[p(x,y,z) =10+\\frac{(\\cos(2x)+\\cos(2y))(\\cos(2x)+2)-2}{16}\\]\n\nwith a pressure value corresponding to a Mach number \\(M\\approx 0.26\\). The triperiodic domain has side length \\(2\\pi\\) in all directions and is discretized using \\(32\\times 32\\times 32\\) nodes. The chosen CFL value is sufficiently small that linear invariants are exactly conserved to machine precision for all schemes. The time evolution of the entropy integral for this test is shown in Fig. 2 and it is in agreement with the previous results. In this test, since the pressure is not constant, \\(A\\rho\\)-\\(He\\) is no longer equivalent to \\(A\\rho\\)-\\(Ap\\); in this case we have better performances from \\(A\\rho\\)-\\(He\\) and \\(G\\rho\\)-\\(Ge\\) when compared to \\(A\\rho\\)-\\(Ap\\) and \\(A\\rho\\)-\\(Ae\\) and this result is found for both fourth-order and six-order accurate fluxes. An improvement can be obtained using an additional term in the expansions and KEEP (1) and AEP are the schemes which more closely achieve a constant value for the entropy integral. Information about the reliability of the scheme can be obtained thorough the study of the evolution of thermodynamic fluctuations in time. We checked that for all the schemes tested, the density and temperature fluctuations do not have an unbound growth (not shown). This is the desired behaviour, since for inviscid isotropic homogeneous turbulence they are reported to level off to a constant value [18, 2].\n\n## 6 Conclusions\n\nWe proposed a new class of asymptotically entropy-preserving fluxes for the discretization of the convective terms in the compressible Euler equations with interesting properties. It provides a consistent asymptotic approximation of an existing entropy-preserving scheme based on the logarithmic mean, and it consists of economical algebraic fluxes based on the harmonic mean. Moreover, at all orders of approximation, the numerical fluxes have the pressure-equilibrium preservation property. The theoretical predictions are confirmed on two test cases, verifying that the new schemes are able to numerically maintain pressure equilibrium and demonstrating good entropy-conservation property. It was also shown that the error on entropy can be reduced by using additional terms in the expansion of the AEC fluxes.\n\nThese results suggest that AEC fluxes could be good candidate for the discretization of compressible flow equations in high performance solvers. Due to the their algebraic form, they are less computationally expensive than the fluxes based on the logarithmic mean, while retaining many important properties. In fact, they guarantee the KEP and PEP properties, combined with arbitrarily small error on entropy preservation.\n\n## Appendix A High-order extension\n\nThe second-order accurate two-point fluxes presented in this article can be extended to higher-order formulations by using the approach proposed by Ranocha [7] in the context of Discontinuous Galerkin discretization of the Euler equations. The main result of interest for us is that contained in Theorem 3.1 of [7], which can be reformulated in FD terms as follows. We consider a numerical flux \\(\\mathcal{F}(\\boldsymbol{w_{i}},\\boldsymbol{w_{i+k}})\\) for a generic quantity \\(\\rho\\varphi\\), which depends on the values of the variables vector **w** in the nodal points \\(i\\) and \\(i+k\\). In our context \\(\\mathcal{F}\\) can be any of the numerical fluxes specified in Eqs. (1)\u2013(4),(9)\u2013(10) or (13)\u2013(15) and **w** is the set of variables \\((\\rho,u,e)\\). We will assume that the numerical flux is smooth, symmetrical (i.e. \\(\\mathcal{F}(\\boldsymbol{w_{i}},\\boldsymbol{w_{i+k}})=\\mathcal{F}(\\boldsymbol{w_ {i+k}},\\boldsymbol{w_{i}})\\)) and consistent with the continuous flux \\(f\\) so that \\(\\mathcal{F}(\\boldsymbol{w_{i}},\\boldsymbol{w_{i}})=f(\\boldsymbol{w_{i}})\\). Under these hypotheses, by following the steps of the proof to Theorem 3.1 in [7], we can show that given a numerical derivative formula of the type \\  "
    ],
    "kosmos": [
      "The initial conditions for the Taylor-Green vortex are\n\n\\[\\rho(x,y,z) =1\\] \\[u(x,y,z) =\\sin(x)\\cos(y)\\cos(z)\\] \\[v(x,y,z) =-\\cos(x)\\sin(y)\\cos(z)\\] \\[w(x,y,z) =0\\] \\[p(x,y,z) =10+\\frac{(\\cos(2x)+\\cos(2y))(\\cos(2x)+2)-2}{16}\\]\n\nwith a pressure value corresponding to a Mach number \\(M\\approx 0.26\\). The triperiodic domain has side length \\(2\\pi\\) in all directions and is discretized using \\(32\\times 32\\times 32\\) nodes. The chosen CFL value is sufficiently small that linear invariants are exactly conserved to machine precision for all schemes. The time evolution of the entropy integral for this test is shown in Fig. 2 and it is in agreement with the previous results. In this test, since the pressure is not constant, \\(A\\rho\\)-\\(He\\) is no longer equivalent to \\(A\\rho\\)-\\(Ap\\); in this case we have better performances from \\(A\\rho\\)-\\(He\\) and \\(G\\rho\\)-\\(Ge\\) when compared to \\(A\\rho\\)-\\(Ap\\) and \\(A\\rho\\)-\\(Ae\\) and this result is found for both fourth-order and six-order accurate fluxes. An improvement can be obtained using an additional term in the expansions and KEEP (1) and AEP (1) are the schemes which more closely achieve a constant value for the entropy integral. Information about the reliability of the scheme can be obtained thorough the study of the evolution of thermodynamic fluctuations in time. We checked that for all the schemes tested, the density and temperature fluctuations do not have an unbound growth (not shown). This is the desired behaviour, since for inviscid isotropic homogeneous turbulence they are reported to level off to a constant value [18, 2].\n\n## 6 Conclusions\n\nWe proposed a new class of asymptotically entropy-preserving fluxes for the discretization of the convective terms in the compressible Euler equations with interesting properties. It provides a consistent asymptotic approximation of an existing entropy-preserving scheme based on the logarithmic mean, and it consists of economical algebraic fluxes based on the harmonic mean. Moreover, at all orders of approximation, the numerical fluxes have the pressure-equilibrium preservation property. The theoretical predictions are confirmed on two test cases, verifying that the new schemes are able to numerically maintain pressure equilibrium and demonstrating good entropy-conservation property. It was also shown that the error on entropy can be reduced by using additional terms in the expansion of the AEC fluxes.\n\nThese results suggest that AEC fluxes could be good candidate for the discretization of compressible flow equations in high performance solvers. Due to the their algebraic form, they are less computationally expensive than the fluxes based on the logarithmic mean, while retaining many important properties. In fact, they guarantee the KEP and PEP properties, combined with arbitrarily small error on entropy preservation.\n\n## Appendix A High-order extension\n\nThe second-order accurate two-point fluxes presented in this article can be extended to higher-order formulations by using the approach proposed by Ranocha [7] in the context of Discontinuous Galerkin discretization of the Euler equations. The main result of interest for us is that contained in Theorem 3.1 of [7], which can be reformulated in FD terms as follows. We consider a numerical flux \\(\\mathcal{F}(\\boldsymbol{w_{i}},\\boldsymbol{w_{i+k}})\\) for a generic quantity \\(\\rho\\varphi\\), which depends on the values of the variables vector **w** in the nodal points \\(i\\) and \\(i+k\\). In our context \\(\\mathcal{F}\\) can be any of the numerical fluxes specified in Eqs. (1)\u2013(4),(9)\u2013(10) or (13)\u2013(15) and **w** is the set of variables \\((\\rho,u,e)\\). We will assume that the numerical flux is smooth, symmetrical (i.e. \\(\\mathcal{F}(\\boldsymbol{w_{i}},\\boldsymbol{w_{i+k}})=\\mathcal{F}(\\boldsymbol{w_ {i+k}},\\boldsymbol{w_{i}})\\)) and consistent with the continuous flux \\(f\\) so that \\(\\mathcal{F}(\\boldsymbol{w_{i}},\\boldsymbol{w_{i}})=f(\\boldsymbol{w_{i}})\\). Under these hypotheses, by following the steps of the proof to Theorem 3.1 in [7], we can show that given a numerical derivative formula of the type \\ "
    ]
  },
  {
    "edit": [
      "C Empirical data\n\nWe observe the changes of activity over time of the empirical networks for the four data sets (Figure 11). The *US school* presents periodic patterns, varying from low contact periods when the students are in class to high contact periods when there is a recreational time. The *US flight* and *Conference* networks have circadian patterns as there are respectively less flights and less contacts at night. Finally, the *Resistance game* does not present any periodic change in its activity as every player of the game is looking at someone else at each time step.\n\n## References\n\n* [1] Petter Holme and Jari Saram\u00a8aki. Temporal networks. *Physics reports*, 519(3):97-125, 2012.\n* [2] Petter Holme. Modern temporal network theory: a colloquium. *The European Physical Journal B*, 88:1-30, 2015.\n* [3] Naoki Masuda and Renaud Lambiotte. *A guide to temporal networks*. World Scientific, 2016.\n* [4] Alain Barrat and Ciro Cattuto. Temporal networks of face-to-face human interactions. *Temporal networks*, pages 191-216, 2013.\n* [5] Sune Lehmann. Fundamental structures in temporal communication networks. *Temporal Network Theory*, pages 25-48, 2019.\n* [6] Mohammed Saqr and Sonsoles Lopez-Pernas. The why, the what and the how to model a dynamic relational learning process with temporal networks. In *Proceedings of the NetSciLA22 workshop*, 2022.\n* "
    ],
    "kosmos": [
      "C Empirical data\n\nWe observe the changes of activity over time of the empirical networks for the four data sets (Figure 11). The *US school* presents periodic patterns, varying from low contact periods when the students are in class to high contact periods when there is a recreational time. The *US flight* and *Conference* networks have circadian patterns as there are respectively less flights and less contacts at night. Finally, the *Resistance game* does not present any periodic change in its activity as every player of the game is looking at someone else at each time step.\n\n## References\n\n* [1] Petter Holme and Jari Saram\u00a8aki. Temporal networks. *Physics reports*, 519(3):97-125, 2012.\n* [2] Petter Holme. Modern temporal network theory: a colloquium. *The European Physical Journal B*, 88:1-30, 2015.\n* [3] Naoki Masuda and Renaud Lambiotte. *A guide to temporal networks*. World Scientific, 2016.\n* [4] Alain Barrat and Ciro Cattuto. Temporal networks of face-to-face human interactions. *Temporal networks*, pages 191-216, 2013.\n* [5] Sune Lehmann. Fundamental structures in temporal communication networks. *Temporal Network Theory*, pages 25-48, 2019.\n* [6] Mohammed Saqr and Sonsoles Lopez-Pernas. The why, the what and the how to model a dynamic relational learning process with temporal networks. In *Proceedings of the NetSciLA22 workshop*, 2022.\n* "
    ]
  },
  {
    "edit": [
      "THE POISSON BOUNDARY OF LAMPSHUFFLER GROUPS\n\n\\[\\gamma_{1}(0)+\\gamma_{2}(0)+\\gamma_{3}(0)+\\ldots\\] \\[=\\sum_{|x|\\leq(n-1)N_{k_{j}-1}}\\beta^{\\ast q}_{k_{j}}(x)\\theta(-x)\\] \\[\\leq\\sum_{|x|\\leq(n-1)N_{k_{j}-1}}\\beta^{\\ast q}_{k_{j}}(x)\\] \\[=\\beta^{\\ast q}_{k_{j}}\\Big{(}\\Big{[}-(n-1)N_{k_{j}-1},(n-1)N_{k_ {j}-1}\\Big{]}\\Big{)}\\] \\[\\leq\\varepsilon_{n},\\]\n\nwhere we used the fact that \\(\\beta_{k_{j}}\\) satisfies Equation ( 2 ), with \\(k_{j}\\geq p_{n}\\).\n\nWe conclude that \\(\\nu^{\\ast n}(0)=\\gamma_{1}(0)+\\gamma_{2}(0)\\leq 2\\varepsilon_{n}\\), which finishes the proof.\n\nIf we weaken the hypothesis E(|supp(\u03c3_{1})|)&lt;\\infty\\) from Lemma 4.10 it is possible that the permutation coordinate never stabilizes. Indeed, with ideas similar to an example of [ 13], we obtain the following.\n\n**Proposition 4.14**.: _The group Shuffler(\\(\\mathbb{Z}\\)) admits probability measures \\(\\mu\\) with an infinite first moment and a finite \\((1-\\varepsilon)\\)-moment, for every \\(0&lt;\\varepsilon&lt;1\\), that induce a transient random walk on \\(\\mathbb{Z}\\) and for which the permutation coordinate of the \\(\\mu\\)-random walk does not stabilize. Such measures can be chosen to satisfy E(|supp(\u03c3_{1})|)=\\infty\\) and E(|supp(\u03c3_{1})|^{1-\\varepsilon})&lt;\\infty\\) for every \\(0&lt;\\varepsilon&lt;1\\)._\n\nProof.: For each \\(n\\geq 1\\), denote by \\(r_{n}:\\mathbb{Z}\\to\\mathbb{Z}\\) the permutation\n\n\\[r_{n}(x)=\\begin{cases}x+1,\\text{ if }0\\leq x<n-1,\\\\ 0,\\text{=\"\" 0,\\text{=\"\" 1,\\text{=\"\" 1,\\text{=\"\" 1,\\\\=\"\" 1, "
    ],
    "kosmos": [
      "THE POISSON BOUNDARY OF LAMPSHUFFLER GROUPS\n\n\\[\\gamma_{1}(0)+\\gamma_{2}(0)+\\gamma_{3}(0)+\\ldots\\] \\[=\\sum_{|x|\\leq(n-1)N_{k_{j}-1}}\\beta^{\\ast q}_{k_{j}}(x)\\theta(-x)\\] \\[\\leq\\sum_{|x|\\leq(n-1)N_{k_{j}-1}}\\beta^{\\ast q}_{k_{j}}(x)\\] \\[=\\beta^{\\ast q}_{k_{j}}\\Bigl{(}\\Bigl{[}-(n-1)N_{k_{j}-1},(n-1)N_{ k_{j}-1}\\Bigr{]}\\Bigr{)}\\] \\[\\leq\\varepsilon_{n},\\]\n\nwhere we used the fact that \\(\\beta_{k_{j}}\\) satisfies Equation (2), with \\(k_{j}\\geq p_{n}\\).\n\nWe conclude that \\(\\nu^{\\ast n}(0)=\\gamma_{1}(0)+\\gamma_{2}(0)\\leq 2\\varepsilon_{n}\\), which finishes the proof.\n\nIf we weaken the hypothesis E(|supp(\u03c3_{1})|)&lt;\\infty\\) from Lemma 4.10 it is possible that the permutation coordinate never stabilizes. Indeed, with ideas similar to an example of [13], we obtain the following.\n\n**Proposition 4.14**.: _The group Shuffler(\\(\\mathbb{Z}\\)) admits probability measures \\(\\mu\\) with an infinite first moment and a finite \\((1-\\varepsilon)\\)-moment, for every \\(0&lt;\\varepsilon&lt;1\\), that induce a transient random walk on \\(\\mathbb{Z}\\) and for which the permutation coordinate of the \\(\\mu\\)-random walk does not stabilize. Such measures can be chosen to satisfy E(|supp(\u03c3_{1})|)=\\infty\\) and E(|supp(\u03c3_{1})|^{1-\\varepsilon})&lt;\\infty\\) for every \\(0&lt;\\varepsilon&lt;1\\)._\n\nProof.: For each \\(n\\geq 1\\), denote by \\(r_{n}:\\mathbb{Z}\\to\\mathbb{Z}\\) the permutation\n\n\\[r_{n}(x)=\\begin{cases}x+1,\\text{ if }0\\leq x<n-1,\\\\ 0,\\text{=\"\" 0,\\text{=\"\" 1,\\text{=\"\" 1,\\text{=\"\" 1,\\\\=\"\" 1, "
    ]
  },
  {
    "edit": [
      "\n\n### Metrics\n\nThree metrics were widely used across the papers: accuracy, specificity, and sensitivity. The equations for these four metrics can be seen below. Accuracy = Correctly classified speech\n\nTotal speech samples\n\nSpecificity = Correctly classified healthy speech\n\nTotal healthy speech\n\nSensitivity = Correctly classified pathological speech\n\nTotal pathological speech\n\nAnother metric was often used in the papers using multi-class classification unweighted average recall (UAR). This metric is calculated by averaging the recall value for each of the specific pathologies included in the dataset. Equation 4 shows how it is calculated, where N is the number of pathologies in the dataset and R~i is the recall of the ith pathology in the dataset. UAR = \u2211~i~N~i~R~i~\n\n### Binary Classification Literature\n\n? (2020) investigate the classification of cancer patients from healthy controls using six machine learning algorithms. The dataset used includes recordings of the prolonged vowel /ah/ from 50 male laryngeal cancer patients. The authors use a binary classifier to classify the patients into two groups: cancer and healthy controls. The authors also use a multi-class classifier to classify the patients into two groups: cancer and healthy controls. The authors use a binary classifier to classify the patients into two groups: cancer and healthy controls. The authors use a multi-class classifier to classify the patients into two groups: cancer and healthy controls. The authors use a binary classifier to classify the patients into two groups: cancer and healthy controls. "
    ],
    "kosmos": [
      "\n\n### Metrics\n\nThree metrics were widely used across the papers: accuracy, specificity, and sensitivity. The equations for these four metrics can be seen below.\n\nAccuracy = Correctly classified speech\n\nTotal speech samples\n\nSpecificity = Correctly classified healthy speech\n\nTotal healthy speech\n\nSensitivity = Correctly classified pathological speech\n\nTotal pathological speech\n\nAnother metric was often used in the papers using multi-class classification - unweighted average recall (UAR). This metric is calculated by averaging the recall value for each of the specific pathologies included in the dataset. Equation 4 shows how it is calculated, where N is the number of pathologies in the dataset and R~i is the recall of the ith pathology in the dataset.\n\nUAR = \u2211~i~N~i~R~i~\n\n### Binary Classification Literature\n\n? (2020) investigate the classification of cancer patients from healthy controls using six machine learning algorithms. The dataset used includes recordings of the prolonged vowel /ah/ from 50 male laryngeal cancer patients. The authors use a binary classifier to classify the patients into two groups: cancer and healthy controls. The authors also use a multi-class classifier to classify the patients into two groups: cancer and healthy controls. The authors use a binary classifier to classify the patients into two groups: cancer and healthy controls. The authors use a multi-class classifier to classify the patients into two groups: cancer and healthy controls. The authors use a binary classifier to classify the patients into two groups: cancer and healthy controls.\n\n"
    ]
  },
  {
    "edit": [
      "for this paper, since a majority of features in keystroke sounds are within the lower frequencies [15, 3, 4] and would therefore be less distinguishable on a linear scale. Meanwhile, MFCC involves performing the discrete cosine transform on a mel-spectrogram, producing a compressed representation that prioritises the frequencies used in human speech. Since, for this paper, human speech is not the target, and the removal of frequencies could risk the loss of relevant data, MFCC was decided to be less suitable than mel-spectrograms. **Data augmentation:** Prior to feature extraction, signals were time-shifted randomly by up to 40% in either direction. This time shifting is an instance of data augmentation, in which the amount of data input to a DL model is artificially increased by slightly adjusting existing inputs [28]. The mel-spectrograms were then generated using 64 mel bands, a window length of 1024 samples and hop length of 500 (255 for the MacBook keystrokes, given their shorter length), resulting in 64x64 images. Using the spectrograms, a second method of data augmentation was implemented called masking. This method involves taking a random 10% of both the time and frequency axis and setting all values within those ranges to the mean of the spectrogram, essentially \u2018blocking out\u2019 a portion of the image. Using time warping and spectrogram masking combined is called SpecAugment and was found to encourage the model to generalise and avoid overfitting the training data [25, 10]. Having converted keystrokes from each data set into a more visual medium, more direct comparisons could be made. MacBook keystrokes (similar to the keystrokes examined in the literature [4, 39, 6]) have only 2 visible peaks: the \u2018push\u2019 and \u2018release\u2019 peaks respectively. The 2 peak structures shown in Fig. 2 are similar to each other, implying that such a structure is native to the MacBook keyboard regardless of recording method, a noticeable difference however is the large range of frequencies present in the zoom recording. The Zoom peaks extend much higher than that of the phone-based recordings, indicating significant data in multiple frequencies that were not present when recorded via phone. The overall data preparation procedure for our data was inspired by the structure presented in [10] and is shown in Fig. 3.\n\n### Model Selection and Implementation\n\nFigure 2: Waveform and corresponding mel-spectrogram of Left: Phone recording, and Right: Zoom recording.\n\n "
    ],
    "kosmos": [
      "for this paper, since a majority of features in keystroke sounds are within the lower frequencies [15, 3, 4] and would therefore be less distinguishable on a linear scale. Meanwhile, MFCC involves performing the discrete cosine transform on a mel-spectrogram, producing a compressed representation that prioritises the frequencies used in human speech. Since, for this paper, human speech is not the target, and the removal of frequencies could risk the loss of relevant data, MFCC was decided to be less suitable than mel-spectrograms.\n\n**Data augmentation:** Prior to feature extraction, signals were time-shifted randomly by up to 40% in either direction. This time shifting is an instance of data augmentation, in which the amount of data input to a DL model is artificially increased by slightly adjusting existing inputs [28]. The mel-spectrograms were then generated using 64 mel bands, a window length of 1024 samples and hop length of 500 (255 for the MacBook keystrokes, given their shorter length), resulting in 64x64 images. Using the spectrograms, a second method of data augmentation was implemented called masking. This method involves taking a random 10% of both the time and frequency axis and setting all values within those ranges to the mean of the spectrogram, essentially \u2018blocking out\u2019 a portion of the image. Using time warping and spectrogram masking combined is called SpecAugment and was found to encourage the model to generalise and avoid overfitting the training data [25, 10].\n\nHaving converted keystrokes from each data set into a more visual medium, more direct comparisons could be made. MacBook keystrokes (similar to the keystrokes examined in the literature [4, 39, 6]) have only 2 visible peaks: the \u2018push\u2019 and \u2018release\u2019 peaks respectively. The 2 peak structures shown in Fig. 2 are similar to each other, implying that such a structure is native to the MacBook keyboard regardless of recording method, a noticeable difference however is the large range of frequencies present in the zoom recording. The Zoom peaks extend much higher than that of the phone-based recordings, indicating significant data in multiple frequencies that were not present when recorded via phone.\n\nThe overall data preparation procedure for our data was inspired by the structure presented in [10] and is shown in Fig. 3.\n\n### Model Selection and Implementation\n\nFigure 2: Waveform and corresponding mel-spectrogram of Left: Phone recording, and Right: Zoom recording.\n\n "
    ]
  },
  {
    "edit": [
      "CNPq through grant 308900/2019-7. R. Clemente acknowledges partial support from CNPq through grant 304454/2022-2. 99\n\nAbreu, E., \u00b4 O, J. & Medeiros, E. Properties of positive harmonic functions on the half-space with a nonlinear boundary condition. _J. Differential Equations_. **248**, 617-637 (2010), Adams, R. & Fournier, J. Sobolev spaces. (Elsevier/Academic Press, Amsterdam,2003) Aleksandrov, A. Uniqueness theorems for surfaces in the large. I. _Amer. Math. Soc. Transl. (2)_. **21** pp. 341-354 (1962), [https://doi.org/10.1016/j.jde.2009.07.006](https://doi.org/10.1016/j.jde.2009.07.006) [https://doi.org/10.1090/trans2/021/09](https://doi.org/10.1090/trans2/021/09) Alexandrov, A. A characteristic property of spheres. _Ann. Mat. Pura Appl. (4)_. **58** pp. 303-315 (1962), [https://doi.org/10.1007/BF02413056](https://doi.org/10.1007/BF02413056) Allegretto, W. & Huang, Y. A Picone\u2019s identity for the p-Laplacian and applications. _Nonlinear Anal._. **32**, 819-830 (1998), [https://doi.org/10.1016/S0362-546X](https://doi.org/10.1016/S0362-546X)(97)00530-0 Bonder, J. & Rossi, J. Existence results for the p-Laplacian with nonlinear boundary conditions. _J. Math. Anal. Appl._. **263**, 195-223 (2001), Chipot, M., Chleb\u0131\u00b4\u0131k, M., Fila, M. & Shafrir, I. Existence of positive solutions of a semilinear elliptic equation in \\(\\mathbb{R}^{n}_{+}\\) with a nonlinear boundary condition. _J. Math. Anal. Appl._. **223**, 429-471 (1998), [https://doi.org/10.1006/jmaa.2001.7609](https://doi.org/10.1006/jmaa.2001.7609) [https://doi.org/10.1006/jmaa.1998.5958](https://doi.org/10.1006/jmaa.1998.5958) Cuesta, M. & Tak\u00b4a\u02c7c, P. A strong comparison principle for positive solutions of degenerate elliptic equations. _Differential Integral Equations_. **13**, 721-746 (2000) Damascelli, L. & Pacella, F. Monotonicity and symmetry of solutions of p-Laplace equations, 1 \\(&lt;p&lt;2\\), via the moving plane method. _Ann. Scuola Norm. Sup. Pisa Cl. Sci. (4)_. **26**, 689-707 (1998), [http://www.numdam.org/item?id=ASNSP](http://www.numdam.org/item?id=ASNSP) 1998 4 26 4 689 0 Damascelli, L. & Sciunzi, B. Regularity, monotonicity and symmetry of positive solutions of m-Laplace equations. _J. Differential Equations_. **206**, 483-515 (2004), Degiovanni, M., Musesti, A. & Squassina, M. On the regularity of solutions in the Pucci-Serrin identity. _Calc. Var. Partial Differential Equations_. **18**, 317-334 (2003), \u00b4O, J. & Medeiros, E. Remarks on least energy solutions for quasilinear elliptic problems in \\(\\mathbb{R}^{N}\\). _Electron. J. Differential Equations_. pp. No. 83, 14 (2003) Escobar, J. Sharp constant in a Sobolev trace inequality. _Indiana Univ. Math. J._. **37**, 687-698 (1988), [https://doi.org/10.1016/j.jde.2004.05.012](https://doi.org/10.1016/j.jde.2004.05.012) [https://doi.org/10.1007/s00526-003-0208-y](https://doi.org/10.1007/s00526-003-0208-y) [https://doi.org/10.1512/iumj.1988.37.37033](https://doi.org/10.1512/iumj.1988.37.37033) Farina, A., Montoro, L. & Sciunzi, B. Monotonicity and one-dimensional symmetry for solutions of \\(-\\Delta_{p}u=f(u)\\) in half-spaces. _Calc. Var. Partial Differential Equations_. **43**, 123-145 (2012), [https://doi.org/10.1007/s00526-011-0405-z](https://doi.org/10.1007/s00526-011-0405-z) Farina, A., Montoro, L. & Sciunzi, B. Monotonicity of solutions of quasilinear degenerate elliptic equation in half-spaces. _Math. Ann._. **357**, 855- "
    ],
    "kosmos": [
      "CNPq through grant 308900/2019-7. R. Clemente acknowledges partial support from CNPq through grant 304454/2022-2.\n\n## References\n\n* [1] Abreu, E., O, J. &amp; Medeiros, E. Properties of positive harmonic functions on the half-space with a nonlinear boundary condition. _J. Differential Equations_. **248**, 617-637 (2010), [https://doi.org/10.1016/j.jde.2009.07.006](https://doi.org/10.1016/j.jde.2009.07.006)\n* [2] Adams, R. &amp; Fournier, J. Sobolev spaces. (Elsevier/Academic Press, Amsterdam,2003)\n* [3] Aleksandrov, A. Uniqueness theorems for surfaces in the large. I. _Amer. Math. Soc. Transl. (2)_. **21** pp. 341-354 (1962), [https://doi.org/10.1090/trans2/021/09](https://doi.org/10.1090/trans2/021/09)\n* [4] Alexandrov, A. A characteristic property of spheres. _Ann. Mat. Pura Appl. (4)_. **58** pp. 303-315 (1962), [https://doi.org/10.1007/BF02413056](https://doi.org/10.1007/BF02413056)\n* [5] Allegretto, W. &amp; Huang, Y. A Picone's identity for the p-Laplacian and applications. _Nonlinear Anal._. **32**, 819-830 (1998), [https://doi.org/10.1016/S0362-546X](https://doi.org/10.1016/S0362-546X)(97)00530-0\n* [6] Bonder, J. &amp; Rossi, J. Existence results for the p-Laplacian with nonlinear boundary conditions. _J. Math. Anal. Appl._. **263**, 195-223 (2001), [https://doi.org/10.1006/jmaa.2001.7609](https://doi.org/10.1006/jmaa.2001.7609)\n* [7] Chipot, M., Chlebuk, M., Fila, M. &amp; Shafrir, I. Existence of positive solutions of a semilinear elliptic equation in \\(\\mathbb{R}^{n}_{+}\\) with a nonlinear boundary condition. _J. Math. Anal. Appl._. **223**, 429-471 (1998), [https://doi.org/10.1006/jmaa.1998.5958](https://doi.org/10.1006/jmaa.1998.5958)\n* [8] Cuesta, M. &amp; Takac, P. A strong comparison principle for positive solutions of degenerate elliptic equations. _Differential Integral Equations_. **13**, 721-746 (2000)\n* [9] Damascelli, L. &amp; Pacella, F. Monotonicity and symmetry of solutions of p-Laplace equations, \\(1&lt;p&lt;2\\), via the moving plane method. _Ann. Scuola Norm. Sup. Pisa Cl. Sci. (4)_. **26**, 689-707 (1998), [http://www.numdam.org/item?id=ASNSP](http://www.numdam.org/item?id=ASNSP) 1998 4 26 4 689 0\n* [10] Damascelli, L. &amp; Sciunzi, B. Regularity, monotonicity and symmetry of positive solutions of m-Laplace equations. _J. Differential Equations_. **206**, 483-515 (2004), [https://doi.org/10.1016/j.jde.2004.05.012](https://doi.org/10.1016/j.jde.2004.05.012)\n* [11] Degiovanni, M., Musesti, A. &amp; Squassina, M. On the regularity of solutions in the Pucci-Serrin identity. _Calc. Var. Partial Differential Equations_. **18**, 317-334 (2003), [https://doi.org/10.1007/s00526-003-0208-y](https://doi.org/10.1007/s00526-003-0208-y)\n* [12] O, J. &amp; Medeiros, E. Remarks on least energy solutions for quasilinear elliptic problems in \\(\\mathbb{R}^{N}\\). _Electron. J. Differential Equations_. pp. No. 83, 14 (2003)\n* [13] Escobar, J. Sharp constant in a Sobolev trace inequality. _Indiana Univ. Math. J._. **37**, 687-698 (1988), [https://doi.org/10.1512/iumj.1988.37.37033](https://doi.org/10.1512/iumj.1988.37.37033)\n* [14] Farina, A., Montoro, L. &amp; Sciunzi, B. Monotonicity and one-dimensional symmetry for solutions of \\(-\\Delta_{p}u=f(u)\\) in half-spaces. _Calc. Var. Partial Differential Equations_. **43**, 123-145 (2012), https:// "
    ]
  },
  {
    "edit": [
      "to \ud835\udc34\ud835\udc65 = \ud835\udc4f has full support. By Proposition 4 , columns of \ud835\udc34 are integrally independent. It follows from Theorem 6 to obtain the bound ( 7 ) given in [ AADLO22 ]. We use the same notation as in the proof of Theorem 6 . For any \ud835\udc5a \u00d7 \ud835\udc5a submatrix of \ud835\udc34 whose columns are indexed by \ud835\udc3d where |\ud835\udc3d| = \ud835\udc5a, we have\n\n\\[\\left|\\det(A_{[\ud835\udc5a]\u00d7\ud835\udc57})\\right| =\\left|\\det(D)\\right|\\cdot\\left|\\det((\ud835\udc48^{-1})_{[\ud835\udc5a]\u00d7\ud835\udc57})\\right|\\] \\[=\\gcd(A)\\cdot\\left|\\det((\ud835\udc48^{-1})_{[\ud835\udc5a]\u00d7\ud835\udc57})\\right|\\] \\[=\\gcd(A)\\cdot\\left|\\det(U_{[\ud835\udc5b]\\setminus \ud835\udc3d\\times[\ud835\udc5a+1:\ud835\udc5b]})\\right|.\\]\n\nWe also know that\n\n\\[\\prod_{\ud835\udc56\u2208[\ud835\udc5b]\\setminus \ud835\udc3d}q_{\ud835\udc56}\\left|\\left|\\det(U_{[\ud835\udc5b]\\setminus \ud835\udc3d\\times[\ud835\udc5a+1:\ud835\udc5b] })\\right|\\right|,\\]\n\nand thus\n\n\\[\\prod_{\ud835\udc56\u2208[\ud835\udc5b]\\setminus \ud835\udc3d}q_{\ud835\udc56}\\left|\\frac{\\left|\\det(A_{[\ud835\udc5a]\u00d7\ud835\udc57})\\right|}{\\gcd (A)},\\right.\\]\n\nwhere \\(q_{\ud835\udc56},i\\in[n]\\setminus \ud835\udc3d\\) are primes numbers and with the same prime repeating at most \ud835\udc5a\\) times in {\ud835\udc5e_{\ud835\udc56}\\mid \ud835\udc56\\in[n]\\setminus \ud835\udc3d}. Recall notation \\(\\Omega_{\ud835\udc5a}(\ud835\udc67)=\\sum_{\ud835\udc56=1}^{\ud835\udc58}\\min\\{\ud835\udc60_{\ud835\udc56},\ud835\udc5a\\}\\) for the prime factorization of \ud835\udc67=r^{\\s_{1}}_{1}\\cdots r^{\\s_{\ud835\udc58}}_{\ud835\udc58}\\) with multiplicities \\(s_{1},...,s_{\ud835\udc58}\\in\\mathbb{Z}_{\\geq 0}\\). Clearly, when \ud835\udc65\\mid \ud835\udc66,\\Omega_{\ud835\udc5a}(\ud835\udc65)\\leqslant\\Omega_{\ud835\udc5a}(\ud835\udc66)\\). Thus, \\(\\Omega_{\ud835\udc5a}(\\prod_{\ud835\udc56\u2208[\ud835\udc5b]\\setminus \ud835\udc3d}q_{\ud835\udc56})\\leqslant\\Omega_{\ud835\udc5a}(\\frac{\\left|\\det (A_{[\ud835\udc5a]\u00d7\ud835\udc57})\\right|}{\\gcd(A)})\\). Moreover, since the multiplicity of each \\(q_{\ud835\udc56}\\) in \\(\\prod_{\ud835\udc56\u2208[\ud835\udc5b]\\setminus \ud835\udc3d}q_{\ud835\udc56}\\) is at most \ud835\udc5a\\), we have \\(\\Omega_{\ud835\udc5a}(\\prod_{\ud835\udc56\u2208[\ud835\udc5b]\\setminus \ud835\udc3d}q_{\ud835\udc56})=\\left|[n]\\setminus \ud835\udc3d|\\right|=\ud835\udc5b-m\\). Therefore, \\(n-m\\leqslant\\Omega_{\ud835\udc5a}(\\frac{\\left|\\det(A_{[\ud835\udc5a]\u00d7\ud835\udc57})\\right|}{\\gcd(A)})\\). Since \ud835\udc3d\\) is an arbitrary subset of \\([n]\\) with cardinality \ud835\udc5a\\), we obtain \\(n\\leqslant m+\\min_{\ud835\udc5f\u2208\\binom{[\ud835\udc5b]}{\ud835\udc5a},\\det(A_{r})\\neq 0}\\Omega_{\ud835\udc5a}\\left(\\frac{ \\left|\\det(A_{r})\\right|}{\\gcd(A)}\\right)\\). Applying the same argument as in the proof of Theorem 2 above, we obtain \ud835\udc53(A)\\leqslant m+\\min_{\ud835\udc5f\u2208\\binom{[\ud835\udc5b]}{\ud835\udc5a},\\det(A_{r})\\neq 0}\\Omega_{\ud835\udc5a}\\left(\\frac{ \\left|\\det(A_{r})\\right|}{\\gcd(A)}\\right)\\).\n\n### Upper bound on \u210e(\ud835\udc5a,\ud835\udc61)\n\nRecall \u210e(\ud835\udc5a,\ud835\udc61) is the maximum "
    ],
    "kosmos": [
      "to \ud835\udc34\ud835\udc65 = \ud835\udc4f has full support. By Proposition 4, columns of \ud835\udc34 are integrally independent. It follows from Theorem 6 that inequality (5) holds.\n\n**Remark 1**.: _We demonstrate how to modify the proof of Theorem 6 to obtain the bound (7) given in [1]. We use the same notation as in the proof of Theorem 6. For any \ud835\udc5a \u00d7 \ud835\udc5a submatrix of \ud835\udc34 whose columns are indexed by \ud835\udc3d where |\ud835\udc3d| = \ud835\udc5a, we have_\n\n\\[\\left|\\det(A_{[\ud835\udc5a]\u00d7\ud835\udc57})\\right| =\\left|\\det(D)\\right|\\cdot\\left|\\det((\ud835\udc48^{-1})_{[\ud835\udc5a]\u00d7\ud835\udc57})\\right|\\] \\[=\\gcd(A)\\cdot\\left|\\det((\ud835\udc48^{-1})_{[\ud835\udc5a]\u00d7\ud835\udc57})\\right|\\] \\[=\\gcd(A)\\cdot\\left|\\det(U_{[\ud835\udc5b]\\setminus \ud835\udc3d\\times[m+1:\ud835\udc5b]})\\right|.\\]\n\n_We also know that_\n\n\\[\\prod_{\ud835\udc56\u2208[\ud835\udc5b]\\setminus \ud835\udc3d}q_{\ud835\udc56}\\ \\Big{|}\\ \\left|\\det(U_{[\ud835\udc5b]\\setminus \ud835\udc3d\\times[m+1:\ud835\udc5b]})\\right|,\\]\n\n_and thus_\n\n\\[\\prod_{\ud835\udc56\u2208[\ud835\udc5b]\\setminus \ud835\udc3d}q_{\ud835\udc56}\\ \\Big{|}\\ \\frac{\\left|\\det(A_{[\ud835\udc5a]\u00d7\ud835\udc57})\\right|}{ \\gcd(A)}\\]\n\n_where \\(q_{\ud835\udc56},i\\in[n]\\setminus \ud835\udc3d\\) are primes numbers and with the same prime repeating at most \ud835\udc5a\\) times in \\(\\{q_{\ud835\udc56}\\mid i\\in[n]\\setminus \ud835\udc3d\\). Recall notation \\(\\Omega_{\ud835\udc5a}(z)=\\sum_{\ud835\udc56=1}^{\ud835\udc58}\\min\\{\ud835\udc60_{\ud835\udc56},\ud835\udc5a\\}\\) for the prime factorization of \\(z=r_{1}^{\ud835\udc60_{1}}\\cdots r_{\ud835\udc58}^{\ud835\udc60_{\ud835\udc58}}\\) with multiplicities \\(s_{1},...,s_{\ud835\udc58}\\in\\mathbb{Z}_{\\geq 0}\\). Clearly, when \\(x\\mid y,\\Omega_{\ud835\udc5a}(x)\\leqslant\\Omega_{\ud835\udc5a}(y)\\). Thus, \\(\\Omega_{\ud835\udc5a}(\\prod_{\ud835\udc56\u2208[\ud835\udc5b]\\setminus \ud835\udc3d}q_{\ud835\udc56})\\leqslant\\Omega_{\ud835\udc5a}(\\frac{\\left|\\det (A_{[\ud835\udc5a]\u00d7\ud835\udc57})\\right|}{\\gcd(A)})\\). Moreover, since the multiplicity of each \\(q_{\ud835\udc56}\\) in \\(\\prod_{\ud835\udc56\u2208[\ud835\udc5b]\\setminus \ud835\udc3d}q_{\ud835\udc56}\\) is at most \ud835\udc5a\\), we have \\(\\Omega_{\ud835\udc5a}(\\prod_{\ud835\udc56\u2208[\ud835\udc5b]\\setminus \ud835\udc3d}q_{\ud835\udc56})=\\left|[n]\\setminus \ud835\udc3d\\right|=n-m\\). Therefore, \\(n-m\\leqslant\\Omega_{\ud835\udc5a}(\\frac{\\left|\\det(A_{[\ud835\udc5a]\u00d7\ud835\udc57})\\right|}{\\gcd(A)})\\). Since \ud835\udc3d\\) is an arbitrary subset of \\([n]\\) with cardinality \ud835\udc5a\\), we obtain \\(n\\leqslant m+\\min_{\ud835\udc5f\u2208\\binom{[\ud835\udc5b]}{\ud835\udc5a},\\det(A_{\ud835\udc5f})\\neq 0}\\Omega_{\ud835\udc5a}\\left(\\frac{ \\left|\\det(A_{\ud835\udc5f})\\right|}{\\gcd(A)}\\right)\\). Applying the same argument as in the proof of Theorem 2 above, we obtain \\(f(A)\\leqslant m+\\min_{\ud835\udc5f\u2208\\binom{[\ud835\udc5b]}{\ud835\udc5a},\\det(A_{\ud835\udc5f})\\neq 0}\\Omega_{\ud835\udc5a}\\left(\\frac{ \\left|\\det(A_{\ud835\udc5f})\\right|}{\\gcd(A)}\\right)\\)._\n\n### Upper bound on \u210e(\ud835\udc5a,\ud835\udc61)\n\nRecall \u210e( "
    ]
  },
  {
    "edit": [
      "Figure 15: **Smaller Network**, Original Images (First Four), Reconstructed Images (Last Four) **(a) Row 1**: codebook size=8192, latent dimension size = 256, 65 images, without positional encoding. **(b) Row 2**: codebook size=8192, latent dimension size = 256, 65 images, with positional encoding. (Figure 16(b))\n\nFigure 16: **(a) Column 1**: codebook sizes = [1024, 8192, 1024], Latent dimensions = 256, image sizes = [2700, 185, 2700] , with, with and without positional encoding respectively. (refer Figure 12 &amp; Figure 8) **(b) Column 2**: **Smaller network**, codebook size = 8192, latent dimension size = 256, image size = 65, without and with positional encoding. (refer Figure 15)\n\n"
    ],
    "kosmos": [
      "Figure 15: **Smaller Network**, Original Images (First Four), Reconstructed Images (Last Four) **(a) Row 1**: codebook size=8192, latent dimension size = 256, 65 images, without positional encoding. **(b) Row 2**: codebook size=8192, latent dimension size = 256, 65 images, with positional encoding. (Figure 16(b))\n\nFigure 16: **(a) Column 1**: codebook sizes = [1024, 8192, 1024], Latent dimensions = 256, image sizes = [2700, 185, 2700] , with, with and without positional encoding respectively. (refer Figure 12 &amp; Figure 8) **(b) Column 2**: **Smaller network**, codebook size = 8192, latent dimension size = 256, image size = 65, without and with positional encoding. (refer Figure 15)\n\n"
    ]
  },
  {
    "edit": [
      "beam alignment case, an excessive number of antennas is adverse to the network coverage probability. As for the perfect beam alignment case, Fig. 6(b) shows that, as the number of antennas becomes large, the outage probability decreases, and the trend of decreasing gradually slows down. This figure implies that from the perspective such as hardware cost, it is not necessary to equip the UAV with too many antennas since the performance gain is very small.\n\n### Effect of UAV Deployment Height\n\nWith regards to the effect of UAV deployment height, Fig. 6(a) shows that the higher deployment of UAVs will lead to a larger outage probability under the imperfect alignment case. The reason is as follows. The higher height implies the larger coverage area of the main-lobe beam on certain tiers, which introduces more interference from more UAVs to the typical UE with the main-lobe pointed. When the beam is mispointed, this kind of effect on SINR becomes worse. Besides that, for the same projection point, the higher height indicates that the transmission link is more likely to be LoS, which means the signal strength from the interfering UAV becomes stronger. Hence, when the misalignment for the beamforming is non-negligible, the lower deployment of UAVs is preferred for both schemes. Under the case of perfect beam alignment, from Fig. 6(b), it can be seen that the outage probability of MAPAS is smaller for the lower height of UAVs. The explanation is the same as before. However, in terms of the performance of CDAS, it seems that the outage probability for the higher altitude of UAVs can be better. This is mainly because of the fact the UE is associated with the closest UAV and their link can be either LoS or NLoS. As mentioned before, for the same projection point, the link from UAV at the lower height is much more likely to be NLoS, which reduces the desired signal strength at the UE whereby degrading the outage performance.\n\n### Effect of UAV Density\n\nFig. 7 plots the outage probability versus the number of antennas for UAV under different UAV densities for both imperfect and perfect alignment cases. Under the imperfect beam alignment scenario, as expected, the outage probability drops at first and then rises with the increase in the number of antennas. Moreover, since more interfering UAVs are involved in the system, the higher UAV density leads to worse outage probability performance. Besides that, Fig. 7(a) shows that the optimal number of UAV antennas increases as the UAV density rises, e.g., for MAPAS, the optimal number antennas is 4 for \\(\\lambda_{k}=0.5\\times 10^{-5}\\) m\\({}^{-2}\\) while it goes to 16 for \\(\\lambda_{k}=5\\times 10^{-5}\\) m\\({}^{-2}\\). The reason is as follows. When the UAV density is very sparse, the number of interfering UAVs falling into the region covered by the main-lobe beam of the UE is very small. In other words, the interference is not that severe. Then the serving UAV needs to ensure that the typical UE is covered by its main-lobe beam; hence, a larger main-lobe beamwidth (equivalently, a smaller number of antennas) is preferred. However, when the interfering UAVs are very dense, the interference becomes very severe. One way to reduce the interference is to reduce the density of interfering UAVs with main-lobe beam pointed to the typical UE. From our analysis, this can be achieved by narrowing the main-lobe beamwidth. Note that it cannot be too narrow, because this can degrade the signal strength from the serving UAV due to the beam misalignment. Hence, a relatively larger number of antennas is preferred for the case of denser UAVs. Under the perfect beam scenario, for the MAPAS, sparse UAVs lead to a lower outage probability as expected. However, this is not the case for the CDAS. Fig. 7(b) shows that the denser UAVs can even result in a better outage probability, especially when the number of antennas is large. The reason is as follows. When the UAV density is very sparse, the closest UAV (i.e., the serving UAV) can be very far away, which consequently leads to a very weak signal strength from the serving UAV. Increasing the density of UAVs somehow improves the signal strength from the serving UAV. However, the effect of denser UAVs is not as obvious as that of sparse UAVs. The reason is that the denser UAVs are more likely to be located in the area of the main-lobe beam, which leads to a weaker signal strength from the serving UAV. Hence, the denser UAVs are preferred for the perfect beam alignment case. The reason is that the denser UAVs are more likely to be located in the area of the main-lobe beam, which leads to a weaker signal strength from the serving UAV. In addition, the denser UAVs "
    ],
    "kosmos": [
      "beam alignment case, an excessive number of antennas is adverse to the network coverage probability. As for the perfect beam alignment case, Fig. 6(b) shows that, as the number of antennas becomes large, the outage probability decreases, and the trend of decreasing gradually slows down. This figure implies that from the perspective such as hardware cost, it is not necessary to equip the UAV with too many antennas since the performance gain is very small.\n\n### Effect of UAV Deployment Height\n\nWith regards to the effect of UAV deployment height, Fig. 6(a) shows that the higher deployment of UAVs will lead to a larger outage probability under the imperfect alignment case. The reason is as follows. The higher height implies the larger coverage area of the main-lobe beam on certain tiers, which introduces more interference from more UAVs to the typical UE with the main-lobe pointed. When the beam is mispointed, this kind of effect on SINR becomes worse. Besides that, for the same projection point, the higher height indicates that the transmission link is more likely to be LoS, which means the signal strength from the interfering UAV becomes stronger. Hence, when the misalignment for the beamforming is non-negligible, the lower deployment of UAVs is preferred for both schemes. Under the case of perfect beam alignment, from Fig. 6(b), it can be seen that the outage probability of MAPAS is smaller for the lower height of UAVs. The explanation is the same as before. However, in terms of the performance of CDAS, it seems that the outage probability for the higher altitude of UAVs can be better. This is mainly because of the fact the UE is associated with the closest UAV and their link can be either LoS or NLoS. As mentioned before, for the same projection point, the link from UAV at the lower height is much more likely to be NLoS, which reduces the desired signal strength at the UE whereby degrading the outage performance.\n\n### Effect of UAV Density\n\nFig. 7 plots the outage probability versus the number of antennas for UAV under different UAV densities for both imperfect and perfect alignment cases. Under the imperfect beam alignment scenario, as expected, the outage probability drops at first and then rises with the increase in the number of antennas. Moreover, since more interfering UAVs are involved in the system, the higher UAV density leads to worse outage probability performance. Besides that, Fig. 7(a) shows that the optimal number of UAV antennas increases as the UAV density rises, e.g., for MAPAS, the optimal number antennas is 4 for \\(\\lambda_{k}=0.5\\times 10^{-5}\\) m\\({}^{-2}\\) while it goes to 16 for \\(\\lambda_{k}=5\\times 10^{-5}\\) m\\({}^{-2}\\). The reason is as follows. When the UAV density is very sparse, the number of interfering UAVs falling into the region covered by the main-lobe beam of the UE is very small. In other words, the interference is not that severe. Then the serving UAV needs to ensure that the typical UE is covered by its main-lobe beam; hence, a larger main-lobe beamwidth (equivalently, a smaller number of antennas) is preferred. However, when the interfering UAVs are very dense, the interference becomes very severe. One way to reduce the interference is to reduce the density of interfering UAVs with main-lobe beam pointed to the typical UE. From our analysis, this can be achieved by narrowing the main-lobe beamwidth. Note that it cannot be too narrow, because this can degrade the signal strength from the serving UAV due to the beam misalignment. Hence, a relatively larger number of antennas is preferred for the case of denser UAVs. Under the perfect beam scenario, for the MAPAS, sparse UAVs lead to a lower outage probability as expected. However, this is not the case for the CDAS. Fig. 7(b) shows that the denser UAVs can even result in a better outage probability, especially when the number of antennas is large. The reason is as follows. When the UAV density is very sparse, the closest UAV (i.e., the serving UAV) can be very far away, which consequently leads to a very weak signal strength from the serving UAV. Increasing the density of UAVs somehow improves the signal strength from the serving UAV. However, the effect of denser UAVs is not as obvious as that of sparse UAVs. The reason is that the denser UAVs are more likely to be located in the area of the main-lobe beam, which leads to a weaker signal strength from the serving UAV. Hence, the denser UAVs are preferred for the perfect beam alignment case. The reason is that the denser UAVs are more likely to be located in the area of the main-lobe beam, which leads to a weaker signal strength from the serving UAV. In addition, the denser UAVs "
    ]
  },
  {
    "edit": [
      "This iteration was introduced by Nadel in [31] where he also proved that periodic points of order two or three must be K\u00a8ahler-Einstein metrics. This was generalized [20, 33] to periodic points \\(\\omega\\) of any order, that is, those satisfying \\(\\rho_{\\omega}^{k}=\\lambda\\omega\\) for any \\(k\\in\\mathbb{N}\\). The iteration (3) can be regarded as a discretization of the K\u00a8ahler\u2013Ricci flow. Observe that, if the manifold \\(M\\) is compact, by Calabi\u2019s conjecture one does not need positivity assumptions to reverse the construction above and define the inverse Ricci\u2013K\u00a8ahler iterations \\(\\rho_{\\omega}^{-k}\\). Both of this discrete dynamical systems have been studied in the literature, see for instance [5, 13]. Here we focus on the following generalized Monge-Amp`ere equation on a complex manifold \\(M\\)\n\n\\[\\rho_{\\omega}^{k}=\\lambda\\Omega \\tag{4}\\]\n\nwhere \\(\\Omega\\) is again a K\u00a8ahler form. In particular we study equation (4) for a special class of wellbehaved K\u00a8ahler metrics. Our second result is the following theorem dealing with K\u00a8ahler metrics induced by the flat metric (which are well-behaved by Example 4 below). Let \\(M\\) be a complex manifold with two metrics \\(g\\) and \\(G\\) induced by the flat metric. If the corresponding K\u00a8ahler forms \\(\\omega,\\Omega\\) satisfy \\(\\rho_{\\omega}^{k}=\\lambda\\Omega\\) for some \\(k\\geqslant 1\\) and \\(\\lambda\\in\\mathbb{R}\\), then \\((M,g)\\) is a totally geodesic submanifold of the flat ambient space. Observe that this result can be seen as a generalization of [40, Theorem 2.1]. Namely, when \\(k=1\\) and \\(g=G\\), Theorem 2 shows that a K\u00a8ahler\u2013Einstein submanifolds of \\(\\mathbb{C}^{n}\\) with the flat metric is necessarily Ricci-flat, hence a totally geodesic submanifold. It is worth pointing out that the metric \\(g\\) is assumed to be induced by a flat one for simplicity, but the hypothesis on \\(g\\) can be sensibly relaxed, cf. Remark 11 below. Our third and last result deals with K\u00a8ahler\u2013Ricci solitons (KRS). Recall that a K\u00a8ahler metric \\(G\\) is a KRS if there exists a holomorphic vector field \\(X\\) (the solitonic vector field) such that \\(\\operatorname{Ric}(G)=\\mu G+L_{X}G\\) where \\(\\operatorname{Ric}(G)\\) denotes the Ricci tensor of \\(G\\) and \\(L_{X}\\) the Lie derivative in the direction of \\(X\\). K\u00a8ahler\u2013Ricci solitons are important generalizations of K\u00a8ahler\u2013Einstein metrics which arise in the study of the K\u00a8ahler\u2013Ricci flow. Our next result show that KRS cannot arise as K\u00a8ahler\u2013Ricci iterations of metrics induced by complex space forms. Let \\(M\\) be a complex manifold with two real analytic K\u00a8ahler metrics \\(g\\) and \\(G\\). Assume that \\(g\\) is induced by a complex space form and \\(G\\) is a KRS. If the corresponding K\u00a8ahler forms \\(\\omega,\\Omega\\) satisfy \\(\\rho_{\\omega}^{k}=\\lambda\\Omega\\) for some \\(\\lambda\\neq 0\\) and \\(k\\geqslant 0\\), then \\(G\\) is trivial, i.e. K\u00a8ahler-Einstein. Observe that, when \\(\\lambda=0\\), one cannot draw any conclusion. Take for instance \\(g\\) to be the flat metric on \\(\\mathbb{C}\\) and \\(G\\) to be Hamilton's cigar KRS [17]. It is worth mentioning that we do not know of any K\u00a8ahler\u2013Einstein. metric \\(G\\) and K\u00a8ahler metric \\(g\\) induced by a complex space form which satisfy \\(\\rho_{\\omega}^{k}=\\lambda\\Omega\\) for \\(\\lambda&lt;0\\) unless \\(g=G\\) and \\(k=1\\). In that case \\(M\\) is forced to be a totally geodesic submanifold in \\(\\mathbb{C}H^{n}\\)[40]. On the other hand, such examples for \\(\\lambda&gt;0\\) are discussed in Section 3, see in particular Proposition 2. Also in this case we generalize some "
    ],
    "kosmos": [
      "This iteration was introduced by Nadel in [31] where he also proved that periodic points of order two or three must be K\u00e4hler-Einstein metrics. This was generalized [20, 33] to periodic points \\(\\omega\\) of any order, that is, those satisfying \\(\\rho_{\\omega}^{k}=\\lambda\\omega\\) for any \\(k\\in\\mathbb{N}\\). The iteration (3) can be regarded as a discretization of the K\u00e4hler\u2013Ricci flow. Observe that, if the manifold \\(M\\) is compact, by Calabi's conjecture one does not need positivity assumptions to reverse the construction above and define the inverse Ricci\u2013K\u00e4hler iterations \\(\\rho_{\\omega}^{-k}\\). Both of this discrete dynamical systems have been studied in the literature, see for instance [5, 13].\n\nHere we focus on the following generalized Monge-Amp`ere equation on a complex manifold \\(M\\)\n\n\\[\\rho_{\\omega}^{k}=\\lambda\\Omega \\tag{4}\\]\n\nwhere \\(\\Omega\\) is again a K\u00e4hler form. In particular we study equation (4) for a special class of well-behaved K\u00e4hler metrics. Our second result is the following theorem dealing with K\u00e4hler metrics induced by the flat metric (which are well-behaved by Example 4 below).\n\n**Theorem 2**.: _Let \\(M\\) be a complex manifold with two metrics \\(g\\) and \\(G\\) induced by the flat metric. If the corresponding K\u00e4hler forms \\(\\omega,\\Omega\\) satisfy \\(\\rho_{\\omega}^{k}=\\lambda\\Omega\\) for some \\(k\\geqslant 1\\) and \\(\\lambda\\in\\mathbb{R}\\), then \\((M,g)\\) is a totally geodesic submanifold of the flat ambient space._\n\nObserve that this result can be seen as a generalization of [40, Theorem 2.1]. Namely, when \\(k=1\\) and \\(g=G\\), Theorem 2 shows that a K\u00e4hler\u2013Einstein submanifolds of \\(\\mathbb{C}^{n}\\) with the flat metric is necessarily Ricci-flat, hence a totally geodesic submanifold. It is worth pointing out that the metric \\(g\\) is assumed to be induced by a flat one for simplicity, but the hypothesis on \\(g\\) can be sensibly relaxed, cf. Remark 11 below.\n\nOur third and last result deals with K\u00e4hler\u2013Ricci solitons (KRS). Recall that a K\u00e4hler metric \\(G\\) is a KRS if there exists a holomorphic vector field \\(X\\) (the solitonic vector field) such that \\(\\operatorname{Ric}(G)=\\mu G+L_{X}G\\) where \\(\\operatorname{Ric}(G)\\) denotes the Ricci tensor of \\(G\\) and \\(L_{X}\\) the Lie derivative in the direction of \\(X\\). K\u00e4hler\u2013Ricci solitons are important generalizations of K\u00e4hler\u2013Einstein metrics which arise in the study of the K\u00e4hler\u2013Ricci flow. Our next result show that KRS cannot arise as K\u00e4hler\u2013Ricci iterations of metrics induced by complex space forms.\n\n**Theorem 3**.: _Let \\(M\\) be a complex manifold with two real analytic K\u00e4hler metrics \\(g\\) and \\(G\\). Assume that \\(g\\) is induced by a complex space form and \\(G\\) is a KRS. If the corresponding K\u00e4hler forms \\(\\omega,\\Omega\\) satisfy \\(\\rho_{\\omega}^{k}=\\lambda\\Omega\\) for some \\(\\lambda\\neq 0\\) and \\(k\\geqslant 0\\), then \\(G\\) is trivial, i.e. K\u00e4hler-Einstein._\n\nObserve that, when \\(\\lambda=0\\), one cannot draw any conclusion. Take for instance \\(g\\) to be the flat metric on \\(\\mathbb{C}\\) and \\(G\\) to be Hamilton's cigar KRS [17]. It is worth mentioning that we do not know of any K\u00e4hler\u2013Einstein metric \\(G\\) and K\u00e4hler metric \\(g\\) induced by a complex space form which satisfy \\(\\rho_{\\omega}^{k}=\\lambda\\Omega\\) for \\(\\lambda<0\\) (and=\"\" 0\\)=\"\" 1.1].=\"\" 3,=\"\" 3,=\"\" [40].=\"\" [25,=\"\" [25,=\"\" [25, theorem=\"\" [26].=\"\" [27].=\"\" [28,=\"\" [29,=\"\" [30].=\"\" [31].=\"\" [32].=\"\" [33].=\"\" [34].=\"\" [35].=\"\" [36].=\"\" [37].\n\n "
    ]
  },
  {
    "edit": [
      "Finally, let us simplify Term 1:\n\nTerm1 = \\(\\exp{(\\{})-i\\alpha_{1}(s)E}\\exp{(\\{})-i\\alpha_{2}(s)P}\\exp{(\\{})-i\\alpha_{3}(s)Q}\\) ( \\(\\exp{(\\{})-i\\alpha_{4}(s)H}\\alpha^{\\prime}_{4}(s)H\\)\n\n= \\(\\alpha^{\\prime}_{4}(s)\\exp{(\\{})-i\\alpha_{1}(s)E}\\exp{(\\{})-i\\alpha_{2}(s)P}\\) ( \\(\\exp{(\\{})-i\\alpha_{3}(s)Q}\\)H) \\(\\exp{(\\{})-i\\alpha_{4}(s)H}\\)\n\n= \\(\\alpha^{\\prime}_{4}(s)e^{-i\\alpha_{1}(s)E}e^{-i\\alpha_{2}(s)P}\\) ( \\(\\exp{(\\{})-i\\alpha_{3}(s)Q}\\)H) \\(e^{i\\alpha_{3}(s)Q}e^{-i\\alpha_{3}(s)Q}\\)\n\nIdentity operator inserted\n\n\\(\\exp{(\\{})-i\\alpha_{4}(s)H}\\)\n\n= \\(\\alpha^{\\prime}_{4}(s)\\exp{(\\{})-i\\alpha_{1}(s)E}\\exp{(\\{})-i\\alpha_{2}(s)P}\\) ( \\(\\exp{(\\{})-i\\alpha_{3}(s)Q}\\)H \\(\\exp{(\\{})-i\\alpha_{3}(s)Q}\\)\n\nApply Baker-Campbell Hausdorff formula\n\n\\(\\exp{(\\{})-i\\alpha_{3}(s)Q}\\exp{(\\{})-i\\alpha_{4}(s)H}\\)\n\n= \\(\\alpha^{\\prime}_{4}(s)\\exp{(\\{})-i\\alpha_{1}(s)E}\\exp{(\\{})-i\\alpha_{2}(s)P}\\) ( \\(H+\\alpha_{3}(s)P+\\frac{\\alpha_{3}(s)^{2}}{2}E\\)\n\n\\(\\exp{(\\{})-i\\alpha_{3}(s)Q}\\exp{(\\{})-i\\alpha_{4}(s)H}\\)\n\n= \\(\\alpha^{\\prime}_{4}(s)\\exp{(\\{})-i\\alpha_{1}(s)E}\\exp{(\\{})-i\\alpha_{2}(s)P}\\)H \\(\\exp{(\\{})-i\\alpha_{3}(s)Q}\\) \\(\\exp{(\\{})-i\\alpha_{4}(s)H}\\)\n\n+ \\(\\alpha^{\\prime}_{4}(s)\\exp{(\\{})-i\\alpha_{1}(s)E}\\exp{(\\{})-i\\alpha_{2}(s)P}\\) \\(\\frac{\\alpha_{3}(s)^{2}}{2}E\\)\n\n+ \\(\\alpha^{\\prime}_{4}(s)\\exp{(\\{})-i\\alpha_{1}(s)E}\\exp{(\\{})-i\\alpha_{2}(s)P}\\) \\(\\frac{\\alpha_{3}(s)^{2}}{2}E\\)\n\n= \\(\\alpha^{\\prime}_{4}(s)\\exp{(\\{})-i\\alpha_{1}(s)E}\\) ( \\(H-\\alpha_{2}(s)Q+\\frac{\\alpha_{2}(s)^{2}}{2}E\\)\n\n\\(\\exp{(\\{})-i\\alpha_{2}(s)P}\\) \\(\\exp{(\\{})-i\\alpha_{3}(s)Q}\\) \\(\\exp{(\\{})-i\\alpha_{4}(s)H}\\)\n\n+ \\(\\alpha^{\\prime}_{4}(s)\\alpha_{3}(s)PU(s)+\\alpha^{\\prime}_{4}(s)\\frac{\\alpha_{3 }(s)^{2}}{2}EU(s)\\)\n\n= \\(\\alpha^{\\prime}_{4}(s)HU(s)-\\alpha^{\\prime}_{4}(s)\\alpha_{2}(s)QU(s)+\\frac{1}{ 2}\\alpha^{\\prime}_{4}(s)\\alpha_{2}(s)^{2}EU(s)\\)\n\n+ \\(\\alpha^{\\prime}_{4}(s)\\alpha_{3}(s)PU(s)+\\frac{1}{2}\\alpha^{\\prime}_{4}(s) \\alpha_{3}(s)^{2}EU(s)\\)\n\n= \\(\\left(\\alpha^{\\prime}_{4}(s)H-\\alpha^{\\prime}_{4}(s)\\alpha_{2}(s)Q+\\frac{1}{ "
    ],
    "kosmos": [
      "Finally, let us simplify Term 1:\n\nTerm1 = \\(\\exp{(\\{})-i\\alpha_{1}(s)E}\\exp{(\\{})-i\\alpha_{2}(s)P}\\exp{(\\{})-i\\alpha_{3}(s)Q}\\) ( \\(\\exp{(\\{})-i\\alpha_{4}(s)H}\\alpha^{\\prime}_{4}(s)H\\)\n\n= \\(\\alpha^{\\prime}_{4}(s)\\exp{(\\{})-i\\alpha_{1}(s)E}\\exp{(\\{})-i\\alpha_{2}(s)P}\\) ( \\(\\exp{(\\{})-i\\alpha_{3}(s)Q}\\)H) \\(\\exp{(\\{})-i\\alpha_{4}(s)H}\\)\n\n= \\(\\alpha^{\\prime}_{4}(s)e^{-i\\alpha_{1}(s)E}e^{-i\\alpha_{2}(s)P}\\) ( \\(\\exp{(\\{})-i\\alpha_{3}(s)Q}\\)H) \\(e^{i\\alpha_{3}(s)Q}e^{-i\\alpha_{3}(s)Q}\\)\n\nIdentity operator inserted\n\n\\(\\exp{(\\{})-i\\alpha_{4}(s)H}\\)\n\n= \\(\\alpha^{\\prime}_{4}(s)\\exp{(\\{})-i\\alpha_{1}(s)E}\\exp{(\\{})-i\\alpha_{2}(s)P}\\) ( \\(\\exp{(\\{})-i\\alpha_{3}(s)Q}\\)H \\(\\exp{(\\{})-i\\alpha_{3}(s)Q}\\)\n\nApply Baker-Campbell Hausdorff formula\n\n\\(\\exp{(\\{})-i\\alpha_{3}(s)Q}\\exp{(\\{})-i\\alpha_{4}(s)H}\\)\n\n= \\(\\alpha^{\\prime}_{4}(s)\\exp{(\\{})-i\\alpha_{1}(s)E}\\exp{(\\{})-i\\alpha_{2}(s)P}\\) ( \\(H+\\alpha_{3}(s)P+\\frac{\\alpha_{3}(s)^{2}}{2}E\\)\n\n\\(\\exp{(\\{})-i\\alpha_{3}(s)Q}\\exp{(\\{})-i\\alpha_{4}(s)H}\\)\n\n= \\(\\alpha^{\\prime}_{4}(s)\\exp{(\\{})-i\\alpha_{1}(s)E}\\exp{(\\{})-i\\alpha_{2}(s)P}\\)H \\(\\exp{(\\{})-i\\alpha_{3}(s)Q}\\) \\(\\exp{(\\{})-i\\alpha_{4}(s)H}\\)\n\n+ \\(\\alpha^{\\prime}_{4}(s)\\exp{(\\{})-i\\alpha_{1}(s)E}\\exp{(\\{})-i\\alpha_{2}(s)P}\\) \\(\\frac{\\alpha_{3}(s)^{2}}{2}E\\)\n\n+ \\(\\alpha^{\\prime}_{4}(s)\\exp{(\\{})-i\\alpha_{1}(s)E}\\exp{(\\{})-i\\alpha_{2}(s)P}\\) \\(\\frac{\\alpha_{3}(s)^{2}}{2}E\\)\n\n= \\(\\alpha^{\\prime}_{4}(s)\\exp{(\\{})-i\\alpha_{1}(s)E}\\) ( \\(H-\\alpha_{2}(s)Q+\\frac{\\alpha_{2}(s)^{2}}{2}E\\)\n\n\\(\\exp{(\\{})-i\\alpha_{2}(s)P}\\) \\(\\exp{(\\{})-i\\alpha_{3}(s)Q}\\) \\(\\exp{(\\{})-i\\alpha_{4}(s)H}\\)\n\n+ \\(\\alpha^{\\prime}_{4}(s)\\alpha_{3}(s)PU(s)+\\alpha^{\\prime}_{4}(s)\\frac{\\alpha_{3 }(s)^{2}}{2}EU(s)\\)\n\n= \\(\\alpha^{\\prime}_{4}(s)HU(s)-\\alpha^{\\prime}_{4}(s)\\alpha_{2}(s)QU(s)+\\frac{1}{ 2}\\alpha^{\\prime}_{4}(s)\\alpha_{2}(s)^{2}EU(s)\\)\n\n+ \\(\\alpha^{\\prime}_{4}(s)\\alpha_{3}(s)PU(s)+\\frac{1}{2}\\alpha^{\\prime}_{4}(s) \\alpha_{3}(s)^{2}EU(s)\\)\n\n= \\(\\left(\\alpha^{\\prime}_{4}(s)H-\\alpha^{\\prime}_{4}(s)\\alpha_{2}(s)Q+\\frac{1}{ "
    ]
  },
  {
    "edit": [
      "The first integral of (5.21) can be estimated as follows: for any \\(\\epsilon&gt;0\\),\n\n\\[\\int_{\\frac{1}{4}}^{1}\\int_{|y^{\\prime}|\\leq 1}(t-s)^{\\frac{1}{2}} ( |x-y^{\\prime}|^{2}+(t-s))^{\\frac{n-1}{2}}\\left|\\partial_{s}g_{k}^{T}(s)\\right| dy^{\\prime}ds\\] \\[\\lesssim\\int_{\\frac{1}{4}}^{1}\\int_{|x|+1+\\sqrt{t-s}}^{1}\\log\\left( 2+\\frac{1}{\\sqrt{t-s}}\\right)ds\\] \\[\\lesssim\\frac{1}{\\langle x\\rangle^{n-1}}\\int_{\\sqrt{t-1}}^{\\sqrt{t -1}}\\log\\left(2+\\frac{1}{u}\\right)du\\lesssim\\frac{1}{\\langle x\\rangle^{n-1}}\\int _{\\sqrt{t-1}}^{\\sqrt{t-1}}\\frac{1}{v^{\\epsilon}}dv\\] \\[\\lesssim\\frac{(t-1)^{\\frac{1}{2}}}{\\langle x\\rangle^{n-1}}.\\]\n\nThe second integral of (5.21) can be estimated as follows:\n\n\\[\\int_{\\frac{1}{4}}^{1}\\int_{|y^{\\prime}|\\leq 1}(t-s)^{\\frac{1}{2}}( |x-y^{\\prime}|^{2}+(t-s))^{\\frac{n-1}{2}}\\left|\\partial_{s}g_{k}^{T}(s)\\right| dy^{\\prime}ds\\] \\[\\lesssim\\int_{\\frac{1}{4}}^{1}\\int_{|y^{\\prime}|\\leq 1}(t-s)^{\\frac {1}{2}}( |x-y^{\\prime}|^{2}+(t-s))^{\\frac{n-1}{2}}(1-s)^{1-a}dy^{\\prime}ds\\] \\[\\lesssim\\int_{\\frac{1}{4}}^{1}\\frac{1}{(t-s)^{\\frac{1}{2}}( |x|+\\sqrt{t-s}+1)^{n-1}}\\frac{1}{(1-s)^{1-a}}ds\\mathbbm{1}_{|x|&lt;2}+\\int_{\\frac{1 }{4}}^{1}\\frac{1}{(t-s)^{\\frac{1}{2}}(1-s)^{1-a}}ds\\frac{1}{|x|^{n-1}}\\mathbbm{ 1}_{|x|&gt;2}\\] \\[\\lesssim\\int_{\\sqrt{t-1}}^{\\sqrt{t-1}}\\frac{1}{(1-t+u^{2})^{1-a}} du\\mathbbm{1}_{|x|&lt;2}+\\frac{\\text{LN}^{*}}{|x|^{n-1}}\\mathbbm{1}_{|x|&gt;2}\\] \\[\\lesssim\\int_{\\sqrt{t-1}}^{\\sqrt{t-1}}\\frac{(1+\\frac{1}{u})^{\\epsilon }}{(1-t+u^{2})^{1-a}}du\\mathbbm{1}_{|x|&lt;2}+\\frac{\\text{LN}^{*}}{|x|^{n-1}} \\mathbbm{1}_{|x|&gt;2}\\] \\[\\lesssim\\int_{\\sqrt{t-1}}^{\\sqrt{t-1}}\\frac{1}{u^{\\epsilon}(1-t+u^{ 2})^{1-a}}du\\mathbbm{1}_{|x|&lt;2}+\\frac{\\text{LN}^{*}}{|x|^{n-1}}\\mathbbm{1}_{|x|&gt;2}\\] \\[\\lesssim\\frac{1}{(t-1)^{\\frac{1}{2}}}\\int_{0}^{\\frac{1}{2}}\\frac{dv }{v^{1-a}(t-1+v)^{\\frac{1}{2}}}\\mathbbm{1}_{|x|&lt;2}+\\frac{\\text{LN}^{*}}{|x|^{n- 1}}\\mathbbm{1}_{| "
    ],
    "kosmos": [
      "The first integral of (5.21) can be estimated as follows: for any \\(\\epsilon&gt;0\\),\n\n\\[\\int_{\\frac{1}{4}}^{1}\\int_{|y^{\\prime}|\\leq 1}(t-s)^{\\frac{1}{2}} ( |x-y^{\\prime}|^{2}+(t-s))^{\\frac{n-1}{2}}\\left|\\partial_{s}g_{k}^{T}(s)\\right| dy^{\\prime}ds\\] \\[\\lesssim\\int_{\\frac{1}{4}}^{1}\\int_{|x|+1+\\sqrt{t-s}}^{1}\\log\\left( 2+\\frac{1}{\\sqrt{t-s}}\\right)ds\\] \\[\\lesssim\\frac{1}{\\langle x\\rangle^{n-1}}\\int_{\\sqrt{t-1}}^{\\sqrt{t -1}}\\log\\left(2+\\frac{1}{u}\\right)du\\lesssim\\frac{1}{\\langle x\\rangle^{n-1}}\\int _{\\sqrt{t-1}}^{\\sqrt{t-1}}\\frac{1}{v^{\\epsilon}}dv\\] \\[\\lesssim\\frac{(t-1)^{\\frac{1}{2}}}{\\langle x\\rangle^{n-1}}.\\]\n\nThe second integral of (5.21) can be estimated as follows:\n\n\\[\\int_{\\frac{1}{4}}^{1}\\int_{|y^{\\prime}|\\leq 1}(t-s)^{\\frac{1}{2}}( |x-y^{\\prime}|^{2}+(t-s))^{\\frac{n-1}{2}}\\left|\\partial_{s}g_{k}^{T}(s)\\right| dy^{\\prime}ds\\] \\[\\lesssim\\int_{\\frac{1}{4}}^{1}\\int_{|y^{\\prime}|\\leq 1}(t-s)^{\\frac {1}{2}}( |x-y^{\\prime}|^{2}+(t-s))^{\\frac{n-1}{2}}(1-s)^{1-a}dy^{\\prime}ds\\] \\[\\lesssim\\int_{\\frac{1}{4}}^{1}\\frac{1}{(t-s)^{\\frac{1}{2}}( |x|+\\sqrt{t-s}+1)^{n-1}}\\frac{1}{(1-s)^{1-a}}ds\\mathbbm{1}_{|x|&lt;2}+\\int_{\\frac{1 }{4}}^{1}\\frac{1}{(t-s)^{\\frac{1}{2}}(1-s)^{1-a}}ds\\frac{1}{|x|^{n-1}}\\mathbbm{ 1}_{|x|&gt;2}\\] \\[\\lesssim\\int_{\\sqrt{t-1}}^{\\sqrt{t-1}}\\frac{1}{(1-t+u^{2})^{1-a}} du\\mathbbm{1}_{|x|&lt;2}+\\frac{\\text{LN}^{*}}{|x|^{n-1}}\\mathbbm{1}_{|x|&gt;2}\\] \\[\\lesssim\\int_{\\sqrt{t-1}}^{\\sqrt{t-1}}\\frac{(1+\\frac{1}{u})^{\\epsilon }}{(1-t+u^{2})^{1-a}}du\\mathbbm{1}_{|x|&lt;2}+\\frac{\\text{LN}^{*}}{|x|^{n-1}} \\mathbbm{1}_{|x|&gt;2}\\] \\[\\lesssim\\int_{\\sqrt{t-1}}^{\\sqrt{t-1}}\\frac{1}{u^{\\epsilon}(1-t+u^{ 2})^{1-a}}du\\mathbbm{1}_{|x|&lt;2}+\\frac{\\text{LN}^{*}}{|x|^{n-1}}\\mathbbm{1}_{|x|&gt;2}\\] \\[\\lesssim\\frac{1}{(t-1)^{\\frac{1}{2}}}\\int_{0}^{\\frac{1}{2}}\\frac{dv }{v^{1-a}(t-1+v)^{\\frac{1}{2}}}\\mathbbm{1}_{|x|&lt;2}+\\frac{\\text{LN}^{*}}{|x|^{n- 1}}\\mathbbm{1}_{| "
    ]
  },
  {
    "edit": [
      "corresponding statistical estimators. In this case, we also use the LP / B03 / ExD model from the spec- z as reference to check the impact of the GaZNet redshift in terms of accuracy and scatter. Basically, the results show that, for the same correlations seen in Fig. 3 , the relative bias of the di ff erent configurations is not worsened, meaning that the accuracy of the mass estimates is not a ff ected by the use of the morphoto- z. This is eventually a consequence of the good accuracy of these latter as seen in Fig. 2 . On the other hand, we register an evident increase of the NMAD as a consequence of the morphoto- z intrinsic statistical errors and outlier fractions, which is also mirrored by the scatter of the residual, at the bottom of the 1-to-1 relations, which is now of the order of 0.23 dex, for log \\(M_{*}/M_{\\odot}&gt;9\\), and 0.49 dex for log \\(M_{*}/M_{\\odot}&lt;9\\), on average. These large scatter at low stellar masses are mainly caused by the trend we see that below log \\(M_{*}/M_{\\odot}=8.5\\), where stellar masses are systematically overestimated compared to those obtained with the spec- z. This is not an e ff ect that comes from the particular set-up of the fitting procedure, as shown by the comparison of the LP / B03 / ExD / morphoto- z against the same set-up with spec- z (bottom / left plot in Fig. 4 ). Even in this latter case, we see that below log \\(M_{*}/M_{\\odot}=8.5\\) the positive bias is similar to the ones of all other configurations. We track the motivation of this systematics to some bias of the GaZNet redshifts for a group of objects at very low redshifts (z \\(&lt;0.05\\) see Fig. 2 ), which turn-out to have also low masses. This can be due to some residual contamination from stars, not picked in the spectra classification, or just a failure of the GaZNet predictions at very low- z, which clearly impact the mass predictions. We will come back to this on Sect. 4 . However, still looking at the LP / B03 / ExD morphoto- z vs. spec- z, above log \\(M_{*}/M_{\\odot}=8.5\\), the bias is almost absent and the only relevant ef ff ect is the GaZNet redshift scatter that, from the NMAD, is quantified in 0.09. This is confirmed by noticing that the general increase of the NMAD from the spectroscopic sample to the morphoto-metric sample, in Table 2 , is compatible with the sum in quadrature of the NMAD of the former with 0.09 coming from the latter, consistently with some pseudo-Gaussian distributions. This is consistent with a log-normal distribution of the uncertainties of the stellar masses, which are confirmed by the outlier fractions that are all of the order of 5-6% above 2\\(\\sigma\\) of the\n\nFigure 4: Stellar mass estimates as for Fig. 3 but using the GaZNet morphoto-metric redshifts.\n\n "
    ],
    "kosmos": [
      "corresponding statistical estimators. In this case, we also use the LP/B03/ExD model from the spec- z as reference to check the impact of the GaZNet redshift in terms of accuracy and scatter. Basically, the results show that, for the same correlations seen in Fig. 3, the relative bias of the different configurations is not worsened, meaning that the accuracy of the mass estimates is not affected by the use of the morphoto- z. This is eventually a consequence of the good accuracy of these latter as seen in Fig. 2. On the other hand, we register an evident increase of the NMAD as a consequence of the morphoto- z intrinsic statistical errors and outlier fractions, which is also mirrored by the scatter of the residual, at the bottom of the 1-to-1 relations, which is now of the order of 0.23 dex, for log M \u2217 / M \u2299 > 9, and 0.49 dex for log M \u2217 / M \u2299 < 9, on average. These large scatter at low stellar masses are mainly caused by the trend we see that below log M \u2217 / M \u2299 = 8.5, where stellar masses are systematically overestimated compared to those obtained with the spec- z. This is not an effect that comes from the particular set-up of the fitting procedure, as shown by the comparison of the LP/B03/ExD/morphoto- z against the same set-up with spec- z (bottom/left plot in Fig. 4). Even in this latter case, we see that below log M \u2217 / M \u2299 = 8.5 the positive bias is similar to the ones of all other configurations. We track the motivation of this systematics to some bias of the GaZNet redshifts for a group of objects at very low redshifts (z < 0.05 see Fig. 2), which turn-out to have also low masses. This can be due to some residual contamination from stars, not picked in the spectra classification, or just a failure of the GaZNet predictions at very low- z, which clearly impact the mass predictions. We will come back to this on Sect. 4. However, still looking at the LP/B03/ExD/morphoto- z vs. spec- z, above log M \u2217 / M \u2299 = 8.5, the bias is almost absent and the only relevant effect is the GaZNet redshift scatter that, from the NMAD, is quantified in 0.09. This is confirmed by noticing that the general increase of the NMAD from the spectroscopic sample to the morphoto-metric sample, in Table 2, is compatible with the sum in quadrature of the NMAD of the former with 0.09 coming from the latter, consistently with some pseudo-Gaussian distributions. This is consistent with a log-normal distribution of the uncertainties of the stellar masses, which are confirmed by the outlier fractions that are all of the order of 5-6% above 2- of the\n\nFigure 4: Stellar mass estimates as for Fig. 3 but using the GaZNet morphoto-metric redshifts.\n\n "
    ]
  },
  {
    "edit": [
      "\n\n# Improving Generalization in Visual Reinforcement Learning via\n\nConflict-aware Gradient Agreement Augmentation\n\nSiao Liu Zhaoyu Chen Yang Liu Yuzheng Wang Dingkang Yang Zhile Zhao\n\nZiqing Zhou Xie Yi Wei Li Wenqiang Zhang Zhongxue Gan\n\nAcademy for Engineering & Technology, Fudan University\n\n{saliu20, yzwang20, dkyang20, fd liwei, wqzhang, ganzhongxue}@fudan.edu.cn\n\n{zhilezhao21, ziqingzhou21, yixie22}@m.fudan.edu.cn\n\n###### Abstract\n\nLearning a policy with great generalization to unseen environments remains challenging but critical in visual re- inforcement learning. Despite the success of augmenta- tion combination in the supervised learning generalization, naively applying it to visual RL algorithms may damage the training efficiency, suffering from serve performance degradation. In this paper, we first conduct qualitative analysis and illustrate the main causes: (i) high-variance gradient magnitudes and (ii) gradient conflicts existed in various augmentation methods. To alleviate these issues, we propose a general policy gradient optimization frame- work, named Conflict-aware Gradient Agreement Augmen- tation (CG2A), and better integrate augmentation combina- tion into visual RL algorithms to address the generalization bias. In particular, CG2A develops a Gradient Agreement Solver to adaptively balance the varying gradient magni- tudes, and introduces a Soft Gradient Surgery strategy to al- leviate the gradient conflicts. Extensive experiments demon- strate that CG2A significantly improves the generalization performance and sample efficiency of visual RL algorithms. [style=unboxed, font-family=Times New Roman, serif, font-size=10pt]\n\n## 1 Introduction\n\nWith the development of deep learning in various tasks [28, 26, 25, 27, 7, 6, 38, 40, 39, 24], visual Reinforcement Learning (RL) has achieved impressive success in various fields such as robotic control [11], autonomous driving [17], and game-playing [35]. Previous works usually formulate it as a Partially Observable Markov Decision Process (POMDP) [33], and the agent receives highdimensional image observations as inputs. As depicted in [15, 14], visual RL generalization refers to the ability of a pretrained RL agent to perform well in unseen environments. Due to the dynamic nature of the real world, even minor perturbations in the environment can result in significant semantic shifts in the visual observations, which makes visual RL generalization challenging. To improve generalization performance, data augmentation [29] is a widely adopted technique in reinforcement learning. Numerous studies [22, 13] utilize data augmentation methods to generate synthetic data and diversify the training environments, yielding considerable performance improvements. However, recent methods [14, 3, 44] mostly select a single augmentation technique to improve the generalization capability, resulting in a poor performance in the environments with observations varying far from the augmented images. For instance, ColorJitter [23] is the preferred choice for addressing color variations, but agents trained with such augmentation still hard to cope with intricate texture patterns. In other words, the generalization ability heavily relies on the selection of specific data augmentation technique, which is so-called generalization bias. Compared to single data augmentation, Augmentation Combination (AC) [16] integrates multiple data augmentation methods to enhance the diversity of augmentations and alleviate the generalization bias, which is a more promising pre-processing solution. Unfortunately, there is a dilemma in incorporating AC into visual RL. Although data augmentation combination can effectively improve generalization capability in the supervised visual tasks, RL algorithms are quite sensitive to excessive variations, resulting in performance degradation and training sample inefficiency. Therefore, it is necessary to rethink why visual RL algorithms cannot benefit from AC as much as supervised learning. From the perspective of gradient optimization, we conduct numerous qualitative analysis to illustrate the causes of performance degradation and training collapse that occur when employing augmentation combinations during train-"
    ],
    "kosmos": [
      "\n\n# Improving Generalization in Visual Reinforcement Learning via\n\nConflict-aware Gradient Agreement Augmentation\n\nSiao Liu Zhaoyu Chen Yang Liu Yuzheng Wang Dingkang Yang Zhile Zhao\n\nZiqing Zhou Xie Yi Wei Li Wenqiang Zhang Zhongxue Gan\n\nAcademy for Engineering &amp; Technology, Fudan University\n\n{saliu20, zhaoyuchen20, yang.liu20, yzwang20, dkyang20, fd liwei, wqzhang, ganzhongxue}@fudan.edu.cn\n\n{zhilezhao21, ziqingzhou21, yixie22}@m.fudan.edu.cn\n\n###### Abstract\n\nLearning a policy with great generalization to unseen environments remains challenging but critical in visual reinforcement learning. Despite the success of augmentation combination in the supervised learning generalization, naively applying it to visual RL algorithms may damage the training efficiency, suffering from serve performance degradation. In this paper, we first conduct qualitative analysis and illustrate the main causes: (i) high-variance gradient magnitudes and (ii) gradient conflicts existed in various augmentation methods. To alleviate these issues, we propose a general policy gradient optimization framework, named Conflict-aware Gradient Agreement Augmentation (CG2A), and better integrate augmentation combination into visual RL algorithms to address the generalization bias. In particular, CG2A develops a Gradient Agreement Solver to adaptively balance the varying gradient magnitudes, and introduces a Soft Gradient Surgery strategy to alleviate the gradient conflicts. Extensive experiments demonstrate that CG2A significantly improves the generalization performance and sample efficiency of visual RL algorithms.\n\n## 1 Introduction\n\nWith the development of deep learning in various tasks [28, 26, 25, 27, 7, 6, 38, 40, 39, 24], visual Reinforcement Learning (RL) has achieved impressive success in various fields such as robotic control [11], autonomous driving [17], and game-playing [35]. Previous works usually formulate it as a Partially Observable Markov Decision Process (POMDP) [33], and the agent receives high-dimensional image observations as inputs. As depicted in [15, 14], visual RL generalization refers to the ability of a pretrained RL agent to perform well in unseen environments. Due to the dynamic nature of the real world, even minor perturbations in the environment can result in significant semantic shifts in the visual observations, which makes visual RL generalization challenging.\n\nTo improve generalization performance, data augmentation [29] is a widely adopted technique in reinforcement learning. Numerous studies [22, 13] utilize data augmentation methods to generate synthetic data and diversify the training environments, yielding considerable performance improvements. However, recent methods [14, 3, 44] mostly select a single augmentation technique to improve the generalization capability, resulting in a poor performance in the environments with observations varying far from the augmented images. For instance, ColorJitter [23] is the preferred choice for addressing color variations, but agents trained with such augmentation still hard to cope with intricate texture patterns. In other words, the generalization ability heavily relies on the selection of specific data augmentation technique, which is so-called generalization bias.\n\nCompared to single data augmentation, Augmentation Combination (AC) [16] integrates multiple data augmentation methods to enhance the diversity of augmentations and alleviate the generalization bias, which is a more promising pre-processing solution. Unfortunately, there is a dilemma in incorporating AC into visual RL. Although data augmentation combination can effectively improve generalization capability in the supervised visual tasks, RL algorithms are quite sensitive to excessive variations, resulting in performance degradation and training sample inefficiency. Therefore, it is necessary to rethink why visual RL algorithms cannot benefit from AC as much as supervised learning.\n\nFrom the perspective of gradient optimization, we conduct numerous qualitative analysis to illustrate the causes of performance degradation and training collapse that occur when employing augmentation combinations during train-"
    ]
  },
  {
    "edit": [
      "\n\n**Lemma 2.9**.: _Let \\(x=\\Delta^{k}\\) for some nonzero integer \\(k\\). We have \\(\\operatorname{SSS}(\\Delta^{k})=\\{\\Delta^{k}\\}\\). The centralizer of \\(\\Delta^{k}\\) in \\(G(m,\\ell)\\) is either \\(G(m,\\ell)\\) if \\(k\\ell\\) is a multiple of \\(m\\), or cyclic and generated by \\(\\Delta\\) otherwise._\n\nProof.: We have that \\(y\\in G(m,\\ell)\\) lies in \\(\\operatorname{SSS}(\\Delta^{k})\\) only if \\(\\inf(y)=k=\\sup(y)\\). The only element satisfying this is \\(\\Delta^{k}\\), which is conjugate to itself. Thus we have \\(\\operatorname{SSS}(\\Delta^{k})=\\{\\Delta^{k}\\}\\). Now, let \\(s(j,q)\\) be a simple element in \\(M(m,\\ell)\\). Since both \\(1\\) and \\(\\Delta\\) conjugate \\(\\Delta^{k}\\) to itself, we can assume that \\(q\\in\\llbracket 1,m-1\\rrbracket\\). We have\n\n\\[s(j,q)^{-1}\\Delta^{k}s(j,q) =s(j,q)\\Delta^{k-1}s(j,q)\\] \\[=s(j+q,\\ell-q)\\Delta^{k-1}s(j,q)\\] \\[=\\Delta^{k-1}s(j+q+(k-1)\\ell,\\ell-q)s(j,q).\\]\n\nIn order for this element to lie in \\(\\operatorname{SSS}(\\Delta^{k})\\), the word \\(s(j+q+(k-1)\\ell,\\ell-q)s(j,q)\\) must not be greedy. This is equivalent to \\(j+k\\ell\\equiv j[m]\\). If \\(k\\ell\\) is a multiple of \\(m\\), this is true for all \\(j\\in\\llbracket 0,m-1\\rrbracket\\), and we obtain \\(s(j,q)^{-1}\\Delta^{k}s(j,q)=\\Delta^{k}\\): the arrows from \\(\\Delta^{k}\\) to itself in \\(\\operatorname{CG}(\\Delta^{k})\\) are given by all the simple elements. Otherwiser, \\(j+k\\ell\\equiv j[m]\\) is never true for \\(j\\in\\llbracket 1,m-1\\rrbracket\\) and the only arrows from \\(\\Delta^{k}\\) to itself in \\(\\operatorname{CG}(\\Delta^{k})\\) are given by \\(1\\) and \\(\\Delta\\).\n\n**Lemma 2.10**.: _Let \\(x=\\Delta^{k}s(i,p)\\) be a periodic element in \\(M(m,\\ell)\\) with \\(p\\in\\llbracket 1,m-1\\rrbracket\\). We have \\(\\operatorname{SSS}(x)=\\{\\Delta^{k}s(n,p)\\mid n\\in\\llbracket 0,m-1\\rrbracket\\}\\). The centralizer of \\(\\Delta^{k}s(0,p)\\) in \\(G(m,\\ell)\\) is cyclic and generated by \\(s(p,m)\\)._\n\nProof.: The assumption that \\(x\\) is periodic is equivalent to \\(k\\ell+p\\equiv 0[m]\\) by Lemma 2.8. Let \\(s(j,q)\\) be a simple element in \\(M(m,\\ell)\\). We have\n\n\\[x^{s(j,q)} =s(j,q)^{-1}\\Delta^{k}s(i,p)s(j,q)\\] \\[=\\Delta^{k-1}s(j+q+(k-1)\\ell,\\ell-q)s(i,p)s(j,q).\\]\n\nAgain, in order for this to lie in SSS( x ), we must have either \\(j+k\\ell\\equiv i[m]\\) or \\(i+p\\equiv j[m]\\). Since \\(k\\ell+p\\equiv 0[m]\\), those two assertions are equivalent. If thery are satisfied, then we have\n\n\\[x^{s(j,q)} =\\Delta^{k}s(j+q-p,p)=\\Delta^{k}s(i+q,p). \\tag{17}\\]\n\nIn particular, \\(s(p,n)\\) gives a conjugating element from \u2206^{k}s(0,p)\\) to \u2206^{k}s(n,p)\\) for \\(n\\in\\llbracket 0,m-1\\rrbracket\\). Moreover, for \u2206^{k}s(n,p)\\in\\operatorname{SSS}(x)\\), the simples \\(s\\) such that \\((\\Delta^{k}s(n,p))^{s}\\in\\operatorname{SSS}(x)\\) are all divisible"
    ],
    "kosmos": [
      "\n\n**Lemma 2.9**.: _Let \\(x=\\Delta^{k}\\) for some nonzero integer \\(k\\). We have \\(\\operatorname{SSS}(\\Delta^{k})=\\{\\Delta^{k}\\}\\). The centralizer of \\(\\Delta^{k}\\) in \\(G(m,\\ell)\\) is either \\(G(m,\\ell)\\) if \\(k\\ell\\) is a multiple of \\(m\\), or cyclic and generated by \\(\\Delta\\) otherwise._\n\nProof.: We have that \\(y\\in G(m,\\ell)\\) lies in \\(\\operatorname{SSS}(\\Delta^{k})\\) only if \\(\\inf(y)=k=\\sup(y)\\). The only element satisfying this is \\(\\Delta^{k}\\), which is conjugate to itself. Thus we have \\(\\operatorname{SSS}(\\Delta^{k})=\\{\\Delta^{k}\\}\\). Now, let \\(s(j,q)\\) be a simple element in \\(M(m,\\ell)\\). Since both \\(1\\) and \\(\\Delta\\) conjugate \\(\\Delta^{k}\\) to itself, we can assume that \\(q\\in\\llbracket 1,m-1\\rrbracket\\). We have\n\n\\[s(j,q)^{-1}\\Delta^{k}s(j,q) =s(j,q)\\Delta^{k-1}s(j,q)\\] \\[=s(j+q,\\ell-q)\\Delta^{k-1}s(j,q)\\] \\[=\\Delta^{k-1}s(j+q+(k-1)\\ell,\\ell-q)s(j,q).\\]\n\nIn order for this element to lie in \\(\\operatorname{SSS}(\\Delta^{k})\\), the word \\(s(j+q+(k-1)\\ell,\\ell-q)s(j,q)\\) must not be greedy. This is equivalent to \\(j+k\\ell\\equiv j[m]\\). If \\(k\\ell\\) is a multiple of \\(m\\), this is true for all \\(j\\in\\llbracket 0,m-1\\rrbracket\\), and we obtain \\(s(j,q)^{-1}\\Delta^{k}s(j,q)=\\Delta^{k}\\): the arrows from \\(\\Delta^{k}\\) to itself in \\(\\operatorname{CG}(\\Delta^{k})\\) are given by all the simple elements. Otherwise, \\(j+k\\ell\\equiv j[m]\\) is never true for \\(j\\in\\llbracket 1,m-1\\rrbracket\\) and the only arrows from \\(\\Delta^{k}\\) to itself in \\(\\operatorname{CG}(\\Delta^{k})\\) are given by \\(1\\) and \\(\\Delta\\).\n\n**Lemma 2.10**.: _Let \\(x=\\Delta^{k}s(i,p)\\) be a periodic element in \\(M(m,\\ell)\\) with \\(p\\in\\llbracket 1,m-1\\rrbracket\\). We have \\(\\operatorname{SSS}(x)=\\{\\Delta^{k}s(n,p)\\mid n\\in\\llbracket 0,m-1\\rrbracket\\}\\). The centralizer of \\(\\Delta^{k}s(0,p)\\) in \\(G(m,\\ell)\\) is cyclic and generated by \\(s(p,m)\\)._\n\nProof.: The assumption that \\(x\\) is periodic is equivalent to \\(k\\ell+p\\equiv 0[m]\\) by Lemma 2.8. Let \\(s(j,q)\\) be a simple element in \\(M(m,\\ell)\\). We have\n\n\\[x^{s(j,q)} =s(j,q)^{-1}\\Delta^{k}s(i,p)s(j,q)\\] \\[=\\Delta^{k-1}s(j+q+(k-1)\\ell,\\ell-q)s(i,p)s(j,q).\\]\n\nAgain, in order for this to lie in \\(\\operatorname{SSS}(x)\\), we must have either \\(j+k\\ell\\equiv i[m]\\) or \\(i+p\\equiv j[m]\\). Since \\(k\\ell+p\\equiv 0[m]\\), those two assertions are equivalent. If they are satisfied, then we have\n\n\\[x^{s(j,q)} =\\Delta^{k}s(j+q-p,p)=\\Delta^{k}s(i+q,p). \\tag{17}\\]\n\nIn particular, \\(s(p,n)\\) gives a conjugating element from \\(\\Delta^{k}s(0,p)\\) to \\(\\Delta^{k}s(n,p)\\) for \\(n\\in\\llbracket 0,m-1\\rrbracket\\). Moreover, for \\(\\Delta^{k}s(n,p)\\in\\operatorname{SSS}(x)\\), the simples \\(s\\) such that \\((\\Delta^{k}s(n,p))^{s}\\in\\operatorname{SSS}("
    ]
  },
  {
    "edit": [
      "ture during a significant decrease in disk contribution, as well as the study of timing characteristics in that state.\n\n## 6 Acknowledgements\n\nWe would like to thank the anonymous referee for their valuable suggestions, which improved the quality of this work. This work has utilized data from *AstroSat* mission which is archived at Indian Space Science Data Centre (ISSDC). We are grateful to the SXT and LAXPC POC teams for providing the data and requisite softwares to perform data analysis. NH acknowledges the financial support provided by Department of Science and Technology (DST) under the INSPIRE fellowship scheme. AG, RM and SS acknowledges the financial support provided by Department of Space, Govt of India (No.DS\\_2B-13012(2)/2/2022-Sec.2).\n\n## 7 Data Availability\n\nThe data used in the publication is publicly available for download at [https://astrobrowse.issdc.gov.in/astro_archive/archive/Home.jsp](https://astrobrowse.issdc.gov.in/astro_archive/archive/Home.jsp) using the observation IDs mentioned in Tab 1.\n\n## References\n\n* P. Agrawal, et al. (2017)The astrophysical implications of the 2016-2017 _AstroSat_ mission. Journal of Astrophysics and Astronomy38 (1), pp. 30. External Links: Document, 1701.00001 Cited by: SS1.\n* H. Antia, et al. (2017)The astrophysical implications of the 2016-2017 _AstroSat_ mission. The Astrophysical Journal Supplement Series231 (1), pp. 10. External Links: Document, 1701.00001 Cited by: SS1.\n* M. Asplund, et al. (2009)The astrophysical implications of the 2009 _AstroSat_ mission. Annual review of astronomy and astrophysics47 (1), pp. 481-501. External Links: Document, 0901.0001 Cited by: SS1.\n* T. M. Belloni, et al. (2011)The astrophysical implications of the 2011 _AstroSat_ mission. arXiv preprint arXiv:1109.3388. Cited by: SS1.\n* Y. Bhargava, et al. (2022)The astrophysical implications of the 2022 _AstroSat_ mission. Monthly Notices of the Royal Astronomical Society512 (3), pp. 6067-6080. External Links: Document, 2207.00001 Cited by: SS1.\n* F. Capitanio, et al. (2009)The astrophysical implications of the 2009 _AstroSat_ mission. Monthly Notices of the Royal Astronomical Society398 (2), pp. 1194-1211. External Links: Document, 0901.0001 Cited by: SS1.\n* C. Chevalier and S. Ilovaisky (1992)The astrophysical implications of the 1992 _AstroSat_ mission. International Astronomical Union Circular5520 (1), pp. 1-12. External Links: Document, astro-ph/9208025 Cited by: SS1.\n* R. Connors, et al. (2021)The astrophysical implications of the 2021 _AstroSat_ mission. The Astronomer's Telegram14725 (1), pp. 1-1. External Links: Document, 2107.00001 Cited by: SS1.\n* T. Dauser, et al. (2014)The astrophysical implications of the 2014 _AstroSat_ mission. Monthly Notices of the Royal Astronomical Society444 (1), pp. L100-L105. External Links: Document, 1406.0001 Cited by: SS1.\n* S. W. Davis, et al. (2005)The astrophysical implications of the 2005 _AstroSat_ mission. The Astrophysical Journal621 (2), pp. 372-385. External Links: Document, astro-ph/0505430 Cited by: SS1.\n* C. Done, et al. (2007)The astrophysical implications of the 2007 _AstroSat_ mission. The Astronomy and Astrophysics Review15 (1), pp. 1-12. External Links: Document, astro-ph/0702266 Cited by: SS1.\n* P. A. Draghis, et al. (2022)The astrophysical implications of the 2022 _AstroSat_ mission. arXiv preprint arXiv:2210.02479. Cited by: SS1.\n* K. Ebisawa, et al. (1993)The astrophysical implications of the 1993 _AstroSat "
    ],
    "kosmos": [
      "ture during a significant decrease in disk contribution, as well as the study of timing characteristics in that state.\n\n## 6 Acknowledgements\n\nWe would like to thank the anonymous referee for their valuable suggestions, which improved the quality of this work. This work has utilized data from *AstroSat* mission which is archived at Indian Space Science Data Centre (ISSDC). We are grateful to the SXT and LAXPC POC teams for providing the data and requisite softwares to perform data analysis. NH acknowledges the financial support provided by Department of Science and Technology (DST) under the INSPIRE fellowship scheme. AG, RM and SS acknowledges the financial support provided by Department of Space, Govt of India (No.DS\\_2B-13012(2)/2/2022-Sec.2).\n\n## 7 Data Availability\n\nThe data used in the publication is publicly available for download at [https://astrobrowse.issdc.gov.in/astro_archive/archive/Home.jsp](https://astrobrowse.issdc.gov.in/astro_archive/archive/Home.jsp) using the observation IDs mentioned in Tab 1.\n\n## References\n\n* P. Agrawal, et al. (2017)The astrophysical implications of the 2016-2017 _AstroSat_ mission. Journal of Astrophysics and Astronomy38 (1), pp. 30. External Links: Document, 1701.00001 Cited by: SS1.\n* H. Antia, et al. (2017)The astrophysical implications of the 2016-2017 _AstroSat_ mission. The Astrophysical Journal Supplement Series231 (1), pp. 10. External Links: Document, 1701.00001 Cited by: SS1.\n* M. Asplund, et al. (2009)The astrophysical implications of the 2009 _AstroSat_ mission. Annual review of astronomy and astrophysics47 (1), pp. 481-501. External Links: Document, 0901.0001 Cited by: SS1.\n* T. M. Belloni, et al. (2011)The astrophysical implications of the 2011 _AstroSat_ mission. arXiv preprint arXiv:1109.3388. Cited by: SS1.\n* Y. Bhargava, et al. (2022)The astrophysical implications of the 2022 _AstroSat_ mission. Monthly Notices of the Royal Astronomical Society512 (3), pp. 6067-6080. External Links: Document, 2207.00001 Cited by: SS1.\n* F. Capitanio, et al. (2009)The astrophysical implications of the 2009 _AstroSat_ mission. Monthly Notices of the Royal Astronomical Society398 (2), pp. 1194-1211. External Links: Document, 0901.0001 Cited by: SS1.\n* C. Chevalier and S. Ilovaisky (1992)The astrophysical implications of the 1992 _AstroSat_ mission. International Astronomical Union Circular5520 (1), pp. 1-12. External Links: Document, astro-ph/9208025 Cited by: SS1.\n* R. Connors, et al. (2021)The astrophysical implications of the 2021 _AstroSat_ mission. The Astronomer's Telegram14725 (1), pp. 1-1. External Links: Document, 2107.00001 Cited by: SS1.\n* T. Dauser, et al. (2014)The astrophysical implications of the 2014 _AstroSat_ mission. Monthly Notices of the Royal Astronomical Society444 (1), pp. L100-L105. External Links: Document, 1406.0001 Cited by: SS1.\n* S. W. Davis, et al. (2005)The astrophysical implications of the 2005 _AstroSat_ mission. The Astrophysical Journal621 (2), pp. 372-385. External Links: Document, astro-ph/0505430 Cited by: SS1.\n* C. Done, et al. (2007)The astrophysical implications of the 2007 _AstroSat_ mission. The Astronomy and Astrophysics Review15 (1), pp. 1-12. External Links: Document, astro-ph/0702266 Cited by: SS1.\n* P. A. Draghis, et al. (2022)The astrophysical implications of the 2022 _AstroSat_ mission. arXiv preprint arXiv:2210.02479. Cited by: SS1.\n* K. Ebisawa, et al. (1993)The astrophysical implications of the 1993 _AstroSat "
    ]
  },
  {
    "edit": [
      "the presynaptic and postsynaptic neuron activities have low correlation their connection are likely to be removed. The latter process is called synaptic pruning and it is considered essential for optimizing activity propagation and memory capacity(Chklovskii, Mel, and Svoboda, 2004; Knoblauch *et al.*, 2014; Knoblauch and Sommer, 2016). Furthermore, it is commonly believed that synaptic pruning and rewiring dysfunction are one of the neural correlate of developmental disorders such as autism or schizophrenia (Bourgeron, 2009; Moyer, Shelton, and Sweet, 2015), leading to, respectively, an higher or lower synaptic density with respect to neurotypical subjects(Hutsler and Zhang, 2010; Pagani *et al.*, 2021; Glantz and Lewis, 2000). In the last decades computational neuroscience has investigated brain dynamics at different scales, from cellular (Markram *et al.*, 2015) to mesoscopic and macroscopic through mean-field approaches (Wilson and Cowan, 1972; Amit and Brunel, 1997; Hopfield, 1984; Renart, Brunel, and Wang, 2004; Leon *et al.*, 2013; di Santo *et al.*, 2018; Capone *et al.*, 2019; Carlu *et al.*, 2020). Regarding synaptic plasticity, computational models were mostly focused on plasticity mechanisms that involve strengthening or weakening of existing synapses, like short-term plasticity (STP) (Tsodyks, Pawelzik, and Markram, 1998) or spike timing-dependent plasticity (STDP) (G\u00a8utig *et al.*, 2003) and on their role in short-term, long-term, working memory and learning (Mongillo, Barak, and Tsodyks, 2008; Tiddia *et al.*, 2022b; Song, Miller, and Abbott, 2000; qiang Bi and ming Poo, 2001; Golosio *et al.*, 2021; Capone *et al.*, 2022). Only in recent times computational models of structural plasticity and connectivity rearrangements during learning were developed, showing intriguing results. Knoblauch *et al.* (2014) and Knoblauch and Sommer (2016) describe a model of structural plasticity based on \u201deffectual connectivity\u201d, defined in these works as the fraction of synapses able to represent a memory stored in a network. By structural plasticity, effectual connectivity is improved, since synapses that do not code for the memory are moved in order to optimize network\u2019s connectivity. Their model defines synapses using a Markov model of three states: potential (i.e. not instantiated), instantiated but silent or instantiated and consolidated. Structural plasticity is thus related to the passage of the synapses from a potential state to an instantiated state (and vice versa), whereas changes only related to the synaptic weight are described by the consolidation of the instantiated synapses. With such a model, it is possible to show that networks with structural plasticity have higher or comparable memory capacity to networks with dense connectivity and it is possible to explain some cognitive mechanism such as the spacing effect (Knoblauch *et al.*, 2014). Spiess *et al.* (2016) simulated a spiking neural network with structural plasticity and STDP, showing that structural plasticity reduces the amount of noise of the network after a learning process, thus making the network able to have a clearer output. Furthermore, such a network with structural plasticity shows higher learning speed than the same network with only STDP implemented. Some new insights about the importance of synaptic pruning are also shown in Navlakha, Barth, and Bar-Joseph (2015), in which different pruning rates were studied suggesting that a slowly decreasing rate of pruning over time leads to more efficient network architectures. As discussed above, the biochemical and biophysical mechanisms underlying structural plasticity are extremely complex and only partially understood to date. For this reason, rather than attempting to build a biologically detailed model, this work exploits a relatively simple phenomenological model, including both the activity-driven and the homeostatic contributions; despite the lower complexity, this model accounts for the effects of structural plasticity in terms of the consolidation of synaptic connections between neurons with a high activity correlation as well as those of pruning and rewiring the connections for which this correlation is lower. This approach is also justified by the requirement for a simple and effective computational model suitable for simulating networks with a relatively large number of neurons and connections and for representing learning processes with sizable numbers of training and validation patterns. This model will then serve as the foundation for the creation of a mean-field-based theoretical framework for learning through synaptic plasticity capable of accounting for a variety of biological network properties. This framework will be used in a training and validation procedure to characterize learning "
    ],
    "kosmos": [
      "the presynaptic and postsynaptic neuron activities have low correlation their connection are likely to be removed. The latter process is called synaptic pruning and it is considered essential for optimizing activity propagation and memory capacity(Chklovskii, Mel, and Svoboda, 2004; Knoblauch *et al.*, 2014; Knoblauch and Sommer, 2016). Furthermore, it is commonly believed that synaptic pruning and rewiring dysfunction are one of the neural correlate of developmental disorders such as autism or schizophrenia (Bourgeron, 2009; Moyer, Shelton, and Sweet, 2015), leading to, respectively, an higher or lower synaptic density with respect to neurotypical subjects(Hutsler and Zhang, 2010; Pagani *et al.*, 2021; Glantz and Lewis, 2000).\n\nIn the last decades computational neuroscience has investigated brain dynamics at different scales, from cellular (Markram *et al.*, 2015) to mesoscopic and macroscopic through mean-field approaches (Wilson and Cowan, 1972; Amit and Brunel, 1997; Hopfield, 1984; Renart, Brunel, and Wang, 2004; Leon *et al.*, 2013; di Santo *et al.*, 2018; Capone *et al.*, 2019; Carlu *et al.*, 2020). Regarding synaptic plasticity, computational models were mostly focused on plasticity mechanisms that involve strengthening or weakening of existing synapses, like short-term plasticity (STP) (Tsodyks, Pawelzik, and Markram, 1998) or spike timing-dependent plasticity (STDP) (G\u00fctig *et al.*, 2003) and on their role in short-term, long-term, working memory and learning (Mongillo, Barak, and Tsodyks, 2008; Tiddia *et al.*, 2022b; Song, Miller, and Abbott, 2000; qiang Bi and ming Poo, 2001; Golosio *et al.*, 2021; Capone *et al.*, 2022). Only in recent times computational models of structural plasticity and connectivity rearrangements during learning were developed, showing intriguing results. Knoblauch *et al.* (2014) and Knoblauch and Sommer (2016) describe a model of structural plasticity based on \"effectual connectivity\", defined in these works as the fraction of synapses able to represent a memory stored in a network. By structural plasticity, effectual connectivity is improved, since synapses that do not code for the memory are moved in order to optimize network's connectivity. Their model defines synapses using a Markov model of three states: potential (i.e. not instantiated), instantiated but silent or instantiated and consolidated. Structural plasticity is thus related to the passage of the synapses from a potential state to an instantiated state (and vice versa), whereas changes only related to the synaptic weight are described by the consolidation of the instantiated synapses. With such a model, it is possible to show that networks with structural plasticity have higher or comparable memory capacity to networks with dense connectivity and it is possible to explain some cognitive mechanism such as the spacing effect (Knoblauch *et al.*, 2014).\n\nSpiess *et al.* (2016) simulated a spiking neural network with structural plasticity and STDP, showing that structural plasticity reduces the amount of noise of the network after a learning process, thus making the network able to have a clearer output. Furthermore, such a network with structural plasticity shows higher learning speed than the same network with only STDP implemented.\n\nSome new insights about the importance of synaptic pruning are also shown in Navlakha, Barth, and Bar-Joseph (2015), in which different pruning rates were studied suggesting that a slowly decreasing rate of pruning over time leads to more efficient network architectures.\n\nAs discussed above, the biochemical and biophysical mechanisms underlying structural plasticity are extremely complex and only partially understood to date. For this reason, rather than attempting to build a biologically detailed model, this work exploits a relatively simple phenomenological model, including both the activity-driven and the homeostatic contributions; despite the lower complexity, this model accounts for the effects of structural plasticity in terms of the consolidation of synaptic connections between neurons with a high activity correlation as well as those of pruning and rewiring the connections for which this correlation is lower. This approach is also justified by the requirement for a simple and effective computational model suitable for simulating networks with a relatively large number of neurons and connections and for representing learning processes with sizable numbers of training and validation patterns. This model will then serve as the foundation for the creation of a mean-field-based theoretical framework for learning through synaptic plasticity capable of accounting for a variety of biological network properties. This framework will be used in a training and validation procedure to characterize learning "
    ]
  },
  {
    "edit": [
      "length \u03be L which is given by\n\n\\[\\xi_{L}^{2}\\equiv\\frac{1}{4\\sin^{2}(k/2)}\\left(\\frac{\\langle q^{2}\\rangle}{ \\langle\\mid q(\\vec{k})\\mid 2\\rangle}-1\\right), \\tag{11}\\]\n\nwhere q(\\(\\vec{k}\\)) is\n\n\\[q(\\vec{k})\\equiv\\frac{1}{N}\\sum_{j}s_{j}^{(1)}s_{j}^{(2)}e^{\\mid\\vec{k}\\cdot \\vec{r}_{j}\\mid}, \\tag{12}\\]\n\nwith \\(\\vec{r}_{j}\\) the position of the j-th NP, \\(\\vec{k}=(2\\pi/L,0,0)\\) and k = \u2225 \u20d7k\u2225 = 2\u03c0/L. 39\n\nErrors in the measurements of these quantities have been calculated as the mean squared deviations of the sample-to-sample fluctuations.\n\n## III Results\n\n### Phase diagram for isotropic HS-like configurations\n\nIn this section we investigate the magnetic order as a function of the volume fraction \u03a6 for frozen configurations obtained from equilibrium states of hard sphere fluids in the range "
    ],
    "kosmos": [
      "length \u03be L which is given by\n\n\\[\\xi_{L}^{2}\\equiv\\frac{1}{4\\sin^{2}(k/2)}\\left(\\frac{\\langle q^{2}\\rangle}{ \\langle\\mid q(\\vec{k})\\mid 2\\rangle}-1\\right), \\tag{11}\\]\n\nwhere q(\\(\\vec{k}\\)) is\n\n\\[q(\\vec{k})\\equiv\\frac{1}{N}\\sum_{j}s_{j}^{(1)}s_{j}^{(2)}e^{\\mid\\vec{k}\\cdot \\vec{r}_{j}\\mid}, \\tag{12}\\]\n\nwith \\(\\vec{r}_{j}\\) the position of the j-th NP, \\(\\vec{k}=(2\\pi/L,0,0)\\) and k = \u2225 \u20d7k\u2225 = 2\u03c0/L. 39\n\nErrors in the measurements of these quantities have been calculated as the mean squared deviations of the sample-to-sample fluctuations.\n\n## III Results\n\n### Phase diagram for isotropic HS-like configurations\n\nIn this section we investigate the magnetic order as a function of the volume fraction \u03a6 for frozen configurations obtained from equilibrium states of hard sphere fluids in the range 0 < \u03a6 < 0.49. (which=\"\" 1.2.=\"\" 1(a)=\"\" 1(b)=\"\" 1.9.=\"\" 2(a)=\"\" 2(b)=\"\" 2.=\"\" 2. [width=\"85mm]figure1.pdf\" [width=\"85mm]figure2.pdf\" [width=\"82mm]figure3.pdf\" [width=\"85mm]figure4.pdf\" [width=\"85mm]figure5.pdf\" [width=\"85mm]figure6.pdf\" [width=\"85mm]figure7.pdf\" [width=\"85mm]figure8.pdf\" [width=\"85mm]figure9.pdf\" [width=\"85mm]figure10.pdf\" [width=\"85mm]figure11.pdf\" [width=\"85mm]figure12.pdf\" [width=\"85mm]figure13.pdf\" [width=\"85mm]figure14.pdf\" [width=\"85mm]figure15.pdf\" [width=\"85mm]figure16.pdf\" [width=\"85mm]figure17.pdf\" [width=\"85mm]figure18.pdf\" [width=\"85mm]figure19.pdf\" [width=\"85mm]figure20.pdf\" [width=\"85mm]figure21.pdf\" [width=\"85mm]figure22.pdf\" [width=\"85mm]figure23.pdf\" [width=\"85mm]figure24.pdf\" [width=\"85mm]figure25.pdf\" [width=\"85mm]figure26.pdf\" [width=\"85mm]figure27.pdf\" [width=\"85mm]figure28.pdf\" [width=\"85mm]figure29.pdf\" [width=\"85mm]figure30.pdf\" [width=\"85mm]figure31.pdf\" [width=\"85mm]figure32.pdf\" [width=\"85mm]figure33.pdf\" [width=\"85mm]figure34.pdf\" [width=\"85mm]figure35.pdf\" [width=\"85mm]figure36.pdf\" [width=\"85mm]figure37.pdf\" [width=\"85mm]figure38.pdf\" [width=\"85mm]figure39.pdf\" [width=\"85mm]figure40.pdf\" [width=\"85mm]figure41.pdf\" [width=\"85mm]figure42.pdf\" [width=\"85mm]figure43.pdf\" [width=\"85mm]figure44.pdf\" [width=\"85mm]figure45.pdf\" [width=\"85mm]figure46.pdf\" [width=\"85mm]figure47.pdf\" [width=\"85mm]figure48.pdf\" [width=\"85mm]figure49.pdf\" [width=\"85mm]figure50.pdf\" [width=\"85mm]figure51.pdf\" [width=\"85mm]figure52.pdf\" [width=\"85mm]figure53.pdf\" [width=\"85mm]figure54.pdf\" [width=\"85mm]figure55.pdf\" [width=\"85mm]figure56.pdf\" [width=\"85mm]figure57.pdf\" [width=\"85mm]figure58.pdf\" [width=\"85mm]figure59.pdf\" [width=\"85mm]figure60.pdf\" [width=\"85mm]figure61.pdf\" [width=\"85mm]figure62.pdf\" [width=\"85mm]figure63.pdf\" [width=\"85mm]figure64.pdf\" [width=\"85mm]figure65.pdf\" [width=\"85mm]figure66.pdf\" [width=\"85mm]figure67.pdf\" [width=\"85mm]figure68.pdf\" [width=\"85mm]figure69.pdf\" [width=\"85mm]figure70.pdf\" [width=\"85mm]figure71.pdf\" [width "
    ]
  },
  {
    "edit": [
      "Let us now informally discuss the case when \u03b1 &gt; 0. For simplicity, we consider (13). The limit on the right-hand side is non-zero, which suggests that there is a residual dependence between the k(N) spins under the Gibbs measure. The reason for the non-zero limit is the fact that the distribution of P k(N) and the corresponding binomial distribution satisfy central limit theorems with different variances, the variance of P k(N) being strictly larger, which comes from the fact that the spins are positively correlated under the Gibbs measure. The distance between these normal distributions appears on the right-hand side of (13). In Theorem 3.5, we shall determine a *mixed* binomial distribution which approximates the distribution of P k(N) under \u00b5 N. In some sense, this describes the residual dependence between the spins under the Gibbs measure.\n\n_Remark 1.2_.: The exchangeability of the measure \u00b5 N has been used to investigate the Curie-Weiss model for example, in [17, Section 5.2] and [2]. In particular, an explicit representation of \u00b5 N as a mixture of Bernoulli measures (valid for each fixed N) can be found in [17, Theorem 5.6]. A general propagation of chaos principle stating that the distribution of k entries in a finite exchangeable vector of length n can be approximated by a mixture of i.i.d. distributions can found in [7].\n\nThe paper is organized as follows. Our proof relies on local limit theorems for the magnetization m N and also for the total number of positive spins P N under \u00b5 N. In some regimes those are known. We collect the corresponding results in Section 2 below. The proofs of these local limit theorems, which we have not been able to locate in the literature, are given in Section 4. The proof of Theorem 1.1 is given in Section 3, including the statement of residual dependence. Two auxiliary technical results related to calculations of the total variation distance are presented in Section 5.\n\n## 2. Local limit theorem for the magnetization\n\nDenote by N (m, v^2) a Gaussian distribution with mean m and variance v^2, so\n\n\\[N(\\m, v^2)(A)=\\int_{A}\\varphi(t;\\m, v^2)\\mathrm{d}t,\\quad A\\in B(R).\\]\n\nPut \u03b4 N : = (1-(-1)^{N})/2\\). This correction term appears below in the local limit theorems for m N, since Nm N always has the same parity as N.\n\n**Proposition 2.1**.: _Assume that h \u0338= 0 or 0 &lt; \\(\\beta&lt;1\\). Then_\n\n\\[\\mu_{N}\\left(\\sqrt{N}(m_{N}-\\m(\\beta,h))\\in\\cdot\\right)\\implies N\\left(0,v^{\\beta ,h}_{\\beta,h}\\right),\\quad N\\longrightarrow\\infty,\\]\n\n_and the following local limit theorem holds true:_\n\n\\[\\lim_{N\\longrightarrow\\infty}\\sqrt{N}\\sup_{\\ell\\in\\mathbb{Z}}\\mu_{N}\\left(\\frac{ Nm_{N}+\\delta_{N}}{2}=\\ell\\right)-\\varphi\\left(\\ell,\\frac{N\\m(\\beta,h)}{2},\\frac{Nv^ {\\beta,h}_{\\beta,h}}{4}\\right)\\Bigg{|}=0.\\] (16) "
    ],
    "kosmos": [
      "Let us now informally discuss the case when \u03b1 &gt; 0. For simplicity, we consider (13). The limit on the right-hand side is non-zero, which suggests that there is a residual dependence between the k(N) spins under the Gibbs measure. The reason for the non-zero limit is the fact that the distribution of P k(N) and the corresponding binomial distribution satisfy central limit theorems with different variances, the variance of P k(N) being strictly larger, which comes from the fact that the spins are positively correlated under the Gibbs measure. The distance between these normal distributions appears on the right-hand side of (13). In Theorem 3.5, we shall determine a *mixed* binomial distribution which approximates the distribution of P k(N) under \u00b5 N. In some sense, this describes the residual dependence between the spins under the Gibbs measure.\n\n_Remark 1.2_.: The exchangeability of the measure \u00b5 N has been used to investigate the Curie-Weiss model for example, in [17, Section 5.2] and [2]. In particular, an explicit representation of \u00b5 N as a mixture of Bernoulli measures (valid for each fixed N) can be found in [17, Theorem 5.6]. A general propagation of chaos principle stating that the distribution of k entries in a finite exchangeable vector of length n can be approximated by a mixture of i.i.d. distributions can found in [7].\n\nThe paper is organized as follows. Our proof relies on local limit theorems for the magnetization m N and also for the total number of positive spins P N under \u00b5 N. In some regimes those are known. We collect the corresponding results in Section 2 below. The proofs of these local limit theorems, which we have not been able to locate in the literature, are given in Section 4. The proof of Theorem 1.1 is given in Section 3, including the statement of residual dependence. Two auxiliary technical results related to calculations of the total variation distance are presented in Section 5.\n\n## 2. Local limit theorem for the magnetization\n\nDenote by N (m, v^2) a Gaussian distribution with mean m and variance v^2, so\n\n\\[N(\\m, v^2)(A)=\\int_{A}\\varphi(t;\\m, v^2)\\mathrm{d}t,\\quad A\\in B(R).\\]\n\nPut \u03b4 N : = (1-(-1)^{N})/2\\). This correction term appears below in the local limit theorems for m N, since Nm N always has the same parity as N.\n\n**Proposition 2.1**.: _Assume that h \u0338= 0 or 0 &lt; \\(\\beta&lt;1\\). Then_\n\n\\[\\mu_{N}\\left(\\sqrt{N}(m_{N}-\\m(\\beta,h))\\in\\cdot\\right)\\implies N\\left(0,v^{\\beta ,h}_{\\beta,h}\\right),\\quad N\\longrightarrow\\infty,\\]\n\n_and the following local limit theorem holds true:_\n\n\\[\\lim_{N\\longrightarrow\\infty}\\sqrt{N}\\sup_{\\ell\\in\\mathbb{Z}}\\mu_{N}\\left(\\frac{ Nm_{N}+\\delta_{N}}{2}=\\ell\\right)-\\varphi\\left(\\ell,\\frac{N\\m(\\beta,h)}{2},\\frac{Nv^ {\\beta,h}_{\\beta,h}}{4}\\right)\\Bigg{|}=0.\\] (16) "
    ]
  },
  {
    "edit": [
      "Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing with Non-Learnable Primitives\n\nChuntao Ding1\\({}^{*}\\) Zhichao Lu2\\({}^{\\dagger}\\) Shangguang Wang3 Ran Cheng4 Vishnu N. Boddeti5\n\n1 Beijing Jiaotong University 2 Sun Yat-sen University 3 Beijing University of Posts and Telecommunications\n\n4 Southern University of Science and Technology 5 Michigan State University. chuntaoding@163.com {luzhichaocn, ranchengcn}@gmail.com sgwang@bupt.edu.cn vishnu@msu.edu\n\n###### Abstract\n\nMulti-task learning (MTL) seeks to learn a single model to accomplish multiple tasks by leveraging shared information among the tasks. Existing MTL models, however, have been known to suffer from negative interference among tasks. Efforts to mitigate task interference have focused on either loss/gradient balancing or implicit parameter partitioning with partial overlaps among the tasks. In this paper, we propose ETR-NLP to mitigate task interference through a synergistic combination of non-learnable primitives (NLPs) and explicit task routing (). Our key idea is to employ non-learnable primitives to extract a diverse set of task-agnostic features and recombine them into a shared branch common to all tasks and explicit task-specific branches reserved for each task. The non-learnable primitives and the explicit decoupling of learnable parameters into shared and task-specific ones afford the flexibility needed for minimizing task interference. We evaluate the efficacy of ETR-NLP networks for both image-level classification and pixel-level dense prediction MTL problems. Experimental results indicate that ETR-NLP significantly outperforms state-of-the-art baselines with fewer learnable parameters and similar FLOPs across all datasets. Code is available at this URL.\n\n+\nFootnote \u2020: dagger}\\) Corresponding author\n\n+\nFootnote \u2020: dagger}\\) Corresponding author\n\n+\nFootnote \u2020: dagger}\\) Corresponding author\n\n## 1 Introduction\n\nMulti-task learning (MTL) is commonly employed to improve learning efficiency and performance of multiple tasks by using supervised signals from other related tasks [6, 33, 49]. These models have led to impressive results across numerous tasks. However, there is well-documented evidence [18, 28, 41, 53] that these models are suffering from _task interference_ [53], thereby limiting multi-task networks (MTNs) from realizing their full potential.\n\nFor instance, consider the learning progression of an MTN with a standard learnable convolutional layer in Figure 1a (blue curve). Observe that the model learns rapidly, we posit, by exploiting all the shared information between the tasks, i.e., gradients pointing in similar directions. However, the performance starts degrading on further training since the model needs to exploit dissimilar information between the tasks for further improvement, i.e., gradients point in different directions. The latter can be verified by observing the similarity (centered kernel alignment [19]), or the lack thereof, between the gradients for each pair of tasks in Figure 1b.\n\nSeveral approaches were proposed for mitigating task interference in MTNs, including loss/gradient balancing [17, 21, 22, 34, 52], parameter partitioning [2, 28, 30, 37] and architectural design [8, 18, 29]. Despite the diversity of these approaches, they share two common characteristics, (i) all parameters are learned, either for a pre-trained task or for the multiple tasks at hand, (ii) the learned parameters are either fully shared across all tasks or are shared across a partial set of tasks through implicit partitioning, i.e., with no direct control over which parameters are shared across which tasks. Both of these features limit the flexibility of existing\n\nFigure 1: (a) Learning progression of multi-task networks (MTNs) on CelebA for eight tasks. Hard-sharing models with fully learnable parameters (gray) learn rapidly and then suffer from performance degradation due to conflicting gradients from task interference. Networks with non-learnable primitives (NLPs; blue) do not suffer from task interference by design, while explicit task routing (ETR; green), and ETR with NLPs (red) do not eliminate but suffer less from task interference. (b) Gradient correlations measured via CKA [19] across all pairs of tasks for different layers of a standard MTN at the end of training. Observe the acute lack of correlation between tasks (low off-diagonal magnitude).\n\n "
    ],
    "kosmos": [
      "Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing with Non-Learnable Primitives\n\nChuntao Ding1\n\nWork done as a visiting scholar at Michigan State University.\n\nZhichao Lu2\n\nCorresponding author\n\nShangguang Wang3\n\nRan Cheng4\n\nVishnu N. Boddeti5\n\n1 Beijing Jiaotong University 2 Sun Yat-sen University 3 Beijing University of Posts and Telecommunications\n\n4 Southern University of Science and Technology 5 Michigan State University\n\nchuntaoding@163.com {luzhichaocn, ranchengcn}@gmail.com sgwang@bupt.edu.cn vishnu@msu.edu\n\n###### Abstract\n\nMulti-task learning (MTL) seeks to learn a single model to accomplish multiple tasks by leveraging shared information among the tasks. Existing MTL models, however, have been known to suffer from negative interference among tasks. Efforts to mitigate task interference have focused on either loss/gradient balancing or implicit parameter partitioning with partial overlaps among the tasks. In this paper, we propose ETR-NLP to mitigate task interference through a synergistic combination of non-learnable primitives (NLPs) and explicit task routing (). Our key idea is to employ non-learnable primitives to extract a diverse set of task-agnostic features and recombine them into a shared branch common to all tasks and explicit task-specific branches reserved for each task. The non-learnable primitives and the explicit decoupling of learnable parameters into shared and task-specific ones afford the flexibility needed for minimizing task interference. We evaluate the efficacy of ETR-NLP networks for both image-level classification and pixel-level dense prediction MTL problems. Experimental results indicate that ETR-NLP significantly outperforms state-of-the-art baselines with fewer learnable parameters and similar FLOPs across all datasets. Code is available at this URL.\n\n## 1 Introduction\n\nMulti-task learning (MTL) is commonly employed to improve learning efficiency and performance of multiple tasks by using supervised signals from other related tasks [6, 33, 49]. These models have led to impressive results across numerous tasks. However, there is well-documented evidence [18, 28, 41, 53] that these models are suffering from _task interference_ [53], thereby limiting multi-task networks (MTNs) from realizing their full potential.\n\nFor instance, consider the learning progression of an MTN with a standard learnable convolutional layer in Figure 1a (blue curve). Observe that the model learns rapidly, we posit, by exploiting all the shared information between the tasks, i.e., gradients pointing in similar directions. However, the performance starts degrading on further training since the model needs to exploit dissimilar information between the tasks for further improvement, i.e., gradients point in different directions. The latter can be verified by observing the similarity (centered kernel alignment [19]), or the lack thereof, between the gradients for each pair of tasks in Figure 1b.\n\nSeveral approaches were proposed for mitigating task interference in MTNs, including loss/gradient balancing [17, 21, 22, 34, 52], parameter partitioning [2, 28, 30, 37] and architectural design [8, 18, 29]. Despite the diversity of these approaches, they share two common characteristics, (i) all parameters are learned, either for a pre-trained task or for the multiple tasks at hand, (ii) the learned parameters are either fully shared across all tasks or are shared across a partial set of tasks through implicit partitioning, i.e., with no direct control over which parameters are shared across which tasks. Both of these features limit the flexibility of existing\n\nFigure 1: (a) Learning progression of multi-task networks (MTNs) on CelebA for eight tasks. Hard-sharing models with fully learnable parameters (gray) learn rapidly and then suffer from performance degradation due to conflicting gradients from task interference. Networks with non-learnable primitives (NLPs; blue) do not suffer from task interference by design, while explicit task routing (ETR; green), and ETR with NLPs (red) do not eliminate but suffer less from task interference. (b) Gradient correlations measured via CKA [19] across all pairs of tasks for different layers of a standard MTN at the end of training. Observe the acute lack of correlation between tasks (low off-diagonal magnitude).\n\n "
    ]
  },
  {
    "edit": [
      "\n\n## 1. Introduction\n\nLet \\(\\Gamma\\) be a discrete group acting on a complex projective space. We denote by \\(\\Gamma\\) the set of all discrete subgroups of \u0393. We say that a discrete subgroup of \u0393 is _purely parabolic_ if it is conjugate to a subgroup of the Heisenberg group. The study of purely parabolic subgroups of discrete groups is a very active area of research. In [8], we introduced the notion of _control group_ and showed that the control group is a discrete subgroup of the Heisenberg group. The control group is a discrete subgroup of the Heisenberg group that is not conjugate to a subgroup of the fundamental group of a complex projective space.\n\n"
    ],
    "kosmos": [
      "\n\n## 1. Introduction\n\nLet \\(\\Gamma\\) be a discrete group acting on a complex projective space. We denote by \\(\\Gamma\\) the set of all discrete subgroups of \u0393. We say that a discrete subgroup of \u0393 is _purely parabolic_ if it is conjugate to a subgroup of the Heisenberg group. The study of purely parabolic subgroups of discrete groups is a very active area of research. In [8], we introduced the notion of _control group_ and showed that the control group is a discrete subgroup of the Heisenberg group. The control group is a discrete subgroup of the Heisenberg group that is not conjugate to a subgroup of the fundamental group of a complex projective space.\n\n"
    ]
  },
  {
    "edit": [
      "\n\n# Topportunities at the LHC: Rare Top Decays with Light Singlets\n\nHenning Bahl\n\nhbahl@uchicago.edu\n\nSeth Koren\n\nsethk@uchicago.edu\n\nLian-Tao Wang\n\nliantaow@uchicago.edu Department of Physics and Enrico Fermi Institute, University of Chicago, 5720 South Ellis Avenue, Chicago, IL 60637 USA"
    ],
    "kosmos": [
      "\n\n# Topportunities at the LHC:\n\nRare Top Decays with Light Singlets\n\nHenning Bahl\n\nhbahl@uchicago.edu\n\nSeth Koren\n\nsethk@uchicago.edu\n\nLian-Tao Wang\n\nliantaow@uchicago.edu Department of Physics and Enrico Fermi Institute, University of Chicago, 5720 South Ellis Avenue, Chicago, IL 60637 USA"
    ]
  },
  {
    "edit": [
      "\n\n## 7.3 Computational Performance of the Optimization Proxies\n\nThis section presents numerical experiments used to assess the performance of the proposed optimization proxies (Proxies) against the optimization models (GDO) and the greedy heuristic (GH). **Optimality Gap:**\n\nTable 3 presents the optimality gaps of various approaches, including the results of Model (1) under various time constraints. In the table, the columns under \u201cGap of Model (1) \u201d denote the optimality gaps of the model under various time limits. Similarly, columns *Gap* for GH and Proxies denote optimality gaps for GH and the optimization proxies. In addition, columns *Time(s)* denote the solving times for GH and Proxies. **Table 3:** *Optimality Gap* (%) *with respect to the Total Trailer Cost*\n\n<table>\n<thead>\n<tr>\n<th>\nInstance\n</th>\n<th>\nModel (1)\n</th>\n<th>\n</th>\n<th>\n</th>\n<th>\nGH\n</th>\n<th>\n</th>\n<th>\nProxies\n</th>\n<th>\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>\n1s\n</td>\n<td>\n5s\n</td>\n<td>\n10s\n</td>\n<td>\n30s\n</td>\n<td>\n60s\n</td>\n<td>\n1800s\n</td>\n<td>\nGap\n</td>\n<td>\nTime (s)\n</td>\n<td>\nGap\n</td>\n<td>\nTime (s)\n</td>\n</tr>\n<tr>\n<td>\nM\n</td>\n<td>\n2.59\n</td>\n<td>\n0.55\n</td>\n<td>\n0.48\n</td>\n<td>\n0.48\n</td>\n<td>\n0.48\n</td>\n<td>\n0.48\n</td>\n<td>\n3.84\n</td>\n<td>\n3.12\n</td>\n<td>\n1.14\n</td>\n<td>\n0.33\n</td>\n</tr>\n<tr>\n<td>\nL\n</td>\n<td>\n51.15\n</td>\n<td>\n5.22\n</td>\n<td>\n2.18\n</td>\n<td>\n1.71\n</td>\n<td>\n1.41\n</td>\n<td>\n1.39\n</td>\n<td>\n12.85\n</td>\n<td>\n13.28\n</td>\n<td>\n3.80\n</td>\n<td>\n1.10\n</td>\n</tr>\n<tr>\n<td>\nXL\n</td>\n<td>\n77.35\n</td>\n<td>\n14.02\n</td>\n<td>\n10.41\n</td>\n<td>\n2.93\n</td>\n<td>\n2.07\n</td>\n<td>\n0.93\n</td>\n<td>\n17.01\n</td>\n<td>\n121.55\n</td>\n<td>\n5.21\n</td>\n<td>\n2.49\n</td>\n</tr>\n</tbody>\n</table>\n\nRecall that Model (1) produces solutions that exhibit considerable variability when the total commodity volume is perturbed as detailed in Table 4 and 5. As such, it is unlikely to be practical in scenarios with planners in the loop. Hence, the table compares the optimization proxies and the heuristics GH with an \u201cidealized\u201d benchmark. With this caveat in place, observe the performance of the optimization proxies under tight time constraints. Proxies generate solutions with low optimality gaps and may be up to 10 to 50 times faster than GH, and around 10 times faster than Model (1) solved with Gurobi. Second, although Model (1) efficiently produces solutions with low optimality gaps, closing the optimizality gap proves to be a significant challenge due to the poor LP relaxation. The performance of GH is also impeded by the inefficiencies of the LP relaxation, as it solves the LP relaxations over many iterations; it takes the GH around 30 iterations for terminal L, and more than 1000 iterations for terminal XL to generate a feasible solution. **Consistency:**\n\nTables 4 and 5 report the consistency of solutions obtained from different models in terms of the normalized distance to the reference load plan and the total variation of the generated solutions. As GDO requires running Model (1) and Model (2) using the best upper bound obtained from Model (1) for another 30 seconds. *The high-level result is that proxies are ideally suited to produce consistent plans. Table 4 shows that the proxies accurately predict, in a few seconds, the results produced by GDO after an hour. Furthermore, Table 5 shows that the performance of the optimization proxies is consistent with the results of GDO. The high-level result is that the optimization proxies are ideally suited to produce consistent plans. Table 4 shows that the proxies accurately predict, in a few seconds, the results produced by GDO after an hour. Furthermore, Table 5 shows that"
    ],
    "kosmos": [
      "\n\n## 7.3 Computational Performance of the Optimization Proxies\n\nThis section presents numerical experiments used to assess the performance of the proposed optimization proxies (Proxies) against the optimization models (GDO) and the greedy heuristic (GH).\n\n**Optimality Gap:**\n\nTable 3 presents the optimality gaps of various approaches, including the results of Model (1) under various time constraints. In the table, the columns under \u201cGap of Model (1)\u201d denote the optimality gaps of the model under various time limits. Similarly, columns *Gap* for GH and Proxies denote optimality gaps for GH and the optimization proxies. In addition, columns *Time(s)* denote the solving times for GH and Proxies.\n\n**Table 3:** *Optimality Gap (%) with respect to the Total Trailer Cost*\n\n<table>\n<thead>\n<tr>\n<th>\nInstance\n</th>\n<th>\nModel (1)\n</th>\n<th>\n</th>\n<th>\n</th>\n<th>\nGH\n</th>\n<th>\n</th>\n<th>\nProxies\n</th>\n<th>\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>\n1s\n</td>\n<td>\n5s\n</td>\n<td>\n10s\n</td>\n<td>\n30s\n</td>\n<td>\n60s\n</td>\n<td>\n1800s\n</td>\n<td>\nGap\n</td>\n<td>\nTime (s)\n</td>\n<td>\nGap\n</td>\n<td>\nTime (s)\n</td>\n</tr>\n<tr>\n<td>\nM\n</td>\n<td>\n2.59\n</td>\n<td>\n0.55\n</td>\n<td>\n0.48\n</td>\n<td>\n0.48\n</td>\n<td>\n0.48\n</td>\n<td>\n0.48\n</td>\n<td>\n3.84\n</td>\n<td>\n3.12\n</td>\n<td>\n1.14\n</td>\n<td>\n0.33\n</td>\n</tr>\n<tr>\n<td>\nL\n</td>\n<td>\n51.15\n</td>\n<td>\n5.22\n</td>\n<td>\n2.18\n</td>\n<td>\n1.71\n</td>\n<td>\n1.41\n</td>\n<td>\n1.39\n</td>\n<td>\n12.85\n</td>\n<td>\n13.28\n</td>\n<td>\n3.80\n</td>\n<td>\n1.10\n</td>\n</tr>\n<tr>\n<td>\nXL\n</td>\n<td>\n77.35\n</td>\n<td>\n14.02\n</td>\n<td>\n10.41\n</td>\n<td>\n2.93\n</td>\n<td>\n2.07\n</td>\n<td>\n0.93\n</td>\n<td>\n17.01\n</td>\n<td>\n121.55\n</td>\n<td>\n5.21\n</td>\n<td>\n2.49\n</td>\n</tr>\n</tbody>\n</table>\n\nRecall that Model (1) produces solutions that exhibit considerable variability when the total commodity volume is perturbed as detailed in Table 4 and 5. As such, it is unlikely to be practical in scenarios with planners in the loop. Hence, the table compares the optimization proxies and the heuristics GH with an \u201cidealized\u201d benchmark. With this caveat in place, observe the performance of the optimization proxies under tight time constraints. Proxies generate solutions with low optimality gaps and may be up to 10 to 50 times faster than GH, and around 10 times faster than Model (1) solved with Gurobi. Second, although Model (1) efficiently produces solutions with low optimality gaps, closing the optimizality gap proves to be a significant challenge due to the poor LP relaxation. The performance of GH is also impeded by the inefficiencies of the LP relaxation, as it solves the LP relaxations over many iterations; it takes the GH around 30 iterations for terminal M, 200 iterations for terminal L, and more than 1000 iterations for terminal XL to generate a feasible solution.\n\n**Consistency:**\n\nTables 4 and 5 report the consistency of solutions obtained from different models in terms of the normalized distance to the reference load plan and the total variation of the generated solutions. As GDO requires running Model (1) and Model (2) sequentially, these experiments set the same time limits for the two stages. For example, if a time limit of 30 seconds is set, GDO runs Model (1) for 30 seconds and subsequently runs Model (2) using the best upper bound obtained from Model (1) for another 30 seconds.\n\n*The high-level result is that proxies are ideally suited to produce consistent plans.* Table 4 shows that the proxies accurately predict, in a few seconds, the results produced by GDO after an hour. Furthermore, Table 5 shows that"
    ]
  },
  {
    "edit": [
      "\n\n**Table 2:** Normal-markdown model: ideal cases\n\n<table>\n<thead>\n<tr>\n<th>\n</th>\n<th>\n</th>\n<th>\nOutcome\n</th>\n<th>\n</th>\n<th>\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>\nEmp.\n</td>\n<td>\np10\n</td>\n<td>\np25\n</td>\n<td>\np50\n</td>\n<td>\np90\n</td>\n</tr>\n<tr>\n<td>\n<em>\nPanel A: Regions differ in location parameter, initial min. wage is small\n</em>\n</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>\nMean causal effect\n</td>\n<td>\n-0.011\n</td>\n<td>\n0.026\n</td>\n<td>\n0.013\n</td>\n<td>\n0.007\n</td>\n<td>\n0.003\n</td>\n</tr>\n<tr>\n<td>\nFraction affected\n</td>\n<td>\n-0.013\n</td>\n<td>\n0.028\n</td>\n<td>\n0.014\n</td>\n<td>\n0.008\n</td>\n<td>\n0.004\n</td>\n</tr>\n<tr>\n<td></td>\n<td>\n(0.000)\n</td>\n<td>\n(0.001)\n</td>\n<td>\n(0.000)\n</td>\n<td>\n(0.000)\n</td>\n<td>\n(0.000)\n</td>\n</tr>\n<tr>\n<td>\nGap measure\n</td>\n<td>\n-0.010\n</td>\n<td>\n0.021\n</td>\n<td>\n0.011\n</td>\n<td>\n0.006\n</td>\n<td>\n0.003\n</td>\n</tr>\n<tr>\n<td></td>\n<td>\n(0.000)\n</td>\n<td>\n(0.001)\n</td>\n<td>\n(0.000)\n</td>\n<td>\n(0.000)\n</td>\n<td>\n(0.000)\n</td>\n</tr>\n<tr>\n<td>\n<em>\nPanel B: Regions differ in location parameter, initial min. wage is large\n</em>\n</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>\nMean causal effect\n</td>\n<td>\n-0.076\n</td>\n<td>\n0.155\n</td>\n<td>\n0.079\n</td>\n<td>\n0.049\n</td>\n<td>\n0.025\n</td>\n</tr>\n<tr>\n<td>\nFraction affected\n</td>\n<td>\n-0.079\n</td>\n<td>\n0.112\n</td>\n<td>\n0.081\n</td>\n<td>\n0.054\n</td>\n<td>\n0.031\n</td>\n</tr>\n<tr>\n<td></td>\n<td>\n(0.002)\n</td>\n<td>\n(0.014)\n</td>\n<td>\n(0.003)\n</td>\n<td>\n(0.000)\n</td>\n<td>\n(0.001)\n</td>\n</tr>\n<tr>\n<td>\nGap measure\n</td>\n<td>\n-0.055\n</td>\n<td>\n0.074\n</td>\n<td>\n0.058\n</td>\n<td>\n0.038\n</td>\n<td>\n0.022\n</td>\n</tr>\n<tr>\n<td></td>\n<td>\n(0.003)\n</td>\n<td>\n(0.012)\n</td>\n<td>\n(0.002)\n</td>\n<td>\n(0.001)\n</td>\n<td>\n(0.000)\n</td>\n</tr>\n<tr>\n<td>\n<em>\nPanel C: Identical regions receive different location shocks, initial min. wage is small\n</em>\n</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>\nMean causal effect\n</td>\n<td>\n-0.006\n</td>\n<td>\n0.015\n</td>\n<td>\n0.007\n</td>\n<td>\n0.004\n</td>\n<td>\n0.002\n</td>\n</tr>\n<tr>\n<td>\nEffective min. wage\n</td>\n<td>\n-0.006\n</td>\n<td>\n0.011\n</td>\n<td>\n0.003\n</td>\n<td>\n0.000\n</td>\n<td>\n-0.002\n</td>\n</tr>\n<tr>\n<td></td>\n<td>\n(0.001)\n</td>\n<td>\n(0.001)\n</td>\n<td>\n(0.000)\n</td>\n<td>\n(0.000)\n</td>\n<td>\n(0.000)\n</td>\n</tr>\n<tr>\n<td"
    ],
    "kosmos": [
      "\n\n**Table 2:** Normal-markdown model: ideal cases\n\n<table>\n<thead>\n<tr>\n<th>\n</th>\n<th>\n</th>\n<th>\nOutcome\n</th>\n<th>\n</th>\n<th>\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>\nEmp.\n</td>\n<td>\np10\n</td>\n<td>\np25\n</td>\n<td>\np50\n</td>\n<td>\np90\n</td>\n</tr>\n<tr>\n<td>\n<em>\nPanel A: Regions differ in location parameter, initial min. wage is small\n</em>\n</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>\nMean causal effect\n</td>\n<td>\n-0.011\n</td>\n<td>\n0.026\n</td>\n<td>\n0.013\n</td>\n<td>\n0.007\n</td>\n<td>\n0.003\n</td>\n</tr>\n<tr>\n<td>\nFraction affected\n</td>\n<td>\n-0.013\n</td>\n<td>\n0.028\n</td>\n<td>\n0.014\n</td>\n<td>\n0.008\n</td>\n<td>\n0.004\n</td>\n</tr>\n<tr>\n<td></td>\n<td>\n(0.000)\n</td>\n<td>\n(0.001)\n</td>\n<td>\n(0.000)\n</td>\n<td>\n(0.000)\n</td>\n<td>\n(0.000)\n</td>\n</tr>\n<tr>\n<td>\nGap measure\n</td>\n<td>\n-0.010\n</td>\n<td>\n0.021\n</td>\n<td>\n0.011\n</td>\n<td>\n0.006\n</td>\n<td>\n0.003\n</td>\n</tr>\n<tr>\n<td></td>\n<td>\n(0.000)\n</td>\n<td>\n(0.001)\n</td>\n<td>\n(0.000)\n</td>\n<td>\n(0.000)\n</td>\n<td>\n(0.000)\n</td>\n</tr>\n<tr>\n<td>\n<em>\nPanel B: Regions differ in location parameter, initial min. wage is large\n</em>\n</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>\nMean causal effect\n</td>\n<td>\n-0.076\n</td>\n<td>\n0.155\n</td>\n<td>\n0.079\n</td>\n<td>\n0.049\n</td>\n<td>\n0.025\n</td>\n</tr>\n<tr>\n<td>\nFraction affected\n</td>\n<td>\n-0.079\n</td>\n<td>\n0.112\n</td>\n<td>\n0.081\n</td>\n<td>\n0.054\n</td>\n<td>\n0.031\n</td>\n</tr>\n<tr>\n<td></td>\n<td>\n(0.002)\n</td>\n<td>\n(0.014)\n</td>\n<td>\n(0.003)\n</td>\n<td>\n(0.000)\n</td>\n<td>\n(0.001)\n</td>\n</tr>\n<tr>\n<td>\nGap measure\n</td>\n<td>\n-0.055\n</td>\n<td>\n0.074\n</td>\n<td>\n0.058\n</td>\n<td>\n0.038\n</td>\n<td>\n0.022\n</td>\n</tr>\n<tr>\n<td></td>\n<td>\n(0.003)\n</td>\n<td>\n(0.012)\n</td>\n<td>\n(0.002)\n</td>\n<td>\n(0.001)\n</td>\n<td>\n(0.000)\n</td>\n</tr>\n<tr>\n<td>\n<em>\nPanel C: Identical regions receive different location shocks, initial min. wage is small\n</em>\n</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>\nMean causal effect\n</td>\n<td>\n-0.006\n</td>\n<td>\n0.015\n</td>\n<td>\n0.007\n</td>\n<td>\n0.004\n</td>\n<td>\n0.002\n</td>\n</tr>\n<tr>\n<td>\nEffective min. wage\n</td>\n<td>\n-0.006\n</td>\n<td>\n0.011\n</td>\n<td>\n0.003\n</td>\n<td>\n0.000\n</td>\n<td>\n-0.002\n</td>\n</tr>\n<tr>\n<td></td>\n<td>\n(0.001)\n</td>\n<td>\n(0.001)\n</td>\n<td>\n(0.000)\n</td>\n<td>\n(0.000)\n</td>\n<td>\n(0.000)\n</td>\n</tr>\n<tr>\n<td"
    ]
  },
  {
    "edit": [
      "\n\n**Lemma 3.4.**_Under the hypothesis of Theorem 3.1, for every \\(p_{0}&gt;2\\) there exists a constant \\(C_{1}\\) such that for all \\(p\\geqslant p_{0}\\) and \\(\\zeta\\in C_{0}^{\\infty}(\\mathbb{R}^{n};\\mathbb{R}_{&gt;0})\\)_\n\n\\[\\|\\nabla(\\zeta|\\psi| p/ 2 ) \u2225_{2}\\leqslant C_{1}p^{\\alpha}\\|(|\\nabla\\zeta|+\\zeta)|\\psi| p/ 2 \u2225_{2}, \\tag{29}\\]\n\n_where \\(\\alpha=(\\mu+1)/2\\)._\n\nProof.: The proof of this lemma is based on (22) and it follows the proof of (Theorem 5.1 in [0]). Note that (22) becomes useless for \\(p=2\\). Agmon works with the stronger inequality (19), which doesn\u2019t degenerate at \\(p=2\\). By fixing a number \\(p_{0}&gt;2\\) and looking at numbers \\(p\\geqslant p_{0}\\) we can reuse Agmon\u2019s proof. From (22) and H\u00a8older\u2019s inequality it follows that\n\n\\[(p-2)\\int dx\\,\\zeta^{2}|\\psi| p \u2212 2|\\nabla|\\psi||^{2}\\leqslant 2\\left(\\int dx\\,\\zeta^{2}|\\psi| p \u2212 2|\\nabla|\\psi||\\right)^{1/2}\\left(\\int dx\\,|\\nabla\\zeta|^{2}|\\psi| p \\right)^{1/2}\\] \\[+\\int dx\\,\\zeta^{2}q_{-}|\\psi| p\\]\n\nUsing (26) this can be rewritten as\n\n\\[(p-2)\\frac{4}{p^{2}}\\int dx\\,\\zeta^{2}|\\nabla|\\psi| p/ 2|^{2}\\leqslant 2\\frac{2}{p}\\left(\\int dx\\,\\zeta^{2}|\\nabla|\\psi| p/ 2|^{2}\\right)^{1/2}\\left(\\int dx\\,|\\nabla\\zeta|^{2}(|\\psi| p/ 2)^{2}\\right)^{1/2}\\] \\[=A\\] \\[+\\int dx\\,\\zeta^{2}q_{-}(|\\psi| p/ 2)^{2}.\\]\n\nNote that by Lemma 3.3 and assumption (13) all integrals are finite. The above inequality is equivalent to\n\n\\[A^{2}\\leqslant\\frac{p}{p-2}AB+\\frac{p^{2}}{4(p-2)}C.\\]\n\nNext, we use that \\(p/(p-2)\\leqslant p_{0}/(p_{0}-2)=:D_{0}\\) for \\(p\\geqslant p_{0}\\) and \\(AB\\leqslant\\delta A^{2}+B^{2}/\\delta\\) for all \\(\\delta&gt;0\\):\n\n\\[A^{2}\\leqslant D_{0}\\left(\\delta A^{2}+\\frac{B^{2}}{\\delta}\\right)+\\frac{p}{4}D_ {0}C\\]\n\n\\[\\Longleftrightarrow(1-D_{0}\\delta)A^{2}\\leqslant D_{0}\\frac{B^{2}}{\\delta}+ \\frac{p}{4}D_{0}C.\\]\n\nWe choose \\(\\delta=\\frac{1}{2D_{0}}\\), so that\n\n\\[\\frac{1}{2}A^{2}\\leqslant 2D_{0}^{2}B^{2}+\\frac{p}{4}D_{0}C.\\]\n\nUsing \\(\\sqrt{a+b}\\leqslant\\sqrt{a}+\\sqrt{b}\\) it follows that\n\n\\[A\\leqslant 2D_{0}B+\\sqrt{\\frac{D_{0}}{2}}\\sqrt{p}\\sqrt{C}\\]"
    ],
    "kosmos": [
      "\n\n**Lemma 3.4**.: _Under the hypothesis of Theorem 3.1, for every \\(p_{0}&gt;2\\) there exists a constant \\(C_{1}\\) such that for all \\(p\\geqslant p_{0}\\) and \\(\\zeta\\in C^{\\infty}_{0}(\\mathbb{R}^{n};\\mathbb{R}_{&gt;0})\\)_\n\n\\[\\|\\nabla(\\zeta|\\psi| p/ 2 ) \u2225_{2}\\leqslant C_{1}\\,p^{\\alpha}\\|(|\\nabla\\zeta|+\\zeta)|\\psi| p/ 2 \u2225_{2}, \\tag{29}\\]\n\n_where \\(\\alpha=(\\mu+1)/2\\)._\n\nProof.: The proof of this lemma is based on (22) and it follows the proof of (Theorem 5.1 in [0]). Note that (22) becomes useless for \\(p=2\\). Agmon works with the stronger inequality (19), which doesn\u2019t degenerate at \\(p=2\\). By fixing a number \\(p_{0}&gt;2\\) and looking at numbers \\(p\\geqslant p_{0}\\) we can reuse Agmon\u2019s proof. From (22) and H\u00a8older\u2019s inequality it follows that\n\n\\[(p-2)\\int dx\\,\\zeta^{2}|\\psi| p \u2212 2|\\nabla|\\psi||^{2}\\leqslant 2\\left(\\int dx\\,\\zeta^{2}|\\psi| p \u2212 2|\\nabla|\\psi||\\right)^{1/2}\\left(\\int dx\\,|\\nabla\\zeta|^{2}|\\psi| p \\right)^{1/2}\\] \\[+\\int dx\\,\\zeta^{2}q_{-}|\\psi| p\\]\n\nUsing (26) this can be rewritten as\n\n\\[(p-2)\\frac{4}{p^{2}}\\int dx\\,\\zeta^{2}|\\nabla|\\psi| p/ 2|^{2}\\leqslant 2\\frac{2}{p}\\left(\\int dx\\,\\zeta^{2}|\\nabla|\\psi| p/ 2|^{2}\\right)^{1/2}\\left(\\int dx\\,|\\nabla\\zeta|^{2}(|\\psi| p/ 2)^{2}\\right)^{1/2}\\] \\[=A\\] \\[+\\int dx\\,\\zeta^{2}q_{-}(|\\psi| p/ 2)^{2}.\\]\n\nNote that by Lemma 3.3 and assumption (13) all integrals are finite. The above inequality is equivalent to\n\n\\[A^{2}\\leqslant\\frac{p}{p-2}AB+\\frac{p^{2}}{4(p-2)}C.\\]\n\nNext, we use that \\(p/(p-2)\\leqslant p_{0}/(p_{0}-2)=:D_{0}\\) for \\(p\\geqslant p_{0}\\) and \\(AB\\leqslant\\delta A^{2}+B^{2}/\\delta\\) for all \\(\\delta&gt;0\\):\n\n\\[A^{2}\\leqslant D_{0}\\left(\\delta A^{2}+\\frac{B^{2}}{\\delta}\\right)+\\frac{p}{4}D _{0}C\\]\n\n\\[\\Longleftrightarrow(1-D_{0}\\delta)A^{2}\\leqslant D_{0}\\frac{B^{2}}{\\delta}+ \\frac{p}{4}D_{0}C.\\]\n\nWe choose \\(\\delta=\\frac{1}{2D_{0}}\\), so that\n\n\\[\\frac{1}{2}A^{2}\\leqslant 2D_{0}^{2}B^{2}+\\frac{p}{4}D_{0}C.\\]\n\nUsing \\(\\sqrt{a+b}\\leqslant\\sqrt{a}+\\sqrt{b}\\) it follows that\n\n\\[A\\leqslant 2D_{0}B+\\sqrt{\\frac{D_{0}}{2}}\\sqrt{p}\\,\\sqrt{C}\\]"
    ]
  },
  {
    "edit": [
      "<table>\n<thead>\n<tr>\n<th>\n</th>\n<th>\nkn\n</th>\n<th>\nml\n</th>\n<th>\nte\n</th>\n<th>\nta\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\nkn\n</td>\n<td>\n-\n</td>\n<td>\n<strong>\n7.7\n</strong>\n</td>\n<td>\n1.0\n</td>\n<td>\n0.8\n</td>\n</tr>\n<tr>\n<td>\nml\n</td>\n<td>\n<strong>\n8.9\n</strong>\n</td>\n<td>\n-\n</td>\n<td>\n5.7\n</td>\n<td>\n0.6\n</td>\n</tr>\n<tr>\n<td>\nte\n</td>\n<td>\n0.5\n</td>\n<td>\n3.2\n</td>\n<td>\n-\n</td>\n<td>\n<strong>\n7.4\n</strong>\n</td>\n</tr>\n<tr>\n<td>\nta\n</td>\n<td>\n5.8\n</td>\n<td>\n4.9\n</td>\n<td>\n<strong>\n7.4\n</strong>\n</td>\n<td>\n-\n</td>\n</tr>\n</tbody>\n</table>\n\nTable 3: BLEU score for 4 language pairs. We run experiments with 3 types of language pair combinations: kn\u2194ml,te\u2194ta. Results documented in Table 3\n\n<table>\n<thead>\n<tr>\n<th>\n</th>\n<th>\nkn\n</th>\n<th>\nml\n</th>\n<th>\nte\n</th>\n<th>\nta\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\nkn\n</td>\n<td>\n-\n</td>\n<td>\n<strong>\n7.4\n</strong>\n</td>\n<td>\n0.4\n</td>\n<td>\n0.5\n</td>\n</tr>\n<tr>\n<td>\nml\n</td>\n<td>\n1.0\n</td>\n<td>\n-\n</td>\n<td>\n<strong>\n7.0\n</strong>\n</td>\n<td>\n0.4\n</td>\n</tr>\n<tr>\n<td>\nte\n</td>\n<td>\n0.8\n</td>\n<td>\n4.7\n</td>\n<td>\n-\n</td>\n<td>\n<strong>\n7.1\n</strong>\n</td>\n</tr>\n<tr>\n<td>\nta\n</td>\n<td>\n<strong>\n8.9\n</strong>\n</td>\n<td>\n4.5\n</td>\n<td>\n0.6\n</td>\n<td>\n-\n</td>\n</tr>\n</tbody>\n</table>\n\nTable 4: BLEU score for 4 language pair model 2. The rows are the source language and the columns are the target language. Cells in bold represent the translation directions used in training\n\n<table>\n<thead>\n<tr>\n<th>\n</th>\n<th>\nkn\n</th>\n<th>\nml\n</th>\n<th>\nte\n</th>\n<th>\nta\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\nkn\n</td>\n<td>\n-\n</td>\n<td>\n<strong>\n6.5\n</strong>\n</td>\n<td>\n4.5\n</td>\n<td>\n0.8\n</td>\n</tr>\n<tr>\n<td>\nml\n</td>\n<td>\n6.8\n</td>\n<td>\n-\n</td>\n<td>\n<strong>\n6.4\n</strong>\n</td>\n<td>\n5.5\n</td>\n</tr>\n<tr>\n<td>\nte\n</td>\n<td>\n0.7\n</td>\n<td>\n2.4\n</td>\n<td>\n-\n</td>\n<td>\n<strong>\n6.6\n</strong>\n</td>\n</tr>\n<tr>\n<td>\nta\n</td>\n<td>\n<strong>\n8.1\n</strong>\n</td>\n<td>\n1.7\n</td>\n<td>\n4.5\n</td>\n<td>\n-\n</td>\n</tr>\n</tbody>\n</table>\n\nTable 5: BLEU score for 4 language pair model 3. The rows are the source language and the columns are the target language. Cells in bold represent the translation directions used in training\n\nTable 3: BLEU score for 4 language pair model 1. The rows are the source language and the columns are the target language. Cells in bold represent the translation directions used in training\n\n### 4 language pairs\n\nA single encoder-decoder model is trained on 4 language pairs. We run experiments with 3 types of language pair combinations:\n\n* **Model 1**: 2 unique language pairs in forward and reverse direction: kn\u2194 ml,te\u2194 ta. Results documented in Table 3\n* **Model 2**: 4 unique language pairs: kn\u2194ml, ml\u2194te, te\u2194ta, ta\u2194kn. Results documented in Table 4\n* "
    ],
    "kosmos": [
      "<table>\n<thead>\n<tr>\n<th>\n</th>\n<th>\nkn\n</th>\n<th>\nml\n</th>\n<th>\nte\n</th>\n<th>\nta\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\nkn\n</td>\n<td>\n-\n</td>\n<td>\n<strong>\n7.7\n</strong>\n</td>\n<td>\n1.0\n</td>\n<td>\n0.8\n</td>\n</tr>\n<tr>\n<td>\nml\n</td>\n<td>\n<strong>\n8.9\n</strong>\n</td>\n<td>\n-\n</td>\n<td>\n5.7\n</td>\n<td>\n0.6\n</td>\n</tr>\n<tr>\n<td>\nte\n</td>\n<td>\n0.5\n</td>\n<td>\n3.2\n</td>\n<td>\n-\n</td>\n<td>\n<strong>\n7.4\n</strong>\n</td>\n</tr>\n<tr>\n<td>\nta\n</td>\n<td>\n5.8\n</td>\n<td>\n4.9\n</td>\n<td>\n<strong>\n7.4\n</strong>\n</td>\n<td>\n-\n</td>\n</tr>\n</tbody>\n</table>\n\nTable 3: BLEU score for 4 language pair model 1. The rows are the source language and the columns are the target language. Cells in bold represent the translation directions used in training\n\n<table>\n<thead>\n<tr>\n<th>\n</th>\n<th>\nkn\n</th>\n<th>\nml\n</th>\n<th>\nte\n</th>\n<th>\nta\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\nkn\n</td>\n<td>\n-\n</td>\n<td>\n<strong>\n7.4\n</strong>\n</td>\n<td>\n0.4\n</td>\n<td>\n0.5\n</td>\n</tr>\n<tr>\n<td>\nml\n</td>\n<td>\n1.0\n</td>\n<td>\n-\n</td>\n<td>\n<strong>\n7.0\n</strong>\n</td>\n<td>\n0.4\n</td>\n</tr>\n<tr>\n<td>\nte\n</td>\n<td>\n0.8\n</td>\n<td>\n4.7\n</td>\n<td>\n-\n</td>\n<td>\n<strong>\n7.1\n</strong>\n</td>\n</tr>\n<tr>\n<td>\nta\n</td>\n<td>\n<strong>\n8.9\n</strong>\n</td>\n<td>\n4.5\n</td>\n<td>\n0.6\n</td>\n<td>\n-\n</td>\n</tr>\n</tbody>\n</table>\n\nTable 4: BLEU score for 4 language pair model 2. The rows are the source language and the columns are the target language. Cells in bold represent the translation directions used in training\n\n<table>\n<thead>\n<tr>\n<th>\n</th>\n<th>\nkn\n</th>\n<th>\nml\n</th>\n<th>\nte\n</th>\n<th>\nta\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\nkn\n</td>\n<td>\n-\n</td>\n<td>\n<strong>\n6.5\n</strong>\n</td>\n<td>\n4.5\n</td>\n<td>\n0.8\n</td>\n</tr>\n<tr>\n<td>\nml\n</td>\n<td>\n6.8\n</td>\n<td>\n-\n</td>\n<td>\n<strong>\n6.4\n</strong>\n</td>\n<td>\n5.5\n</td>\n</tr>\n<tr>\n<td>\nte\n</td>\n<td>\n0.7\n</td>\n<td>\n2.4\n</td>\n<td>\n-\n</td>\n<td>\n<strong>\n6.6\n</strong>\n</td>\n</tr>\n<tr>\n<td>\nta\n</td>\n<td>\n<strong>\n8.1\n</strong>\n</td>\n<td>\n1.7\n</td>\n<td>\n4.5\n</td>\n<td>\n-\n</td>\n</tr>\n</tbody>\n</table>\n\nTable 5: BLEU score for 4 language pair model 3. The rows are the source language and the columns are the target language. Cells in bold represent the translation directions used in training\n\nTable 3: BLEU score for 4 language pair model 1. The rows are the source language and the columns are the target language. Cells in bold represent the translation directions used in training\n\n### 4 language pairs\n\nA single encoder-decoder model is trained on 4 language pairs. We run experiments with 3 types of language pair combinations:\n\n* **Model 1**: 2 unique language pairs in forward and reverse direction: kn\u2194 ml,te \u2194 ta. Results documented in Table 3\n* **Model 2**: 4 unique language pairs: kn\u2192ml, ml\u2192te, te\u2192ta, ta\u2192kn. Results documented in Table 4\n* "
    ]
  },
  {
    "edit": [
      "states. As shown in Fig. 8 (c) and (d), the qubit gradually relaxes to \\(|0\\rangle\\) after the SFQ operation. Therefore, when the \\(\\pi\\) pulse is applied to the qubit immediately after the SFQ operation, the qubit cannot be completely excited to the \\(|1\\rangle\\) state. As the recovery time prolongs, the population of \\(|1\\rangle\\) after the \\(\\pi\\) pulse also gradually increases to 1.\n\n## Appendix C Extraction of Quasiparticle Density Near Qubit\n\nThe \\(x_{\\rm QP,qubit}\\) extracted using \\(x_{\\rm QP,qubit}=(\\Gamma(t)-\\Gamma_{0})/C\\) is a kind of average QP density during the \\(T_{1}\\) measurement, which can be regarded as the average density of QP over a period of time \\(t_{\\rm avg}\\) (\\(\\sim T_{1}\\)) after \\(t_{\\rm R}\\). To intuitively illustrate the validity of the \\(x_{\\rm QP,qubit}\\) evolution extracted in this way to analyze the QP propagation mechanism, we compare (i) the \\(x_{\\rm QP,qubit}\\) extracted with a very short measurement time (\\(t_{\\rm avg}=0\\)) in an ideal measurement, and (ii) the \\(x_{\\rm QP,qubit}\\) extracted with several microsecond measurement times (\\(t_{\\rm avg}=3\\), 6, 12, 18 \\(\\mu\\)s) in a real experiment. We assume that the gray curve (\\(t_{\\rm avg}=0\\)) in the Fig. 9 below, which is similar to the trend of the extracted \\(x_{\\rm QP,qubit}\\) evolution in the maintext, is the real QP density evolution. The trends and timescales (\\(\\sim\\)several microseconds) of \\(x_{\\rm QP,qubit}\\) evolution vary little over all values of \\(t_{\\rm avg}\\) listed.\n\n## Appendix D Diffusion of Quasiparticles in Superconducting Quantum-Classical Hybrid Circuits\n\nMost QPs propagate diffusively when the local QP density is relatively low. The local QP density is \\(x_{\\rm QP}=n_{\\rm QP}/n_{\\rm CP}\\), where \\(n_{\\rm QP}\\) is the QP density and \\(n_{\\rm CP}\\) the Cooper pair density. As the local QP density increases, QPs have a greater probability of recombination, and phonon-mediated propagation becomes the leading mechanism. We calculated the diffusion equation by finite element simulation, thereby estimating the contribution of QP diffusion to the QP propagation in the device described in the paper. We set a boundary of\n\nFigure 8: **(a) Pulse sequence applied to measure qubits state after an SFQ pulse train. The transmission of the readout resonator is measured at \\(f_{\\rm r}\\). (b) Qubit readout resonator spectroscopy vs. DC/SFQ converter drive pulse duration t\\({}_{\\rm SFQ}\\). (c) Pulse sequence applied to measure qubits state with variable delay \\(t_{\\rm delay}\\) after a 25\\(\\mu\\)s SFQ pulse train. The transmission of the readout resonator is measured at \\(f_{\\rm r}\\). (d) Qubit readout resonator spectroscopy vs. delay between SFQ circuit operation and readout pulse t\\({}_{\\rm delay}\\). The frequencies corresponding to the readout resonator when the state of qubit is \\(|0\\rangle\\), \\(|1\\rangle\\), and punch out have been marked.**\n\nFigure 9: E\ufb00ect of measurement time on the extracted \\(x_{\\rm QP,qubit}\\) evolution\n\n"
    ],
    "kosmos": [
      "states. As shown in Fig. 8 (c) and (d), the qubit gradually relaxes to \\(|0\\rangle\\) after the SFQ operation. Therefore, when the \\(\\pi\\) pulse is applied to the qubit immediately after the SFQ operation, the qubit cannot be completely excited to the \\(|1\\rangle\\) state. As the recovery time prolongs, the population of \\(|1\\rangle\\) after the \\(\\pi\\) pulse also gradually increases to 1.\n\n## Appendix C Extraction of Quasiparticle Density Near Qubit\n\nThe \\(x_{\\rm QP,qubit}\\) extracted using \\(x_{\\rm QP,qubit}=(\\Gamma(t)-\\Gamma_{0})/C\\) is a kind of average QP density during the \\(T_{1}\\) measurement, which can be regarded as the average density of QP over a period of time \\(t_{\\rm avg}\\) (\\(\\sim T_{1}\\)) after \\(t_{\\rm R}\\). To intuitively illustrate the validity of the \\(x_{\\rm QP,qubit}\\) evolution extracted in this way to analyze the QP propagation mechanism, we compare (i) the \\(x_{\\rm QP,qubit}\\) extracted with a very short measurement time (\\(t_{\\rm avg}=0\\)) in an ideal measurement, and (ii) the \\(x_{\\rm QP,qubit}\\) extracted with several microsecond measurement times (\\(t_{\\rm avg}=3\\), 6, 12, 18 \\(\\mu\\)s) in a real experiment. We assume that the gray curve (\\(t_{\\rm avg}=0\\)) in the Fig. 9 below, which is similar to the trend of the extracted \\(x_{\\rm QP,qubit}\\) evolution in the maintext, is the real QP density evolution. The trends and timescales (\\(\\sim\\)several microseconds) of \\(x_{\\rm QP,qubit}\\) evolution vary little over all values of \\(t_{\\rm avg}\\) listed.\n\n## Appendix D Diffusion of Quasiparticles in Superconducting Quantum-Classical Hybrid Circuits\n\nMost QPs propagate diffusively when the local QP density is relatively low. The local QP density is \\(x_{\\rm QP}=n_{\\rm QP}/n_{\\rm CP}\\), where \\(n_{\\rm QP}\\) is the QP density and \\(n_{\\rm CP}\\) the Cooper pair density. As the local QP density increases, QPs have a greater probability of recombination, and phonon-mediated propagation becomes the leading mechanism. We calculated the diffusion equation by finite element simulation, thereby estimating the contribution of QP diffusion to the QP propagation in the device described in the paper. We set a boundary of\n\nFigure 8: **(a) Pulse sequence applied to measure qubits state after an SFQ pulse train. The transmission of the readout resonator is measured at \\(f_{\\rm r}\\). (b) Qubit readout resonator spectroscopy vs. DC/SFQ converter drive pulse duration t\\({}_{\\rm SFQ}\\). (c) Pulse sequence applied to measure qubits state with variable delay \\(t_{\\rm delay}\\) after a 25\\(\\mu\\)s SFQ pulse train. The transmission of the readout resonator is measured at \\(f_{\\rm r}\\). (d) Qubit readout resonator spectroscopy vs. delay between SFQ circuit operation and readout pulse t\\({}_{\\rm delay}\\). The frequencies corresponding to the readout resonator when the state of qubit is \\(|0\\rangle\\), \\(|1\\rangle\\), and punch out have been marked.**\n\nFigure 9: E\ufb00ect of measurement time on the extracted \\(x_{\\rm QP,qubit}\\) evolution\n\n"
    ]
  },
  {
    "edit": [
      "the other cliques, ensuring a fully connected graph. All nodes and edges are associated with information through embeddings, described below. **GNN stage** The second stage employs a generalized message-passing GNN following [5] to perform several prediction tasks on this graph simultaneously: 1. **keypoint association prediction:** we model association between body keypoints and their corresponding pedicle keypoints as binary edge classification on the over-connected \\(k\\)-NN graph. 2. **body keypoint level prediction: **for body keypoints, we model the spine level prediction as multi-class node classification. 3. **keypoint legitimacy prediction:** to filter out false-positive keypoints, we additionally compute an binary legitimacy prediction for each node. To perform these task, our message-passing GNN maintains edge and node embeddings which are updated in each layer. A message-passing layer performs a node update and edge update operation. Denoting the feature vector of a node \\(v\\) by \\(x_{v}\\), and the feature vector of a directed edge \\((u,v)\\) by \\(x_{uv}\\), the node and edge features are updated as follows: \\[x_{u}^{\\prime}=\\bigoplus_{v\\in\\mathcal{N}_{u}\\cup\\{u\\}}\\psi_{\\text{node}}(x_{u },x_{v},x_{uv}),\\qquad x_{uv}^{\\prime}=\\psi_{\\text{edge}}(x_{u},x_{v},x_{uv})\\] (1) Node update Edge update\n\nHere \\(\\bigoplus\\) denotes a symmetric pooling operation (in our case max pooling) over the neighborhood \\(\\mathcal{N}_{u}\\). \\(\\psi_{\\text{node}}\\) and \\(\\psi_{\\text{edge}}\\) are trainable parametric functions: in our case, two distinct two-layer MLPs with ReLU nonlinearities. After \\(N\\) such message-passing layers we obtain an embedding vector for each node and edge. Each node/edge embedding is passed through a linear layer (distinct for nodes and edges) to obtain a vector of node class logits or a single edge prediction logit, respectively. The last entry in the node prediction vector is interpreted as a node legitimacy prediction score: nodes predicted as illegitimate are discarded for the output. The node input features \\(x_{u}\\in\\mathbb{R}^{7}\\) consist of the one-hot encoded keypoint type (body, left or right pedicle) and the segment input information (a pseudoprobability in \\([0,1]\\) for each of the four spine segments of belonging to that segment, computed by applying a sigmoid to the heatmap network\u2019s output channels which represent the different spine segments). The edge input features \\(x_{uv}\\in\\mathbb{R}^{4}\\) consist of the normalized direction vector of the edge and the distance between the two endpoints. The output of the GNN contains finer spine-level classification (i.e. C1-C7, T1-T13, L1-L6, S1-S2), keypoint-level legitimacy (legitimate vs. false-positive detection) and body-pedicle association via edge prediction, implicitly defining the orientation of each vertebra. Prediction scores of corresponding directed edges \\((u,v)\\) and \\((v,u)\\) are symmetrized by taking the mean. In our experiments we consider variations to our architecture: weight sharing between consecutive GNN layers, multiple heads with a shared backbone (jointly trained) and dedicated networks (separately trained) for edge/node prediction.\n\n "
    ],
    "kosmos": [
      "the other cliques, ensuring a fully connected graph. All nodes and edges are associated with information through embeddings, described below.\n\n**GNN stage** The second stage employs a generalized message-passing GNN following [5] to perform several prediction tasks on this graph simultaneously:\n\n1. **keypoint association prediction:** we model association between body keypoints and their corresponding pedicle keypoints as binary edge classification on the over-connected \\(k\\)-NN graph.\n2. **body keypoint level prediction:** for body keypoints, we model the spine level prediction as multi-class node classification.\n3. **keypoint legitimacy prediction:** to filter out false-positive keypoints, we additionally compute an binary legitimacy prediction for each node.\n\nTo perform these task, our message-passing GNN maintains edge and node embeddings which are updated in each layer. A message-passing layer performs a node update and edge update operation. Denoting the feature vector of a node \\(v\\) by \\(x_{v}\\), and the feature vector of a directed edge \\((u,v)\\) by \\(x_{uv}\\), the node and edge features are updated as follows:\n\n\\[x_{u}^{\\prime}=\\bigoplus_{v\\in\\mathcal{N}_{u}\\cup\\{u\\}}\\psi_{\\text{node}}(x_{u} ,x_{v},x_{uv}),\\qquad x_{uv}^{\\prime}=\\psi_{\\text{edge}}(x_{u},x_{v},x_{uv}) \\tag{1}\\]\n\nNode update Edge update\n\nHere \\(\\bigoplus\\) denotes a symmetric pooling operation (in our case max pooling) over the neighborhood \\(\\mathcal{N}_{u}\\). \\(\\psi_{\\text{node}}\\) and \\(\\psi_{\\text{edge}}\\) are trainable parametric functions: in our case, two distinct two-layer MLPs with ReLU nonlinearities. After \\(N\\) such message-passing layers we obtain an embedding vector for each node and edge. Each node/edge embedding is passed through a linear layer (distinct for nodes and edges) to obtain a vector of node class logits or a single edge prediction logit, respectively. The last entry in the node prediction vector is interpreted as a node legitimacy prediction score: nodes predicted as illegitimate are discarded for the output.\n\nThe node input features \\(x_{u}\\in\\mathbb{R}^{7}\\) consist of the one-hot encoded keypoint type (body, left or right pedicle) and the segment input information (a pseudo-probability in \\([0,1]\\) for each of the four spine segments of belonging to that segment, computed by applying a sigmoid to the heatmap network's output channels which represent the different spine segments). The edge input features \\(x_{uv}\\in\\mathbb{R}^{4}\\) consist of the normalized direction vector of the edge and the distance between the two endpoints.\n\nThe output of the GNN contains finer spine-level classification (i.e. C1-C7, T1-T13, L1-L6, S1-S2), keypoint-level legitimacy (legitimate vs. false-positive detection) and body-pedicle association via edge prediction, implicitly defining the orientation of each vertebra. Prediction scores of corresponding directed edges \\((u,v)\\) and \\((v,u)\\) are symmetrized by taking the mean.\n\nIn our experiments we consider variations to our architecture: weight sharing between consecutive GNN layers, multiple heads with a shared backbone (jointly trained) and dedicated networks (separately trained) for edge/node prediction.\n\n "
    ]
  },
  {
    "edit": [
      "taking \u2126 \u2192 \u2126 + i0 + , where we add infinitesimally small imaginary part as it is required in the retarded Green\u2019s function. Then \\(a=(\\Omega+i0 +)^{2}+...\\) gets small imaginary part \\(2i\\Omega 0 +\\) defining the value of csgn. Taking the limit of infinitisimal 0 + we arrive at the following expression for the trace of the Green\u2019s function\n\n\\[\\text{Tr}G(\\Omega)=2\\Omega\\int_{0}^{\\pi}dk_{y}\\left\\{\\begin{array}{ll}-\\frac{ \\text{sgn}a}{\\sqrt{a^{2}-b^{2}}}&amp;\\text{for }a^{2}&gt;b^{2},\\\\ i\\frac{\\text{sgn}\\Omega}{\\sqrt{b^{2}-a^{2}}}&amp;\\text{for }a^{2}\\leq b^{2},\\end{array}\\right. \\tag{13}\\]\n\nwhere we used the fact that the integrand is an even function of \\(k_{y}\\). Here the values \\(a\\) and \\(b\\) are the functions of the variables \\(k_{y}\\) and \u2126 and the parameter \\(m\\) as determined by Eqs. (10). The DOS is proportional to the imaginary part of Tr G (\u2126) , therefore it is nonzero only in the region of \u2126 where\n\n\\[a^{2}-b^{2}\\leq 0. \\tag{14}\\]\n\nOutside the region (14) Tr G is real and the DOS is equal to zero, i.e., \\(\\rho(\\Omega)=0\\). The next step in evaluation of Eq. (13) is to perform integration over \\(k_{y}\\). It is convenient to replace \\(y=\\cos k_{y}\\) and \\(dk_{y}=-dy(1-y^{2})^{-1/2}\\). The boundaries of the integration are \\(-1\\leq y\\leq 1\\), where changing the order of the integration boundaries results an additional minus sign. The integration over \\(y\\) is performed differently depending on the value of the parameter m. Therefore, in what follows we are considering three cases with |m| = 1, |m| &gt; 1, and |m| &lt; 1, separately.\n\n#### 1.1.1 Case |m| = 1\n\nCalculations are simpler in the special case of |m| = 1. The function in the denominator of the integral (13) can be written explicitly as \\(a^{2}-b^{2}=(\\Omega^{2}-1)(\\Omega^{2}-5-4my)\\). It is linear in y and changes the sign only once at the point \\(y=y_{0}\\,\\text{sgn}\\,m\\), where \\(y_{0}=(\\Omega^{2}-5)/4\\). Solving the condition (14) with respect to \u2126 and using the fact that |y| \u2264 1 we obtain that DOS is nonzero only for 1 \u2264 |\u2126| \u2264 3. The condition (14) considered with respect to y determines the boundaries of integration in Eq. (13). For m = 1 it is satisfied for \\(y_{0}\\leq y\\leq 1\\).Then the imaginary part of Eq. (13) in terms of variable y gives the DOS in the implicit form\n\n\\[\\rho(\\Omega)=\\frac{|\\Omega|}{\\pi^{2}\\sqrt{\\Omega^{2}-1}}\\int_{y_{0}}^{1}\\frac{dy }{\\sqrt{(1-y^{2})(y-y_{0})}}, \\tag{15}\\]\n\nwhere the definition (7) is used. The similar expression will appears for m = \u2212 1: the boundaries of integration are \u22121 \u2264 y \u2264 \u2212y_{0}\\) and denominator in the integrand is \\(\\sqrt{(1-y^{2})(-y-y_{0})}\\). The corresponding DOS can be transformed into the result Eq. (15) by replacing the integration variable y \u2192 \u2212y\\). So Eq. (15) is valid for both cases m = \u00b1 1. The integral over y can be calculated in terms of the complete elliptical integral of the first kind K(x) using the identity (3.131.5) from [20] where \\(u_{3}&gt;u_{2}&gt;u_{1}\\). In Eq. (15) these parameters are \\(u_{1}=-1\\), \\(u_{2}=y_{0}\\), \\(u_{3}=1\\) and the result of integration is equal to \\(\\sqrt{2}K\\left[\\sqrt{(1-y_{0})/2}\\right]\\). Then the DOS for |m "
    ],
    "kosmos": [
      "taking \u2126 \u2192 \u2126 + i0 + , where we add infinitesimally small imaginary part as it is required in the retarded Green\u2019s function. Then \\(a=(\\Omega+i0 +)^{2}+...\\) gets small imaginary part \\(2i\\Omega 0 +\\) defining the value of csgn. Taking the limit of infinitisimal 0 + we arrive at the following expression for the trace of the Green's function\n\n\\[\\text{Tr}G(\\Omega)=2\\Omega\\int_{0}^{\\pi}dk_{y}\\left\\{\\begin{array}{ll}-\\frac{ \\text{sgn}a}{\\sqrt{a^{2}-b^{2}}}&amp;\\text{for }a^{2}&gt;b^{2},\\\\ i\\frac{\\text{sgn}\\Omega}{\\sqrt{b^{2}-a^{2}}}&amp;\\text{for }a^{2}\\leq b^{2},\\end{array}\\right. \\tag{13}\\]\n\nwhere we used the fact that the integrand is an even function of \\(k_{y}\\). Here the values \\(a\\) and \\(b\\) are the functions of the variables \\(k_{y}\\) and \u2126 and the parameter m as determined by Eqs. (10).\n\nThe DOS is proportional to the imaginary part of Tr G (\u2126) , therefore it is nonzero only in the region of \u2126 where\n\n\\[a^{2}-b^{2}\\leq 0. \\tag{14}\\]\n\nOutside the region (14) Tr G is real and the DOS is equal to zero, i.e., \u03c1 (\u2126) = 0.\n\nThe next step in evaluation of Eq. (13) is to perform integration over \\(k_{y}\\). It is convenient to replace \\(y=\\cos k_{y}\\) and \\(dk_{y}=-dy(1-y^{2})^{-1/2}\\). The boundaries of the integration are \\(-1\\leq y\\leq 1\\), where changing the order of the integration boundaries results an additional minus sign. The integration over \\(y\\) is performed differently depending on the value of the parameter m. Therefore, in what follows we are considering three cases with |m| = 1, |m| &gt; 1, and |m| &lt; 1, separately.\n\n#### 1.1.1 Case |m| = 1\n\nCalculations are simpler in the special case of |m| = 1. The function in the denominator of the integral (13) can be written explicitly as \\(a^{2}-b^{2}=(\\Omega^{2}-1)(\\Omega^{2}-5-4my)\\). It is linear in y and changes the sign only once at the point y = y 0 \\(\\text{sgn}\\,m\\), where y 0 = (\u2126^{2}- 5)/4\\). Solving the condition (14) with respect to \u2126 and using the fact that |y| \u2264 1 we obtain that DOS is nonzero only for 1 \u2264 |\u2126| \u2264 3.\n\nThe condition (14) considered with respect to y determines the boundaries of integration in Eq. (13). For m = 1 it is satisfied for y 0 \u2264 y \u2264 1.Then the imaginary part of Eq. (13) in terms of variable y gives the DOS in the implicit form\n\n\\[\\rho(\\Omega)=\\frac{|\\Omega|}{\\pi^{2}\\sqrt{\\Omega^{2}-1}}\\int_{y_{0}}^{1}\\frac{dy }{\\sqrt{(1-y^{2})(y-y_{0})}}, \\tag{15}\\]\n\nwhere the definition (7) is used. The similar expression will appears for m = -1: the boundaries of integration are \u22121 \u2264 y \u2264 \u2212y 0 and denominator in the integrand is \\(\\sqrt{(1-y^{2})(-y-y_{0})}\\). The corresponding DOS can be transformed into the result Eq. (15) by replacing the integration variable y \u2192 \u2212y\\). So Eq. (15) is valid for both cases m = \u00b11.\n\nThe integral over y can be calculated in terms of the complete elliptical integral of the first kind K(x) using the identity (3.131.5) from [20]\n\n\\[\\int_{u_{2}}^{u_{3}}\\frac{dy}{\\sqrt{(u_{3}-y)(y-u_{2})(y-u_{1})}} \\tag{16}\\]\n\nwhere u 3 &gt; u 2 &gt; u 1 . In Eq. (15) these parameters are u 1 = -1, u 2 = y 0, u 3 = 1 and the result of integration is equal to \\(\\sqrt{2 "
    ]
  },
  {
    "edit": [
      "In the context of high-order schemes, mathematical frameworks have been developed in order to construct schemes of arbitrary spatial discretization order [4, 5]. Among the well-documented ones are the Discontinuous Galerkin [6, 7], the Spectral Differences [8] and the Spectral Volumes [9] class of schemes. More recently, another approach for constructing high-order schemes has been introduced in the literature: the Flux Reconstruction [10], also referred to as FR. This framework is capable of creating compact high-order schemes for solving partial differential equations. Special focus was given to the treatment of advection terms [10] and diffusion terms [11], which are integral components of high-fidelity mathematical models used in fluid dynamics, such as the Navier-Stokes equations. In the Flux Reconstruction approach, the solution is known at multiple discrete points, also known as nodes, within a discrete domain element: a cell. A continuous solution is reconstructed within the cell by performing an interpolation using a basis of Lagrange polynomials. In general, the overall resulting solution will be continuous only within a cell, usually displaying discontinuities across the interfaces between two adjacent cells. Therefore, if nothing else is done, there is no interaction between nearby cells. The main idea behind the FR framework is to introduce means for information to propagate through the domain. This is done by defining common values for the property fluxes (and its derivatives, if needed) across a cell interface. For the advection terms, the interface fluxes are usually reconstructed in an upwind manner, with the Roe flux [12] being a popular procedure to be used. For the diffusion terms, the common fluxes are taken to be, in its most general case, a weighted average between the fluxes of the immediate adjacent cells. A corrected, continuous, flux function can, then, be reconstructed within a cell, in such a way that the previously defined common interface values are respected. Finally, the time-derivatives of the solution properties can be evaluated by using the corrected fluxes and, then, integrated by using an appropriate time-march scheme selected by the user. An interesting characteristic of the FR framework is that the continuous reconstruction of the flux terms is performed by using a special set of functions, called \u201ccorrection functions\u201d, and, depending on the function(s) used, different high-order schemes are achieved. For instance, schemes such as the nodal Discontinuous Galerkin [6] and the Staggered Grid [13] can be recovered by using the Flux Reconstruction in combination with an appropriate correction function. One important step that must be made towards the comprehension of the properties of a scheme is the assessment of its stability bounds and effective order of accuracy. In the case of the FR framework, part of this process has already been done by Huynh [10] and documented for a limited number of scheme orders. The present paper aims to expand this analysis by using the FR framework with 5 different correction functions and using cells with up to 10 internal nodes. The resulting schemes are, then, used to solve the 1-D advection model equation. Observations are done regarding the effects of the artificial dissipation over the transient solution, obtained by using a forth-order Runge-Kutta time-marching procedure (RK4). Further insights regarding the stability and accuracy of the resulting schemes are obtained by performing a Fourier analysis on each one of them. NUMERICAL FORMULATION In this section, a brief explanation is given regarding the Flux Reconstruction approach for the construction of high-order schemes. The intention is to give the reader sufficient background knowledge regarding the inner workings of the FR approach before performing the Fourier stability analysis in the next section. The equation of interest is the initial value problem (IVP) given by the 1-D advection model equation: f = au (1) where t is the time coordinate, x is the space coordinate, u(x, t) is the property being transported by a wave, whose dynamics is given by Eq. (1), with constant speed a. Finally, f is the flux term and the main subject of interest to the FR approach. The discrete domain can be constructed by dividing its continuous counterpart into multiple cells E, where the j-th cell is denoted by E j and has a length h j. Each cell is composed of K nodes, which is where the discrete solution, u(x, t), is evaluated at. The centroid of cell E j is located at x j , and the coordinate of the k-th node of the j-th cell is denoted by x j,k. Following the same naming convention, the solution evaluated at x j,k is defined as u j,k, which is a function of time only. The coordinate of the interface located between elements E j and E j+1 is "
    ],
    "kosmos": [
      "In the context of high-order schemes, mathematical frameworks have been developed in order to construct schemes of arbitrary spatial discretization order [4, 5]. Among the well-documented ones are the Discontinuous Galerkin [6, 7], the Spectral Di\ufb00erences [8] and the Spectral Volumes [9] class of schemes. More recently, another approach for constructing high-order schemes has been introduced in the literature: the Flux Reconstruction [10], also referred to as FR. This framework is capable of creating compact high-order schemes for solving partial di\ufb00erential equations. Special focus was given to the treatment of advection terms [10] and di\ufb00usion terms [11], which are integral components of high-\ufb01delity mathematical models used in fluid dynamics, such as the Navier-Stokes equations.\n\nIn the Flux Reconstruction approach, the solution is known at multiple discrete points, also known as nodes, within a discrete domain element: a cell. A continuous solution is reconstructed within the cell by performing an interpolation using a basis of Lagrange polynomials. In general, the overall resulting solution will be continuous only within a cell, usually displaying discontinuities across the interfaces between two adjacent cells. Therefore, if nothing else is done, there is no interaction between nearby cells. The main idea behind the FR framework is to introduce means for information to propagate through the domain. This is done by de\ufb01ning common values for the property fluxes (and its derivatives, if needed) across a cell interface. For the advection terms, the interface fluxes are usually reconstructed in an upwind manner, with the Roe \ufb02ux [12] being a popular procedure to be used. For the di\ufb00usion terms, the common fluxes are taken to be, in its most general case, a weighted average between the fluxes of the immediate adjacent cells. A corrected, continuous, flux function can, then, be reconstructed within a cell, in such a way that the previously de\ufb01ned common interface values are respected. Finally, the time-derivatives of the solution properties can be evaluated by using the corrected fluxes and, then, integrated by using an appropriate time-march scheme selected by the user.\n\nAn interesting characteristic of the FR framework is that the continuous reconstruction of the flux terms is performed by using a special set of functions, called \u201ccorrection functions\u201d, and, depending on the function(s) used, di\ufb00erent high-order schemes are achieved. For instance, schemes such as the nodal Discontinuous Galerkin [6] and the Staggered Grid [13] can be recovered by using the Flux Reconstruc- tion in combination with an appropriate correction function. One important step that must be made towards the comprehension of the properties of a scheme is the assessment of its stability bounds and e\ufb00ective order of accuracy. In the case of the FR framework, part of this process has already been done by Huynh [10] and documented for a limited number of scheme orders. The present paper aims to expand this analysis by using the FR framework with 5 di\ufb00erent correction functions and using cells with up to 10 internal nodes. The resulting schemes are, then, used to solve the 1-D advection model equation. Observations are done regarding the e\ufb00ects of the arti\ufb01cial dissipation over the transient solution, obtained by using a forth-order Runge-Kutta time-marching procedure (RK4). Further insights regarding the stability and accuracy of the resulting schemes are obtained by performing a Fourier analysis on each one of them.\n\n## 2 Numerical Formulation\n\nIn this section, a brief explanation is given regarding the Flux Reconstruction approach for the construction of high-order schemes. The intention is to give the reader su\ufb03cient background knowledge regarding the inner workings of the FR approach before performing the Fourier stability analysis in the next section.\n\nThe equation of interest is the initial value problem (IVP) given by the 1-D advection model equation:\n\n\\[\\begin{cases}\\frac{\\partial u}{\\partial t}+\\frac{\\partial f}{\\partial x}=0,&amp; \\text{with}\\quad f=au\\\\ u(x,t)|_{t=0}=u_{0}(x)\\end{cases} \\tag{1}\\]\n\nwhere t is the time coordinate, x is the space coordinate, u(x,t) is the property being transported by a wave, whose dynamics is given by Eq. (1), with constant speed a. Finally, f is the flux term and the main subject of interest to the FR approach. The discrete domain can be constructed by dividing its continuous counterpart into multiple cells E, where the j-th cell is denoted by E j and has a length h j . Each cell is composed of K nodes, which is where the discrete solution, u(x,t), is evaluated at. The centroid of cell E j is located at "
    ]
  },
  {
    "edit": [
      "further, the time scales becomes sequenced as\n\n\\[t_{c}-\\hat{t}&lt;t_{i}&lt;t_{f}&lt;t_{c}+\\hat{t}, \\tag{48}\\]\n\nwhere the system enters into the S regime. The scenario of the adiabatic-impulse approximation is illustrated in Fig. 7.\n\n## Acknowledgments\n\nWe thank Yan He for useful discussion. This work is supported by NSFC under Grants No. 11074177.\n\n## Appendix A Solution of TDBdG equations\n\nWe can solve the TDBdG equations given by Eq. (6) exactly by mapping them to the Landau-Zener problem. Then, the time-dependent Bogoliubov coefficients can be given by\n\n\\[v_{q}(z)=C_{1}D_{-s_{q \u2212 1}}(iz)+C_{2}D_{-s_{q \u2212 1}}(-iz),\\] (A1)\n\n\\[u_{q}(z)=\\frac{e^{i\\pi/4}}{\\sqrt{\\tau_{Q}}\\sin q}\\left(i\\frac{\\mathrm{d}}{ \\mathrm{d}z}+\\frac{iz}{2}\\right)v_{q}(z),\\] (A2)\n\nwith free complex parameters \\(C_{1}\\) and \\(C_{2}\\). Here, \\(D_{m}(z)\\) is the complex parabolic cylinder function, \\(z=2\\sqrt{\\tau_{Q}}\\left(\\frac{t}{\\tau_{Q}}+\\cos q\\right)e^{i\\pi/4}\\), and \\(s_{q}=-i\\tau_{Q}\\sin^{2}q\\). To reduce the above rigorous solution, we need to apply the asymptotes of \\(D_{m}(z)\\) that are given by [ 89 ]\n\n\\[D_{m}(z)=e^{-z^{2}/4}z^{m},\\;\\forall|\\arg(z)|&lt;3\\pi/4,\\] (A3)\n\n\\[D_{m}(z)= e^{-z^{2}/4}z^{m}-\\frac{\\sqrt{2\\pi}}{\\Gamma(-m)}e^{-im\\pi}e^{z^{2 }/4}z^{-m-1},\\] \\[\\forall-5\\pi/4&lt;\\arg(z)&lt;-\\pi/4,\\] (A4)\n\nfor \\(|z|\\gg 1\\) and\n\n\\[D_{m}(z)=\\frac{2^{m/2}\\sqrt{\\pi}}{\\Gamma(\\frac{1}{2}-\\frac{m}{2})}-\\frac{2^{\\frac {1}{2}+\\frac{m}{2}}\\sqrt{\\pi z}}{\\Gamma(-\\frac{m}{2})}+O(z^{2}),\\] (A5)\n\nfor \\(|z|\\to 0\\).\n\nFurthermore, in numerical simulations, the time-dependent parameter should start at a finite value. We choose a sufficiently large but finite initial transverse field, so the initial conditions of Eqs. (A1) and (A2) can be expanded into a powers of 1/ g i,\n\n\\[u_{q}(t_{i})^{2}=1-\\frac{\\sin^{2}q}{4g_{i}^{2}}+O(\\frac{1}{g_{i}^{ 2}}),\\] (A6) \\[v_{q}(t_{i})^{2}=1-u_{q}(t_{i})^{2}.\\] (A7)\n\nBased on this approximation, the two constants, \\(C_{1}\\) and \\(C_{2}\\), can be expressed as\n\n\\[|C_{1}|^{2}=u_{q}(t_{i})^{2}\\;e^{-\\frac{\\pi}{2}\\tau_{Q}\\sin^{2}q} \\tau_{Q}\\sin^{2}q\\] (A8) \\[|C_{2}|^{2}=0,\\] (A9)\n\nfor \\(|z_{i}|\\gg 1\\), and\n\n\\[C_{1}=\\frac{v_{q}(t_{i})}{\\sqrt{2\\pi}}-\\frac{(-1)^{3/4}u_{q}(t_{i}) \\sqrt{\\tau_{Q}}\\sin q}{2}+O(\\tau_{Q},\\tau_{Q}^{2})\\] (A10) \\[C_{2}=\\frac{v_{q}(t_{i})}{\\sqrt{2\\pi}}+\\frac{(-1)^{3/4}u_{q}(t_{i}) \\sqrt{\\tau_{Q}}\\sin q}{2}+O(\\tau_{Q},\\tau_{Q "
    ],
    "kosmos": [
      "further, the time scales becomes sequenced as\n\n\\[t_{c}-\\hat{t}&lt;t_{i}&lt;t_{f}&lt;t_{c}+\\hat{t}, \\tag{48}\\]\n\nwhere the system enters into the S regime. The scenario of the adiabatic-impulse approximation is illustrated in Fig. 7.\n\n## Acknowledgments\n\nWe thank Yan He for useful discussion. This work is supported by NSFC under Grants No. 11074177.\n\n## Appendix A Solution of TDBdG equations\n\nWe can solve the TDBdG equations given by Eq. (6) exactly by mapping them to the Landau-Zener problem. Then, the time-dependent Bogoliubov coefficients can be given by\n\n\\[v_{q}(z)=C_{1}D_{-s_{q \u2212 1}}(iz)+C_{2}D_{-s_{q \u2212 1}}(-iz),\\] (A1)\n\n\\[u_{q}(z)=\\frac{e^{i\\pi/4}}{\\sqrt{\\tau_{Q}}\\sin q}\\left(i\\frac{\\mathrm{d}}{ \\mathrm{d}z}+\\frac{iz}{2}\\right)v_{q}(z),\\] (A2)\n\nwith free complex parameters \\(C_{1}\\) and \\(C_{2}\\). Here, \\(D_{m}(z)\\) is the complex parabolic cylinder function, \\(z=2\\sqrt{\\tau_{Q}}\\left(\\frac{t}{\\tau_{Q}}+\\cos q\\right)e^{i\\pi/4}\\), and \\(s_{q}=-i\\tau_{Q}\\sin^{2}q\\). To reduce the above rigorous solution, we need to apply the asymptotes of \\(D_{m}(z)\\) that are given by [ 89 ]\n\n\\[D_{m}(z)=e^{-z^{2}/4}z^{m},\\;\\forall|\\arg(z)|&lt;3\\pi/4,\\] (A3)\n\n\\[D_{m}(z)= e^{-z^{2}/4}z^{m}-\\frac{\\sqrt{2\\pi}}{\\Gamma(-m)}e^{-im\\pi}e^{z^{2 }/4}z^{-m-1},\\] \\[\\forall-5\\pi/4&lt;\\arg(z)&lt;-\\pi/4,\\] (A4)\n\nfor \\(|z|\\gg 1\\) and\n\n\\[D_{m}(z)=\\frac{2^{m/2}\\sqrt{\\pi}}{\\Gamma(\\frac{1}{2}-\\frac{m}{2})}-\\frac{2^{\\frac {1}{2}+\\frac{m}{2}}\\sqrt{\\pi z}}{\\Gamma(-\\frac{m}{2})}+O(z^{2}),\\] (A5)\n\nfor \\(|z|\\to 0\\).\n\nFurthermore, in numerical simulations, the time-dependent parameter should start at a finite value. We choose a sufficiently large but finite initial transverse field, so the initial conditions of Eqs. (A1) and (A2) can be expanded into a powers of 1/ g i,\n\n\\[u_{q}(t_{i})^{2}=1-\\frac{\\sin^{2}q}{4g_{i}^{2}}+O(\\frac{1}{g_{i}^{ 2}}),\\] (A6) \\[v_{q}(t_{i})^{2}=1-u_{q}(t_{i})^{2}.\\] (A7)\n\nBased on this approximation, the two constants, \\(C_{1}\\) and \\(C_{2}\\), can be expressed as\n\n\\[|C_{1}|^{2}=u_{q}(t_{i})^{2}\\;e^{-\\frac{\\pi}{2}\\tau_{Q}\\sin^{2}q} \\tau_{Q}\\sin^{2}q\\] (A8) \\[|C_{2}|^{2}=0,\\] (A9)\n\nfor \\(|z_{i}|\\gg 1\\), and\n\n\\[C_{1}=\\frac{v_{q}(t_{i})}{\\sqrt{2\\pi}}-\\frac{(-1)^{3/4}u_{q}(t_{i}) \\sqrt{\\tau_{Q}}\\sin q}{2}+O(\\tau_{Q},\\tau_{Q}^{2})\\] (A10) \\[C_{2}=\\frac{v_{q}(t_{i})}{\\sqrt{2\\pi}}+\\frac{(-1)^{3/4}u_{q}(t_{i}) \\sqrt{\\tau_{Q}}\\sin q}{2}+O(\\tau_{Q},\\tau_{Q "
    ]
  },
  {
    "edit": [
      "\n\n**Proposition 2**.: _Given integers \\(l\\) and \\(k\\) where \\(l,k\\geq 2\\), the shortest negative cycle of \\(\\widetilde{BQ}(\\ell,2k-1)\\) is of length \\(\\min\\{2l,2k\\}\\)._\n\nProof.: We first present two natural choices for a negative cycle, one of length \\(2k\\) and another of length \\(2l\\). The first is a negative cycle on the first two layers. Take a positive edge and connect its two ends with one of the two paths using only the negative edges that connect the two layers. This would result in a negative cycle of length \\(2k\\). The second negative cycle we consider is by taking a positive edge and connecting each of its ends to the vertex \\(u\\) by a shortest path (all edges negative). One of these paths will be of length \\(l\\) and the other would be of length \\(l-1\\). Together with the first chosen edge itself then, they form a negative cycle of length \\(2l\\).\n\nIt remains to show that the shortest of these two types of cycles gives us the negative girth. To that end, we will first show that a shortest negative cycle can only use one positive edge of \\(\\widetilde{BQ}(\\ell,2k-1)\\). Towards a contradiction, let \\(C\\) be a negative cycle with more than two positive edges. We aim to present a negative cycle \\(C^{\\prime}\\) whose length is at most \\(|C|-2\\). We take two positive edges of \\(C\\) that come consecutively on the cyclic order. Assume \\(xy\\) and \\(x^{\\prime}y^{\\prime}\\) are these two edges and that \\(x^{\\prime}\\) is followed by \\(y\\) in the cyclic order of \\(C\\) (that is to say, there is no positive edge in the \\(x^{\\prime}-y\\) path in \\(C\\)). We remove the two positive edges \\(xy\\) and \\(x^{\\prime}y^{\\prime}\\) and the \\(x^{\\prime}y\\) path connecting them in \\(C\\), but then we add a \\(xy^{\\prime}\\) copy of this path (which also has no positive edge). The result is a closed walk whose sign is the same as that of \\(C\\), and whose length is \\(|C|-2\\). But then this closed walk must contain a negative cycle, whose length then is also at most \\(|C|-2\\), a contradiction.\n\nFigure 4: \\(\\widetilde{BQ}(2,3)\\), presented two different ways.\n\nFigure 3: Construction of \\(\\widetilde{BQ}(\\ell,2k-1)\\).\n\n"
    ],
    "kosmos": [
      "\n\n**Proposition 2**.: _Given integers \\(l\\) and \\(k\\) where \\(l,k\\geq 2\\), the shortest negative cycle of \\(\\widetilde{BQ}(\\ell,2k-1)\\) is of length \\(\\min\\{2l,2k\\}\\)._\n\nProof.: We first present two natural choices for a negative cycle, one of length \\(2k\\) and another of length \\(2l\\). The first is a negative cycle on the first two layers. Take a positive edge and connect its two ends with one of the two paths using only the negative edges that connect the two layers. This would result in a negative cycle of length \\(2k\\). The second negative cycle we consider is by taking a positive edge and connecting each of its ends to the vertex \\(u\\) by a shortest path (all edges negative). One of these paths will be of length \\(l\\) and the other would be of length \\(l-1\\). Together with the first chosen edge itself then, they form a negative cycle of length \\(2l\\).\n\nIt remains to show that the shortest of these two types of cycles gives us the negative girth. To that end, we will first show that a shortest negative cycle can only use one positive edge of \\(\\widetilde{BQ}(\\ell,2k-1)\\). Towards a contradiction, let \\(C\\) be a negative cycle with more than two positive edges. We aim to present a negative cycle \\(C^{\\prime}\\) whose length is at most \\(|C|-2\\). We take two positive edges of \\(C\\) that come consecutively on the cyclic order. Assume \\(xy\\) and \\(x^{\\prime}y^{\\prime}\\) are these two edges and that \\(x^{\\prime}\\) is followed by \\(y\\) in the cyclic order of \\(C\\) (that is to say, there is no positive edge in the \\(x^{\\prime}-y\\) path in \\(C\\)). We remove the two positive edges \\(xy\\) and \\(x^{\\prime}y^{\\prime}\\) and the \\(x^{\\prime}y\\) path connecting them in \\(C\\), but then we add a \\(xy^{\\prime}\\) copy of this path (which also has no positive edge). The result is a closed walk whose sign is the same as that of \\(C\\), and whose length is \\(|C|-2\\). But then this closed walk must contain a negative cycle, whose length then is also at most \\(|C|-2\\), a contradiction.\n\nFigure 4: \\(\\widetilde{BQ}(2,3)\\), presented two different ways.\n\nFigure 3: Construction of \\(\\widetilde{BQ}(\\ell,2k-1)\\).\n\n"
    ]
  },
  {
    "edit": [
      "For a set E, E denotes its interior and E its closure.\n\nFor E \u2286 d, C E [0 , \u221e ) denotes the set of probability measures on E. For a stochastic process Z, {F Z t } denotes the filtration generated by Z, i.e. F Z t := \u03c3{Z(s), s \u2264 t}.\n\n## 2 Formulation of the problem and preliminaries\n\nLet W \u2286 d be a piecewise smooth cone with vertex at the origin i.e. W := m W j, W j := {x = rz, z \u2208 S j, r > 0}, where S j is a nonempty domain in the unit sphere S d \u2212 1 with C 2 boundary. Clearly W = {x = rz, z \u2208 S, r > 0}, where S := m S j.\n\nIt is supposed that S = m S j.\n\nThe object of this work is the semimartingale obliquely reflecting Brownian motion (ORBM) in W with drift b, dispersion matrix \u03c3 and radially constant direction of reflection g j on each face, i.e. for x \u2208 \u2202 W j \u2212 {0}, x = rz, z \u2208 \u2202 S j, g j (x) = g j (z), for some unit vector field g j defined on \u2202 S j. This process can be defined as the solution of the following stochastic differential equation with reflection: X(t) = X(0) + b t + \u03c3 W(t) + \\(\\int_{0}^{t}\\gamma(s)\\,d\\lambda(s), t \u2265 0, \\(\\gamma(t)\\in G(X(t)),\\quad|\\gamma(t)|=1,\\quad d\\lambda-a.e.,\\quad t\\geq 0,\\) X(t) \u2208 W, \\(\\lambda(t)=\\int_{0}^{t}11_{\\partial W}(X(s))d\\lambda(s),\\quad t\\geq 0,\\) where G(x) := {g : g = \\sum_{j\\in\\mathcal{J}(x)}u_{j}g^{j}(x),\\ u_{j}\\geq 0}, \\mathcal{J}(x) := {j : x \u2208 \u2202 W j}, x \u2208 \\partial W-0,\\) (5) G(0) := the closed convex cone generated by {g j (x), x \u2208 \\(\\left(\\partial W_{j}\\cap W\\right)\\) - {0}, j = 1, \u00b7 \u00b7 \u00b7 , m}.\n\n "
    ],
    "kosmos": [
      "For a set E, E denotes its interior and E its closure.\n\nFor E \u2286 d, C E [0 , \u221e ) denotes the set of continuous functions from [0 , \u221e ) to E and D E [0 , \u221e ) denotes the set of cadlag functions from [0 , \u221e ) to E.\n\nFor E \u2286 d, P (E) denotes the set of probability measures on E. For a stochastic process Z, {F Z t } denotes the filtration generated by Z, i.e. F Z t := \u03c3\\{Z(s), s \u2264 t\\}.\n\n## 2 Formulation of the problem and preliminaries\n\nLet W \u2286 d be a piecewise smooth cone with vertex at the origin i.e.\n\n\\[\\mathcal{W}:=\\bigcap_{j=1}^{m}\\mathcal{W}_{j},\\quad\\mathcal{W}_{j}:=\\{x=rz, z\\in\\mathcal{S}_{j},\\,r&gt;0\\}, \\tag{1}\\]\n\nwhere \\(\\mathcal{S}_{j}\\) is a nonempty domain in the unit sphere S^d\u22121 with C^2 boundary. Clearly\n\n\\[\\mathcal{W}=\\{x=rz,\\,z\\in\\mathcal{S},\\,r&gt;0\\},\\quad\\text{where }\\mathcal{S}:= \\bigcap_{j=1}^{m}\\mathcal{S}_{j}. \\tag{2}\\]\n\nIt is supposed that\n\n\\[\\mathcal{S}=\\bigcap_{j=1}^{m}\\mathcal{S}_{j}.\\]\n\nThe object of this work is the semimartingale obliquely reflecting Brownian motion (ORBM) in W with drift b, dispersion matrix \u03c3 and radially constant direction of reflection g j on each face, i.e. for x \u2208 \u2202\\mathcal{W}_{j}-\\{0\\},\\,x=rz,\\,z\\in\\partial\\mathcal{S}_{j},\\)\n\n\\[g j (x)=g j (z), \\tag{3}\\]\n\nfor some unit vector field g j defined on \u2202\\mathcal{S}_{j}\\). This process can be defined as the solution of the following stochastic differential equation with reflection:\n\n\\[X(t) =X(0)+b\\,t+\\sigma\\,W(t)+\\int_{0}^{t}\\gamma(s)\\,d\\lambda(s),\\quad t \\geq 0, \\tag{4}\\] \\[\\gamma(t) \\in G(X(t)),\\quad|\\gamma(t)|=1,\\quad d\\lambda-a.e.,\\quad t\\geq 0,\\] \\[X(t) \\in\\overline{W},\\quad\\lambda(t)=\\int_{0}^{t}11_{\\partial\\mathcal{ W}}(X(s))d\\lambda(s),\\quad t\\geq 0,\\]\n\nwhere\n\n\\[G(x):=\\{g:g=\\sum_{j\\in\\mathcal{J}(x)}u_{j}g j(x),\\,u_{j}\\geq 0\\},\\quad\\mathcal{J}(x):=\\{j:\\,x\\in\\partial\\mathcal{W}_{j}\\}, \\quad x\\in\\partial\\mathcal{W}-0,\\]\n\n\\[G(0):=\\text{the closed convex cone generated by}\\,\\,\\{g j(x),\\ x\\in\\big{(} \\partial\\mathcal{W}_{j}\\cap\\overline{W}\\big{)}-\\{0\\},\\ j=1,\\cdots\\,,m\\}.\\] (5) "
    ]
  },
  {
    "edit": [
      "\n\n**Lemma 1.1.** [ 0 ] If \\(G\\) is a graph such that for \\(u,v\\in V(G)\\), \\(uv\\notin E(G)\\), then \\(\\rho(G)&lt;\\rho(G+uv)\\).\n\nLet \\(A\\) be a real symmetric matrix whose rows and columns are indexed by \\(V=\\{1,2,\\cdots,n\\}\\). Let \\(\\{V_{1},V_{2},\\cdots,V_{k}\\}\\) be a partition of \\(V\\) such that the block partition of the matrix \\(A\\) according to \\(\\{V_{1},V_{2},\\cdots,V_{k}\\}\\) can be expressed as\n\n\\[A=\\begin{bmatrix}A_{11}&amp;A_{12}&amp;\\cdots&amp;A_{1k}\\\\ A_{21}&amp;A_{22}&amp;\\cdots&amp;A_{2k}\\\\ \\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots\\\\ A_{k1}&amp;A_{k2}&amp;\\cdots&amp;A_{kk}\\end{bmatrix},\\]\n\nwhere \\(A_{ij}\\) denotes the block formed by intersection of the rows in \\(V_{i}\\) and the columns in \\(V_{j}\\). Let \\(q_{ij}(A)\\) denote the average row sum of \\(A_{ij}\\). Then, the quotient matrix of \\(A\\) with respect to the partition \\(\\{V_{1},V_{2},\\cdots,V_{k}\\}\\) is given by\n\n\\[Q(A)=[q_{ij}(A)].\\]\n\nMoreover, if the row sum of each block \\(A_{ij}\\) is constant then we say that the partition is equitable and \\(Q(A)\\) is called an equitable quotient matrix of \\(A\\). There is a nice relation between the spectrum of \\(A\\) and that of \\(Q(A)\\), which is stated now as a theorem. [0] Let \\(A\\) be a real symmetric matrix such that it has an equitable quotient matrix \\(Q(A)\\), then, \\(\\sigma(Q(A))\\subset\\sigma(Q(A))\\). Moreover, if \\(A\\) is nonnegative, then \\(\\rho(A)=\\rho(Q(A))\\), i.e., the spectral radius of \\(Q(A)\\) is actually the spectral radius of \\(A\\).\n\nA vertex \\(v\\) of a connected graph \\(G\\) is a cut vertex of \\(G\\) if \\(G-v\\) is disconnected. A block of the graph \\(G\\) is a maximal connected subgraph of G that has no cut-vertex. Given two blocks \\(F\\) and \\(H\\) of graph \\(G\\) are said to be adjacent if they are connected via a cut-vertex. We denote \\(F\\oplus H\\), to represent the induced subgraph on the vertex set of two adjacent blocks \\(F\\) and \\(H\\).\n\nA complete graph is a graph where each vertex is adjacent to every other vertex. A complete graph on \\(n\\) vertices is denoted by \\(K_{n}\\). A connected graph is called a clique tree if each of its blocks is a clique. Let \\(G\\) be a clique tree with \\(d_{1}\\) blocks of \\(K_{n_{1}}\\), \\(d_{2}\\) blocks of \\(K_{n_{2}}\\), so on up to \\(d_{b}\\) blocks of \\(K_{n_{b}}\\), then we write \\(G\\) with blocks \\(K_{n_{1}}^{(d_{1})}-K_{n_{2}}^{(d_{2})}-\\cdots-K_{n_{b}}^{(d_{b})}\\) (for example see Figure 1). Here the above notation only gives information about the blocks of \\(G\\), but not about the structure of the graph. But for a special case, if \\(G\\) has a central cut vertex, then all the blocks are adjacent via the central cut vertex and we denote it by\n\n\\[G=K_{n_{1}}^{(d_{1})}\\oplus K_{n_{2}}^{(d_{2})}\\oplus\\cdots\\oplus K_{n_{b}}^{( d_{b})}.\\]\n\nA block \\(H\\) of a clique tree \\(G\\) is a pendent block of \\(G\\) if \\(H\\) has exactly one cut-vertex of \\(G\\). Let \\(v\\) be a cut-vertex of \\(G\\). If \\(G-v\\) consists of two disjoint graphs \\(W_{1}\\) and \\(W_{2}\\) and let \\(G_{i}(i=1,2)\\) be the subgraph of \\(G\\) induced by \\(\\{v\\}\\cup V(W_{i})\\), then \\(G\\) is called the"
    ],
    "kosmos": [
      "\n\n**Lemma 1.1**.: _[_0_]_ _If \\(G\\) is a graph such that for \\(u,v\\in V(G)\\), \\(uv\\notin E(G)\\), then \\(\\rho(G)&lt;\\rho(G+uv)\\)._\n\nLet \\(A\\) be a real symmetric matrix whose rows and columns are indexed by \\(V=\\{1,2,\\cdots,n\\}\\). Let \\(\\{V_{1},V_{2},\\cdots,V_{k}\\}\\) be a partition of \\(V\\) such that the block partition of the matrix \\(A\\) according to \\(\\{V_{1},V_{2},\\cdots,V_{k}\\}\\) can be expressed as\n\n\\[A=\\begin{bmatrix}A_{11}&amp;A_{12}&amp;\\cdots&amp;A_{1k}\\\\ A_{21}&amp;A_{22}&amp;\\cdots&amp;A_{2k}\\\\ \\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots\\\\ A_{k1}&amp;A_{k2}&amp;\\cdots&amp;A_{kk}\\end{bmatrix},\\]\n\nwhere \\(A_{ij}\\) denotes the block formed by intersection of the rows in \\(V_{i}\\) and the columns in \\(V_{j}\\). Let \\(q_{ij}(A)\\) denote the average row sum of \\(A_{ij}\\). Then, the quotient matrix of \\(A\\) with respect to the partition \\(\\{V_{1},V_{2},\\cdots,V_{k}\\}\\) is given by\n\n\\[Q(A)=[q_{ij}(A)].\\]\n\nMoreover, if the row sum of each block \\(A_{ij}\\) is constant then we say that the partition is equitable and \\(Q(A)\\) is called an equitable quotient matrix of \\(A\\). There is a nice relation between the spectrum of \\(A\\) and that of \\(Q(A)\\), which is stated now as a theorem.\n\n**Theorem 1.2**.: _[_0_]_ _Let \\(A\\) be a real symmetric matrix such that it has an equitable quotient matrix \\(Q(A)\\), then, \\(\\sigma(Q(A))\\subset\\sigma(Q(A))\\). Moreover, if \\(A\\) is nonnegative, then \\(\\rho(A)=\\rho(Q(A))\\), i.e., the spectral radius of \\(Q(A)\\) is actually the spectral radius of \\(A\\)._\n\nA vertex \\(v\\) of a connected graph \\(G\\) is a cut vertex of \\(G\\) if \\(G-v\\) is disconnected. A block of the graph \\(G\\) is a maximal connected subgraph of G that has no cut-vertex. Given two blocks \\(F\\) and \\(H\\) of graph \\(G\\) are said to be adjacent if they are connected via a cut-vertex. We denote \\(F\\mathbin{\\otimes}H\\), to represent the induced subgraph on the vertex set of two adjacent blocks \\(F\\) and \\(H\\).\n\nA complete graph is a graph where each vertex is adjacent to every other vertex. A complete graph on \\(n\\) vertices is denoted by \\(K_{n}\\). A connected graph is called a clique tree if each of its blocks is a clique. Let \\(G\\) be a clique tree with \\(d_{1}\\) blocks of \\(K_{n_{1}}\\), \\(d_{2}\\) blocks of \\(K_{n_{2}}\\), so on up to \\(d_{b}\\) blocks of \\(K_{n_{b}}\\), then we write \\(G\\) with blocks \\(K_{n_{1}}^{(d_{1})}-K_{n_{2}}^{(d_{2})}-\\cdots-K_{n_{b}}^{(d_{b})}\\) (for example see Figure 1). Here the above notation only gives information about the blocks of \\(G\\), but not about the structure of the graph. But for a special case, if \\(G\\) has a central cut vertex, then all the blocks are adjacent via the central cut vertex and we denote it by\n\n\\[G=K_{n_{1}}^{(d_{1})}\\mathbin{\\otimes}K_{n_{2}}^{(d_{2})}\\mathbin{\\otimes} \\cdots\\mathbin{\\otimes}K_{n_{b}}^{(d_{b})}.\\]\n\nA block \\(H\\) of a clique tree \\(G\\) is a pendent block of \\(G\\) if \\(H\\) has exactly one cut-vertex of \\(G\\). Let \\(v\\) be a cut-vertex of \\(G\\). If \\(G-v\\) consists of two disjoint graphs \\(W_{1}\\) and \\(W_{2}\\) and let \\(G_{i}(i=1,2)\\"
    ]
  },
  {
    "edit": [
      "where \\(\\rho_{w}\\) is the density of water and \\(C\\) the ocean function that equals one where water is present and zero otherwise. In the case of ice loading, the direct term is given by\n\n\\[\\zeta=\\rho_{i}(1-C)\\Delta I, \\tag{2.12}\\]\n\nwhere \\(\\rho_{i}\\) is the density of ice, and \\(\\Delta I\\) the change in ice thickness. The factor, \\(1-C\\), within this expression accounts for the possibility of floating ice (e.g. Crawford et al. 2018, equations 31\u201334). Consider again a pair of solutions \\((\\mathbf{u},\\phi)\\) and \\((\\mathbf{u}^{\\dagger},\\phi^{\\dagger})\\) of the loading problem associated, respectively, to with loads \\(\\sigma\\) and \\(\\sigma^{\\dagger}\\). Here, however, we assume that these loads are decomposed as in eq.(2.11) into water and direct terms that share a common ocean function. From the above expression for sea level change we can write\n\n\\[\\mathbf{u}\\cdot\\nabla\\Phi+\\phi=-g\\,\\Delta SL+\\Phi_{g},\\quad\\mathbf{u}^{\\dagger} \\cdot\\nabla\\Phi+\\phi^{\\dagger}=-g\\,\\Delta SL^{\\dagger}+\\Phi_{g}^{\\dagger}, \\tag{2.13}\\]\n\nand hence eq.(2.8) becomes\n\n\\[\\int_{\\partial M}(-g\\,\\Delta SL^{\\dagger}+\\Phi_{g}^{\\dagger})\\,\\sigma\\,\\mathrm{d }S=\\int_{\\partial M}(-g\\,\\Delta SL+\\Phi_{g})\\,\\sigma^{\\dagger}\\,\\mathrm{d}S. \\tag{2.14}\\]\n\nThe terms involving the constants \\(\\Phi_{g}\\) and \\(\\Phi_{g}^{\\dagger}\\) vanish due to conservation of mass, and so the identity simplifies to\n\n\\[\\int_{\\partial M}\\Delta SL^{\\dagger}\\,\\sigma\\,\\mathrm{d}S=\\int_{\\partial M}\\Delta SL \\,\\sigma^{\\dagger}\\,\\mathrm{d}S. \\tag{2.15}\\]\n\nIf we now substitute into the equality the decompositions of the loads, \\(\\sigma\\) and \\(\\sigma^{\\dagger}\\), we find\n\n\\[\\int_{\\partial M}\\Delta SL^{\\dagger}(\\rho_{w}C\\,\\Delta SL+\\zeta)\\,\\mathrm{d}S= \\int_{\\partial M}\\Delta SL\\,(\\rho_{w}C\\,\\Delta SL^{\\dagger}+\\zeta^{\\dagger})\\, \\mathrm{d}S, \\tag{2.16}\\]\n\nand cancelling the terms symmetric in \\(\\Delta SL\\) and \\(\\Delta SL^{\\dagger}\\) we arrive at\n\n\\[\\int_{\\partial M}\\Delta SL^{\\dagger}\\,\\zeta\\,\\mathrm{d}S=\\int_{\\partial M}\\Delta SL \\,\\zeta^{\\dagger}\\,\\mathrm{d}S. \\tag{2.17}\\]\n\nAgain, this is a known result, being implied as a special case by the adjoint theory of Crawford et al. (2018) for sea level change in a viscoelastic earth model. What is new is the explicit statement as a reciprocity theorem along with the more elementary derivation that has been facilitated by restricting attention to the elastic fingerprint problem. **Symmetry of the Green\u2019s function**\n\nBecause the fingerprint problem is linear, its solution must take the form\n\n\\[\\Delta SL(\\mathbf{x})=\\int_{\\partial M}G(\\mathbf{x},\\mathbf{x}^{\\prime})\\zeta( \\mathbf{x}^{\\prime})\\,\\mathrm{d}S_{\\mathbf{x}^{\\prime}}, \\tag{2.18}\\]\n\nfor an appropriate Green\u2019s function, where we have added a subscript to the surface element to make clear which variable it is defined with respect to. Suppose that, within eq.(2.17), we take "
    ],
    "kosmos": [
      "where \\(\\rho_{w}\\) is the density of water and \\(C\\) the ocean function that equals one where water is present and zero otherwise. In the case of ice loading, the direct term is given by\n\n\\[\\zeta=\\rho_{i}(1-C)\\Delta I, \\tag{2.12}\\]\n\nwhere \\(\\rho_{i}\\) is the density of ice, and \\(\\Delta I\\) the change in ice thickness. The factor, \\(1-C\\), within this expression accounts for the possibility of floating ice (e.g. Crawford et al. 2018, equations 31-34).\n\nConsider again a pair of solutions \\((\\mathbf{u},\\phi)\\) and \\((\\mathbf{u}^{\\dagger},\\phi^{\\dagger})\\) of the loading problem associated, respectively, to with loads \\(\\sigma\\) and \\(\\sigma^{\\dagger}\\). Here, however, we assume that these loads are decomposed as in eq.(2.11) into water and direct terms that share a common ocean function. From the above expression for sea level change we can write\n\n\\[\\mathbf{u}\\cdot\\nabla\\Phi+\\phi=-g\\,\\Delta SL+\\Phi_{g},\\quad\\mathbf{u}^{\\dagger} \\cdot\\nabla\\Phi+\\phi^{\\dagger}=-g\\,\\Delta SL^{\\dagger}+\\Phi_{g}^{\\dagger}, \\tag{2.13}\\]\n\nand hence eq.(2.8) becomes\n\n\\[\\int_{\\partial M}(-g\\,\\Delta SL^{\\dagger}+\\Phi_{g}^{\\dagger})\\,\\sigma\\,\\mathrm{d }S=\\int_{\\partial M}(-g\\,\\Delta SL+\\Phi_{g})\\,\\sigma^{\\dagger}\\,\\mathrm{d}S. \\tag{2.14}\\]\n\nThe terms involving the constants \\(\\Phi_{g}\\) and \\(\\Phi_{g}^{\\dagger}\\) vanish due to conservation of mass, and so the identity simplifies to\n\n\\[\\int_{\\partial M}\\Delta SL^{\\dagger}\\,\\sigma\\,\\mathrm{d}S=\\int_{\\partial M}\\Delta SL \\,\\sigma^{\\dagger}\\,\\mathrm{d}S. \\tag{2.15}\\]\n\nIf we now substitute into the equality the decompositions of the loads, \\(\\sigma\\) and \\(\\sigma^{\\dagger}\\), we find\n\n\\[\\int_{\\partial M}\\Delta SL^{\\dagger}(\\rho_{w}C\\,\\Delta SL+\\zeta)\\,\\mathrm{d}S= \\int_{\\partial M}\\Delta SL\\,(\\rho_{w}C\\,\\Delta SL^{\\dagger}+\\zeta^{\\dagger})\\, \\mathrm{d}S, \\tag{2.16}\\]\n\nand cancelling the terms symmetric in \\(\\Delta SL\\) and \\(\\Delta SL^{\\dagger}\\) we arrive at\n\n\\[\\int_{\\partial M}\\Delta SL^{\\dagger}\\,\\zeta\\,\\mathrm{d}S=\\int_{\\partial M}\\Delta SL \\,\\zeta^{\\dagger}\\,\\mathrm{d}S. \\tag{2.17}\\]\n\nAgain, this is a known result, being implied as a special case by the adjoint theory of Crawford et al. (2018) for sea level change in a viscoelastic earth model. What is new is the explicit statement as a reciprocity theorem along with the more elementary derivation that has been facilitated by restricting attention to the elastic fingerprint problem.\n\n### Symmetry of the Green's function\n\nBecause the fingerprint problem is linear, its solution must take the form\n\n\\[\\Delta SL(\\mathbf{x})=\\int_{\\partial M}G(\\mathbf{x},\\mathbf{x}^{\\prime})\\zeta( \\mathbf{x}^{\\prime})\\,\\mathrm{d}S_{\\mathbf{x}^{\\prime}}, \\tag{2.18}\\]\n\nfor an appropriate Green's function, where we have added a subscript to the surface element to make clear which variable it is defined with respect to. Suppose that, within eq.(2.17), we take "
    ]
  },
  {
    "edit": [
      "<table>\n<colgroup>\n<col/>\n<col/>\n<col/>\n<col/>\n</colgroup>\n<thead>\n<tr>\n<th>\n</th>\n<th>\n(1)\n</th>\n<th>\n(2)\n</th>\n<th>\n(3)\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>\nNumber of posts\n</td>\n<td>\nNumber of questions\n</td>\n<td>\nWeekday posts\n</td>\n</tr>\n<tr>\n<td>\nStack Overflow \u00d7 Post-GPT\n</td>\n<td>\n-0.170**\n</td>\n<td>\n-0.112+\n</td>\n<td>\n-0.149*\n</td>\n</tr>\n<tr>\n<td></td>\n<td>\n(0.0607)\n</td>\n<td>\n(0.0619)\n</td>\n<td>\n(0.0636)\n</td>\n</tr>\n<tr>\n<td>\nObservations\n</td>\n<td>\n1,150\n</td>\n<td>\n1,150\n</td>\n<td>\n1,150\n</td>\n</tr>\n<tr>\n<td>\nR-squared\n</td>\n<td>\n0.995\n</td>\n<td>\n0.994\n</td>\n<td>\n0.993\n</td>\n</tr>\n<tr>\n<td>\nR-squared within\n</td>\n<td>\n0.290\n</td>\n<td>\n0.315\n</td>\n<td>\n0.232\n</td>\n</tr>\n<tr>\n<td>\nOutcome mean\n</td>\n<td>\n16363\n</td>\n<td>\n7273\n</td>\n<td>\n13191\n</td>\n</tr>\n<tr>\n<td>\nOutcome std. dev.\n</td>\n<td>\n29088\n</td>\n<td>\n12661\n</td>\n<td>\n23685\n</td>\n</tr>\n</tbody>\n</table>\n\nTable 1: Results of a difference-in-differences model, estimating the change in activity observed weekly on Stack Overflow following the release of ChatGPT, relative to activity on four other platforms less likely to have been impacted. All regressions comprise platform fixed effects, week fixed effects, and platform-specific linear time-trends. The standard-error of the estimate clustered on month is reported in parentheses. Significance codes: ***: p < 0.001, ***: p < 0.01, *: p < 0.05, +: p < 0.1.\nTable 1: Results of a difference-in-differences model, estimating the change in activity observed weekly on Stack Overflow following the release of ChatGPT, relative to activity on four other platforms less likely to have been impacted. All regressions comprise platform fixed effects, week fixed effects, and platform-specific linear time-trends. The standard-error of the estimate clustered on month is reported in parentheses. Significance codes: ***: p < 0.001, ***: p < 0.01, *: p < 0.05, +: p < 0.1.\nFigure 2 shows that the impact of ChatGPT is increasing over time and is by the end of our study greater in magnitude than the average post-ChatGPT effect estimated in Table 1. By the end of April 2023, the estimated effect stabilizes at around 25%. Interestingly, ChatGPT use, in general, peaked around this time.3\n\ndecline in activity on Stack Overflow.\n\nWe report the estimated effect of our difference-in-differences model in Table 1 and visualize the weekly estimates of the relative change in the Stack Overflow activity in Figure 2. Table 1 indicates that ChatGPT decreased posting activity on Stack Overflow by 15.6% (1 \u2212 e \u22120 . 17 ). These results are robust to changes in the controls and starting point of the data time series. We also tested for heterogeneity in subsets of the data: considering only questions (rather than counting both questions and answers) and posts on weekdays. In both subsets our estimates did not deviate significantly from the main result: we estimate a 12% relative decrease in questions and 14% relative decrease in posts on weekdays.\n\nFigure 2\n\n"
    ],
    "kosmos": [
      "<table>\n<colgroup>\n<col/>\n<col/>\n<col/>\n<col/>\n</colgroup>\n<thead>\n<tr>\n<th>\n</th>\n<th>\n(1)\n</th>\n<th>\n(2)\n</th>\n<th>\n(3)\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td>\nNumber of posts\n</td>\n<td>\nNumber of questions\n</td>\n<td>\nWeekday posts\n</td>\n</tr>\n<tr>\n<td>\nStack Overflow \u00d7 Post-GPT\n</td>\n<td>\n-0.170**\n</td>\n<td>\n-0.112+\n</td>\n<td>\n-0.149*\n</td>\n</tr>\n<tr>\n<td></td>\n<td>\n(0.0607)\n</td>\n<td>\n(0.0619)\n</td>\n<td>\n(0.0636)\n</td>\n</tr>\n<tr>\n<td>\nObservations\n</td>\n<td>\n1,150\n</td>\n<td>\n1,150\n</td>\n<td>\n1,150\n</td>\n</tr>\n<tr>\n<td>\nR-squared\n</td>\n<td>\n0.995\n</td>\n<td>\n0.994\n</td>\n<td>\n0.993\n</td>\n</tr>\n<tr>\n<td>\nR-squared within\n</td>\n<td>\n0.290\n</td>\n<td>\n0.315\n</td>\n<td>\n0.232\n</td>\n</tr>\n<tr>\n<td>\nOutcome mean\n</td>\n<td>\n16363\n</td>\n<td>\n7273\n</td>\n<td>\n13191\n</td>\n</tr>\n<tr>\n<td>\nOutcome std. dev.\n</td>\n<td>\n29088\n</td>\n<td>\n12661\n</td>\n<td>\n23685\n</td>\n</tr>\n</tbody>\n</table>\n\nTable 1: Results of a difference-in-differences model, estimating the change in activity observed weekly on Stack Overflow following the release of ChatGPT, relative to activity on four other platforms less likely to have been impacted. All regressions comprise platform fixed effects, week fixed effects, and platform-specific linear time-trends. The standard-error of the estimate clustered on month is reported in parentheses. Significance codes: ***: p < 0.001, ***: p < 0.01, *: p < 0.05, +: p < 0.1.\nTable 1: Results of a difference-in-differences model, estimating the change in activity observed weekly on Stack Overflow following the release of ChatGPT, relative to activity on four other platforms less likely to have been impacted. All regressions comprise platform fixed effects, week fixed effects, and platform-specific linear time-trends. The standard-error of the estimate clustered on month is reported in parentheses. Significance codes: ***: p < 0.001, ***: p < 0.01, *: p < 0.05, +: p < 0.1.\nFigure 2 shows that the impact of ChatGPT is increasing over time and is by the end of our study greater in magnitude than the average post-ChatGPT effect estimated in Table 1. By the end of April 2023, the estimated effect stabilizes at around 25%. Interestingly, ChatGPT use, in general, peaked around this time.3\n\ndecline in activity on Stack Overflow.\n\nWe report the estimated effect of our difference-in-differences model in Table 1 and visualize the weekly estimates of the relative change in the Stack Overflow activity in Figure 2. Table 1 indicates that ChatGPT decreased posting activity on Stack Overflow by 15.6% (1 \u2212 e \u22120 . 17 ). These results are robust to changes in the controls and starting point of the data time series. We also tested for heterogeneity in subsets of the data: considering only questions (rather than counting both questions and answers) and posts on weekdays. In both subsets our estimates did not deviate significantly from the main result: we estimate a 12% relative decrease in questions and 14% relative decrease in posts on weekdays.\n\nFigure 2\n\n"
    ]
  },
  {
    "edit": [
      "Astronomy &amp; Astrophysics manuscript no. 46892corr\n\nMay 26, 2024\n\nA large topographic feature on the surface of the trans-Neptunian object (307261) 2002 MS4 measured from stellar occultations\n\nF. L. Rommel 3, 2, 3, F. Braga-Ribas , 2, 2, J. L. Ortiz 4, B. Sicardy 5, P. Santos-Sanz 4, J. Desmars 6 , 7, J. I. B. Camargo 2 , R. Vieira-Martins 2, 2, M. Assafin 8 , 2, B. E. Morgado , 1, 2, R. C. Boufleur 1, 2, G. Benedetti-Rossi 9, 5, 2, A. R. Gomes-J\u00fanior , 2, 3, 2, E. Fern\u00e1ndez-Valenzuela 11, B. J. Holler 12, D. Souami , 13, 14 , R. Du ff ard 4 , G. Margoti 2, 2, M. Vara-Lubiano 4, J. Lecacheux 4, J. L. Plouvier 15, N. Morales 4, A. Maury 16, J. Fabrega 17, P. Cerasolo 18, E. Jehin 19, D. Albanese 20, H. Mariey 21, S. Cikota 22, 23, D. Ru\u017edjak 24, A. Cikota 25, R. Szak\u00e1ts 26, 27, D. Baba Aissa 28, Z. Gringahcene 28, V. Kashuba 29, N. Koshkin 29, V. Zhukov 30, S. Fisek 31, 32, O. \u00c7ak\u0131r 33, 34, S. \u00d6zer 35, 36, M. Schnabel , 37, 38, M. Schnabel , 38, J. Signoret 39, L. Morrone 40, 41, T. Santana-Ros 42, 43, C. L. Pereira , 2, 2, 3, M. Emilio , 44, 1, 3, A. Y. Burdanov 45, D. de Wit 45, J. Barkaoui 46, 45, 47, M. Gillon 46, G. Leto 48, A. Frasca 48, G. Catanzaro 48, R. Zanmar Sanchez 48, U. Tagliaferr 49, M. Di Sora 49, G. Isopi 49, Y. Krugly 50, 51, I. Slyusarev 50, V. Chiorony 50, H. Miku\u017e 52, 53, B. Bacci 54, M. Maestripieri 54, M. D. Grazia 54, I. de la Cueva 55, M. Yuste-Moreno 55, F. Ciabattari 56, O. M. Kozhukhov 57, M. Serra-Ricart 47, 58, M. R. Alarcon 47, 58, J. Licandro 47, 58, G. Masi 59, R. Bacci 60, J. M. Bosch 61, R. Behem 62, J.-P. Prost 62, S. Renner 63, M. Conjat 21, M. Bachini 63, G. Succi 64, L. Stoian 65, A. Juravle 65, D. Carosati 66, B. Gowe 67, J. Carrillo 68, A. P. Zheleznyak 50, N. Montigiani 69, C. R. Foster 70, M.Mannucci 69, N. Ruocco 71, F. Cuevas 72, P. Di Marcantonio 73, I. Coretti 73, G. Iafrate 73, S. Baldini 73, M. Collins 74, A. Pal 26, B. Cs\u00e1k 26, E. Fern\u00e1ndez-Garcia 4, A. J. Castro-Tirado 4, L. Hudin 75, J. M. Madiedo 4, R. M. Anghel 76, J. F. Calvo-Fern\u00e1ndez 77 "
    ],
    "kosmos": [
      "Astronomy &amp; Astrophysics manuscript no. 46892corr\n\nMay 26, 2024\n\nA large topographic feature on the surface of the trans-Neptunian object (307261) 2002 MS4 measured from stellar occultations\n\nF. L. Rommel1,2,3, F. Braga-Ribas3,1,2, J. L. Ortiz4, B. Sicardy5, P. Santos-Sanz4, J. Desmars6,7, J. I. B. Camargo1,2, R. Vieira-Martins1,2, M. Assafin8,2, B. E. Morgado8,2,1, R. C. Boufleur1,2, G. Benedetti-Rossi9,5,2,\n\nA. R. Gomes-J\u00fanior10,3,2, E. Fern\u00e1ndez-Valenzuela11, B. J. Holler12, D. Souami3,13,14\\*, R. Duffard4, G. Margot3,2, M. Vara-Lubiano4, J. Lecacheux3, J. L. Plouvier15, N. Morales4, A. Maury16, J. Fabrega17, P. Ceravolo18, E. Jehin19, D. Albanese20, H. Mariey21, S. Cikota22,23, D. Ru\u017edjak24, A. Cikota25, R. Szak\u00e1ts26,27, D. Baba Aissa28, Z. Gringahcene28, V. Kashuba29, N. Koshkin29, V. Zhukov30, S. Fisek31,32, O. \u00c7ak\u0131r33,34, S. \u00d6zer35,36, M. Schnabel37,38, M. Schnabel38, J. Signoret39, L. Morrone40,41, T. Santana-Ros42,43, C. L. Pereira1,2,43, M. Emilio44,1,3, A. Y. Burdanov45, D. de Wit45, B. Barkaoui46,45,47, M. Gillon48, G. Leto48, A. Frasca48, G. Catanzaro48, R. Zanmar Sanchez48, U. Tagliaferrr49, M. Di Sora49, G. Isopi49, Y. Krugly50,51, I. Slyusarev50, V. Chiorony50, H. Miku\u017e52,53, B. Bacci54, M. Maestripieri54, M. D. Grazia54, I. de la Cueva55, M. Yuste-Moreno55, F. Ciabattari56, O. M. Kozhukhov57, M. Serra-Ricart47,58, M. R. Alarcon47,58, J. Licandro47,58, G. Masi59, R. Bacci60, J. M. Bosch61, R. Behem62, J.-P. Prost62, S. Renner63,64, M. Conjat21, M. Bachini64, G. Succi64, L. Stoian65, A. Juravle65, D. Carosati66, B. Gowe67, J. Carrillo68, A. P. Zheleznyak50, N. Montigiani69, C. R. Foster70, M.Mannucci69, N. Ruocco71, F. Cuevas72, P. Di Marcantonio73, I. Coretti73, G. Iafrate73, S. Baldini73, M. Collins74, A. Pal26, B. Cs\u00e1k26, E. Fern\u00e1ndez-Garcia4, A. J. Castro-Tirado4, L. Hudin75, J. M. Madiedo4, R. M. Anghel76, J. F. Calvo-Fern\u00e1ndez77, A. Valvasori78,79, E. Guido80, R. M. Gherase81, S. Kamoun82,83, R. Fafet62, M. S\u00e1nchez-Gonz\u00e1lez84,85, L. Curelaru86, C. D. Vintdevarg87, C. A. Danescu88, J.-F. Gout89, C. J. Schmitz90, A. Sota4, I. Belskaya5,50, M. Rodr\u00edguez-Marco91, Z. Kilic92,93, E. Frappa94, "
    ]
  },
  {
    "edit": [
      "Proposition 5.1 characterizes the slow dynamics of an N -spike quasi-equilibrium solution on the long time-scale. We remark that this time-scale is longer than the time-scale of slow spike dynamics for the GM and Gray-Scott models ([ 23 ], [ 8 ], [ 12 ]), where there are no chemotactic e ff ects. In Appendix H , we show that , as given in ( 5.17 ), can be calculated asymptotically by retaining only the contribution from the sub-inner solution. In particular, in Appendix we provide the leading order estimate\n\nMoreover, in Appendix H we show at the steady-state spike locations that , with given in ( 4.28 ). To illustrate our results, we now compare the dynamics computed from the DAE system ( 5.24 ) with corresponding numerical results computed from the full PDE system ( 1.2 ) using FLEXPDE7 [ 14 ]. In our comparison, we computed the integrals defining numerically from ( 5.17 ). The results for a oneand two-spike dynamics are shown in Figure 8 for the parameter values in the figure caption. In Figure 8a , where we chose the initial condition , the asymptotic and numerical spike trajectories are favorably compared for a one-spike quasi-equilibrium pattern. In Figure 8b a similar favorable comparison is shown for the case of two-spike dynamics starting from the initial condition and .\n\n### Computation of Jacobian Matrix for Balancing Conditions\n\nIn this subsection, and as remarked in \u00a7 4 , we show that when the matrix in ( 4.29 ) arises from the linearization of the DAE dynamics ( 5.24 ) in Proposition 5.1 about the steady-state spike locations. Our approach below is inspired by a related analysis for the GM model in [ 58 ]. To this end, we use the Green\u2019s function in ( 2.24 ) together with its decomposition in ( 5.20 ) to define\n\n(5.27) "
    ],
    "kosmos": [
      "Proposition 5.1 characterizes the slow dynamics of an N -spike quasi-equilibrium solution on the long time-scale. We remark that this time-scale is longer than the time-scale of slow spike dynamics for the GM and Gray-Scott models ([ 23 ], [ 8 ], [ 12 ]), where there are no chemotactic effects.\n\nIn Appendix H, we show that , as given in (5.17), can be calculated asymptotically by retaining only the contribution from the sub-inner solution. In particular, in Appendix H we provide the leading order estimate\n\nMoreover, in Appendix H we show at the steady-state spike locations that , with given in (4.28).\n\nTo illustrate our results, we now compare the dynamics computed from the DAE system (5.24) with corresponding numerical results computed from the full PDE system (1.2) using FLEXPDE7 [ 14 ]. In our comparison, we computed the integrals defining numerically from (5.17). The results for a one- and two-spike dynamics are shown in Figure 8 for the parameter values in the figure caption. In Figure 8a, where we chose the initial condition , the asymptotic and numerical spike trajectories are favorably compared for a one-spike quasi-equilibrium pattern. In Figure 8b a similar favorable comparison is shown for the case of two-spike dynamics starting from the initial condition and .\n\n### Computation of Jacobian Matrix for Balancing Conditions\n\nIn this subsection, and as remarked in SS4, we show that when is from the linearization of the DAE dynamics (5.24) in Proposition 5.1 about the steady-state spike locations. Our approach below is inspired by a related analysis for the GM model in [ 58 ].\n\nTo this end, we use the Green's function in (2.24) together with its decomposition in (5.20) to define\n\n(5.27) "
    ]
  },
  {
    "edit": [
      "<table>\n<thead>\n<tr>\n<th>\n</th>\n<th>\nModels A and B\n</th>\n<th>\nSW [ 13 ]\n</th>\n<th>\nExperimental [ 78 ]\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\n$F_{V_{0}}^{1/2}$\n</td>\n<td>\n276\n</td>\n<td>\n261\n</td>\n<td>\n346 . 2 \u00b1 1 . 4\n</td>\n</tr>\n<tr>\n<td>\n$F_{V_{1}}^{1/2}$\n</td>\n<td>\n341\n</td>\n<td></td>\n<td>\n433 \u00b1 13\n</td>\n</tr>\n<tr>\n<td>\n$F_{V_{2}}^{1/2}$\n</td>\n<td>\n384\n</td>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n\nTable 6 . The decay constants of the vector mesons (in MeV) obtained in our work (same result for models A and B), compared against the result obtained using the soft wall model (SW) [ 13 ]. In order to compare with experimental results of [ 78 ], we need to identify \\(F_{V}\\) with \\(g_{ \u03c1}\\).\n\nwhere \\(S_{n}(u)=e \u2212 B S \u03c8 s n (u)\\). To get the normalization constant we first consider the asymptotic expansion of the normalizable solution, \\(S_{n}(u)\\), close to the boundary, \\(S_{n}(u)=N s n u 3 + \u00b7 \u00b7 \u00b7\\), where \\(N s n\\) is the normalization constant, which is obtained by plugging \\(S_{n}(u)\\) in ( 4.3 ). Then decay constants of the scalar mesons are given by the following dictionary [ 40 ]:\n\n\\[F_{s n} = \\zeta\\,u\\,e^{3 A s \u2212 \u03a6}\\partial_{u}S_{n}\\Bigg{|}_{u=\\epsilon}=3 \\frac{\\sqrt{N_{c}}}{2\\pi}\\,N_{s n}. \\tag{4.4}\\]\n\nWe first investigate the behavior of the scalar meson decay constants as a function of the parameter \\(a_{0}\\) in the chiral limit with the other parameters fixed as in model A. Our numerical results are displayed on the left panel of Fig. 14 . As can be seen from the plot, the results show a smooth behavior of the decay constants in the region of interest, i.e., \\(a_{0}&lt;a c 0\\), where \\(a c 0\\approx 0 . 0974\\). However, the behavior of the decay constant increases close to \\(a c 0\\) for the ground state while it decreases for the scalar resonances.\n\nIn addition, we calculate the scalar meson decay constants as a function of the quark mass. Our numerical results are displayed on the right panel of Fig. 14 . As can be seen from the plot, the decay constant increases in the region of small quark mass until it reaches some maximum value and then decreases when the quark mass grows. Finally, we calculate the decay constant using the final set of parameters for models A and B displayed in Table 1 .\n\n "
    ],
    "kosmos": [
      "<table>\n<thead>\n<tr>\n<th>\n</th>\n<th>\nModels A and B\n</th>\n<th>\nSW [ 13 ]\n</th>\n<th>\nExperimental [ 78 ]\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\n$F_{V_{0}}^{1/2}$\n</td>\n<td>\n276\n</td>\n<td>\n261\n</td>\n<td>\n346 . 2 \u00b1 1 . 4\n</td>\n</tr>\n<tr>\n<td>\n$F_{V_{1}}^{1/2}$\n</td>\n<td>\n341\n</td>\n<td></td>\n<td>\n433 \u00b1 13\n</td>\n</tr>\n<tr>\n<td>\n$F_{V_{2}}^{1/2}$\n</td>\n<td>\n384\n</td>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n\nTable 6 . The decay constants of the vector mesons (in MeV) obtained in our work (same result for models A and B), compared against the result obtained using the soft wall model (SW) [ 13 ]. In order to compare with experimental results of [ 78 ], we need to identify \\(F_{V}\\) with \\(g_{ \u03c1}\\).\n\nwhere \\(S_{n}(u)=e \u2212 B S \u03c8 s n (u)\\). To get the normalization constant we first consider the asymptotic expansion of the normalizable solution, \\(S_{n}(u)\\), close to the boundary, \\(S_{n}(u)=N s n u 3 + \u00b7 \u00b7 \u00b7\\), where \\(N s n\\) is the normalization constant, which is obtained by plugging \\(S_{n}(u)\\) in ( 4.3 ). Then decay constants of the scalar mesons are given by the following dictionary [ 40 ]\n\n\\[F_{s n} = \\zeta\\,u\\,e^{3 A s \u2212 \u03a6}\\partial_{u}S_{n}\\Bigg{|}_{u=\\epsilon}=3 \\frac{\\sqrt{N_{c}}}{2\\pi}\\,N_{s n}. \\tag{4.4}\\]\n\nWe first investigate the behavior of the scalar meson decay constants as a function of the parameter \\(a_{0}\\) in the chiral limit with the other parameters fixed as in model A. Our numerical results are displayed on the left panel of Fig. 14 . As can be seen from the plot, the results show a smooth behavior of the decay constants in the region of interest, i.e., \\(a_{0}&lt;a c 0\\), where \\(a c 0\\approx 0 . 0974\\). However, the behavior of the decay constant increases close to \\(a c 0\\) for the ground state while it decreases for the scalar resonances.\n\nIn addition, we calculate the scalar meson decay constants as a function of the quark mass. Our numerical results are displayed on the right panel of Fig. 14 . As can be seen from the plot, the decay constant increases in the region of small quark mass until it reaches some maximum value and then decreases when the quark mass grows. Finally, we calculate the decay constant using the final set of parameters for models A and B displayed in Table 1 .\n\n "
    ]
  },
  {
    "edit": [
      "Conference'17, July 2017, Washington, DC, USA 1\n\ndetection methods [9, 15, 29, 37] only focus on coping with presentation attacks, which construct artificial fingers [34] to circumvent the fingerprint recognition system. To model the physiological differences between the artificial and real fingers, existing liveness detection methods usually rely on collecting the pulse rate, skin odor, finger elasticity, etc., from the sensor of the fingerprint recognition system. Unfortunately, the replacement attack, i.e., replacing the real fingerprint images with the GAN-generated fingerprint images, usually happens after the fingerprint collection step and the fake fingerprint images are generated without the defects of artificial fingers. Besides, the GAN-generated fingerprint images also exist some generation artifacts, which are induced by specific processing operations in GANs and does not exist in the captured impressions. Under such circumstance, the existing fingerprint liveness detection methods can hardly be directly applied to detect the GAN-generated fingerprint images.\n\nTo efficiently and effectively identify the GAN-generated fingerprint images with decent robustness against the existing anti-forensic method [11], in this paper, we propose a **R**obust deep **F**orgery **D**etection method **for** GAN-generated **Fin**gerprint images (RFDforFin). To take full advantage of the fingerprint characteristics and the generation artifacts of the fake fingerprint images, we construct a lightweight yet robust two-stream neural network, by exploiting the unique features of the fingerprint images and generation artifacts in frequency domain. Considering the impact of sweat on grayscale variations along the ridges of fingerprints [10], we construct a special ridge stream which utilizes this unique fingerprint characteristic. In the generation artifact stream, inspired by [13], which discovers the obvious generation artifacts in the Discrete Cosine Transform (DCT) frequency domain, we transform the input fingerprint image into different frequency domains, analyze the generation artifacts in these spectrum, and construct a simple yet effective convolutional neural network to learn more robust generation artifacts between the generated and real images from the FFT frequency spectrum. To simultaneously utilize the features extracted from the ridges and generation artifacts in the final prediction, we build a simple yet effective fusion module. Since typical CNNs can learn certain frequency information from the input image or its frequency transformed spectrum, by utilizing the unique 1D ridge features jointly with the 2D generation artifact features, our method can avoid overfitting to certain frequency information, which can improve the robustness of our method.\n\nOur main contributions are summarized as follows:\n\n* We propose the first deep forgery detection method for GAN-generated fingerprint images, by jointly exploiting the unique 1D ridge features and 2D generation artifact features via a lightweight two-stream neural network to ensure the robustness and efficiency of the proposed work.\n* We propose to exploit fingerprint-related characteristic and construct a ridge stream, which exploits the grayscale variations along the ridges. With this ridge stream, our method can avoid overfitting to certain frequency information, which can be easily interfered by existing anti-forensic method [11] and thus improves the overall robustness.\n* We analyze the frequency spectrum of the real and generated fingerprint images and construct a simple yet effective generation artifact stream, i.e., a shallow convolutional neural network, to extract frequency-domain inconsistencies.\n* Comprehensive experiments demonstrate that our method is effective, efficient, and robust to the anti-forensic method [11].\n\n## 2 Related Work\n\n### Fingerprint Image Synthesis\n\nSeveral recent methods have been proposed for generating realistic fingerprint images automatically [3, 4, 12, 17, 35, 39, 44]. [3] and [35] combine a convolution autoencoder (CAE) and a GANbased method (, DCGAN [38], WGAN [18]) directly. [4] presents a GAN-based pipeline followed by a stochastic search algorithm over the latent variable space, to search for suitable latent variable and generate DeepMasterPrints which are synthetic fingerprints and can be matched against a large number of fingerprints. These techniques can be considered as single-staged architectures, whose input is a random vector and output is a generated image.\n\nTo make the synthetic results more realistic and obtain multiple impressions for a virtual identity, many multi-staged methods [12, 17, 39, 44] firstly synthesize a binary masterprint which defines a ridge structure and represents a new identity. After nonlinear distortion and cropping, which simulate various pressures and different contact regions between the finger and the fingerprint sensor, the distorted masterprint is fed into another generative neural network to generate a realistic fingerprint image. The differences between these multi-staged methods lie in the way of the masterprint generation scheme, the distortion simulation method, and the final impression (fingerprint) generation approach. [12] utilizes the GAN-based methods (, BigGAN [5]) in these three steps. [17] attempts to employ conventional rotation instead of the deep learning method in the distortion module. [39] adopts StyleGAN2 "
    ],
    "kosmos": [
      "Conference'17, July 2017, Washington, DC, USA 1\n\ndetection methods [9, 15, 29, 37] only focus on coping with presentation attacks, which construct artificial fingers [34] to circumvent the fingerprint recognition system. To model the physiological differences between the artificial and real fingers, existing liveness detection methods usually rely on collecting the pulse rate, skin odor, finger elasticity, etc., from the sensor of the fingerprint recognition system. Unfortunately, the replacement attack, i.e., replacing the real fingerprint images with the GAN-generated fingerprint images, usually happens after the fingerprint collection step and the fake fingerprint images are generated without the defects of artificial fingers. Besides, the GAN-generated fingerprint images also exist some generation artifacts, which are induced by specific processing operations in GANs and does not exist in the captured impressions. Under such circumstance, the existing fingerprint liveness detection methods can hardly be directly applied to detect the GAN-generated fingerprint images.\n\nTo efficiently and effectively identify the GAN-generated fingerprint images with decent robustness against the existing anti-forensic method [11], in this paper, we propose a **R**obust deep **F**orgery **D**etection method **for** GAN-generated **Fin**gerprint images (RFDforFin). To take full advantage of the fingerprint characteristics and the generation artifacts of the fake fingerprint images, we construct a lightweight yet robust two-stream neural network, by exploiting the unique features of the fingerprint images and generation artifacts in frequency domain. Considering the impact of sweat on grayscale variations along the ridges of fingerprints [10], we construct a special ridge stream which utilizes this unique fingerprint characteristic. In the generation artifact stream, inspired by [13], which discovers the obvious generation artifacts in the Discrete Cosine Transform (DCT) frequency domain, we transform the input fingerprint image into different frequency domains, analyze the generation artifacts in these spectrum, and construct a simple yet effective convolutional neural network to learn more robust generation artifacts between the generated and real images from the FFT frequency spectrum. To simultaneously utilize the features extracted from the ridges and generation artifacts in the final prediction, we build a simple yet effective fusion module. Since typical CNNs can learn certain frequency information from the input image or its frequency transformed spectrum, by utilizing the unique 1D ridge features jointly with the 2D generation artifact features, our method can avoid overfitting to certain frequency information, which can improve the robustness of our method.\n\nOur main contributions are summarized as follows:\n\n* We propose the first deep forgery detection method for GAN-generated fingerprint images, by jointly exploiting the unique 1D ridge features and 2D generation artifact features via a lightweight two-stream neural network to ensure the robustness and efficiency of the proposed work.\n* We propose to exploit fingerprint-related characteristic and construct a ridge stream, which exploits the grayscale variations along the ridges. With this ridge stream, our method can avoid overfitting to certain frequency information, which can be easily interfered by existing anti-forensic method [11] and thus improves the overall robustness.\n* We analyze the frequency spectrum of the real and generated fingerprint images and construct a simple yet effective generation artifact stream, i.e., a shallow convolutional neural network, to extract frequency-domain inconsistencies.\n* Comprehensive experiments demonstrate that our method is effective, efficient, and robust to the anti-forensic method [11].\n\n## 2 Related Work\n\n### Fingerprint Image Synthesis\n\nSeveral recent methods have been proposed for generating realistic fingerprint images automatically [3, 4, 12, 17, 35, 39, 44]. [3] and [35] combine a convolution autoencoder (CAE) and a GANbased method (, DCGAN [38], WGAN [18]) directly. [4] presents a GAN-based pipeline followed by a stochastic search algorithm over the latent variable space, to search for suitable latent variable and generate DeepMasterPrints which are synthetic fingerprints and can be matched against a large number of fingerprints. These techniques can be considered as single-staged architectures, whose input is a random vector and output is a generated image.\n\nTo make the synthetic results more realistic and obtain multiple impressions for a virtual identity, many multi-staged methods [12, 17, 39, 44] firstly synthesize a binary masterprint which defines a ridge structure and represents a new identity. After nonlinear distortion and cropping, which simulate various pressures and different contact regions between the finger and the fingerprint sensor, the distorted masterprint is fed into another generative neural network to generate a realistic fingerprint image. The differences between these multi-staged methods lie in the way of the masterprint generation scheme, the distortion simulation method, and the final impression (fingerprint) generation approach. [12] utilizes the GAN-based methods (, BigGAN [5]) in these three steps. [17] attempts to employ conventional rotation instead of the deep learning method in the distortion module. [39] adopts StyleGAN2 "
    ]
  },
  {
    "edit": [
      "_Remark 2.1_.:\n1. It should be noted that for any potential \\(u\\), the eigenvalues \\((\\nu_{n})\\) of \\(L_{u}\\) cannot be all simple. For instance, take \\(u(x)=\\mathrm{e}^{ix}\\) , one can easily check that for \\(L_{u}=D-T_{u}T_{u}\\) , \\(L_{u}1=L_{u}\\mathrm{e}^{ix}=0\\) .\n2. Inequality ( 2.7 ) implies that as \\(n&gt;&gt;0\\), the lower bound of the distance between two consecutive eigenvalues \\(\\nu_{n}\\) gets closer to \\(1\\) .\n\nProof.: All the presented inequalities are a direct consequence of the max-min principle\n\n\\[\\lambda_{n} =\\max_{F\\subseteq L_{+}^{2}}\\min\\left\\{\\left\\langle\\tilde{L}_{u }h\\mid h\\right\\rangle\\ ;\\ h\\in F^{\\perp}\\cap H^{\\frac{1}{2}}_{+}(T)\\ ,\\ \\|h\\|_{L^{2}}=1\\right\\}\\] \\[\\nu_{n} =\\max_{F\\subseteq L_{+}^{2}}\\min\\left\\{\\left\\langle L_{u}h\\mid h \\right\\rangle\\ ;\\ h\\in F^{\\perp}\\cap H^{\\frac{1}{2}}_{+}(T)\\ ,\\ \\|h\\|_{L^{2}}=1 \\right\\}\\ .\\]\n\nSpectrum of \\(\\tilde{L}_{u}\\) . Let \\(F\\) be any subspace of \\(L_{+}^{2}(T)\\) of dimension \\(n\\) , and consider \\(E:=\\mathbb{C}1\\oplus S(F)\\) , where \\(S\\) is the shift operator, then\n\n\\[\\lambda_{n+1} \\geq\\min\\{\\langle\\tilde{L}_{u}h\\mid h\\rangle\\ ;\\ \\|h\\|_{L^{2}}=1 ,\\ h\\in E^{\\perp}\\cap H^{\\frac{1}{2}}_{+}\\}\\]\n\nObserve that \\(E^{\\perp}=S\\left(F^{\\perp}\\right)\\) , thus by ( 2.2 ) ,\n\n\\[\\lambda_{n+1} \\geq\\min\\left\\{\\langle\\tilde{L}_{u}g\\mid g\\rangle+1+\\left|\\langle Sg \\mid u\\rangle\\right|^{2}\\ ;\\ \\|g\\|_{L^{2}}=1,\\ g\\in F^{\\perp}\\cap H^{\\frac{1}{2 }}_{+}\\right\\}\\ .\\]\n\nIn addition, since \\(|\\langle Sg\\mid u\\rangle|^{2}\\geq 0\\) , we infer for all \\(n\\in\\mathbb{N}_{\\geq 0}\\) ,\n\n\\[\\lambda_{n+1} \\geq\\lambda_{n}+1\\ .\\]\n\nSpectrum of \\(L_{u}\\) -Inequality ( 2.6 ) . let \\(F\\) be any subspace of \\(L_{+}^{2}(T)\\) of dimension \\(n\\), and take \\(G:=\\mathbb{C}1\\oplus S(F)+\\mathbb{C}u\\) . Then,\n\n\\[\\nu_{n+2}(u) \\geq\\min\\{\\left\\langle L_{u}h\\mid h\\right\\rangle\\ ;\\ \\|h\\|_{L^{2} }=1,\\ h\\in G^{\\perp}\\cap H^{\\frac{1}{2}}_{+}\\}\\,.\\]\n\nSince \\(G^{\\perp}=S\\left(F^{\\perp}\\cap(S^{*}u)^{\\perp}\\right)\\), then\n\n\\[\\nu_{n+2} \\geq\\min\\left\\{\\left\\langle L_{u}Sg\\mid Sg\\right\\rangle\\ ;\\ \\|g\\|_ {L^{2}}=1,\\ g\\in F^{\\perp}\\cap(S^{*}u)^{\\perp}\\cap H^{\\frac{1}{2}}_{+}(T)\\right\\}\\,.\\]\n\nNote that \\(g\\perp S^{*}u\\), then by ( 2.2 ),\n\n\\[\\nu_{n+2} \\geq\\min\\left\\{\\left\\langle L_{u}g\\mid g\\rangle+1;\\ \\|g\\|_{L^{2} }=1,\\ g\\in "
    ],
    "kosmos": [
      "_Remark 2.1_.:\n1. It should be noted that for any potential \\(u\\), the eigenvalues \\((\\nu_{n})\\) of \\(L_{u}\\) cannot be all simple. For instance, take \\(u(x)=\\mathrm{e}^{ix}\\) , one can easily check that for \\(L_{u}=D-T_{u}T_{u}\\) , \\(L_{u}1=L_{u}\\mathrm{e}^{ix}=0\\) .\n2. Inequality ( 2.7 ) implies that as \\(n&gt;&gt;0\\), the lower bound of the distance between two consecutive eigenvalues \\(\\nu_{n}\\) gets closer to \\(1\\) .\n\nProof.: All the presented inequalities are a direct consequence of the max-min principle\n\n\\[\\lambda_{n} =\\max_{F\\subseteq L_{+}^{2}}\\min\\left\\{\\left\\langle\\tilde{L}_{u }h\\mid h\\right\\rangle\\ ;\\ h\\in F^{\\perp}\\cap H^{\\frac{1}{2}}_{+}(T)\\ ,\\ \\|h\\|_{L^{2}}=1\\right\\}\\] \\[\\nu_{n} =\\max_{F\\subseteq L_{+}^{2}}\\min\\left\\{\\left\\langle L_{u}h\\mid h \\right\\rangle\\ ;\\ h\\in F^{\\perp}\\cap H^{\\frac{1}{2}}_{+}(T)\\ ,\\ \\|h\\|_{L^{2}}=1 \\right\\}\\ .\\]\n\nSpectrum of \\(\\tilde{L}_{u}\\) . Let \\(F\\) be any subspace of \\(L_{+}^{2}(T)\\) of dimension \\(n\\) , and consider \\(E:=\\mathbb{C}1\\oplus S(F)\\) , where \\(S\\) is the shift operator, then\n\n\\[\\lambda_{n+1} \\geq\\min\\{\\langle\\tilde{L}_{u}h\\mid h\\rangle\\ ;\\ \\|h\\|_{L^{2}}=1 ,\\ h\\in E^{\\perp}\\cap H^{\\frac{1}{2}}_{+}\\}\\]\n\nObserve that \\(E^{\\perp}=S\\left(F^{\\perp}\\right)\\) , thus by ( 2.2 ) ,\n\n\\[\\lambda_{n+1} \\geq\\min\\left\\{\\langle\\tilde{L}_{u}g\\mid g\\rangle+1+\\left|\\langle Sg \\mid u\\rangle\\right|^{2}\\ ;\\ \\|g\\|_{L^{2}}=1,\\ g\\in F^{\\perp}\\cap H^{\\frac{1}{2 }}_{+}\\right\\}\\ .\\]\n\nIn addition, since \\(|\\langle Sg\\mid u\\rangle|^{2}\\geq 0\\) , we infer for all \\(n\\in\\mathbb{N}_{\\geq 0}\\) ,\n\n\\[\\lambda_{n+1} \\geq\\lambda_{n}+1\\ .\\]\n\nSpectrum of \\(L_{u}\\) -Inequality ( 2.6 ) . let \\(F\\) be any subspace of \\(L_{+}^{2}(T)\\) of dimension \\(n\\), and take \\(G:=\\mathbb{C}1\\oplus S(F)+\\mathbb{C}u\\) . Then,\n\n\\[\\nu_{n+2}(u) \\geq\\min\\{\\left\\langle L_{u}h\\mid h\\right\\rangle\\ ;\\ \\|h\\|_{L^{2} }=1,\\ h\\in G^{\\perp}\\cap H^{\\frac{1}{2}}_{+}\\}\\,.\\]\n\nSince \\(G^{\\perp}=S\\left(F^{\\perp}\\cap(S^{*}u)^{\\perp}\\right)\\), then\n\n\\[\\nu_{n+2} \\geq\\min\\left\\{\\left\\langle L_{u}Sg\\mid Sg\\right\\rangle\\ ;\\ \\|g\\|_ {L^{2}}=1,\\ g\\in F^{\\perp}\\cap(S^{*}u)^{\\perp}\\cap H^{\\frac{1}{2}}_{+}(T)\\right\\}\\,.\\]\n\nNote that \\(g\\perp S^{*}u\\), then by ( 2.2 ),\n\n\\[\\nu_{n+2} \\geq\\min\\left\\{\\left\\langle L_{u}g\\mid g\\rangle+1;\\ \\|g\\|_{L^{2} }=1,\\ g\\in "
    ]
  },
  {
    "edit": [
      "From embedding L 2n (\u2126) \\(\\hookrightarrow\\) L 2(1 \u2212\u03c3) (\u2126) and D ( A 1 \u2212\u03c3 ) \\(\\hookrightarrow\\) L 2(1+\u03c3) (\u2126), ( 1.10) and the Young inequality, we conclude\n\n\\[(f_{1}\\left(v_{1}(t)\\right),A^{\\sigma}v_{2}(t)) \\leqslant C\\int_{\\Omega}(1+|v_{1}(t)|^{\\gamma})\\left|A^{\\sigma}v_ {2}(t)\\right|dx\\] \\[\\leqslant C\\left(1+\\|v_{1}(t)\\|^{\\gamma}_{L 2n\\gamma (\u2126)}\\right)\\|A^{\\sigma}v_{2}(t)\\|_{L 2n\\gamma (\u2126)}\\] \\[\\leqslant C\\left(1+\\|v_{1}(t)\\|^{\\gamma}_{L 2n (\u2126)}\\right)\\|A^{\\sigma}v_{2}(t)\\|_{L 2n (\u2126)}\\] \\[\\leqslant C\\left(1+\\|v_{1}(t)\\|^{\\gamma}_{L 2n (\u2126)}\\right)\\|A^{\\frac{1+\\sigma}{2}}v_{2}(t)\\|\\] \\[\\leqslant C\\left(1+\\|v_{1}(t)\\|^{2\\gamma}_{L 2n (\u2126)}\\right)+\\frac{1}{16}\\|A^{\\frac{1+\\sigma}{2}}v_{2}(t)\\|^{2}. \\tag{4.72}\\]\n\nUsing ( 1.7 ), we derive\n\n\\[\\left|(f(u(t))-f\\left(v_{1}(t)\\right),A^{\\sigma}v_{2}(t))\\right| \\leqslant C\\int_{\\Omega}\\left|(f^{\\prime}\\left((1-\\mu)u(t)\\right)+\\mu v_{1}(t) \\right|\\left|u(t)-v_{1}(t)\\right|\\left|A^{\\sigma}v_{2}(t)\\right|dx \\tag{4.73}\\] \\[\\leqslant C\\int_{\\Omega}\\left(1+|u(t)|^{\\frac{4}{n-2}}+|v_{1}(t)|^ {\\frac{4}{n-2}}\\right)|v_{2}(t)|\\left|A^{\\sigma}v_{2}(t)\\right|dx,\\]\n\nwhere 0 < \u00b5< 1.=\"\" 2.11 , the Cauchy and Young inequalities, we obtain there exists positive constants \\(\\widetilde{C}_{4}\\) and \\(\\widetilde{C}_{5}\\) such that\n\n\\[\\int_{\\Omega}|v_{2}(t)|\\left|A^{\\sigma}v_{2}(t)\\right|dx\\leqslant \\widetilde{C}_{4}+\\widetilde{C}_{5}\\|A^{\\frac{1+\\sigma}{2}}v_{2}(t)\\|^{2}. \\tag{4.74}\\]\n\nConducting similar calculations to ( 4.72 ), then by Corollary 4.8 , we deduce\n\n\\[\\int_{\\Omega}|v_{1}(t)|^{\\frac{4}{n-2}}|v_{2}(t)||A^{\\sigma}v_{2}( t)|dx \\leqslant C\\|v_{1}(t)\\|^{\\frac{4}{n-2}}_{L 2n (\u2126)}\\|v_{2}(t)\\|_{L 2n (\u2126)}\\|A^{\\sigma}v_{2}(t)\\|_{L 2n (\u2126)}\\] (4.75) \\[\\leqslant C\\|A^{\\frac{1}{2}}v_{1}(t)\\|^{\\frac{4}{n-2}}\\|A^{\\frac{1+ \\sigma}{2}}v_{2}(t)\\|^{2}\\] \\[\\leqslant\\frac{1}{16}\\|A^{\\frac{1+\\sigma}{2}}v_{2}(t)\\|^{2}+C \\widetilde{C}_{3}\\,\\|\\nabla v_{1}(t)\\|^{2}\\,\\|A^{\\frac{1+\\sigma}{2}}v_{2}(t)\\|^{ 2}.\\]</ \u00b5<> "
    ],
    "kosmos": [
      "From embedding L 2n (\u2126) \\(\\hookrightarrow\\) L 2(1 \u2212\u03c3) (\u2126) and D ( A 1 \u2212\u03c3 ) \\(\\hookrightarrow\\) L 2(1+\u03c3) (\u2126), ( 1.10) and the Young inequality, we conclude\n\n\\[(f_{1}\\left(v_{1}(t)\\right),A^{\\sigma}v_{2}(t)) \\leqslant C\\int_{\\Omega}(1+|v_{1}(t)|^{\\gamma})\\left|A^{\\sigma}v_ {2}(t)\\right|dx\\] \\[\\leqslant C\\left(1+\\|v_{1}(t)\\|^{\\gamma}_{L 2n\\gamma (\u2126)}\\right)\\|A^{\\sigma}v_{2}(t)\\|_{L 2n\\gamma (\u2126)}\\] \\[\\leqslant C\\left(1+\\|v_{1}(t)\\|^{\\gamma}_{L 2n (\u2126)}\\right)\\|A^{\\sigma}v_{2}(t)\\|_{L 2n (\u2126)}\\] \\[\\leqslant C\\left(1+\\|v_{1}(t)\\|^{\\gamma}_{L 2n (\u2126)}\\right)\\|A^{\\frac{1+\\sigma}{2}}v_{2}(t)\\|\\] \\[\\leqslant C\\left(1+\\|v_{1}(t)\\|^{2\\gamma}_{L 2n (\u2126)}\\right)+\\frac{1}{16}\\|A^{\\frac{1+\\sigma}{2}}v_{2}(t)\\|^{2}. \\tag{4.72}\\]\n\nUsing ( 1.7), we derive\n\n\\[\\left|(f(u(t))-f\\left(v_{1}(t)\\right),A^{\\sigma}v_{2}(t))\\right| \\leqslant C\\int_{\\Omega}\\left|(f^{\\prime}\\left((1-\\mu)u(t)\\right)+\\mu v_{1}(t) \\right|\\left|u(t)-v_{1}(t)\\right|\\left|A^{\\sigma}v_{2}(t)\\right|dx \\tag{4.73}\\] \\[\\leqslant C\\int_{\\Omega}\\left(1+|u(t)|^{\\frac{4}{n-2}}+|v_{1}(t)| ^{\\frac{4}{n-2}}\\right)|v_{2}(t)|\\left|A^{\\sigma}v_{2}(t)\\right|dx,\\]\n\nwhere 0 < \u00b5< 1.=\"\" 2.11,=\"\" 4.8,=\"\" 4.74)=\"\" 4.75)=\"\" 4.76)=\"\" 4.77)=\"\" 4.78,=\"\" 4.79)=\"\" 4.80)=\"\" 4.81)=\"\" 4.82)=\"\" 4.83)=\"\" 4.84)=\"\" 4.85)=\"\" 4.86)=\"\" 4.87)=\"\" 4.88)=\"\" 4.89)=\"\" 4.90)=\"\" 4.91)=\"\" 4.92)=\"\" 4.93)=\"\" 4.94)=\"\" 4.95)=\"\" 4.96)=\"\" 4.97)=\"\" 4.98)=\"\" 4.99)=\"\" 5.00)=\"\" 6.00)=\"\" 7.00)=\"\" 8.00)=\"\" 9.00)=\"\" 10.00)=\"\" 10.00),=\"\" 10.00),=\"\" 10.\n\n "
    ]
  },
  {
    "edit": [
      "\n\n## 4 TinySiamese Network\n\nThe proposed TinySiamese neural network takes on a new look and a new way of working which is different from the standard Siamese network. The difference first appears in the input processing of the network. Instead of having images as input, the input was the output feature vector of a pre-trained CNN model. In other words, all input images would be transformed into feature vectors using a feature extractor (such as a pre-trained CNN model) as illustrated in Fig. 3. Then, the Tiny-Siamese encoded the features in a small set of layers and finally calculated the distance between two encoded feature vectors and generated similarity score. Using this score, the model was trained from scratch with the Adam optimization algorithm and binary cross-entropy loss function.\n\n### Architecture\n\nUnlike the standard Siamese, the input of the TinySiamese was the encoded image as a feature vector. The backbone layers first aimed to extract relevant features using a linear fully-connected layer and a ReLU layer and then amplify them using another linear fully-connected layer and Sigmoid layer. The output size of the first linear layer had the half size of the input (n, n/2) and was followed by a non-linear ReLU layer. The second linear layer took n/2 features in input and came back to the same first input size in output (n/2, n). This layer was followed by a non-linear Sigmoid layer. The outputs of the TinySiamese sub-networks were encoded into an n-dimensional vector using inputs of a size equal to n. Siamese networks are usually trained\n\nFigure 3: The Proposed Architecture Based on TinySiamese Network for Verification.\n\n"
    ],
    "kosmos": [
      "\n\n## 4 TinySiamese Network\n\nThe proposed TinySiamese neural network takes on a new look and a new way of working which is different from the standard Siamese network. The difference first appears in the input processing of the network. Instead of having images as input, the input was the output feature vector of a pre-trained CNN model. In other words, all input images would be transformed into feature vectors using a feature extractor (such as a pre-trained CNN model) as illustrated in Fig. 3. Then, the Tiny-Siamese encoded the features in a small set of layers and finally calculated the distance between two encoded feature vectors and generated similarity score. Using this score, the model was trained from scratch with the Adam optimization algorithm and binary cross-entropy loss function.\n\n### Architecture\n\nUnlike the standard Siamese, the input of the TinySiamese was the encoded image as a feature vector. The backbone layers first aimed to extract relevant features using a linear fully-connected layer and a ReLU layer and then amplify them using another linear fully-connected layer and Sigmoid layer. The output size of the first linear layer had the half size of the input (n, n/2) and was followed by a non-linear ReLU layer. The second linear layer took n/2 features in input and came back to the same first input size in output (n/2, n). This layer was followed by a non-linear Sigmoid layer. The outputs of the TinySiamese sub-networks were encoded into an n-dimensional vector using inputs of a size equal to n. Siamese networks are usually trained\n\nFigure 3: The Proposed Architecture Based on TinySiamese Network for Verification.\n\n"
    ]
  },
  {
    "edit": [
      "\n\n# Natural Language is All a Graph Needs\n\nRuosong Ye\n\nRutgers University\n\nruosong.ye@rutgers.edu\n\nCaiqi Zhang\n\nUniversity of Cambridge\n\ncz391@cam.ac.uk\n\nRunhui Wang\n\nRutgers University\n\nrunhui.wang@rutgers.edu\n\nShuyuan Xu\n\nRutgers University\n\nshuyuan.xu@rutgers.edu\n\nYongfeng Zhang\n\nRutgers University\n\nyongfeng.zhang@rutgers.edu\n\n###### Abstract\n\nThe emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformersbased large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose **InstructGLM** (**Instruc**tion-finetuned **G**raph **L**anguage **M**odel), systematically design highly scalable prompts based on natural language instructions, and use natural language to describe the geometric structure and node features of the graph for instruction tuning an LLM to perform learning and inference on graphs in a generative manner. Our method exceeds all competitive GNN baselines on ogbn-arxiv, Cora and PubMed datasets, which demonstrates the effectiveness of our method and sheds light on generative large language models as the foundation model for graph machine learning.\n\n## 1 Introduction\n\nBefore the advent of Transformers [1], various artificial intelligence domains with different inductive biases had diverse foundational model architectures. For instance, CNNs [2, 3] were designed with considerations for spatial invariance in images, leading to superior performance in computer vision tasks [4, 5]. Memory-enhanced models like RNNs [6] and LSTM [7, 8] were widely used for handling sequential data such as natural language [9] and audio [10]. Graph Neural Networks (GNNs) excel in capturing topological information by employing message passing and aggregation mechanisms, making them a preferred choice in the field of graph learning for a long time [11, 12, 13].\n\nIn recent years, the AI community has witnessed the emergence of numerous powerful pre-trained Large Language Models (LLMs) [14, 15, 16, 17, 18], which are driving huge advancements and lead to the pursuit of possible Artificial General Intelligence (AGI) [19]. Under this background, there is a trend towards unification in model architectures across different domains. Specifically, pre-trained Transformers have demonstrated remarkable performance on various modalities, such as images [20] and videos [21] in computer vision, text in natural language processing [22], structured data in graph machine learning [23], decision sequences in reinforcement learning [24], and visual-text pairs in multimodal tasks [25]. There has even been Transformers capable of handling twelve modalities [26].\n\nBesides model architecture, the unification of processing method in handling multimodal data is also a significant trend worth attention. T5 [15] established a text-to-text framework, unifying all NLP"
    ],
    "kosmos": [
      "\n\n# Natural Language is All a Graph Needs\n\nRuosong Ye\n\nRutgers University\n\nruosong.ye@rutgers.edu\n\nCaiqi Zhang\n\nUniversity of Cambridge\n\ncz391@cam.ac.uk\n\nRunhui Wang\n\nRutgers University\n\nrunhui.wang@rutgers.edu\n\nShuyuan Xu\n\nRutgers University\n\nshuyuan.xu@rutgers.edu\n\nYongfeng Zhang\n\nRutgers University\n\nyongfeng.zhang@rutgers.edu\n\n###### Abstract\n\nThe emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformersbased large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose **InstructGLM** (**Instruc**tion-finetuned **G**raph **L**anguage **M**odel), systematically design highly scalable prompts based on natural language instructions, and use natural language to describe the geometric structure and node features of the graph for instruction tuning an LLM to perform learning and inference on graphs in a generative manner. Our method exceeds all competitive GNN baselines on ogbn-arxiv, Cora and PubMed datasets, which demonstrates the effectiveness of our method and sheds light on generative large language models as the foundation model for graph machine learning.\n\n## 1 Introduction\n\nBefore the advent of Transformers [1], various artificial intelligence domains with different inductive biases had diverse foundational model architectures. For instance, CNNs [2, 3] were designed with considerations for spatial invariance in images, leading to superior performance in computer vision tasks [4, 5]. Memory-enhanced models like RNNs [6] and LSTM [7, 8] were widely used for handling sequential data such as natural language [9] and audio [10]. Graph Neural Networks (GNNs) excel in capturing topological information by employing message passing and aggregation mechanisms, making them a preferred choice in the field of graph learning for a long time [11, 12, 13].\n\nIn recent years, the AI community has witnessed the emergence of numerous powerful pre-trained Large Language Models (LLMs) [14, 15, 16, 17, 18], which are driving huge advancements and lead to the pursuit of possible Artificial General Intelligence (AGI) [19]. Under this background, there is a trend towards unification in model architectures across different domains. Specifically, pre-trained Transformers have demonstrated remarkable performance on various modalities, such as images [20] and videos [21] in computer vision, text in natural language processing [22], structured data in graph machine learning [23], decision sequences in reinforcement learning [24], and visual-text pairs in multimodal tasks [25]. There has even been Transformers capable of handling twelve modalities [26].\n\nBesides model architecture, the unification of processing method in handling multimodal data is also a significant trend worth attention. T5 [15] established a text-to-text framework, unifying all NLP"
    ]
  },
  {
    "edit": [
      "each root subgroup may be expressed in terms of commutators of other root subgroups. If some commutative unital ring \\(K\\) acts on the root subgroups in a natural way, then the resulting odd form ring \\((R,\\Delta)\\) is an augmented odd form \\(K\\)-algebra and in the last claim of the theorem the maps of the root subgroups are isomorphisms. The Chevalley commutator formula is\n\n\\[[G_{\\alpha},G_{\\beta}]\\leq\\prod_{\\begin{subarray}{c}i\\alpha+j\\beta\\in\\Phi\\\\ i,j\\in\\{1,2,...\\}\\end{subarray}}G_{i\\alpha+j\\beta},\\]\n\nwe also assume that \\(G_{2\\alpha}\\leq G_{\\alpha}\\) are 2-step nilpotent filtrations for any ultrashort root \\(\\alpha\\). In other words, \\(G\\) is a group with BC\u2113-commutator relations in the sense of [8]. Alternatively, we may assume only that \\(G\\) contains groups indexed by a root system of type B\u2113 and\n\n\\[[G_{\\alpha},G_{\\beta}]\\leq\\prod_{\\begin{subarray}{c}i\\alpha+j\\beta\\in\\Phi\\\\ i,j\\in\\mathbb{R}_{+}\\end{subarray}}G_{i\\alpha+j\\beta}.\\]\n\nThese formulas turn out to be equivalent (modulo other natural conditions) up to a choice of the nilpotent filtrations \\(G_{2\\alpha}\\leq G_{\\alpha}\\). The paper is organized as follows. In section 2 we recall the definitions of odd form rings and associated unitary groups. In sections 3 and 4 we list the precise conditions on \\(G\\) and its subgroups. Namely, the conditions are (C1)\u2013(C5) without using a commutative unital ring \\(K\\) or (C1)\u2013(C8) involving \\(K\\). In section 4 we also discuss the case \\(\\ell=3\\): under additional \u201cassociativity conditions\u201d (A1)\u2013(A4) the main results still hold, otherwise there are counterexamples (e.g. Chevalley groups of types E6 and E7). These associativity conditions always hold for \\(\\ell\\geq 4\\) by theorem 1. Sections 5 and 6 contain the proof of the main theorem 7 we prove theorem 3 for groups satisfying only (C1)\u2013(C5). In the last section 7 we prove theorem 2 for groups satisfying only (C1)\u2013(C5).\n\n## 2 Odd form groups and odd form rings\n\nIn this paper we build a lot of 2-step nilpotent groups with various operations, so it is useful to develop some technique to simplify such constructions. The group operation of 2-step nilpotent groups is usually denoted by \\(\\dot{+}\\). All lemmas in this section may be checked directly or using the machinery of polyquadratic maps [14, \u00a7 1.3]. We say that \\((M,H)\\) is a _hermitian group_ if \\(M\\) and \\(H\\) are abelian groups, the following is an automorphism\n\n\\((-)\\colon H\\to H\\) of order at most 2, and there is a biadditive pairing \\(\\langle-,-\\rangle\\colon M\\times M\\to H\\) such that\n\n\\(\\langle m,m^{\\prime}\\rangle\\)\n\n= \\(\\langle m^{\\prime},m\\rangle\\) for all \\(m,m^{\\prime}\\in M\\).\n\n "
    ],
    "kosmos": [
      "each root subgroup may be expressed in terms of commutators of other root subgroups. If some commutative unital ring \\(K\\) acts on the root subgroups in a natural way, then the resulting odd form ring \\((R,\\Delta)\\) is an augmented odd form \\(K\\)-algebra and in the last claim of the theorem the maps of the root subgroups are isomorphisms.\n\nThe Chevalley commutator formula is\n\n\\[[G_{\\alpha},G_{\\beta}]\\leq\\prod_{\\begin{subarray}{c}i\\alpha+j\\beta\\in\\Phi\\\\ i,j\\in\\{1,2,...\\}\\end{subarray}}G_{i\\alpha+j\\beta},\\]\n\nwe also assume that \\(G_{2\\alpha}\\leq G_{\\alpha}\\) are 2-step nilpotent filtrations for any ultrashort root \\(\\alpha\\). In other words, \\(G\\) is a group with BC\u2113-commutator relations in the sense of [8]. Alternatively, we may assume only that \\(G\\) contains groups indexed by a root system of type B\u2113 and\n\n\\[[G_{\\alpha},G_{\\beta}]\\leq\\prod_{\\begin{subarray}{c}i\\alpha+j\\beta\\in\\Phi\\\\ i,j\\in\\mathbb{R}_{+}\\end{subarray}}G_{i\\alpha+j\\beta}.\\]\n\nThese formulas turn out to be equivalent (modulo other natural conditions) up to a choice of the nilpotent filtrations \\(G_{2\\alpha}\\leq G_{\\alpha}\\).\n\nThe paper is organized as follows. In section 2 we recall the definitions of odd form rings and associated unitary groups. In sections 3 and 4 we list the precise conditions on \\(G\\) and its subgroups. Namely, the conditions are (C1)\u2013(C5) without using a commutative unital ring \\(K\\) or (C1)\u2013(C8) involving \\(K\\). In section 4 we also discuss the case \u2113 = 3: under additional \u201cassociativity conditions\u201d (A1)\u2013(A4) the main results still hold, otherwise there are counterexamples (e.g. Chevalley groups of types E6 and E7). These associativity conditions always hold for \u2113 \u2265 4 by theorem 1. Sections 5 and 6 contain the proof of the main theorem 2 for groups satisfying (C1)\u2013(C8). In the last section 7 we prove theorem 3 for groups satisfying only (C1)\u2013(C5).\n\n## 2 Odd form groups and odd form rings\n\nIn this paper we build a lot of 2-step nilpotent groups with various operations, so it is useful to develop some technique to simplify such constructions. The group operation of 2-step nilpotent groups is usually denoted by \\(\\dot{+}\\). All lemmas in this section may be checked directly or using the machinery of polyquadratic maps [14, \u00a71.3].\n\nWe say that \\((M,H)\\) is a _hermitian group_ if \\(M\\) and \\(H\\) are abelian groups, there is an automorphism\n\n\\((-)\\colon H\\to H\\) of order at most 2, and there is a biadditive pairing \\(\\langle-,-\\rangle\\colon M\\times M\\to H\\) such that\n\n\\(\\langle m,m^{\\prime}\\rangle\\)\n\n= \\(\\langle m^{\\prime},m\\rangle\\) for all \\(m,m^{\\prime}\\in M\\).\n\n "
    ]
  },
  {
    "edit": [
      "ones where there are only a few particles, we opt to first bin the data points by \\(X_{\\rm max}\\). In each bin we calculate the mean value of the spectral parameter and the corresponding standard deviation. If a bin contains less than two data points, which can occur because not all showers might contain particles in that slice, we do not consider it for the parabolic fit. All the other bins are then fed to a least-squares fitting routine. We end up with a spectral function describing the spectral parameter value as function of the shower \\(X_{\\rm max}\\) in a given slice, for a particular antenna,\n\n\\[a(r_{\\rm ant},X_{\\rm slice},\\ X_{\\rm max}) = p_{0}^{a}+p_{1}^{a}\\cdot\\ X_{\\rm max}+p_{2}^{a}\\cdot\\ X_{\\rm max }^{2}\\] \\[b(r_{\\rm ant},X_{\\rm slice},\\ X_{\\rm max}) = p_{0}^{b}+p_{1}^{b}\\cdot\\ X_{\\rm max}+p_{2}^{b}\\cdot\\ X_{\\rm max }^{2}\\] \\[c(r_{\\rm ant},X_{\\rm slice},\\ X_{\\rm max}) = p_{0}^{c}+p_{1}^{c}\\cdot\\ X_{\\rm max}+p_{2}^{c}\\cdot\\ X_{\\rm max }^{2}\\ .\\]\n\nIn Figure 3 we present a schematic overview of how we extract the spectral functions. These need to be determined only once for the air shower geometry under consideration. Generalising them to arbitrary geometries will be the subject of future work.\n\n### Construction of the template\n\nThe final ingredient of template synthesis, is the _template_ itself. With this object and the spectral functions, we have all the necessary information to synthesise the emission from an air shower with arbitrary longitudinal profile.\n\nIn order to construct our template, we use a single microscopic simulation called the _origin_ shower. The origin is a microscopic simulation, sliced using the same procedure as the simulation set that was used to extract the spectral functions. From the origin shower we calculate the amplitude frequency spectrum \\(A_{\\rm origin}\\) and phase frequency spectrum \\(\\phi_{\\rm origin}\\) in each antenna and every slice, as shown in Figure 4. Using the spectral functions with the \\(X_{\\rm max}\\) of the origin shower, we can normalise these to obtain the spectra of the template,\n\n\\[A_{\\rm template}(r_{\\rm ant},f,X_{\\rm slice}) = A_{\\rm origin}(r_{\\rm ant},f,X_{\\rm slice})\\] \\[{}\\cdot\\left[\u02dc A(r_{\\rm ant},f,X_{\\rm slice}\\ X_{\\rm max}^{\\rm origin })\\right]^{-1}\\] \\[\\phi_{\\rm template}(r_{\\rm ant},f,X_{\\rm slice}) = \\phi_{\\rm origin}(r_{\\rm ant},f,X_{\\rm slice})\\ .\\]\n\nIn this equation the parameterised amplitude frequency spectrum \\(\\tilde{A}(f)\\) also depends on \\(X_{\\rm max}\\) through the spectral coefficients, which are calculated using the fitted spectral functions. As a result, in the following the frequency \\(f\\) is restricted to the range used to fit the spectral functions.\n\nThe template thus contains a normalised version of the origin shower. It acquires its phase frequency spectrum as is, but the origin amplitude frequency spectrum is corrected for the longitudinal evolution (particle number in\n\nFigure 3: Schematic overview of how we extract the spectral functions from our simulation set. In every antenna, we decompose the emission from every slice in the geomagnetic (GEO) and charge-excess (CE) components. These are then fitted using Equations (4) and (5) respectively. Finally, we fit a parabola to each spectral parameter as a function of \\(X_{\\rm max}\\). The result of this are the spectral functions.\n\n "
    ],
    "kosmos": [
      "ones where there are only a few particles, we opt to first bin the data points by \\(X_{\\rm max}\\). In each bin we calculate the mean value of the spectral parameter and the corresponding standard deviation. If a bin contains less than two data points, which can occur because not all showers might contain particles in that slice, we do not consider it for the parabolic fit. All the other bins are then fed to a least-squares fitting routine. We end up with a spectral function describing the spectral parameter value as function of the shower \\(X_{\\rm max}\\) in a given slice, for a particular antenna,\n\n\\[a(r_{\\rm ant},X_{\\rm slice},\\ X_{\\rm max}) = p_{0}^{a}+p_{1}^{a}\\cdot\\ X_{\\rm max}+p_{2}^{a}\\cdot\\ X_{\\rm max }^{2}\\] \\[b(r_{\\rm ant},X_{\\rm slice},\\ X_{\\rm max}) = p_{0}^{b}+p_{1}^{b}\\cdot\\ X_{\\rm max}+p_{2}^{b}\\cdot\\ X_{\\rm max }^{2}\\] \\[c(r_{\\rm ant},X_{\\rm slice},\\ X_{\\rm max}) = p_{0}^{c}+p_{1}^{c}\\cdot\\ X_{\\rm max}+p_{2}^{c}\\cdot\\ X_{\\rm max }^{2}\\ .\\]\n\nIn Figure 3 we present a schematic overview of how we extract the spectral functions. These need to be determined only once for the air shower geometry under consideration. Generalising them to arbitrary geometries will be the subject of future work.\n\n### Construction of the template\n\nThe final ingredient of template synthesis, is the _template_ itself. With this object and the spectral functions, we have all the necessary information to synthesise the emission from an air shower with arbitrary longitudinal profile.\n\nIn order to construct our template, we use a single microscopic simulation called the _origin_ shower. The origin is a microscopic simulation, sliced using the same procedure as the simulation set that was used to extract the spectral functions. From the origin shower we calculate the amplitude frequency spectrum \\(A_{\\rm origin}\\) and phase frequency spectrum \\(\\phi_{\\rm origin}\\) in each antenna and every slice, as shown in Figure 4. Using the spectral functions with the \\(X_{\\rm max}\\) of the origin shower, we can normalise these to obtain the spectra of the template,\n\n\\[A_{\\rm template}(r_{\\rm ant},f,X_{\\rm slice}) = A_{\\rm origin}(r_{\\rm ant},f,X_{\\rm slice})\\] \\[{}\\cdot\\left[\u02dc A(r_{\\rm ant},f,X_{\\rm slice}\\ X_{\\rm max}^{\\rm origin })\\right]^{-1}\\] \\[\\phi_{\\rm template}(r_{\\rm ant},f,X_{\\rm slice}) = \\phi_{\\rm origin}(r_{\\rm ant},f,X_{\\rm slice})\\ .\\]\n\nIn this equation the parameterised amplitude frequency spectrum \\(\\tilde{A}(f)\\) also depends on \\(X_{\\rm max}\\) through the spectral coefficients, which are calculated using the fitted spectral functions. As a result, in the following the frequency \\(f\\) is restricted to the range used to fit the spectral functions.\n\nThe template thus contains a normalised version of the origin shower. It acquires its phase frequency spectrum as is, but the origin amplitude frequency spectrum is corrected for the longitudinal evolution (particle number in\n\nFigure 3: Schematic overview of how we extract the spectral functions from our simulation set. In every antenna, we decompose the emission from every slice in the geomagnetic (GEO) and charge-excess (CE) components. These are then fitted using Equations (4) and (5) respectively. Finally, we fit a parabola to each spectral parameter as a function of \\(X_{\\rm max}\\). The result of this are the spectral functions.\n\n "
    ]
  },
  {
    "edit": [
      "<table>\n<thead>\n<tr>\n<th>\nBrightness range\n</th>\n<th>\nOriginal\n</th>\n<th>\nColor transfer\n</th>\n<th>\nGaussian blur\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\n1 (AdvPatch)\n</td>\n<td>\n89.4%\n</td>\n<td>\n90.8%\n</td>\n<td>\n47.8%\n</td>\n</tr>\n<tr>\n<td>\n0.35\n</td>\n<td>\n89.5%\n</td>\n<td>\n87.9%\n</td>\n<td>\n22.7%\n</td>\n</tr>\n<tr>\n<td>\n0.24\n</td>\n<td>\n74.2%\n</td>\n<td>\n75.3%\n</td>\n<td>\n10.1%\n</td>\n</tr>\n</tbody>\n</table>\n\nTable 1: Performance with color transfer and Gaussian blur\n\n<table>\n<thead>\n<tr>\n<th>\nBrightness range\n</th>\n<th>\nOriginal\n</th>\n<th>\n10% drift\n</th>\n<th>\n15% drift\n</th>\n<th>\n20% drift\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\n1 (AdvPatch)\n</td>\n<td>\n89.4%\n</td>\n<td>\n87.6%\n</td>\n<td>\n85.9%\n</td>\n<td>\n83.3%\n</td>\n</tr>\n<tr>\n<td>\n0.35\n</td>\n<td>\n89.5%\n</td>\n<td>\n84.2%\n</td>\n<td>\n77.6%\n</td>\n<td>\n67.2%\n</td>\n</tr>\n<tr>\n<td>\n0.24\n</td>\n<td>\n74.2%\n</td>\n<td>\n68.0%\n</td>\n<td>\n43.2%\n</td>\n<td>\n27.3%\n</td>\n</tr>\n</tbody>\n</table>\n\nTable 2: Performance with different color drift\n\n### Effectiveness of different features in the physical world\n\n#### 4.3.1 Color transfer and texture blurring\n\nIn physical-world deployments, attack patches are often affected by color transfer due to lighting conditions or blurring caused by camera focus or smudging. To analyze if these changes will affect the performance of the attack patches, we compare the performance of an original patch, a color-adjusted patch, and a local texture adjusted patch. The color-adjusted patch is applied by adding a value ( ) to all values in RGB channels and making sure will not lead to an overflow. This color transfer will not change the texture information of the patch. The local texture adjustment is applied by using a 3x3 Gaussian blur. Figure 4 demonstrates the two types of feature adjustments applying to a patch with brightness range=0.24.\n\nThe performances of different patches are shown in Table 1. Regardless of the lightness restriction, the color transfer patch achieves almost the same success rate as the original patch. This performance shows that the patch does not need to maintain a specific color to deceive the target network. On the other hand, the blurred patch exhibits a significant decrease in success rate, suggesting that local texture is the key feature in deceiving target networks. Using these findings, we can apply the proposed hue mapping method to adjust the color of the patch and enhance its integration with the target environment, resulting in further reduced visibility. This process does not require any learning and can be quickly applied when deploying the patch in the physical world.\n\n#### 4.3.2 Random color variations\n\nWhen printing an attack patch, it is important to consider that normal printers are not able to produce a patch with precisely the same color as the digital version. Therefore, the patch\u2019s robustness to random color variations must be evaluated.\n\nTo replicate the color drift that commonly occurs during printing, we generate random noise within a restricted range that corresponds to a percentage of the original value. This approach allows us to simulate different levels of drift, and the results are shown in Table 2.\n\n "
    ],
    "kosmos": [
      "<table>\n<thead>\n<tr>\n<th>\nBrightness range\n</th>\n<th>\nOriginal\n</th>\n<th>\nColor transfer\n</th>\n<th>\nGaussian blur\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\n1 (AdvPatch)\n</td>\n<td>\n89.4%\n</td>\n<td>\n90.8%\n</td>\n<td>\n47.8%\n</td>\n</tr>\n<tr>\n<td>\n0.35\n</td>\n<td>\n89.5%\n</td>\n<td>\n87.9%\n</td>\n<td>\n22.7%\n</td>\n</tr>\n<tr>\n<td>\n0.24\n</td>\n<td>\n74.2%\n</td>\n<td>\n75.3%\n</td>\n<td>\n10.1%\n</td>\n</tr>\n</tbody>\n</table>\n\nTable 1: Performance with color transfer and Gaussian blur\n\n<table>\n<thead>\n<tr>\n<th>\nBrightness range\n</th>\n<th>\nOriginal\n</th>\n<th>\n10% drift\n</th>\n<th>\n15% drift\n</th>\n<th>\n20% drift\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>\n1 (AdvPatch)\n</td>\n<td>\n89.4%\n</td>\n<td>\n87.6%\n</td>\n<td>\n85.9%\n</td>\n<td>\n83.3%\n</td>\n</tr>\n<tr>\n<td>\n0.35\n</td>\n<td>\n89.5%\n</td>\n<td>\n84.2%\n</td>\n<td>\n77.6%\n</td>\n<td>\n67.2%\n</td>\n</tr>\n<tr>\n<td>\n0.24\n</td>\n<td>\n74.2%\n</td>\n<td>\n68.0%\n</td>\n<td>\n43.2%\n</td>\n<td>\n27.3%\n</td>\n</tr>\n</tbody>\n</table>\n\nTable 2: Performance with different color drift\n\n### Effectiveness of different features in the physical world\n\n#### 4.3.1 Color transfer and texture blurring\n\nIn physical-world deployments, attack patches are often affected by color transfer due to lighting conditions or blurring caused by camera focus or smudging. To analyze if these changes will affect the performance of the attack patches, we compare the performance of an original patch, a color-adjusted patch, and a local texture adjusted patch. The color-adjusted patch is applied by adding a value ( ) to all values in RGB channels and making sure will not lead to an overflow. This color transfer will not change the texture information of the patch. The local texture adjustment is applied by using a 3x3 Gaussian blur. Figure 4 demonstrates the two types of feature adjustments applying to a patch with brightness range=0.24.\n\nThe performances of different patches are shown in Table 1. Regardless of the lightness restriction, the color transfer patch achieves almost the same success rate as the original patch. This performance shows that the patch does not need to maintain a specific color to deceive the target network. On the other hand, the blurred patch exhibits a significant decrease in success rate, suggesting that local texture is the key feature in deceiving target networks. Using these findings, we can apply the proposed hue mapping method to adjust the color of the patch and enhance its integration with the target environment, resulting in further reduced visibility. This process does not require any learning and can be quickly applied when deploying the patch in the physical world.\n\n#### 4.3.2 Random color variations\n\nWhen printing an attack patch, it is important to consider that normal printers are not able to produce a patch with precisely the same color as the digital version. Therefore, the patch\u2019s robustness to random color variations must be evaluated.\n\nTo replicate the color drift that commonly occurs during printing, we generate random noise within a restricted range that corresponds to a percentage of the original value. This approach allows us to simulate different levels of drift, and the results are shown in Table 2.\n\n "
    ]
  },
  {
    "edit": [
      "The cyan dashed line represents the linear least-squares best fit performed on the logarithm of the points for high densities ( \ud835\udf0c thr > 1.1 \u00d7 10\\({}^{-21}\\) g cm\\({}^{-3}\\)). The best fit of \ud835\udf05 = 0.47 \u00b1 0.03 is consistent with the strong-field limit of \ud835\udc35 \u221d \ud835\udf0c^{\\rm 0.5}\\). We have already shown in the previous section (Section 5 ) that our structures are on average highly elongated, and magnetic fields clearly help to deform the shape of the forming structures. It is therefore not unexpected that we find a shallower scaling compared to the weak field limit (\ud835\udf05 = 0.67).\n\nWe see that, while there is no clear transition from the subto the super-Alfv\u00e9nic regime, there is clearly a trend that higher Alfv\u00e9nic Mach numbers are preferentially obtained at the higher density end. This is confirmed by a Kolmogorov-Smirnov (KS) two-sample test, which compares if two distributions belong to the same population. In this case, we compare the \ud835\udf0c thr-distributions of structures with \\(\\mathcal{M}_{A}&gt;1\\) and \\(\\mathcal{M}_{A}\\leq 1\\). We find the \ud835\udc5d\\(-\\)values3 to be very low: 6 \u00d7 10\\({}^{-4}\\) at 2 Myr and 5.2 \u00d7 10\\({}^{-15}\\) at 3.5 Myr (see Table 6).\n\nFootnote 3: If the \ud835\udc5d\\)-value is larger than a certain value (typically 0.05), this means that we cannot reject the null hypothesis that the sub-Alfv\u00e9nic and super-Alfv\u00e9nic structures have the same underlying density distribution.\n\nCrutcher et al. (2010) found that the observed magnetic field distribution is rather flat at low density, in agreement with the idea that denser clouds are swept up along the magnetic field lines on large scales, while at higher density there is a power-law increase of the magnetic field strength. If spherical clouds start to collapse and the magnetic field is not strong enough to stop the collapse, one expects a power-law slope of \ud835\udf05 = 0.5 \u2212 0.67 (see above).\n\nIn the case of our clouds, we find that the high-density end is well consistent with \ud835\udf05 = 0.5, and the lower-density end clearly shows a much shallower slope. Nonetheless, there does not seem to be a clear single density at which there is a sharp change in slope. Simulations by Li et al. (2015), Mocz et al. (2017), Girichidis et al. (2018), Zhang et al. (2019) find similarly the lack of a sharp transition density. Auddy et al. (2022) predict that the transition density depends on the fourth power of \\(\\mathcal{M}_{A}\\). While of potential interest, this is unfortunately not demonstrable from the present analysis.\n\n### Impact of magnetic fields on the energetics of sub-structures\n\nWe are also interested in assessing the energetic relevance of magnetic fields over different length scales in the MCs, especially with respect to potentially star-forming structures. For this purpose, we compute the volume term of the magnetic energy and compare it with the kinetic and potential energies. Similar work for the same simulations has been performed by Ganguly et al. (2022), who assess the virial balance of the cloud sub-structures. Here, we extend the range of our analysis to include the dynamics of lower-density gas (between 10\\({}^{-24}\\) and 10\\({}^{-22}\\) g cm\\({}^{-3}\\); _low-den_ dendrogram analysis, see Table 2).\n\nThe magnetic energy of a given structure is computed as\n\n\\[E_{B}=\\int_{V}\\frac{1}{8\\pi}|\\mathbf{B}|^{2}\\mathrm{d}^{3}r, \\tag{21}\\]\n\nwhere the integration is computed over the entire volume \ud835\udc49 of the structure. The kinetic energy is computed using the following relation:\n\n\\[E_{\\rm KE}=\\frac{1}{2}\\int_{V}\\rho(\\mathbf{v}-\\mathbf{v}_{0})^{2}\\mathrm{d}^{3}r. \\tag{22}\\]\n\nHere, \\(\\mathbf{v}_{0}\\) is the centre of mass velocity computed from Eq. 17. The self-gravitating potential energy of a given structure is obtained using the following relation:\n\n\\[E_{\\rm PE}=-\\frac{1}{2}G\\int_{V}\\frac{\\rho(\\mathbf{r})\\rho(\\mathbf{r}^{\\prime})}\n\n "
    ],
    "kosmos": [
      "The cyan dashed line represents the linear least-squares best fit performed on the logarithm of the points for high densities (\\(\\rho_{\\rm thr}&gt;1.1\\times 10^{-21}\\) g cm\\({}^{-3}\\)). The best fit of \\(\\kappa=0.47\\pm 0.03\\) is consistent with the strong-field limit of \\(B\\propto\\rho^{0.5}\\). We have already shown in the previous section (Section 5) that our structures are on average highly elongated, and magnetic fields clearly help to deform the shape of the forming structures. It is therefore not unexpected that we find a shallower scaling compared to the weak field limit (\\(\\kappa=0.67\\)).\n\nWe see that, while there is no clear transition from the sub- to the super-Alfv\u00e9nic regime, there is clearly a trend that higher Alfv\u00e9nic Mach numbers are preferentially obtained at the higher density end. This is confirmed by a Kolmogorov-Smirnov (KS) two-sample test, which compares if two distributions belong to the same population. In this case, we compare the \\(\\rho_{\\rm thr}\\)-distributions of structures with \\(\\mathcal{M}_{A}&gt;1\\) and \\(\\mathcal{M}_{A}\\leq 1\\). We find the \\(p\\)-values3 to be very low: \\(6\\times 10^{-4}\\) at 2 Myr and \\(5.2\\times 10^{-15}\\) at 3.5 Myr (see Table 6).\n\nFootnote 3: If the \\(p\\)-value is larger than a certain value (typically 0.05), this means that we cannot reject the null hypothesis that the sub-Alfv\u00e9nic and super-Alfv\u00e9nic structures have the same underlying density distribution.\n\nCrutcher et al. (2010) found that the observed magnetic field distribution is rather flat at low density, in agreement with the idea that denser clouds are swept up along the magnetic field lines on large scales, while at higher density there is a power-law increase of the magnetic field strength. If spherical clouds start to collapse and the magnetic field is not strong enough to stop the collapse, one expects a power-law slope of \\(\\kappa=0.5-0.67\\) (see above).\n\nIn the case of our clouds, we find that the high-density end is well consistent with \\(\\kappa=0.5\\), and the lower-density end clearly shows a much shallower slope. Nonetheless, there does not seem to be a clear single density at which there is a sharp change in slope. Simulations by Li et al. (2015), Mocz et al. (2017), Girichidis et al. (2018), Zhang et al. (2019) find similarly the lack of a sharp transition density. Auddy et al. (2022) predict that the transition density depends on the fourth power of \\(\\mathcal{M}_{A}\\). While of potential interest, this is unfortunately not demonstrable from the present analysis.\n\n### Impact of magnetic fields on the energetics of sub-structures\n\nWe are also interested in assessing the energetic relevance of magnetic fields over different length scales in the MCs, especially with respect to potentially star-forming structures. For this purpose, we compute the volume term of the magnetic energy and compare it with the kinetic and potential energies. Similar work for the same simulations has been performed by Ganguly et al. (2022), who assess the virial balance of the cloud sub-structures. Here, we extend the range of our analysis to include the dynamics of lower-density gas (between \\(10^{-24}\\) and \\(10^{-22}\\) g cm\\({}^{-3}\\); _low-den_ dendrogram analysis, see Table 2).\n\nThe magnetic energy of a given structure is computed as\n\n\\[E_{B}=\\int_{V}\\frac{1}{8\\pi}|\\mathbf{B}|^{2}\\mathrm{d}^{3}r, \\tag{21}\\]\n\nwhere the integration is computed over the entire volume \\(V\\) of the structure. The kinetic energy is computed using the following relation:\n\n\\[E_{\\rm KE}=\\frac{1}{2}\\int_{V}\\rho(\\mathbf{v}-\\mathbf{v}_{0})^{2}\\mathrm{d}^{3}r. \\tag{22}\\]\n\nHere, \\(\\mathbf{v}_{0}\\) is the centre of mass velocity computed from Eq. 17. The self-gravitating potential energy of a given structure is obtained using the following relation:\n\n\\[E_{\\rm PE}=-\\frac{1}{2}G\\int_{V}\\frac{\\rho(\\mathbf{r})\\rho(\\mathbf{r}^{\\prime})}\n\n "
    ]
  },
  {
    "edit": [
      "instructions given in the prompt. Past research has shown the importance of stating the actions a model can take, such as outputting \u201cI don\u2019t know.\u201d (Zhou et al., 2023). Similarly, how strongly the prompt encourages a model to incorporate feedback can favor overoptimization.\n\n**Introducing Errors** Finally, effective feedback may communicate information on where the learner is failing, requiring an understanding of the possible error modes for a given task, and which ones the learner is likely in. For example, guessing and committing systematic reasoning mistakes are reflections of differing understandings. Exploring the error space and identifying the mistakes made by a learner is an important extension to the base framework directly derived from pedagogical and psychology of education research.\n\n### Feedback Integration\n\nThe method used to transmit the feedback to the model influences how it is subsequently processed. Fernandes et al. (2023) identify three common feedback integration mechanisms: feedback-based imitation learning, joint-feedback modeling, and reinforcement learning. In addition to this, we also consider feedback use in in-context learning (Brown et al., 2020). The training objective will necessarily influence how the model is processing and incorporating feedback. Typically, the training relies upon either scalar feedback (a single number encoding how much the model should be rewarded for its output) or a ranking (how well a given output did in relation to other candidate answers). However, this is simple information, and does not leverage the rich and complex information encoded in natural language feedback. Section 5 therefore comprehensively explores the different types of information that can be encoded in feedback.\n\n## 5 Feedback Content Taxonomy\n\nIn Section 4, we presented an overview of the complex ecosystem of feedback, including an expansion specifically for LLMs (i.e., FELT) that connects various background elements (e.g.,the learner, the task, the error types) to the actual feedback that must be given. In this section, we expand on our analysis of the *content* dimension of feedback in FELT. Specifically, we present a taxonomy of feedback content under two different forms: a set of 10 broad axes along which feedback can vary, and a more concrete set of nine emergent categories for feedback topic. Figure 4 presents an overview of the two different presentations of this taxonomy, and the mapping between them.\n\nWe motivate this taxonomy to finely categorize current approaches to textual feedback that implicitly formulate feedback solely for *utility* (i.e.,how useful is the feedback for guiding a model toward a suitable response). However, they do not categorize its content, leaving a conceptual gap about *what* makes feedback useful. Our taxonomy stratifies the feedback space, allowing a deliberate and systematic study of feedback content.\n\n### General Taxonomy\n\nWe break down feedback content along ten dimensions that influence how feedback is formulated:\n\n1. *length*, an indication of how much feedback feedback is given, possibly measured by counting its number of tokens,\n2. *granularity*, a measure of the level of detail with which the feedback addresses the original answer \u2014 it is not a measure of how much of the answer is being considered, but rather of the level of detail with which it is being considered,8 Footnote 8: For an open-answer example task, feedback might range from global learning meta-feedback, to global but task-specific, to paragraph-level, to sentence-level, to word-level, to token-level feedback.\n3. *applicability of instructions*, expressing both whether the feedback contains instructions, as well as how applicable those instructions are for the learner and their current understanding and approach to solving the task,\n4. *answer coverage*, which registers how much of the learner\u2019s answer is considered to generate the given feedback. The feedback could be independent of the answer, or only relate to parts of the answer (e.g.,, focusing on a particular mistake), or the feedback might take the complete answer into consideration,\n5. *criteria*, denoting which criteria the answer is being evaluated on: global evaluation, specific dimensions (e.g., fluency, engagement, etc.), or, alternatively, no dimensions (the answer is not being evaluated),\n6. *information novelty*, indicating the degree to which learner already had access to the information provided in the feedback, ranging from all information being previously known\n7. *task complexity*, indicating the complexity of the task, ranging from simple to complex, and including both the number of instructions and the number of tokens.\n\n "
    ],
    "kosmos": [
      "instructions given in the prompt. Past research has shown the importance of stating the actions a model can take, such as outputting \u201cI don\u2019t know.\u201d (Zhou et al., 2023). Similarly, how strongly the prompt encourages a model to incorporate feedback can favor overoptimization.\n\n**Introducing Errors** Finally, effective feedback may communicate information on where the learner is failing, requiring an understanding of the possible error modes for a given task, and which ones the learner is likely in. For example, guessing and committing systematic reasoning mistakes are reflections of differing understandings. Exploring the error space and identifying the mistakes made by a learner is an important extension to the base framework directly derived from pedagogical and psychology of education research.\n\n### Feedback Integration\n\nThe method used to transmit the feedback to the model influences how it is subsequently processed. Fernandes et al. (2023) identify three common feedback integration mechanisms: feedback-based imitation learning, joint-feedback modeling, and reinforcement learning. In addition to this, we also consider feedback use in in-context learning (Brown et al., 2020). The training objective will necessarily influence how the model is processing and incorporating feedback. Typically, the training relies upon either scalar feedback (a single number encoding how much the model should be rewarded for its output) or a ranking (how well a given output did in relation to other candidate answers). However, this is simple information, and does not leverage the rich and complex information encoded in natural language feedback. Section 5 therefore comprehensively explores the different types of information that can be encoded in feedback.\n\n## 5 Feedback Content Taxonomy\n\nIn Section 4, we presented an overview of the complex ecosystem of feedback, including an expansion specifically for LLMs (i.e., FELT) that connects various background elements (e.g.,the learner, the task, the error types) to the actual feedback that must be given. In this section, we expand on our analysis of the *content* dimension of feedback in FELT. Specifically, we present a taxonomy of feedback content under two different forms: a set of 10 broad axes along which feedback can vary, and a more concrete set of nine emergent categories for feedback topic. Figure 4 presents an overview of the two different presentations of this taxonomy, and the mapping between them.\n\nWe motivate this taxonomy to finely categorize current approaches to textual feedback that implicitly formulate feedback solely for *utility* (i.e.,how useful is the feedback for guiding a model toward a suitable response). However, they do not categorize its content, leaving a conceptual gap about *what* makes feedback useful. Our taxonomy stratifies the feedback space, allowing a deliberate and systematic study of feedback content.\n\n### General Taxonomy\n\nWe break down feedback content along ten dimensions that influence how feedback is formulated:\n\n1. *length*, an indication of how much feedback feedback is given, possibly measured by counting its number of tokens,\n2. *granularity*, a measure of the level of detail with which the feedback addresses the original answer \u2014 it is not a measure of how much of the answer is being considered, but rather of the level of detail with which it is being considered,8 Footnote 8: For an open-answer example task, feedback might range from global learning meta-feedback, to global but task-specific, to paragraph-level, to sentence-level, to word-level, to token-level feedback.\n3. *applicability of instructions*, expressing both whether the feedback contains instructions, as well as how applicable those instructions are for the learner and their current understanding and approach to solving the task,\n4. *answer coverage*, which registers how much of the learner\u2019s answer is considered to generate the given feedback. The feedback could be independent of the answer, or only relate to parts of the answer (e.g.,, focusing on a particular mistake), or the feedback might take the complete answer into consideration,\n5. *criteria*, denoting which criteria the answer is being evaluated on: global evaluation, specific dimensions (e.g., fluency, engagement, etc.), or, alternatively, no dimensions (the answer is not being evaluated),\n6. *information novelty*, indicating the degree to which learner already had access to the information provided in the feedback, ranging from all information being previously known\n7. *task complexity*, indicating the complexity of the task, ranging from simple to complex, and including both the number of instructions and the number of tokens.\n\n "
    ]
  },
  {
    "edit": [
      "individual escape fractions, we select a subsample of galaxies not used for the fitting in order to avoid problems due to over-fitting. As a measure for this accuracy we use the average relative deviation\n\n\\[r=\\frac{1}{N_{\\rm test}}\\sum_{i=1}^{N_{\\rm test}}\\frac{|f_{\\rm esc ,pred,i}-f_{\\rm esc,i}|}{f_{\\rm esc,i}}, \\tag{15}\\]\n\nwith \\(f_{\\rm esc,pred,i}\\) and \\(f_{\\rm esc,i}\\) being the predicted and modelled escape fraction of the \\(i\\)-th galaxy respectively, and \\(N_{\\rm test}\\) the number of test galaxies. We only used galaxies with \\(f_{\\rm esc}&gt;0.01\\), as this measure is not useful for \\(f_{\\rm esc}\\) approaching 0. We find a value of \\(r\\approx 1.2\\), i.e. the average estimation error is of the order of a factor of 2, and as such the accuracy of predicting the escape fraction of a single halo is limited. However, for large scale studies where the statistical distribution of the escape fraction is more important, this model performs significantly better. Indeed, the average escape fraction obtained with the fitting formula is \\(\\bar{f}_{\\rm esc,pred,i}=0.121\\pm 0.086\\), with the modelled escape fraction being \\(\\bar{f}_{\\rm esc,i}=0.117\\pm 0.1334\\).\n\nIn fig. 9 we show how well the fitting formula is able to reproduce the behaviour of the escape fraction in relation to the stellar mass and redshift that we examined in fig. 2. We see that the evolution of the escape fraction with redshift is successfully reproduced. However, the large gradients in \\(\\langle f_{\\rm esc}\\rangle\\) that are seen in fig. 8 are smoothed out. The reason for this likely lies in the optimization process used to find the fitting formula, as the mean squared error was used for optimization, and thus large gradients in the fitting function were disfavored because they led to large errors for the outer mass ranges.\n\nFig. 10 shows that the fitting formula is able to successfully predict the bimodality in the escape fraction, as seen in fig. 4. However the boundary between the two modes is less pronounced. This is likely caused by the smoothing effect of the optimization process of the fitting function discussed above.\n\nFinally, by comparing fig. 11 to fig. 3, we see that the fitting formula is able to reproduce all important trends, namely, the decrease in peak escape fraction with redshift and the approximate locations and values of the peaks. We also reproduce both the minima and maxima in the dependence of \\(f_{\\rm esc}\\) on \\(M_{\\rm gas}\\).\n\nAs mentioned earlier, it is important to emphasize that our modeling aims to capture the overall trends of LyC escape with galactic properties. Considering the inherent limitations in resolution and simplifications involved in estimating the LyC flux, it is crucial to scale the absolute value predicted by the fitting formula using a free parameter, which should be determined based on the specific ionizing photon budget required for reionization. We intend to investigate the large scale implication of these results and to determine scaling parameters in subsequent work.\n\n## 5 Discussion and Conclusions\n\nTo gain a better understanding of the correlation of LyC escape with galactic properties, we have applied the physically motivated model for the LyC escape fraction developed in F23 to \\(\\approx 600,000\\) galaxies extracted from the TNG50 simulation (Nelson et al., 2019; Pillepich et al., 2019) in the range \\(5.2&lt;z&lt;20\\).\n\nGiven the large uncertainties in the subgrid modeling of LyC escape, attempting a quantitative comparison of our results to those of previous studies would be impractical. Therefore, we focus our discussion on qualitative results. Numerous previous numerical studies, such as the First Billion Year project (Paardekooper et al., 2015), CODA-II (Lewis et al., 2020), FIRE-II (Ma et al., 2020), THE-SAN (Yeh et al., 2022), SPHINX (Rosdahl et al., 2022) and TNG50\n\nFigure 6: Galaxies with disk scale heights \\(H&gt;10^{21.3}\\)cm selected for their high and low SFR, i.e. \\(\\log({\\rm SFR}/{\\rm M_{\\odot}yr^{-1}})=0.14,0.20\\) and \\(0.40\\) (top) and \\(\\log({\\rm SFR}/{\\rm M_{\\odot}yr^{-1}})=-4.\n\n "
    ],
    "kosmos": [
      "individual escape fractions, we select a subsample of galaxies not used for the fitting in order to avoid problems due to over-fitting. As a measure for this accuracy we use the average relative deviation\n\n\\[r=\\frac{1}{N_{\\rm test}}\\sum_{i=1}^{N_{\\rm test}}\\frac{|f_{\\rm esc ,pred,i}-f_{\\rm esc,i}|}{f_{\\rm esc,i}}, \\tag{15}\\]\n\nwith \\(f_{\\rm esc,pred,i}\\) and \\(f_{\\rm esc,i}\\) being the predicted and modelled escape fraction of the \\(i\\)-th galaxy respectively, and \\(N_{\\rm test}\\) the number of test galaxies. We only used galaxies with \\(f_{\\rm esc}&gt;0.01\\), as this measure is not useful for \\(f_{\\rm esc}\\) approaching 0. We find a value of \\(r\\approx 1.2\\), i.e. the average estimation error is of the order of a factor of 2, and as such the accuracy of predicting the escape fraction of a single halo is limited. However, for large scale studies where the statistical distribution of the escape fraction is more important, this model performs significantly better. Indeed, the average escape fraction obtained with the fitting formula is \\(\\bar{f}_{\\rm esc,pred,i}=0.121\\pm 0.086\\), with the modelled escape fraction being \\(\\bar{f}_{\\rm esc,i}=0.117\\pm 0.1334\\).\n\nIn fig. 9 we show how well the fitting formula is able to reproduce the behaviour of the escape fraction in relation to the stellar mass and redshift that we examined in fig. 2. We see that the evolution of the escape fraction with redshift is successfully reproduced. However, the large gradients in \\(\\langle f_{\\rm esc}\\rangle\\) that are seen in fig. 8 are smoothed out. The reason for this likely lies in the optimization process used to find the fitting formula, as the mean squared error was used for optimization, and thus large gradients in the fitting function were disfavored because they led to large errors for the outer mass ranges.\n\nFig. 10 shows that the fitting formula is able to successfully predict the bimodality in the escape fraction, as seen in fig. 4. However the boundary between the two modes is less pronounced. This is likely caused by the smoothing effect of the optimization process of the fitting function discussed above.\n\nFinally, by comparing fig. 11 to fig. 3, we see that the fitting formula is able to reproduce all important trends, namely, the decrease in peak escape fraction with redshift and the approximate locations and values of the peaks. We also reproduce both the minima and maxima in the dependence of \\(f_{\\rm esc}\\) on \\(M_{\\rm gas}\\).\n\nAs mentioned earlier, it is important to emphasize that our modeling aims to capture the overall trends of LyC escape with galactic properties. Considering the inherent limitations in resolution and simplifications involved in estimating the LyC flux, it is crucial to scale the absolute value predicted by the fitting formula using a free parameter, which should be determined based on the specific ionizing photon budget required for reionization. We intend to investigate the large scale implication of these results and to determine scaling parameters in subsequent work.\n\n## 5 Discussion and Conclusions\n\nTo gain a better understanding of the correlation of LyC escape with galactic properties, we have applied the physically motivated model for the LyC escape fraction developed in F23 to \\(\\approx 600,000\\) galaxies extracted from the TNG50 simulation (Nelson et al., 2019; Pillepich et al., 2019) in the range \\(5.2&lt;z&lt;20\\).\n\nGiven the large uncertainties in the subgrid modeling of LyC escape, attempting a quantitative comparison of our results to those of previous studies would be impractical. Therefore, we focus our discussion on qualitative results. Numerous previous numerical studies, such as the First Billion Year project (Paardekooper et al., 2015), CODA-II (Lewis et al., 2020), FIRE-II (Ma et al., 2020), THE-SAN (Yeh et al., 2022), SPHINX (Rosdahl et al., 2022) and TNG50\n\nFigure 6: Galaxies with disk scale heights \\(H&gt;10^{21.3}\\)cm selected for their high and low SFR, i.e. \\(\\log({\\rm SFR}/{\\rm M_{\\odot}yr^{-1}})=0.14,0.20\\) and \\(0.40\\) (top) and \\(\\log({\\rm SFR}/{\\rm M_{\\odot}yr^{-1}})=-4.\n\n "
    ]
  },
  {
    "edit": [
      "level accuracy, we can obtain the message that LeViLM is not capable of performing complicated (multi-hop) reasoning over the scene knowledge and producing accurate predictions. Besides, the prediction process is black-box and can not be explainable, which can be further studied in the future. The answer is that (i) The current baselines can only achieve strong results on easy or medium tasks and are unable to perform well on the hard task; (ii) The interpretability of the baselines is poor. **4.5. Case Study** To further investigate the effects of knowledge, we perform qualitative analysis on four cases in the SK-VG dataset. Figure 5 shows the grounding results of four baselines on four referring expressions. It is observed that in the first case, all the baselines can ground the \u201c*cane*\u201d in the image even without the knowledge since there is only one cane presented. In the second case, the finetuned LeViLM can detect the target object even without knowledge, while it can not detect the \u201c*Brandon\u2019s servant*\u201d without knowledge in the third case. In the last case, all the baselines can not ground the referred object correctly, and the last three baselines all treat the \u201c*Spider-Man*\u201d as the \u201c*enemy*\u201d. This shows that the baseline models can not perform accurate reasoning in some complicated cases, demonstrating the challenges. **5. Concluding Remarks** The visual grounding field has emerged as a prominent attractive research direction, where the models are required to reason over vision and language to ground the target objects. Yet, the language part of the existing VG benchmarks is only simple description texts, which can not evaluate the reasoning capability of the models comprehensively. To take a step in this direction, we propose a new benchmark dataset called SK-VG, which requires models to reason over the (image, scene knowledge, query) triples to perform accurate reasoning. We propose two approaches to perform this new task: Knowledge-embedded Vision-Language Interaction and Linguistic-enhanced Vision-Language Matching. Experimental results confirm the validity of the proposed approaches but also show that there is still substantial room for improvement, e.g., reasoning and interpretability. **Acknowledgement** This work was also sponsored by Tencent CCF Open Fund (NO. RBFR2022009).\n\n "
    ],
    "kosmos": [
      "level accuracy, we can obtain the message that LeViLM is not capable of performing complicated (multi-hop) reasoning over the scene knowledge and producing accurate predictions. Besides, the prediction process is black-box and can not be explainable, which can be further studied in the future. The answer is that (i) The current baselines can only achieve strong results on easy or medium tasks and are unable to perform well on the hard task; (ii) The interpretability of the baselines is poor.\n\n### Case Study\n\nTo further investigate the effects of knowledge, we perform qualitative analysis on four cases in the SK-VG dataset. Figure 5 shows the grounding results of four baselines on four referring expressions. It is observed that in the first case, all the baselines can ground the \u201c*cane*\u201d in the image even without the knowledge since there is only one cane presented. In the second case, the finetuned LeViLM can detect the target object even without knowledge, while it can not detect the \u201c*Brandon's servant*\u201d without knowledge in the third case. In the last case, all the baselines can not ground the referred object correctly, and the last three baselines all treat the \u201c*Spider-Man*\u201d as the \u201c*enemy*\u201d. This shows that the baseline models can not perform accurate reasoning in some complicated cases, demonstrating the challenges.\n\n## 5 Concluding Remarks\n\nThe visual grounding field has emerged as a prominent attractive research direction, where the models are required to reason over vision and language to ground the target objects. Yet, the language part of the existing VG benchmarks is only simple description texts, which can not evaluate the reasoning capability of the models comprehensively. To take a step in this direction, we propose a new benchmark dataset called SK-VG, which requires models to reason over the (image, scene knowledge, query) triples to perform accurate reasoning. We propose two approaches to perform this new task: Knowledge-embedded Vision-Language Interaction and Linguistic-enhanced Vision-Language Matching. Experimental results confirm the validity of the proposed approaches but also show that there is still substantial room for improvement, e.g., reasoning and interpretability.\n\n## Acknowledgement\n\nThis work was supported in part by the Chinese Key-Area Research and Development Program of Guangdong Province (2020B0101350001), in part by the Guangdong Basic and Applied Basic Research Foundation (NO. 2020B1515020048), in part by the National Natural Science Foundation of China (NO. 61976250), in part by the Shenzhen Science and Technology Program (NO. JCYJ20220530141211024, NO. JCYJ20220818103001002), in part by the Fundamental Research Funds for the Central Universities under Grant 22lgqb25 and in part by the Guangdong Provincial Key Laboratory of Big Data Computing, The Chinese University of Hong Kong, Shenzhen. This work was also sponsored by Tencent CCF Open Fund (NO. RBFR2022009).\n\n "
    ]
  },
  {
    "edit": [
      "\n\n# Unique continuation for an elliptic interface problem using unfitted isoparametric finite elements\n\nErik\n\nDepartment of Mathematics, University College London, Gower Street, London, WC1E 6BT, United Kingdom.\n\nJanosch\n\nCorresponding author(s). E-mail(s): j.preuss@ucl.ac.uk; Contributing authors: e.burman@ucl.ac.uk;\n\n###### Abstract\n\nWe study unique continuation over an interface using a stabilized unfitted finite element method tailored to the conditional stability of the problem. The interface is approximated using an isoparametric transformation of the background mesh and the corresponding geometrical error is included in our error analysis. To counter possible destabilizing effects caused by non-conformity of the discretization and cope with the interface conditions, we introduce adapted regularization terms. This allows to derive error estimates based on conditional stability. Numerical experiments suggest that the presence of an interface seems to be of minor importance for the continuation of the solution beyond the data domain. On the other hand, certain convexity properties of the geometry are crucial as has already been observed for many other problems without interfaces. **Keywords:** unfitted finite element method, unique continuation, interface problems, isoparametric finite element method, geometry errors, conditional H\u00a8older stability\n\n**MSC Classification:** 35J15 , 65N12 , 65N20 , 65N30 , 86-08\n\n## 1 Introduction\n"
    ],
    "kosmos": [
      "\n\n# Unique continuation for an elliptic interface problem using unfitted isoparametric finite elements\n\nErik\n\nDepartment of Mathematics, University College London, Gower Street, London, WC1E 6BT, United Kingdom.\n\nJanosch\n\nCorresponding author(s). E-mail(s): j.preuss@ucl.ac.uk; Contributing authors: e.burman@ucl.ac.uk;\n\n###### Abstract\n\nWe study unique continuation over an interface using a stabilized unfitted finite element method tailored to the conditional stability of the problem. The interface is approximated using an isoparametric transformation of the background mesh and the corresponding geometrical error is included in our error analysis. To counter possible destabilizing effects caused by non-conformity of the discretization and cope with the interface conditions, we introduce adapted regularization terms. This allows to derive error estimates based on conditional stability. Numerical experiments suggest that the presence of an interface seems to be of minor importance for the continuation of the solution beyond the data domain. On the other hand, certain convexity properties of the geometry are crucial as has already been observed for many other problems without interfaces.\n\n**Keywords:** unfitted finite element method, unique continuation, interface problems, isoparametric finite element method, geometry errors, conditional Holder stability\n\n**MSC Classification:** 35J15 , 65N12 , 65N20 , 65N30 , 86-08\n\n## 1 Introduction\n"
    ]
  }
]