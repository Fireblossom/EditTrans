[{"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In Figure 9, we show the Leavitt laws for a sample of Cepheids in the LMC (Persson et al. 2004) and in NGC 7250 (Owens et al. 2023). The scatter in the JWST F115W data for NGC 7250 is a factor of two smaller than in the SH0ES F160W data; i.e., the improved resolution and higher signal-to-noise ratio of the JWST data results in a lower variance ($\\\\sigma^2$) for the F115W relation by almost a factor of four. This is all the more remarkable since the $J$-band data are single-phase observations only, while the HST observations have been corrected to mean light. The HST data exhibit more than three times the scatter of the $H$-band data for the LMC, the latter of which reflects the expected scatter for that band, as exemplified by the LMC data.\\n\\n12. SUMMARY\\n\\nThe accuracy of the Cepheid distance scale has continued to improve over the century during which it has been used to measure the distances to nearby galaxies and set the scale for the determination of $H_0$. Still, challenges remain in overcoming systematic uncertainties. Many of these challenges will be overcome with new capabilities provided by the JWST.\\n\\nNew JWST data for the nearby galaxy NGC 7250 already demonstrate that (1) many of the Cepheids observed with HST/WFC3 are significantly crowded (and biased to brighter apparent magnitudes) by nearby neighbors. A re-analysis of the SH0ES optical data, then coupled with the new high-resolution and higher signal-to-noise JWST F115W data, leads to significantly reduced effects of crowding and smaller photometric uncertainties. (2) These improvements result in a factor of two lower scatter in the near-infrared Leavitt law for JWST F115W compared with HST F160W, even with single-epoch F115W JWST photometry.\\n\\nThe galaxies in our JWST CCHP program sample have all been selected to have with distances $\\\\lesssim 20$ Mpc, close enough to minimize crowding effects. As for the case of NGC 7250 presented here, these data will be combined with a re-analysis of the SH0ES HST optical data for the Cepheids. TRGB, carbon star, and Cepheid distances to the same sample of galaxies being observed as part of the CCHP will allow measurement of three independent distances to each\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In Figure 9, we show the Leavitt laws for a sample of Cepheids in the LMC (Persson et al. 2004) and in NGC 7250 (Owens et al. 2023). The scatter in the JWST F115W data for NGC 7250 is a factor of two smaller than in the SHoES F160W data; i.e., the improved resolution and higher signal-to-noise ratio of the JWST data results in a lower variance (\u03c3\u00b2) for the F115W relation by almost a factor of four. This is all the more remarkable since the J-band data are single-phase observations only, while the HST observations have been corrected to mean light. The HST data exhibit more than three times the scatter of the H-band data for the LMC, the latter of which reflects the expected scatter for that band, as exemplified by the LMC data. \\n\\n12. SUMMARY\\n\\nThe accuracy of the Cepheid distance scale has continued to improve over the century during which it has been used to measure the distances to nearby galaxies and set the scale for the determination of H 0. Still, challenges remain in overcoming systematic uncertainties. Many of these challenges will be overcome with new capabilities provided by the JWST . New JWST data for the nearby galaxy NGC 7250 already demonstrate that (1) many of the Cepheids observed with are significantly crowded (and biased to brighter apparent magnitudes) by nearby neighbors. A re-analysis of the SH0ES optical data, then coupled with the new high-resolution and higher signal-to-noise JWST F115W data, leads to significantly reduced effects of crowding and smaller photometric uncertainties. (2) These improvements result in a factor of two lower scatter in the near-infrared Leavitt law for JWST F115W compared with HST F160W, even with single-epoch F115W JWST photometry.\\n\\nThe galaxies in our JWST CCHP program sample have all been selected to have with distances \u227220 Mpc, close enough to minimize crowding effects. As for the case of NGC 7250 presented here, these data will be combined with a re-analysis of the SH0ES HST optical data for the Cepheids. TRGB, carbon star, and Cepheid distances to the same sample of galaxies being observed as part of the CCHP will allow measurement of three independent distances to each\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\\\[ \\\\alpha_{\\\\text{eff}} = \\\\beta_{\\\\text{eff}} \\\\]\\n\\\\[ \\\\Rightarrow \\\\alpha \\\\frac{N_1}{L} = \\\\beta \\\\left(1 - \\\\frac{N_2}{L}\\\\right) \\\\]\\n\\nBelow we obtain the exact location \\\\( x_w \\\\) and height \\\\( \\\\Delta \\\\) of the LDW in terms of the control parameters.\\n\\nIn the DW phase, particle number in \\\\( T \\\\) can be expressed as\\n\\\\[ N_T = L \\\\int_0^1 \\\\rho(x) dx, \\\\]\\nwhere a multiplicative factor \\\\( L \\\\) is introduced in the right-hand side to rescale the integration limit of the position variable \\\\( x \\\\). Using (48) and (51) in (52), we get:\\n\\\\[ N_T = L \\\\left[ \\\\alpha \\\\frac{N_1}{L} (2x_w - 1) + 1 - x_w \\\\right]. \\\\]\\n\\nIdentifying the steady state TASEP current in the DW phase as \\\\( J_T = \\\\rho_{\\\\text{LD}}(1 - \\\\rho_{\\\\text{LD}}) \\\\) or \\\\( J_T = \\\\rho_{\\\\text{HD}}(1 - \\\\rho_{\\\\text{HD}}) \\\\) and substituting the expressions of \\\\( N_T \\\\) [see eq. (53)] and \\\\( J_T \\\\) in eq. (17a) together with PNC, we obtain the following two equations coupled in \\\\( N_1/L \\\\) and \\\\( x_w \\\\):\\n\\\\[ \\\\frac{N_1}{L} \\\\left[ 1 - \\\\frac{\\\\alpha}{\\\\beta} + \\\\alpha(2x_w - 1) \\\\right] - x_w + 1 = \\\\mu - 1, \\\\]\\n\\\\[ \\\\frac{N_1}{L} = \\\\frac{k_2}{k_1 + k_2} \\\\left[ \\\\mu - \\\\frac{\\\\alpha}{\\\\beta} \\\\frac{N_1}{L} (2x_w - 1) - 1 + x_w \\\\right] \\\\]\\n\\\\[ - \\\\frac{1}{L(k_1 + k_2)} \\\\frac{N_1}{L} \\\\left( 1 - \\\\frac{N_1}{L} \\\\right). \\\\]\\n\\nWhile solving eqs. (54) and (55) for \\\\( N_1/L \\\\) and \\\\( x_w \\\\), we get a quadratic equation for \\\\( N_1/L \\\\) with two solutions:\\n\\\\[ \\\\left( \\\\frac{N_1}{L} \\\\right)^\\\\pm = \\\\left( \\\\frac{1}{2\\\\alpha} + \\\\frac{k_{10}}{2\\\\alpha^2} + \\\\frac{k_{20}}{2\\\\alpha^2} \\\\right) \\\\pm \\\\left[ \\\\left( \\\\frac{1}{2\\\\alpha} + \\\\frac{k_{10}}{2\\\\alpha^2} + \\\\frac{k_{20}}{2\\\\alpha^2} \\\\right)^2 - \\\\frac{k_{20}}{\\\\alpha^2} \\\\right]^{\\\\frac{1}{2}}. \\\\]\\n\\nThe density in the LD part of the DW is thus\\n\\\\[ \\\\rho_{\\\\text{LD}} = \\\\alpha \\\\frac{N_1}{L} = \\\\left( \\\\frac{1}{2} + \\\\frac{k_{10}}{2\\\\alpha} + \\\\frac{k_{20}}{2\\\\beta} \\\\right) \\\\pm \\\\left[ \\\\left( \\\\frac{1}{2} + \\\\frac{k_{10}}{2\\\\alpha} + \\\\frac{k_{20}}{2\\\\beta} \\\\right)^2 - \\\\frac{k_{20}}{\\\\alpha^2} \\\\right]^{\\\\frac{1}{2}}. \\\\]\\n\\nAt the boundary between the LD and the DW phases, MFT must predict identical (low) density in the bulk of \\\\( T \\\\). We now argue that in (57) the solution with a negative discriminant is actually the physically acceptable solution. Equating the density in LD phase [eq. (25)] and density in the LD domain of DW phase [eq. (57) with negative discriminant], we obtain the LD-DW boundary as follows:\\n\\\\[ \\\\left( \\\\frac{1 + k_{20}}{2} + \\\\frac{k_{10} + k_{20}}{2\\\\alpha} \\\\right) \\\\]\\n\\\\[ - \\\\left[ \\\\left( \\\\frac{1 + k_{20}}{2} + \\\\frac{k_{10} + k_{20}}{2\\\\alpha} \\\\right)^2 - \\\\mu k_{20} \\\\right]^{\\\\frac{1}{2}} \\\\]\\n\\\\[ = \\\\left( \\\\frac{1}{2} + \\\\frac{k_{10}}{2\\\\alpha} + \\\\frac{k_{20}}{2\\\\beta} \\\\right) - \\\\left[ \\\\left( \\\\frac{1}{2} + \\\\frac{k_{10}}{2\\\\alpha} + \\\\frac{k_{20}}{2\\\\beta} \\\\right)^2 - k_{20} \\\\right]^{\\\\frac{1}{2}}. \\\\]\\n\\nThe LD-DW boundary equation (58) can be simplified further (detailed calculation is given in Appendix) after which it reduces to a form which we will obtain later in subsection V E. We find\\n\\\\[ \\\\frac{N_1}{L} \\\\left( 1 + \\\\alpha - \\\\frac{\\\\alpha}{\\\\beta} \\\\right) = \\\\mu - 1 \\\\]\\nas the LD-DW phase boundary, where \\\\( N_1/L \\\\) is given by\\n\\\\[ \\\\frac{N_1}{L} = \\\\left( \\\\frac{1}{2\\\\alpha} + \\\\frac{k_{10}}{2\\\\alpha^2} + \\\\frac{k_{20}}{2\\\\alpha^2} \\\\right) \\\\]\\n\\\\[ - \\\\left[ \\\\left( \\\\frac{1}{2\\\\alpha} + \\\\frac{k_{10}}{2\\\\alpha^2} + \\\\frac{k_{20}}{2\\\\alpha^2} \\\\right)^2 - \\\\frac{k_{20}}{\\\\alpha^2} \\\\right]^{\\\\frac{1}{2}}. \\\\]\\n\\nTherefore, the acceptable solution of \\\\( N_1/L \\\\) is the one with a negative discriminant; see eq. (60). Next, the expression for \\\\( N_2 \\\\) can be obtained using eq. (51). One finds\\n\\\\[ \\\\frac{N_2}{L} = 1 - \\\\frac{\\\\alpha}{\\\\beta} \\\\frac{N_1}{L} \\\\]\\n\\\\[ = 1 - \\\\left( \\\\frac{1}{2\\\\beta} + \\\\frac{k_{10}}{2\\\\alpha\\\\beta} + \\\\frac{k_{20}}{2\\\\beta^2} \\\\right) \\\\]\\n\\\\[ + \\\\left[ \\\\left( \\\\frac{1}{2\\\\beta} + \\\\frac{k_{10}}{2\\\\alpha\\\\beta} + \\\\frac{k_{20}}{2\\\\beta^2} \\\\right)^2 - \\\\frac{k_{20}}{\\\\beta^2} \\\\right]^{\\\\frac{1}{2}}, \\\\]\\n\\nOnce we obtain the expression for \\\\( N_1/L \\\\) in eq. (60), the position \\\\( x_w \\\\) of the domain wall can be obtained using eq. (54). We find:\\n\\\\[ x_w = \\\\frac{\\\\left( 1 - \\\\alpha - \\\\frac{\\\\alpha}{\\\\beta} \\\\right) \\\\frac{N_1}{L} - \\\\mu + 2}{1 - 2\\\\alpha \\\\frac{N_1}{L}}. \\\\]\\n\\nHaving the expression of \\\\( N_1/L \\\\), it is straightforward to obtain the low and high densities in the DW phase:\\n\\\\[ \\\\rho_{\\\\text{LD}} = \\\\left( \\\\frac{1}{2} + \\\\frac{k_{10}}{2\\\\alpha} + \\\\frac{k_{20}}{2\\\\beta} \\\\right) - \\\\left[ \\\\left( \\\\frac{1}{2} + \\\\frac{k_{10}}{2\\\\alpha} + \\\\frac{k_{20}}{2\\\\beta} \\\\right)^2 - k_{20} \\\\right]^{\\\\frac{1}{2}}, \\\\]\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\\\[ \\\\alpha_{\\\\text{eff}} = \\\\beta_{\\\\text{eff}} \\\\]\\n\\\\[ \\\\Rightarrow \\\\alpha \\\\frac{N_1}{L} = \\\\beta \\\\left(1 - \\\\frac{N_2}{L}\\\\right) \\\\]\\n\\nBelow we obtain the exact location \\\\( x_w \\\\) and height \\\\( \\\\Delta \\\\) of the LDW in terms of the control parameters. In the DW phase, particle number in \\\\( T \\\\) can be expressed as\\n\\n\\\\[ N_T = L \\\\int_0^1 \\\\rho(x) dx, \\\\]\\n\\nwhere a multiplicative factor \\\\( L \\\\) is introduced in the righthand side to rescale the integration limit of the position variable \\\\( x \\\\). Using (48) and (51) in (52), we get:\\n\\n\\\\[ N_T = L \\\\left[ \\\\alpha \\\\frac{N_1}{L} (2x_w - 1) + 1 - x_w \\\\right]. \\\\]\\n\\nIdentifying the steady state T . Using ( 48 ) and ( 51 ) in ( 52 ), we get: \\n\\n\\\\[ \\\\frac{N_1}{L} \\\\left[ 1 - \\\\frac{\\\\alpha}{\\\\beta} + \\\\alpha (2x_w - 1) \\\\right] - x_w + 1 = \\\\mu - 1, \\\\quad (54) \\\\]\\n\\n\\\\[ \\\\frac{N_1}{L} = \\\\frac{k_2}{k_1 + k_2} \\\\left[ \\\\mu - \\\\alpha \\\\frac{N_1}{L} (2x_w - 1) - 1 + x_w \\\\right] \\\\]\\n\\n\\\\[ - \\\\frac{1}{L(k_1 + k_2)} \\\\frac{N_1}{L} \\\\left( 1 - \\\\frac{N_1}{L} \\\\right). \\\\]\\n\\nWhile solving eqs. (54) and (55) for \\\\( N_1/L \\\\) and \\\\( x_w \\\\), we get a quadratic equation for \\\\( N_1/L \\\\) with two solutions:\\n\\n\\\\[ \\\\left( \\\\frac{N_1}{L} \\\\right)_{\\\\pm} = \\\\left( \\\\frac{1}{2} + \\\\frac{k_{10}}{2\\\\alpha^2} + \\\\frac{k_{20}}{2\\\\beta} \\\\right) \\\\pm \\\\left[ \\\\left( \\\\frac{1}{2} + \\\\frac{k_{10}}{2\\\\alpha^2} + \\\\frac{k_{20}}{2\\\\beta} \\\\right)^2 - \\\\frac{k_{20}}{\\\\alpha^2} \\\\right]^{\\\\frac{1}{2}}. \\\\]\\n\\nThe density in the LD part of the DW is thus\\n\\n\\\\[ \\\\rho_{\\\\text{LD}} = \\\\alpha \\\\frac{N_1}{L} = \\\\left( \\\\frac{1}{2} + \\\\frac{k_{10}}{2\\\\alpha^2} + \\\\frac{k_{20}}{2\\\\beta} \\\\right) \\\\pm \\\\left[ \\\\left( \\\\frac{1}{2} + \\\\frac{k_{10}}{2\\\\alpha^2} + \\\\frac{k_{20}}{2\\\\beta} \\\\right)^2 - \\\\frac{k_{20}}{\\\\alpha^2} \\\\right]^{\\\\frac{1}{2}}. \\\\]\\n\\nAt the boundary between the LD and the DW phases, MFT must predict identical (low) density in the bulk of \\\\( T \\\\). We now argue that in (57) the solution with a negative discriminant is actually the physically acceptable solution. Equating the density in LD phase [eq. (25)] and density in the LD domain of DW phase [eq. (57) with negative discriminant], we obtain the LD-DW boundary as follows:\\n\\n\\\\[ \\\\left( \\\\frac{1 + k_{20}}{2} + \\\\frac{k_{10} + k_{20}}{2\\\\alpha} \\\\right) \\\\]\\n\\n\\\\[ - \\\\left[ \\\\left( \\\\frac{1 + k_{20}}{2} + \\\\frac{k_{10} + k_{20}}{2\\\\alpha} \\\\right)^2 - \\\\mu k_{20} \\\\right]^{\\\\frac{1}{2}} \\\\]\\n\\n\\\\[ = \\\\left( \\\\frac{1}{2} + \\\\frac{k_{10}}{2\\\\alpha} + \\\\frac{k_{20}}{2\\\\beta} \\\\right) - \\\\left[ \\\\left( \\\\frac{1}{2} + \\\\frac{k_{10}}{2\\\\alpha} + \\\\frac{k_{20}}{2\\\\beta} \\\\right)^2 - k_{20} \\\\right]^{\\\\frac{1}{2}}. \\\\]\\n\\nThe LD-DW boundary equation (58) can be simplified further (detailed calculation is given in Appendix) after which it reduces to a form which we will obtain later in subsection V E. We find\\n\\n\\\\[ \\\\frac{N_1}{L} \\\\left( 1 + \\\\alpha - \\\\frac{\\\\alpha}{\\\\beta} \\\\right) = \\\\mu - 1 \\\\]\\n\\nas the LD-DW phase boundary, where \\\\( N_1/L \\\\) is given by\\n\\n\\\\[ \\\\frac{N_1}{L} = \\\\left( \\\\frac{1}{2\\\\alpha} + \\\\frac{k_{10}}{2\\\\alpha^2} + \\\\frac{k_{20}}{2\\\\beta} \\\\right) \\\\]\\n\\n\\\\[ - \\\\left[ \\\\left( \\\\frac{1}{2\\\\alpha} + \\\\frac{k_{10}}{2\\\\alpha^2} + \\\\frac{k_{20}}{2\\\\beta} \\\\right)^2 - \\\\frac{k_{20}}{\\\\alpha^2} \\\\right]^{\\\\frac{1}{2}}. \\\\]\\n\\nTherefore, the acceptable solution of \\\\( N_1/L \\\\) is the one with a negative discriminant; see eq. (60). Next, the expression for \\\\( N_2 \\\\) can be obtained using eq. (51). One finds\\n\\n\\\\[ \\\\frac{N_2}{L} = 1 - \\\\frac{\\\\alpha}{\\\\beta} \\\\frac{N_1}{L} \\\\]\\n\\n\\\\[ = 1 - \\\\left( \\\\frac{1}{2\\\\beta} + \\\\frac{k_{10}}{2\\\\alpha\\\\beta} + \\\\frac{k_{20}}{2\\\\beta^2} \\\\right) \\\\]\\n\\n\\\\[ + \\\\left[ \\\\left( \\\\frac{1}{2\\\\beta} + \\\\frac{k_{10}}{2\\\\alpha\\\\beta} + \\\\frac{k_{20}}{2\\\\beta^2} \\\\right)^2 - \\\\frac{k_{20}}{\\\\beta^2} \\\\right]^{\\\\frac{1}{2}}, \\\\]\\n\\nOnce we obtain the expression for \\\\( N_1/L \\\\) in eq. (60), the position \\\\( x_w \\\\) of the domain wall can be obtained using eq. (54). We find:\\n\\n\\\\[ x_w = \\\\frac{\\\\left( 1 - \\\\alpha - \\\\frac{\\\\alpha}{\\\\beta} \\\\right) \\\\frac{N_1}{L} - \\\\mu + 2}{1 - 2\\\\alpha \\\\frac{N_1}{L}}. \\\\]\\n\\nHaving the expression of \\\\( N_1/L \\\\), it is straightforward to obtain the low and high densities in the DW phase as follows:\\n\\n\\\\[ \\\\rho_{\\\\text{LD}} = \\\\left( \\\\frac{1}{2} + \\\\frac{k_{10}}{2\\\\alpha} + \\\\frac{k_{20}}{2\\\\beta} \\\\right) - \\\\left[ \\\\left( \\\\frac{1}{2} + \\\\frac{k_{10}}{2\\\\alpha} + \\\\frac{k_{20}}{2\\\\beta} \\\\right)^2 - k_{20} \\\\right]^{\\\\frac{1}{2}}. \\\\]\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of threads/CPU cores. For the FL training we utilized Flower framework \\\\(^3\\\\). We simulate a computer vision IoT training task using dataset CIFAR-10 \\\\(^4\\\\) and the computer vision model SqueezeNet which is light-weight and deemed to be suitable for edge computer visions applications. We assign higher kernel priority to baseload process to ensure the FL training doesn\u2019t affect the CPU time of baseload in co-running scenario. For the energy consumption measurement, we utilized WLAN power socket switch\\\\(^5\\\\). We report mean energy per sample and samples per second values for different power-modes and CPU core baseloads.\\n\\n\\\\[\\nEPS = \\\\frac{P_{\\\\text{total}} - P_{\\\\text{BL}}}{N}\\n\\\\]\\n\\n\\\\(EPS\\\\) : Energy per Sample  \\n\\\\(P_{\\\\text{total}}\\\\) : Total power consumption (FL and Baseload)  \\n\\\\(P_{\\\\text{BL}}\\\\) : Power consumption due to Baseload  \\n\\\\(N\\\\) : Number of Samples\\n\\nEnergy per sample values were calculated using Eq. 1. Figure 1a illustrates the mean energy per sample and 95% confidence intervals for each powermode, based on 10 repeated measurements. We observe significant difference in energy per sample and samples per second values when there is no non-FL base load (0 baseload cores) compared to a scenario when non-FL baseload is executing and utilizing all CPU cores. We also observe that while samples per second (Figure 1b) doesn\u2019t vary significantly when non-FL baseload is co-running with FL, energy per sample values fluctuate for baseloads 3 and 4.\\n\\n\\\\(^3\\\\)[https://flower.dev](https://flower.dev)  \\n\\\\(^4\\\\)[https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)  \\n\\\\(^5\\\\)[https://www.delock.com/produkt/11826/merkmale.html](https://www.delock.com/produkt/11826/merkmale.html)\\n\\nFor our experiments, ondemand mode with baseload cores 3 has an optimum energy usage when calculating same number of samples compared to other baseload and samples per second combinations.\\n\\nIV. CONCLUSION AND FUTURE WORK\\n\\nRecent research studies have focused on energy-efficient and carbon-efficient FL scheduling and client selection. However, most of the research assumes simplistic energy consumption models for underlying FL clients. In this work, we showed that how energy per sample values under real-world scenarios such as different power modes and non-FL baseloads at CPU cores can vary and exhibit complex operational behavior patterns.\\n\\nFor future work, following open research questions and possibilities could be explored further,\\n\\n- How do current FL systems communicate FL clients\u2019 energy related information? How to collect energy per sample, throughput per second and uncertainty related information at runtime?\\n- How can we predict the power-performance characteristics, what are the relevant metrics? With more data about real-world impact factors affecting energy footprint of edge devices, can we build predictive models for forecasting?\\n- How often do we need to measure before we can be certain? Can we report the uncertainty to be used in scheduling? FL trainings are usually executed multiple times due to data distribution drifts and hyperparameter search. This repetitive FL training execution could be leveraged to collect more data about power-performance behavior patterns of FL clients.\\n- What\u2019s the impact of hardware accelerated edge devices such as jetson nano on energy related metrics? What are the energy efficiency opportunities in FL and non-FL co-running scenarios?\\n\\nREFERENCES\\n\\n[1] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, \u201cCommunication-efficient learning of deep networks from decentralized data,\u201d in AISTATS, 2016.\\n[2] X. Qiu, T. Parcollet, D. J. Beutel, T. Topal, A. Mathur, and N. D. Lane, \u201cA first look into the carbon footprint of federated learning,\u201d CoRR, vol. abs/2010.06537, 2020.\\n[3] X. Zhou, J. Zhao, H. Han, and C. Guet, \u201cJoint optimization of energy consumption and completion time in federated learning,\u201d in ICDCS, IEEE, 2022.\\n[4] C. W. Zaw, S. R. Pandey, K. Kim, and C. S. Hong, \u201cEnergy-aware resource management for federated learning in multi-access edge computing systems,\u201d IEEE Access, vol. 9, pp. 34938\u201334950, 2021.\\n[5] Y. G. Kim and C.-J. Wu, \u201cAutofl: Enabling heterogeneity-aware energy efficient federated learning,\u201d in IEEE/ACM International Symposium on Microarchitecture (MICRO), pp. 183\u2013198, 2021.\\n[6] P. Wiesner, R. Khalili, D. Grimwald, P. Agrawal, L. Thamsen, and O. Kao, \u201cFedzero: Leveraging renewable excess energy in federated learning,\u201d arXiv preprint arXiv:2305.15092, 2023.\\n[7] B. G\u00fcler and A. Yener, \u201cA framework for sustainable federated learning,\u201d in International Symposium on Modeling and Optimization in Mobile, Ad hoc, and Wireless Networks (WiOpt), IEEE, 2021.\\n[8] B. Rupprecht, D. Hajo, and B. Vogel-Heuser, \u201cPerformance evaluation of ai algorithms on heterogeneous edge devices for manufacturing,\u201d in International Conference on Automation Science and Engineering (CASE), pp. 2132\u20132139, IEEE, 2022.\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of threads/CPU cores. For the FL training we utilized Flower framework 3 . We simulate a computer vision IoT training task [https://flower.dev](https://flower.dev) using dataset CIFAR-10 4 and the computer vision model [https://www.cs.toronto.edu/](https://www.cs.toronto.edu/) kriz/cifar.html SqueezeNet which is light-weight and deemed to be suitable for edge computer visions applications. We assign higher kernel priority to baseload process to ensure the FL training doesn\u2019t affect the CPU time of baseload in co-running scenario. For the energy consumption measurement, we utilized WLAN power socket switch 5 . We report mean energy per [https://www.delock.com/produkt/11826/merkmale.html](https://www.delock.com/produkt/11826/merkmale.html) sample and samples per second values for different powermodes and CPU core baseloads. \\n\\n\\\\[\\nEPS = \\\\frac{P_{\\\\text{total}} - P_{\\\\text{BL}}}{N}\\n\\\\]  \\n\\n(1)\\n\\n\\\\(EPS\\\\) : Energy per Sample  \\n\\\\(P_{\\\\text{total}}\\\\) : Total power consumption (FL and Baseload)  \\n\\\\(P_{\\\\text{BL}}\\\\) : Power consumption due to Baseload  \\n\\\\(N\\\\) : Number of Samples\\n\\nEnergy per sample values were calculated using Eq. 1. Figure 1a illustrates the mean energy per sample and 95% confidence intervals for each powermode, based on 10 repeated measurements. We observe significant difference in energy per sample and samples per second values when there is no nonFL base load (0 baseload cores) compared to a scenario when non-FL baseload is executing and utilizing all CPU cores. We also observe that while samples per second (Figure 1b) doesn\u2019t vary significantly when non-FL baseload is co-running with FL, energy per sample values fluctuate for baseloads 3 and 4. \\n\\nFor our experiments, ondemand mode with baseload cores 3 has an optimum energy usage when calculating same number of samples compared to other baseload and samples per second combinations. \\n\\nIV. CONCLUSION AND F UTURE W ORK Recent research studies have focused on energy-efficient and carbon-efficient FL scheduling and client selection. However, most of the research assumes simplistic energy consumption models for underlying FL clients. In this work, we showed that how energy per sample values under real-world scenarios such as different power modes and non-FL baseloads at CPU cores can vary and exhibit complex operational behavior patterns. For future work, following open research questions and possibilities could be explored further, \\n\\n- How do current FL systems communicate FL clients\u2019 energy related information? How to collect energy per sample, throughput per second and uncertainty related information at runtime? \\n- How can we predict the power-performance characteristics, what are the relevant metrics? With more data about real-world impact factors affecting energy footprint of edge devices, can we build predictive models for forecasting? \\n- How often do we need to measure before we can be certain? Can we report the uncertainty to be used in scheduling? FL trainings are usually executed multiple times due to data distribution drifts and hyperparameter search. This repetitive FL training execution could be leveraged to collect more data about power-performance behavior patterns of FL clients. \\n- What\u2019s the impact of hardware accelerated edge devices such as jetson nano on energy related metrics? What are the energy efficiency opportunities in FL and non-FL corunning scenarios? \\n\\nREFERENCES\\n\\n[1] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, \u201cCommunication-efficient learning of deep networks from decentralized data,\u201d in AISTATS, 2016. [2] X. Qiu, T. Parcollet, D. J. Beutel, T. Topal, A. Mathur, and N. D. Lane, \u201cA first look into the carbon footprint of federated learning,\u201d CoRR, vol. abs/2010.06537, 2020. [3] X. Zhou, J. Zhao, H. Han, and C. Guet, \u201cJoint optimization of energy consumption and completion time in federated learning,\u201d in ICDCS, IEEE, 2022. [4] C. W. Zaw, S. R. Pandey, K. Kim, and C. S. Hong, \u201cEnergy-aware resource management for federated learning in multi-access edge computing systems,\u201d IEEE Access, vol. 9, pp. 34938\u201334950, 2021. [5] Y. G. Kim and C.-J. Wu, \u201cAutofl: Enabling heterogeneity-aware energy efficient federated learning,\u201d in IEEE/ACM International Symposium on Microarchitecture (MICRO), pp. 183\u2013198, 2021. [6] P. Wiesner, R. Khalili, D. Grimwald, P. Agrawal, L. Thamsen, and O. Kao, \u201cFedzero: Leveraging renewable excess energy in federated learning,\u201d arXiv preprint arXiv:2305.15092, 2023. [7] B. G\u00fcler and A. Yener, \u201cA framework for sustainable federated learning,\u201d in International Symposium on Modeling and Optimization in Mobile, Ad hoc, and Wireless Networks (WiOpt), IEEE, 2021. [8] B. Rupprecht, D. Hajo, and B. Vogel-Heuser, \u201cPerformance evaluation of ai algorithms on heterogeneous edge devices for manufacturing,\u201d in International Conference on Automation Science and Engineering (CASE), pp. 2132\u20132139, IEEE, 2022.\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the process of scanning the database, the system identifies all the songs that have the word 'good' either in their titles or in their lyrics. The first song that meets this criterion is 'Good Life' by Kehlani and G-Eazy. This song contains the word 'good' not only in its title but also within its lyrical content.\\n\\nIt\u2019s important to note that the search doesn\u2019t stop at 'Good Life.' There might be many other songs with the word 'good' in their titles or lyrics within the database. The given information only indicates the first result that the system presents in response to the query 'good.'\\n\\nTo summarize, the system is a music-centric search tool that uses the provided query to locate songs in its database containing the term 'good' either in their titles or lyrics. The first song it finds is 'Good Life' by Kehlani and G-Eazy, which contains 'good' both in its title and lyrics. The system can provide more results if the user wishes to explore other songs with the word 'good.'\\n\\nIn Figure 7, the dataset consists of song lyrics from the year 1950 to 2019. This provides a well-spread dataset which allows us to get results from over 60 years, thus making the search engine efficient enough to provide songs from every year. The year 2017 has the most songs amounting up to 660 songs from various different genres and emotions.\\n\\nIn Figure 8, the dataset has been categorized into 7 major genres namely - pop, country, blues, rock, jazz, reggae and hip hop. There are about 7042 songs under the genre pop. This includes pop\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the process of scanning the database, the system identifies all the songs that have the word \u2019good\u2019 either in their titles or in their lyrics. The first song that meets this criterion is \u2019Good Life\u2019 by Kehlani and G-Eazy. This song contains the word \u2019good\u2019 not only in its title but also within its lyrical content. It\u2019s important to note that the search doesn\u2019t stop at \u2019Good Life.\u2019 There might be many other songs with the word \u2019good\u2019 in their titles or lyrics within the database. The given information only indicates the first result that the system presents in response to the query \u2019good.\u2019\\n\\nTo summarize, the system is a music-centric search tool that uses the provided query to locate songs in its database containing the term \u2019good\u2019 either in their titles or lyrics. The first song it finds is \u2019Good Life\u2019 by Kehlani and G-Eazy, which contains \u2019good\u2019 both in its title and lyrics. The system can provide more results if the user wishes to explore other songs with the word \u2019good.\u2019\\n\\nIn Figure 7, the dataset consists of song lyrics from the year 1950 to 2019. This provides a wellspread dataset which allows us to get results from over 60 years, thus making the search engine efficient enough to provide songs from every year. The year 2017 has the most songs amounting upto 660 songs from various different genres and emotions. In Figure 8, the dataset has been categorized into 7 major genres namely pop, country, blues, rock, jazz, reggae and hip hop. There are about 7042 songs under the genre pop. This includes pop\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3 Experiments with C++ data\\n\\nTo answer the first research question, we used the original C++ method-level vulnerability dataset from [1]. After parsing, we obtained the following statistics of the input graphs: 11788 train graphs (956 vulnerable), 1667 validation graphs (133 vulnerable), 3385 test graphs (286 vulnerable).\\n\\nTo test each dimension of RQ 1, we performed 10 trials of training the model. In each trial, the dataset was split into train, validation, and test parts anew. The results can be found in Table 1.\\n\\n| Configuration          | Median F1 | Median ROC AUC |\\n|------------------------|-----------|----------------|\\n| Baseline               | 27.29     | 0.696          |\\n| Without SMOTE & RL     | 21.45     | 0.730          |\\n| Without AST edges      | 27.65     | 0.706          |\\n| With pruning           | 30.83     | 0.724          |\\n| Majority downsampling  | 26.61     | 0.678          |\\n\\nTable 1. Results of experiments for research question 1\\n\\n3.1 Excluding SMOTE and RL\\n\\nThe model without SMOTE and RL achieves the worst performance with respect to the F1 score and the best performance with respect to the ROC AUC measure.\\n\\n3.2 AST edges\\n\\nThe model performs slightly better without including AST edges. This is likely due to including too much of fine-grained information or too many nodes. The model becomes more likely to overfit to irrelevant features in the input and fail to generalize.\\n\\n3.3 Pruning\\n\\nThe experiments also showed that the model performs better with pruning at operator nodes. Pruning makes a graph simpler and less entangled for the model to understand.\\n\\n3.4 Downsampling\\n\\nTable 1 shows that the model performs worse with balancing the train set by downsampling non-vulnerable methods. We think that a rough balancing of the train part impacts the score negatively since it turns off SMOTE.\\n\\n4 Experiments with Java data\\n\\nTo answer the rest of research questions, we trained and tested the model on different parts of the Java dataset (1): P1, P2, and P3. In particular, we varied k in the range from 1 to 14. Then, we plotted the resulting ROC AUC scores against k, and draw conclusions based on the observed dynamics. To make set P3 to be independent of k, we fixed it to be the complement of P1. That is, P3 consisted of functions that remained unchanged in the commits where only one function was changed. Also, in order to balance different parts involved in training and testing, we restricted the size of P3:\\n\\n\\\\[ |P_3| = |P_1| + |P_2| \\\\]\\n\\nDuring the data cleaning phase, we ensured that in each experiment, P3 did not contain functions that are contained in \\\\( P_1 \\\\cup P_2 \\\\). Also, we removed any duplicate functions from each of the parts \\\\( P_1, P_2, \\\\) and \\\\( P_3 \\\\), and removed methods contained in the training data from the test data.\\n\\nTable 2 shows the distribution of the collected Java methods after stratification by k and cleaning the data:\\n\\n| k   | P1 train | P1 test | P2 train | P2 test |\\n|-----|----------|---------|----------|---------|\\n| 1   | 410 (205)| 135 (68)| 0 (0)    | 0 (0)   |\\n| 2   | 399 (200)| 145 (73)| 343 (171)| 122 (61)|\\n| 3   | 416 (208)| 132 (66)| 696 (347)| 228 (113)|\\n| 4   | 414 (207)| 128 (65)| 960 (479)| 346 (172)|\\n| 5   | 415 (210)| 129 (64)| 1159 (575)| 433 (217)|\\n| 6   | 414 (208)| 131 (65)| 1393 (692)| 506 (254)|\\n| 7   | 421 (212)| 120 (60)| 1583 (789)| 596 (296)|\\n| 8   | 394 (197)| 151 (75)| 1870 (938)| 572 (284)|\\n| 9   | 410 (207)| 135 (67)| 2027 (1012)| 664 (330)|\\n| 10  | 411 (206)| 131 (66)| 2195 (1089)| 632 (314)|\\n| 11  | 399 (199)| 150 (75)| 2439 (1215)| 708 (353)|\\n| 12  | 400 (202)| 144 (72)| 2545 (1270)| 769 (383)|\\n| 13  | 397 (200)| 143 (72)| 2619 (1303)| 872 (434)|\\n| 14  | 409 (204)| 136 (68)| 2853 (1421)| 845 (419)|\\n\\nTable 2. Statistics of collected Java methods after stratification by k and cleaning. Each cell has the format \\\\( N_1(N_2) \\\\), where \\\\( N_1 \\\\) is the total number of methods and \\\\( N_2 \\\\) is the number of vulnerable ones.\\n\\n4.1 Research question 2\\n\\nIn this research question, we investigate training on different combinations of sets \\\\( P_1, P_2, \\\\) and \\\\( P_3 \\\\), and testing on \\\\( P_1 \\\\cup P_2 \\\\cup P_3 \\\\) or \\\\( P_1 \\\\cup P_3 \\\\), which is a stricter test. The results can be found in Figures 2 and 3.\\n\\nFigures 2 and 3 allow us to conclude that if the test set includes part \\\\( P_3 \\\\), then the inclusion of part \\\\( P_3 \\\\) into training is critical to achieving a high performance. Overall, parts \\\\( P_2 \\\\) and \\\\( P_3 \\\\) contribute the most to the prediction, as seen by the red and blue lines on Figures 2 and 3.\\n\\nAlso, on Figure 3, we see a slight degradation of performance corresponding to training on \\\\( P_2 \\\\cup P_3 \\\\) (red line) as \\\\( k \\\\) increases. This might indicate the increasing amount of\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3 Experiments with C++ data\\n\\nTo answer the first research question, we used the original C++ method-level vulnerability dataset from [1]. After parsing, we obtained the following statistics of the input graphs: 11788 train graphs (956 vulnerable), 1667 validation graphs (133 vulnerable), 3385 test graphs (286 vulnerable).\\n\\nTo test each dimension of RQ 1, we performed 10 trials of training the model. In each trial, the dataset was split into train, validation, and test parts anew. The results can be found in Table 1.\\n\\n| Configuration          | Median F1 | Median ROC AUC |\\n|------------------------|-----------|----------------|\\n| Baseline               | 27.29     | 0.696          |\\n| Without SMOTE & RL     | 21.45     | 0.730          |\\n| Without AST edges      | 27.65     | 0.706          |\\n| With pruning           | 30.83     | 0.724          |\\n| Majority downsampling  | 26.61     | 0.678          |\\n\\nTable 1. Results of experiments for research question 1\\n\\n3.1 Excluding SMOTE and RL\\n\\nThe model without SMOTE and RL achieves the worst performance with respect to the F1 score and the best performance with respect to the ROC AUC measure.\\n\\n3.2 AST edges\\n\\nThe model performs slightly better without including AST edges. This is likely due to including too much of fine-grained information or too many nodes. The model becomes more likely to overfit to irrelevant features in the input and fail to generalize.\\n\\n3.3 Pruning\\n\\nThe experiments also showed that the model performs better with pruning at operator nodes. Pruning makes a graph simpler and less entangled for the model to understand.\\n\\n3.4 Downsampling\\n\\nTable 1 shows that the model performs worse with balancing the train set by downsampling non-vulnerable methods. We think that a rough balancing of the train part impacts the score negatively since it turns off SMOTE.\\n\\n4 Experiments with Java data\\n\\nTo answer the rest of research questions, we trained and tested the model on different parts of the Java dataset (1): P1, P2, and P3. In particular, we varied k in the range from 1 to 14. Then, we plotted the resulting ROC AUC scores against k, and draw conclusions based on the observed dynamics. To make set P3 to be independent of k, we fixed it to be the complement of P1. That is, P3 consisted of functions that remained unchanged in the commits where only one function was changed. Also, in order to balance different parts involved in training and testing, we restricted the size of P3:\\n\\n\\\\[ |P_3| = |P_1| + |P_2| \\\\]\\n\\nDuring the data cleaning phase, we ensured that in each experiment, P3 did not contain functions that are contained in \\\\( P_1 \\\\cup P_2 \\\\). Also, we removed any duplicate functions from each of the parts \\\\( P_1, P_2, \\\\) and \\\\( P_3 \\\\), and removed methods contained in the training data from the test data.\\n\\nTable 2 shows the distribution of the collected Java methods after stratification by k and cleaning the data:\\n\\n| k  | P1 train | P1 test | P2 train | P2 test |\\n|----|---------|---------|----------|---------|\\n| 1  | 410 (205) | 135 (68) | 0 (0) | 0 (0) |\\n| 2  | 399 (200) | 145 (73) | 343 (171) | 122 (61) |\\n| 3  | 416 (208) | 132 (66) | 696 (347) | 228 (113) |\\n| 4  | 414 (207) | 128 (65) | 960 (479) | 346 (172) |\\n| 5  | 415 (210) | 129 (64) | 1159 (575) | 433 (217) |\\n| 6  | 414 (208) | 131 (65) | 1393 (692) | 506 (254) |\\n| 7  | 421 (212) | 120 (60) | 1583 (789) | 596 (296) |\\n| 8  | 394 (197) | 151 (75) | 1870 (938) | 572 (284) |\\n| 9  | 410 (207) | 135 (67) | 2027 (1012) | 664 (330) |\\n| 10 | 411 (206) | 131 (66) | 2195 (1089) | 632 (314) |\\n| 11 | 399 (199) | 150 (75) | 2439 (1215) | 708 (353) |\\n| 12 | 400 (202) | 144 (72) | 2545 (1270) | 769 (383) |\\n| 13 | 397 (200) | 143 (72) | 2619 (1303) | 872 (434) |\\n| 14 | 409 (204) | 136 (68) | 2853 (1421) | 845 (419) |\\n\\nTable 2. Statistics of collected Java methods after stratification by k and cleaning. Each cell has the format \\\\( N_1(N_2) \\\\), where \\\\( N_1 \\\\) is the total number of methods and \\\\( N_2 \\\\) is the number of vulnerable ones.\\n\\n4.1 Research question 2\\n\\nIn this research question, we investigate training on different combinations of sets \\\\( P_1, P_2, \\\\) and \\\\( P_3, \\\\) and testing on \\\\( P_1 \\\\cup P_2 \\\\cup P_3 \\\\) or \\\\( P_1 \\\\cup P_3 \\\\), which is a stricter test. The results can be found in Figures 2 and 3.\\n\\nFigures 2 and 3 allow us to conclude that if the test set includes part \\\\( P_3 \\\\), then the inclusion of part \\\\( P_3 \\\\) into training is critical to achieving a high performance. Overall, parts \\\\( P_2 \\\\) and \\\\( P_3 \\\\) contribute the most to the prediction, as seen by the red and blue lines on Figures 2 and 3.\\n\\nAlso, on Figure 3, we see a slight degradation of performance corresponding to training on \\\\( P_2 \\\\cup P_3 \\\\) (red line) as \\\\( k \\\\) increases. This might indicate the increasing amount of\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"After expanding further the minors of above determinant based on Theorem 1, we see that this result is the same as the result of Example 2.\\n\\nFor \\\\( i = 2 \\\\), we have:\\n\\n\\\\[\\nA = \\\\begin{pmatrix}\\n3 & 0 & -4 & -2 & 4 & 0 & 5 & 1 & 0 \\\\\\\\\\n2 & 5 & -1 & -3 & 0 & 3 & 1 & 2 \\\\\\\\\\n0 & 3 & -2 & -3 & 2 & 5 & 0 & 4 & 3\\n\\\\end{pmatrix}\\n\\\\]\\n\\n\\\\[\\n= 2 \\\\cdot \\\\begin{pmatrix}\\n4 & 0 & 1 & 0 \\\\\\\\\\n2 & 5 & 4 & 3\\n\\\\end{pmatrix} - 5 \\\\cdot \\\\begin{pmatrix}\\n-2 & 0 & 5 & 0 \\\\\\\\\\n-3 & 3 & 3 & 2\\n\\\\end{pmatrix} + (-1) \\\\cdot \\\\begin{pmatrix}\\n-2 & 4 & 5 & 1 \\\\\\\\\\n-3 & 2 & 0 & 4\\n\\\\end{pmatrix} - (-3) \\\\cdot \\\\begin{pmatrix}\\n0 & -4 & 1 & 0 \\\\\\\\\\n3 & -2 & 4 & 3\\n\\\\end{pmatrix} +\\n\\\\]\\n\\n\\\\[\\n0 \\\\cdot \\\\begin{pmatrix}\\n3 & -4 & 5 & 0 \\\\\\\\\\n0 & -2 & 0 & 3\\n\\\\end{pmatrix} - 3 \\\\cdot \\\\begin{pmatrix}\\n3 & 0 & 5 & 1 \\\\\\\\\\n0 & 3 & 0 & 4\\n\\\\end{pmatrix} + 5 \\\\cdot \\\\begin{pmatrix}\\n5 & -1 & 0 & 3 \\\\\\\\\\n3 & -2 & 2 & 5\\n\\\\end{pmatrix} - 1 \\\\cdot \\\\begin{pmatrix}\\n2 & -1 & -3 & 3 \\\\\\\\\\n0 & -2 & -3 & 5\\n\\\\end{pmatrix} +\\n\\\\]\\n\\n\\\\[\\n0 \\\\cdot \\\\begin{pmatrix}\\n2 & 5 & -3 & 0 \\\\\\\\\\n0 & 3 & -3 & 2\\n\\\\end{pmatrix} = 326.\\n\\\\]\\n\\nAfter expanding further the minors of above determinant based on Theorem 1, we see that this result is the same as the result of Example 2.\\n\\nFor \\\\( i = 3 \\\\), we have:\\n\\n\\\\[\\nA = \\\\begin{pmatrix}\\n3 & 0 & -4 & -2 & 4 & 0 & 5 & 1 & 0 \\\\\\\\\\n2 & 5 & -1 & -3 & 0 & 3 & 1 & 2 \\\\\\\\\\n0 & 3 & -2 & -3 & 2 & 5 & 0 & 4 & 3\\n\\\\end{pmatrix}\\n\\\\]\\n\\n\\\\[\\n= 0 \\\\cdot \\\\begin{pmatrix}\\n4 & 0 & 1 & 0 \\\\\\\\\\n0 & 3 & 1 & 2\\n\\\\end{pmatrix} - 5 \\\\cdot \\\\begin{pmatrix}\\n-2 & 0 & 5 & 0 \\\\\\\\\\n-3 & 3 & 3 & 2\\n\\\\end{pmatrix} + (-2) \\\\cdot \\\\begin{pmatrix}\\n-2 & 4 & 5 & 1 \\\\\\\\\\n-3 & 2 & 0 & 4\\n\\\\end{pmatrix} - (-3) \\\\cdot \\\\begin{pmatrix}\\n0 & -4 & 1 & 0 \\\\\\\\\\n5 & -1 & 1 & 2\\n\\\\end{pmatrix} +\\n\\\\]\\n\\n\\\\[\\n2 \\\\cdot \\\\begin{pmatrix}\\n3 & -4 & 5 & 0 \\\\\\\\\\n2 & -1 & 3 & 2\\n\\\\end{pmatrix} - 5 \\\\cdot \\\\begin{pmatrix}\\n3 & 0 & 5 & 1 \\\\\\\\\\n2 & 5 & 3 & 1\\n\\\\end{pmatrix} + 0 \\\\cdot \\\\begin{pmatrix}\\n5 & -1 & 0 & 3 \\\\\\\\\\n5 & -1 & 0 & 3\\n\\\\end{pmatrix} - 4 \\\\cdot \\\\begin{pmatrix}\\n2 & -1 & -3 & 3 \\\\\\\\\\n2 & -1 & -3 & 3\\n\\\\end{pmatrix} +\\n\\\\]\\n\\n\\\\[\\n3 \\\\cdot \\\\begin{pmatrix}\\n2 & 5 & -3 & 0 \\\\\\\\\\n2 & 5 & -3 & 0\\n\\\\end{pmatrix} = 326.\\n\\\\]\\n\\nAfter expanding further the minors of above determinant based on Theorem 1, we see that this result is the same as the result of Example 2.\\n\\nFor \\\\( j = 1 \\\\), we have:\\n\\n\\\\[\\nA = \\\\begin{pmatrix}\\n3 & 0 & -4 & -2 & 4 & 0 & 5 & 1 & 0 \\\\\\\\\\n2 & 5 & -1 & -3 & 0 & 3 & 1 & 2 \\\\\\\\\\n0 & 3 & -2 & -3 & 2 & 5 & 0 & 4 & 3\\n\\\\end{pmatrix}\\n\\\\]\\n\\n\\\\[\\n= 3 \\\\cdot \\\\begin{pmatrix}\\n0 & 3 & 1 & 2 \\\\\\\\\\n2 & 5 & 4 & 3\\n\\\\end{pmatrix} - 2 \\\\cdot \\\\begin{pmatrix}\\n4 & 0 & 1 & 0 \\\\\\\\\\n2 & 5 & 4 & 3\\n\\\\end{pmatrix} + 0 \\\\cdot \\\\begin{pmatrix}\\n4 & 0 & 1 & 0 \\\\\\\\\\n0 & 3 & 1 & 2\\n\\\\end{pmatrix} - (-2) \\\\cdot \\\\begin{pmatrix}\\n5 & -1 & 1 & 2 \\\\\\\\\\n3 & -2 & 4 & 3\\n\\\\end{pmatrix} +\\n\\\\]\\n\\n\\\\[\\n(-3) \\\\cdot \\\\begin{pmatrix}\\n0 & -4 & 1 & 0 \\\\\\\\\\n3 & -2 & 4 & 3\\n\\\\end{pmatrix} - (-3) \\\\cdot \\\\begin{pmatrix}\\n0 & -4 & 1 & 0 \\\\\\\\\\n5 & -1 & 1 & 2\\n\\\\end{pmatrix} + 5 \\\\cdot \\\\begin{pmatrix}\\n5 & -1 & 0 & 3 \\\\\\\\\\n3 & -2 & 2 & 5\\n\\\\end{pmatrix} - 3 \\\\cdot \\\\begin{pmatrix}\\n0 & -4 & 1 & 0 \\\\\\\\\\n3 & -2 & 2 & 5\\n\\\\end{pmatrix} +\\n\\\\]\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\\\[\\n4 \\\\cdot \\\\begin{vmatrix} 2 & -1 & 3 \\\\\\\\ 0 & -2 & 0 \\\\\\\\ 3 & 2 & 3 \\\\end{vmatrix} - 0 \\\\cdot \\\\begin{vmatrix} 2 & 5 & 3 \\\\\\\\ 0 & 3 & 0 \\\\\\\\ 4 & 1 & 4 \\\\end{vmatrix} + 5 \\\\cdot \\\\begin{vmatrix} 5 & -1 & 0 \\\\\\\\ 3 & -2 & 2 \\\\\\\\ 5 & 1 & 3 \\\\end{vmatrix} - 1 \\\\cdot \\\\begin{vmatrix} 2 & -1 & -3 \\\\\\\\ 0 & -2 & -3 \\\\\\\\ 3 & 5 & 3 \\\\end{vmatrix} + \\\\\\\\\\n0 \\\\cdot \\\\begin{vmatrix} 2 & 5 \\\\\\\\ 0 & 3 \\\\\\\\ -3 & 2 \\\\end{vmatrix} = 326.\\n\\\\]\\n\\nAfter expanding further the minors of above determinant based on Theorem 1, we see that this result is the same as the result of Example 2. For \\\\( j = 2 \\\\), we have:\\n\\n\\\\[\\nA = \\\\begin{pmatrix} 3 & 0 & -4 & -2 & 4 & 0 & 5 & 1 & 0 \\\\\\\\ 2 & 5 & -1 & -3 & 0 & 3 & 1 & 2 \\\\\\\\ 0 & 3 & -2 & -3 & 2 & 5 & 0 & 4 & 3 \\\\end{pmatrix}\\n\\\\]\\n\\n\\\\[\\n= 2 \\\\cdot \\\\begin{vmatrix} 4 & 0 & 1 \\\\\\\\ 2 & 5 & 4 \\\\\\\\ 0 & 3 & 1 \\\\end{vmatrix} - 5 \\\\cdot \\\\begin{vmatrix} -2 & 0 & 5 \\\\\\\\ -3 & 3 & 2 \\\\\\\\ 0 & 3 & 0 \\\\end{vmatrix} + (-1) \\\\cdot \\\\begin{vmatrix} -2 & 4 & 5 \\\\\\\\ -3 & 2 & 0 \\\\\\\\ 0 & 3 & 1 \\\\end{vmatrix} - (-3) \\\\cdot \\\\begin{vmatrix} 0 & -4 & 1 \\\\\\\\ 3 & -2 & 4 \\\\\\\\ 0 & 3 & 1 \\\\end{vmatrix} + \\\\\\\\\\n0 \\\\cdot \\\\begin{vmatrix} 2 & 5 \\\\\\\\ 0 & 3 \\\\\\\\ -3 & 2 \\\\end{vmatrix} = 326.\\n\\\\]\\n\\nAfter expanding further the minors of above determinant based on Theorem 1, we see that this result is the same as the result of Example 2. For \\\\( i = 3 \\\\), we have:\\n\\n\\\\[\\nA = \\\\begin{pmatrix} 3 & 0 & -4 & -2 & 4 & 0 & 5 & 1 & 0 \\\\\\\\ 2 & 5 & -1 & -3 & 0 & 3 & 1 & 2 \\\\\\\\ 0 & 3 & -2 & -3 & 2 & 5 & 0 & 4 & 3 \\\\end{pmatrix}\\n\\\\]\\n\\n\\\\[\\n= 0 \\\\cdot \\\\begin{vmatrix} 4 & 0 & 1 \\\\\\\\ 0 & 3 & 1 \\\\\\\\ 2 & 5 & 4 \\\\end{vmatrix} - 5 \\\\cdot \\\\begin{vmatrix} -2 & 0 & 5 \\\\\\\\ -3 & 3 & 2 \\\\\\\\ 0 & 3 & 0 \\\\end{vmatrix} + (-2) \\\\cdot \\\\begin{vmatrix} -2 & 4 & 5 \\\\\\\\ -3 & 2 & 0 \\\\\\\\ 0 & 3 & 1 \\\\end{vmatrix} - (-3) \\\\cdot \\\\begin{vmatrix} 0 & -4 & 1 \\\\\\\\ 3 & -2 & 4 \\\\\\\\ 0 & 3 & 1 \\\\end{vmatrix} + \\\\\\\\\\n2 \\\\cdot \\\\begin{vmatrix} 3 & -4 & 5 \\\\\\\\ 2 & -1 & 3 \\\\\\\\ 0 & 3 & 2 \\\\end{vmatrix} - 5 \\\\cdot \\\\begin{vmatrix} 3 & 0 & 5 \\\\\\\\ 2 & 5 & 3 \\\\\\\\ 1 & 0 \\\\end{vmatrix} + 0 \\\\cdot \\\\begin{vmatrix} 5 & -1 & 0 \\\\\\\\ 3 & -2 & 2 \\\\\\\\ 0 & 3 & 1 \\\\end{vmatrix} - 4 \\\\cdot \\\\begin{vmatrix} 2 & -1 & -3 \\\\\\\\ 0 & -2 & -3 \\\\\\\\ 3 & 5 & 3 \\\\end{vmatrix} + \\\\\\\\\\n3 \\\\cdot \\\\begin{vmatrix} 2 & 5 \\\\\\\\ 0 & 3 \\\\\\\\ -3 & 2 \\\\end{vmatrix} = 326.\\n\\\\]\\n\\nAfter expanding further the minors of above determinant based on Theorem 1, we see that this result is the same as the result of Example 2. For \\\\( j = 1 \\\\), we have:\\n\\n\\\\[\\nA = \\\\begin{pmatrix} 3 & 0 & -4 & -2 & 4 & 0 & 5 & 1 & 0 \\\\\\\\ 2 & 5 & -1 & -3 & 0 & 3 & 1 & 2 \\\\\\\\ 0 & 3 & -2 & -3 & 2 & 5 & 0 & 4 & 3 \\\\end{pmatrix}\\n\\\\]\\n\\n\\\\[\\n= 3 \\\\cdot \\\\begin{vmatrix} 0 & 3 & 1 \\\\\\\\ 2 & 5 & 4 \\\\\\\\ 3 & 2 & 3 \\\\end{vmatrix} - 2 \\\\cdot \\\\begin{vmatrix} 4 & 0 & 1 \\\\\\\\ 2 & 5 & 4 \\\\\\\\ 3 & 2 & 3 \\\\end{vmatrix} + 0 \\\\cdot \\\\begin{vmatrix} 4 & 0 & 1 \\\\\\\\ 0 & 3 & 1 \\\\\\\\ 2 & 5 & 4 \\\\end{vmatrix} - (-2) \\\\cdot \\\\begin{vmatrix} 5 & -1 & 1 \\\\\\\\ 3 & -2 & 4 \\\\\\\\ 0 & 3 & 1 \\\\end{vmatrix} + \\\\\\\\\\n(-3) \\\\cdot \\\\begin{vmatrix} 0 & -4 & 1 \\\\\\\\ 3 & -2 & 4 \\\\\\\\ 0 & 3 & 2 \\\\end{vmatrix} - (-3) \\\\cdot \\\\begin{vmatrix} 0 & -4 & 1 \\\\\\\\ 5 & -1 & 1 \\\\\\\\ 2 & 5 & 3 \\\\end{vmatrix} + 5 \\\\cdot \\\\begin{vmatrix} 5 & -1 & 0 \\\\\\\\ 3 & -2 & 2 \\\\\\\\ 5 & 1 & 3 \\\\end{vmatrix} - 3 \\\\cdot \\\\begin{vmatrix} 0 & -4 & 1 \\\\\\\\ 3 & -2 & 2 \\\\\\\\ 5 & 1 & 3 \\\\end{vmatrix} + \\\\\\\\\\n14 \\\\cdot \\\\begin{vmatrix} 0 & -4 & 1 \\\\\\\\ 3 & -2 & 4 \\\\\\\\ 0 & 3 & 2 \\\\end{vmatrix} = 326.\\n\\\\]\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Let us denote by $\\\\mathcal{T} = (\\\\kappa \\\\times \\\\omega \\\\times \\\\text{acc}(\\\\kappa)) \\\\times \\\\gamma \\\\times \\\\ "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Let us denote by $\\\\mathcal{T} = (\\\\kappa \\\\times \\\\omega \\\\times \\\\text{acc}(\\\\kappa) \\\\times \\\\gamma \\\\times \\\\kappa \\\\times \\\\kappa \\\\times \\\\kappa \\\\times \\\\kappa \\\\times \\\\kappa) \\\\leq \\\\gamma$. For every $\\\\xi \\\\in \\\\mathcal{T}$ there are functions $\\\\{\\\\xi_i \\\\in \\\\kappa^{\\\\leq \\\\omega} \\\\mid 0 < i \\\\leq 8\\\\}$ such that for all $i \\\\leq 8$, $\\\\text{dom}(\\\\xi_i) = \\\\text{dom}(\\\\xi)$ and for all $n \\\\in \\\\text{dom}(\\\\xi)$, $\\\\xi(n) = (\\\\xi_1(n), \\\\xi_2(n), \\\\xi_3(n), \\\\xi_4(n), \\\\xi_5(n), \\\\xi_6(n), \\\\xi_7(n), \\\\xi_8(n))$. For every $\\\\xi \\\\in \\\\mathcal{T}$ let us denote $(\\\\xi_4, \\\\xi_5, \\\\xi_6, \\\\xi_7, \\\\xi_8)$ by $\\\\bar{\\\\xi}$.\\n\\n**Definition 3.12.** For all $\\\\alpha \\\\in \\\\text{acc}(\\\\kappa)$ and $\\\\eta \\\\in \\\\mathcal{T}$ with $\\\\bar{\\\\eta} \\\\in J_f$, $\\\\text{dom}(\\\\eta) = m < \\\\gamma$ define $\\\\Gamma^\\\\alpha_\\\\eta$ as follows:\\n\\nIf $\\\\bar{\\\\eta} \\\\in J_f^\\\\alpha$, then $\\\\Gamma^\\\\alpha_\\\\eta$ is the set of elements $\\\\xi$ of $\\\\mathcal{T}$ such that:\\n\\n1. $\\\\xi \\\\upharpoonright m = \\\\eta$,\\n2. $\\\\bar{\\\\xi} \\\\upharpoonright \\\\text{dom}(\\\\xi) \\\\setminus m \\\\in W^\\\\alpha$,\\n3. $\\\\xi_3$ is constant on $\\\\text{dom}(\\\\xi) \\\\setminus m$,\\n4. $\\\\xi_3(m) = \\\\alpha$,\\n5. for all $n \\\\in \\\\text{dom}(\\\\xi) \\\\setminus m$, let $\\\\xi_2(n)$ be the unique $r < \\\\omega$ such that $\\\\sigma^\\\\alpha_\\\\eta(r) = \\\\bar{\\\\xi}(n)$, where $\\\\xi = \\\\bar{\\\\xi} \\\\upharpoonright n$.\\n\\nIf $\\\\bar{\\\\eta} \\\\notin J_f^\\\\alpha$, then $\\\\Gamma^\\\\alpha_\\\\eta = \\\\emptyset$.\\n\\nFor $\\\\eta \\\\in \\\\mathcal{T}$ with $\\\\bar{\\\\eta} \\\\in J_f$, $\\\\text{dom}(\\\\eta) = m < \\\\gamma$ define $\\\\Gamma(\\\\eta) = \\\\bigcup_{\\\\alpha \\\\in \\\\text{acc}(\\\\kappa)} \\\\Gamma^\\\\alpha_\\\\eta$.\\n\\nFinally we can define $A^f$ by induction. Let $T_f(0) = \\\\{\\\\emptyset\\\\}$ and for all $n < \\\\gamma$,\\n\\n$$T_f(n + 1) = T_f(n) \\\\cup \\\\bigcup_{\\\\eta \\\\in T_f(n) \\\\text{ dom}(\\\\eta) = n} \\\\Gamma(\\\\eta).$$\\n\\nFor $n \\\\leq \\\\gamma$ a limit ordinal,\\n\\n$$T_f(n) = \\\\bigcup_{m < n} T_f(m)$$\\n\\nand\\n\\n$$T_f(n) = T_f(n) \\\\cup \\\\{\\\\eta \\\\in \\\\mathcal{T} \\\\mid \\\\text{dom}(\\\\eta) = n \\\\& \\\\forall m < n (\\\\eta \\\\upharpoonright m \\\\in T_f(n))\\\\}.$$\\n\\nFor $0 < i \\\\leq 8$ let us denote by $s_i(\\\\eta) = \\\\sup\\\\{\\\\eta_i(n) \\\\mid n < \\\\gamma\\\\}$ and $s_\\\\gamma(\\\\eta) = \\\\max\\\\{s_i(\\\\eta) \\\\mid i \\\\leq 8\\\\}$. Finally\\n\\n$$A^f = T_f(\\\\gamma).$$\\n\\nDefine the color function $d_f$ by\\n\\n$$d_f(\\\\eta) = \\\\begin{cases} c_f(\\\\bar{\\\\eta}) & \\\\text{if } s_1(\\\\eta) < s_\\\\gamma(\\\\eta) \\\\\\\\ f(s_1(\\\\eta)) & \\\\text{if } s_1(\\\\eta) = s_\\\\gamma(\\\\eta). \\\\end{cases}$$\\n\\nIt is clear that $A^f$ is closed under initial segments, indeed the relations $\\\\prec$, $(P_n)_{n \\\\leq \\\\gamma}$, and $\\\\wedge$ of Definition 3.11 have a canonical interpretation in $A^f$.\\n\\nNow we finish the construction of $A^f$ by using the $\\\\kappa$-colorable linear order $I$ of Section 2. We have to define $\\\\prec | \\\\text{Suc}_{A^f}(\\\\eta)$ for all $\\\\eta \\\\in A^f$ with domain smaller than $\\\\gamma$. Properly speaking, $A^f$ will not be an ordered coloured tree as in Definition 3.11, but it will be isomorphic to an ordered coloured tree as in Definition 3.11. Let us proceed to define $\\\\prec | \\\\text{Suc}_{A^f}(\\\\eta)$. Let $F : I \\\\to \\\\kappa$ be a $\\\\kappa$-coloration of $I$.\\n\\nFor any $\\\\eta \\\\in A^f$ with domain $m + 1 < \\\\gamma$, we will define the order $\\\\prec | \\\\text{Suc}_{A^f}(\\\\eta)$ such that it is isomorphic to $I$ and satisfies the following:\\n\\n1. For any set $B \\\\subset \\\\text{Suc}_{A^f}(\\\\eta)$ of size less than $\\\\kappa$, $p(x)$ a type of basic formulas over $B$ in the variable $x$, and any tuple $(\\\\vartheta_2, \\\\vartheta_3) \\\\in \\\\omega \\\\times \\\\text{acc}(\\\\kappa)$ with $\\\\vartheta_3 \\\\geq \\\\eta_3(m)$, if $p(x)$ is realized in $\\\\text{Suc}_{A^f}(\\\\eta)$, then there are $\\\\kappa$ many $\\\\alpha < \\\\kappa$ such that $\\\\eta^\\\\alpha(\\\\alpha, \\\\vartheta_2, \\\\vartheta_3, \\\\sigma^\\\\alpha_\\\\eta(\\\\vartheta_2)) \\\\models p$. \\n\\n\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"whenever $\\\\text{tr}(\\\\rho^2) \\\\leq 2/d$. Secondly, Lemma 7 and the vectorization of matrices, Theorem 8 and Theorem 11 give hints to the similarity of the role of $\\\\vartheta(G)$ in quantum contextuality and joint expectation values.\\n\\nIV. SELFADJOINT UNITARY REPRESENTATIONS\\n\\nThe set of operators, where any pair either commutes or anticommutes, plays an important role as exemplified by the Pauli strings. The commutation and anticommutation relations of such a set $\\\\{S_i\\\\}$ can be encoded into a so-called frustration graph $G$ [11, 12], where $i \\\\sim j$ if $\\\\{S_i, S_j\\\\} = 0$, and $i \\\\not\\\\sim j$ if $[S_i, S_j] = 0$. By checking extensive examples, it is conjectured in Ref. [34] that\\n\\n$$q(\\\\{S_i\\\\}) = \\\\alpha(G). \\\\quad (20)$$\\n\\nWhether Eq. (20) can be violated is also an open question in Ref. [25]. Conversely, for a given graph $G$, we can consider its representation by a set $\\\\{S_i\\\\}$ of selfadjoint unitaries, in the sense that $\\\\{S_i, S_j\\\\} = 0$ if $i \\\\sim j$ and $[S_i, S_j] = 0$ if $i \\\\not\\\\sim j$. This representation is called selfadjoint unitary representation (SAUR) [41]. By taking the graph-theoretic approach instead of starting from a special set, we denote $\\\\beta(G, w) = q(S_{\\\\text{ac}}(G), w)$, where $S_{\\\\text{ac}}(G)$ is the set of all SAURs of $G$. The conjecture in Eq. (20) is equivalent to $\\\\beta(G) = \\\\alpha(G)$. In Ref. [25], no such an example is known that $\\\\beta(G) > \\\\alpha(G)$. To continue, we first introduce the standard SAUR of a given graph, which is defined deductively. The standard SAUR can help us to reduce the complexity of considerations, since we only need to focus on the standard SAUR to obtain $\\\\beta(G)$ as we will see later.\\n\\n**Definition 12.** For a given graph $G$ and one of its edges $(i_0, j_0)$, other vertices except $i_0$ and $j_0$ can be devided into four groups $V_0, V_1, V_2, V_3$, such that\\n\\n- $i \\\\not\\\\sim i_0$ and $i \\\\not\\\\sim j_0$ for any $i \\\\in V_0$;\\n- $i \\\\not\\\\sim i_0$ and $i \\\\sim j_0$ for any $i \\\\in V_1$;\\n- $i \\\\sim i_0$ and $i \\\\sim j_0$ for any $i \\\\in V_2$;\\n- $i \\\\sim i_0$ and $i \\\\not\\\\sim j_0$ for any $i \\\\in V_3$.\\n\\nThe subgraph $G'$ of $G$ with vertices in $\\\\bigcup_{i=0}^3 V_i$ is said to be a Pauli-$(i_0, j_0)$-induced subgraph of $G$ if\\n\\n- when $i \\\\in V_{k_1}, j \\\\in V_{k_2}$ where $k_1 \\\\neq k_2 \\\\in \\\\{1, 2, 3\\\\}$, we have $i \\\\sim j$ (or $i \\\\not\\\\sim j$) in $G'$ if and only if $i \\\\not\\\\sim j$ (or $i \\\\sim j$) in $G$;\\n- otherwise, $i \\\\sim j$ in $G'$ if and only if $i \\\\sim j$ in $G$.\\n\\n**Definition 13.** For a given graph $G$ and one of its edges $(i_0, j_0)$, denote $G'$ the Pauli-$(i_0, j_0)$-induced subgraph of $G$, from a standard SAUR $\\\\{S'_i\\\\}$ of $G'$, we call the following SAUR a standard one of $G$:\\n\\n$$\\\\bigcup_{k=0}^3 \\\\{\\\\sigma_k \\\\otimes S'_i\\\\}_{i \\\\in V_k} \\\\cup \\\\{X_{i_0} \\\\otimes \\\\text{id}, Z_{j_0} \\\\otimes \\\\text{id}\\\\}. \\\\quad (21)$$\\n\\nIf $G$ has no edge, we assign $\\\\text{id}$ or $1$ to all its vertices.\\n\\nIf we take the pentagon $C_5$ in Fig. 2 as the original graph, and $(1, 3)$ as the edge, then the Pauli-$(1, 3)$-induced subgraph $G''$ is a triangle. Continually, the Pauli-$(4, 5)$-induced subgraph $G'''$ of $G'$ is just the vertex 2. Hence, the standard SAURs $\\\\{S''_i\\\\}, \\\\{S'''_i\\\\}$ and $\\\\{S_i\\\\}$ of $G''$, $G'''$ and $G$, respectively, are\\n\\n$$S''_1 = 1, S''_2 = Y, S''_3 = X, S''_4 = Z, \\\\quad (22)$$\\n\\n$$S_1 = X \\\\text{id}, S_2 = \\\\text{id} Y, S_3 = Z \\\\text{id}, S_4 = X X, S_5 = Z Z, \\\\quad (23)$$\\n\\nwhere we have omitted the symbol of tensor product, and $X \\\\text{id}$ means $X \\\\otimes \\\\text{id}$ etc.\\n\\n**Theorem 14 (Cf. [41]).** For a given graph $G$ and one SAUR $\\\\{S_i\\\\}$ of $G$, there is a unitary $U$ and standard SAUR $\\\\{S_i\\\\}$ such that $U S_i U^\\\\dagger = \\\\bar{S}_i \\\\otimes D_i$, where $\\\\{D_i\\\\}$ is a set of commuting selfadjoint unitaries.\\n\\nThe standard SAUR is succinct, however, it loses the information of the symmetry in the graph. To reflect the structure of the graph, we introduce the edge SAUR.\\n\\n**Definition 15.** For a given graph $G$ with $n$ vertices and the edge set $E$, the set of selfadjoint operators $\\\\{A_e\\\\}_{e=1}^n$, with $A_e = \\\\otimes_{x \\\\in E} O_{e,x}$ is called the edge SAUR of $G$, where $O_{e,x} = X$ if $i$ is the start of $e$, and $O_{e,x} = Z$ if $i$ is the end of $e$, otherwise, $O_{e,x} = \\\\text{id}$.\\n\\nFor different SAURs of the same graph $G$, their joint expectation values are related.\\n\\n**Theorem 16.** For a given graph $G$, $q(\\\\{S_i\\\\}) = q(\\\\{\\\\bar{S}_i\\\\})$ where $\\\\{S_i\\\\}$ is a SAUR of $G$ and $\\\\{\\\\bar{S}_i\\\\}$ is a standard one.\\n\\n**Proof.** From the convexity of $\\\\sum_i \\\\langle S_i \\\\rangle^2$, we know that we only need to prove for the case that $\\\\rho$ is a pure state $|\\\\psi\\\\rangle \\\\langle \\\\psi|$. Since $D_i$ commutes with each other, we can assume $D_i$'s are diagonal matrices. Denote $d_2$ the dimension of $D_i$'s, then we have the decomposition\\n\\n$$U |\\\\psi\\\\rangle = \\\\sum_{i=1}^{d_2} \\\\sqrt{p_i} |\\\\phi_i\\\\rangle \\\\otimes |i\\\\rangle, \\\\quad (24)$$\\n\\nwhere $p_i \\\\geq 0$ and $\\\\sum_i p_i = 1$.\\n\\nHence,\\n\\n$$\\\\langle S_i \\\\rangle = \\\\sum_{k,l} \\\\sqrt{p_k p_l} \\\\langle \\\\phi_k | S_i | \\\\phi_l \\\\rangle \\\\langle k | D_i | l \\\\rangle = \\\\sum_k p_k \\\\langle \\\\phi_k | S_{i,k} | \\\\phi_k \\\\rangle, \\\\quad (25)$$\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"whenever $\\\\text{tr}(\\\\rho^2) \\\\leq 2/d$. Secondly, Lemma 7 and the vechints to the similarity of the role of $\\\\vartheta(G)$ in quantum contextuality and joint expectation values.\\n\\nIV. SELFADJOINT UNITARY REPRESENTATIONS\\n\\nThe set of operators, where any pair either commutes or anticommutes, plays an important role as exemplified by the Pauli strings. The commutation and anticommutation relations of such a set $\\\\{S_i\\\\}$ can be encoded into a so-called frustration graph , 12 ], where $i \\\\sim j$ if $\\\\{S_i, S_j\\\\} = 0$, and $i \\\\not\\\\sim j$ if $\\\\{S_i, S_j\\\\} = 0$. By checking extensive examples, it is conjectured in Ref. [ 25 ]. Conversely, for a given graph $G$, we can consider its representation by a set $\\\\{S_i\\\\}$ of selfadjoint unitaries, in the sense that $\\\\{S_i, S_j\\\\} = 0$ if $i \\\\sim j$ and $\\\\{S_i, S_j\\\\} = 0$ if $i \\\\not\\\\sim j$. This representation is called a special set, we denote $\\\\beta(G, w) = q(S_{\\\\text{ac}}(G), w)$, where $S_{\\\\text{ac}}(G)$ is the set of all SAURs of $G$. The conjecture in Eq. (20) is equivalent to $\\\\beta(G) = \\\\alpha(G)$. In Ref. [ 25 ], no such an example is known that $\\\\beta(G) > \\\\alpha(G)$. To continue, we first introduce the standard SAUR of a given graph, which is defined deductively. The standard SAUR can help us to reduce the complexity of considerations, since we only need to focus on the standard SAUR to obtain $\\\\beta(G)$ as we will see later. \\n\\nDefinition 12. For a given graph $G$ and one of its edges $(i_0, j_0)$, other vertices except $i_0$ and $j_0$ can be divided into four groups $V_0, V_1, V_2, V_3$, such that\\n\\n- $i \\\\not\\\\sim i_0$ and $i \\\\not\\\\sim j_0$ for any $i \\\\in V_0$;\\n- $i \\\\not\\\\sim i_0$ and $i \\\\sim j_0$ for any $i \\\\in V_1$;\\n- $i \\\\sim i_0$ and $i \\\\sim j_0$ for any $i \\\\in V_2$;\\n- $i \\\\sim i_0$ and $i \\\\not\\\\sim j_0$ for any $i \\\\in V_3$.\\n\\nThe subgraph $G'$ of $G$ with vertices in $\\\\bigcup_{i=0}^3 V_i$ is said to be a Pauli-$(i_0, j_0)$-induced subgraph of $G$ if\\n\\n- when $i \\\\in V_{k_1}, j \\\\in V_{k_2}$ where $k_1 \\\\neq k_2 \\\\in \\\\{1, 2, 3\\\\}$, we have $i \\\\sim j$ (or $i \\\\not\\\\sim j$) in $G'$ if and only if $i \\\\not\\\\sim j$ (or $i \\\\sim j$) in $G$;\\n- otherwise, $i \\\\sim j$ in $G'$ if and only if $i \\\\sim j$ in $G$.\\n\\nDefinition 13. For a given graph $G$ and one of its edges $(i_0, j_0)$, denote $G'$ the Pauli-$(i_0, j_0)$-induced subgraph of $G$, from a standard SAUR $\\\\{S_i\\\\}$ of $G'$, we call the following SAUR a standard one of $G$:\\n\\n$$\\\\bigcup_{k=0}^3 \\\\{\\\\sigma_k \\\\otimes S_i\\\\}_{i \\\\in V_k} \\\\cup \\\\{X_{i_0} \\\\otimes \\\\text{id}, Z_{j_0} \\\\otimes \\\\text{id}\\\\}. \\\\quad (21)$$\\n\\nIf $G$ has no edge, we assign $\\\\text{id}$ or $1$ to all its vertices.\\n\\nIf we take the pentagon $C_5$ in Fig. 2 as the original graph, and $(1, 3)$ as the edge, then the Pauli-$(1, 3)$-induced subgraph $G''$ is a triangle. Continually, the Pauli-$(4, 5)$-induced subgraph $G'''$ of $G'$ is just the vertex $2$. Hence, the standard SAURs $\\\\{S''_i\\\\}, \\\\{S'''_i\\\\}$ and $\\\\{S_i\\\\}$ of $G''$, $G'''$ and $G$, respectively, are\\n\\n$$S''_1 = 1, S''_2 = Y, S''_3 = X, S''_4 = Z, \\\\quad (22)$$\\n\\n$$S_1 = X \\\\text{id}, S_2 = \\\\text{id} Y, S_3 = Z \\\\text{id}, S_4 = X X, S_5 = Z Z, \\\\quad (23)$$\\n\\nwhere we have omitted the symbol of tensor product, and $X \\\\text{id}$ means $X \\\\otimes \\\\text{id}$ etc.\\n\\nTheorem 14 (Cf. [ 41 ]). For a given graph $G$ and one SAUR $\\\\{S_i\\\\}$ of $G$, there is a unitary $U$ and standard SAUR $\\\\{S_i\\\\}$ such that $U S_i U^\\\\dagger = \\\\bar{S}_i \\\\otimes D_i$, where $\\\\{D_i\\\\}$ is a set of commuting selfadjoint unitaries.\\n\\nThe standard SAUR is succinct, however, it loses the information of the symmetry in the graph. To reflect the structure of the graph, we introduce the edge SAUR.\\n\\nDefinition 15. For a given graph $G$ with $n$ vertices and the edge set $E$, the set of selfadjoint operators $\\\\{A_i\\\\}_{i=1}^n$, with $A_i = \\\\otimes_{e \\\\in E} O_{e,i}$ is called the edge SAUR of $G$, where $O_{e,i} = X$ if $i$ is the start of $e$, and $O_{e,i} = Z$ if $i$ is the end of $e$, otherwise, $O_{e,i} = \\\\text{id}$.\\n\\nFor different SAURs of the same graph $G$, their joint expectation values are related.\\n\\nTheorem 16. For a given graph $G$, $q(\\\\{S_i\\\\}) = q(\\\\{\\\\bar{S}_i\\\\})$ where $\\\\{S_i\\\\}$ is a SAUR of $G$ and $\\\\{\\\\bar{S}_i\\\\}$ is a standard one.\\n\\nProof. From the convexity of $\\\\sum_i \\\\langle S_i \\\\rangle^2$, we know that we only need to prove for the case that $\\\\rho$ is a pure state $|\\\\psi\\\\rangle \\\\langle \\\\psi|$. Since $D_i$ commutes with each other, we can assume $D_i$'s are diagonal matrices. Denote $d_2$ the dimension of $D_i$'s, then we have the decomposition\\n\\n$$U |\\\\psi\\\\rangle = \\\\sum_{i=1}^{d_2} \\\\sqrt{p_i} |\\\\phi_i\\\\rangle \\\\otimes |i\\\\rangle, \\\\quad (24)$$\\n\\nwhere $p_i \\\\geq 0$ and $\\\\sum_i p_i = 1$.\\n\\nHence,\\n\\n$$\\\\langle S_i \\\\rangle = \\\\sum_{k,l} \\\\sqrt{p_k p_l} \\\\langle \\\\phi_k | S_i | \\\\phi_l \\\\rangle \\\\langle k | D_i | l \\\\rangle = \\\\sum_k p_k \\\\langle \\\\phi_k | s_{ik} \\\\bar{S}_i | \\\\phi_k \\\\rangle, \\\\quad (25)$$\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.2 Checking SUSY for the Fibered Background\\n\\nHere we aim to compute how many supercharges are preserved by the the backgrounds presented in this paper. For the un-fibered background in eq.(2.1) we refer the reader to [12], [19], where it is shown that this solution preserves 16 Supercharges in an interesting way: the anti-commutator of two supercharges includes the $R$-Symmetry generators. Now we present the analysis for the fibered background in eq.(2.3). We perform all the analysis in the S-dual system, in terms of NS5 branes, where we only have $H_3$ flux.\\n\\nFirst, note that the dilatino variation is a matrix equation of the form $M\\\\varepsilon = 0$. In order to have non-trivial solutions to this equation, we require $M$ to be non-invertible, for which we need to impose $\\\\det(M) = 0$. It is also possible to obtain a matrix equation from the gravitino variation. Noting that we can write the gravitino variation as a covariant derivative, for which we define the connection\\n\\n$$W_\\\\mu = \\\\frac{1}{4} \\\\omega_\\\\mu^{ab} \\\\Gamma_{ab} + \\\\frac{1}{4 \\\\cdot 2!} H_{\\\\mu\\\\nu\\\\lambda} \\\\Gamma^{\\\\nu\\\\lambda} \\\\sigma^3 + \\\\frac{e^\\\\Phi}{8} \\\\left( F_\\\\mu \\\\Gamma^\\\\mu (i\\\\sigma_2) + \\\\frac{1}{3!} F_{\\\\mu\\\\nu\\\\lambda} \\\\Gamma^{\\\\mu\\\\nu\\\\lambda} \\\\sigma^1 + \\\\frac{1}{2 \\\\cdot 5!} F_{\\\\mu\\\\nu\\\\lambda\\\\rho\\\\sigma} \\\\Gamma^{\\\\mu\\\\nu\\\\lambda\\\\rho\\\\sigma} (i\\\\sigma_2) \\\\right) \\\\Gamma_\\\\mu,$$\\n\\nthen we can write the gravitino variation as\\n\\n$$\\\\delta \\\\psi_\\\\mu dx^\\\\mu = (\\\\partial_\\\\mu \\\\varepsilon + W_\\\\mu \\\\varepsilon) dx^\\\\mu \\\\equiv \\\\mathcal{D} \\\\varepsilon. \\\\quad (A.11)$$\\n\\nWe can get rid of the partial derivative of the spinor by acting with $\\\\mathcal{D}$ a second time\\n\\n$$\\\\mathcal{D} \\\\wedge \\\\mathcal{D} \\\\varepsilon = (dW + W \\\\wedge W) \\\\varepsilon = \\\\frac{1}{2} \\\\Theta_{\\\\mu\\\\nu} dx^\\\\mu \\\\wedge dx^\\\\nu \\\\varepsilon. \\\\quad (A.12)$$\\n\\nEach of the components of $\\\\Theta_{\\\\mu\\\\nu}$ defines a matrix equation, giving a total of 45 independent equations. We need to make sure that $\\\\det(\\\\Theta_{\\\\mu\\\\nu}) = 0$ for each of the components. The equations\\n\\n$$M \\\\varepsilon = 0, \\\\quad \\\\Theta_{\\\\mu\\\\nu} \\\\varepsilon = 0, \\\\quad (A.13)$$\\n\\nconstrain the number of independent components of the spinor. After this procedure we use the gravitino variation to solve the dependence of the spinor on the spacetime coordinates.\\n\\nSpecialising to our background, the determinant of the Dilatino variation for the background in eq.(2.3) reads\\n\\n$$\\\\det(M) \\\\sim (4(e_B Q_A - e_A Q_B)^2 + m^2)^8 \\\\left( 4(e_B Q_A + e_A Q_B)^2 + m^2 \\\\right)^8. \\\\quad (A.14)$$\\n\\nIn order to have non-trivial solutions we need to impose the following BPS conditions on the parameters of the background\\n\\n$$e_A Q_B = \\\\pm e_B Q_A, \\\\quad m = 0. \\\\quad (A.15)$$\\n\\nWith this conditions it is possible to check that $\\\\det(\\\\Theta_{\\\\mu\\\\nu}) = 0$ is also satisfied. Solving these matrix equations shows that the spinor has 8 independent components. Then, solving for the gravitino variation shows that these components are not independents, and in fact, the total number of independent components its reduced to 4. The solution for the spinor is\\n\\n$$\\\\varepsilon_1 = \\\\bar{\\\\varepsilon} \\\\quad (A.16)$$\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.2 Checking SUSY for the Fibered Background\\n\\nHere we aim to compute how many supercharges are preserved by the the backgrounds presented in this paper. For the un-fibered background in eq.( 2.1 ) we refer the reader to [ 12 ], [ 19 ], where it is shown that this solution preserves 16 Supercharges in an interesting way: the anti-commutator of two supercharges includes the $R$-Symmetry generators. Now we present the analysis for the fibered background in eq.( 2.3 ). We perform all the analysis in the S-dual system, in terms of NS5 branes, where we only have $H_3$ flux.\\n\\nFirst, note that the dilatino variation is a matrix equation of the form $M \\\\varepsilon = 0$. In order to have non-trivial solutions to this equation, we require $M$ to be non-invertible, for which we need to impose det( $M$ ) = 0. It is also possible to obtain a matrix equation from the gravitino variation. Noting that we can write the gravitino variation as a covariant derivative, for which we define the connection\\n\\n$$W_\\\\mu = \\\\frac{1}{4} \\\\omega_\\\\mu^{ab} \\\\Gamma_{ab} + \\\\frac{1}{4 \\\\cdot 2!} H_{\\\\mu \\\\nu \\\\lambda} \\\\Gamma^{\\\\nu \\\\lambda} \\\\sigma^3 + \\\\frac{e^\\\\Phi}{8} \\\\left( F_\\\\mu \\\\Gamma^\\\\mu (i \\\\sigma_2) + \\\\frac{1}{3!} F_{\\\\mu \\\\nu \\\\lambda} \\\\Gamma^{\\\\mu \\\\nu \\\\lambda} \\\\sigma^1 + \\\\frac{1}{2 \\\\cdot 5!} F_{\\\\mu \\\\nu \\\\lambda \\\\rho \\\\sigma} \\\\Gamma^{\\\\mu \\\\nu \\\\lambda \\\\rho \\\\sigma} (i \\\\sigma_2) \\\\right) \\\\Gamma_\\\\mu,$$\\n\\nthen we can write the gravitino variation as\\n\\n$$\\\\delta \\\\psi_\\\\mu dx^\\\\mu = (\\\\partial_\\\\mu \\\\varepsilon + W_\\\\mu \\\\varepsilon) dx^\\\\mu \\\\equiv \\\\mathcal{D} \\\\varepsilon.$$  \\\\hspace{1cm} (A.11)\\n\\nWe can get rid of the partial derivative of the spinor by acting with $\\\\mathcal{D}$ a second time\\n\\n$$\\\\mathcal{D} \\\\wedge \\\\mathcal{D} \\\\varepsilon = (dW + W \\\\wedge W) \\\\varepsilon = \\\\frac{1}{2} \\\\Theta_{\\\\mu \\\\nu} dx^\\\\mu \\\\wedge dx^\\\\nu \\\\varepsilon.$$  \\\\hspace{1cm} (A.12)\\n\\nEach of the components of $\\\\Theta_{\\\\mu \\\\nu}$ defines a matrix equation, giving a total of 45 independent equations. We need to make sure that det( $\\\\Theta_{\\\\mu \\\\nu}$ ) = 0 for each of the components. The equations\\n\\n$$M \\\\varepsilon = 0, \\\\quad \\\\Theta_{\\\\mu \\\\nu} \\\\varepsilon = 0,$$  \\\\hspace{1cm} (A.13)\\n\\nconstrain the number of independent components of the spinor. After this procedure we use the gravitino variation to solve the dependence of the spinor on the spacetime coordinates. Specialising to our background, the determinant of the Dilatino variation for the background in eq.( 2.3 ) reads\\n\\n$$\\\\det(M) \\\\sim (4(e_B Q_A - e_A Q_B)^2 + m^2)^8 \\\\left( 4(e_B Q_A + e_A Q_B)^2 + m^2 \\\\right)^8.$$  \\\\hspace{1cm} (A.14)\\n\\nIn order to have non-trivial solutions we need to impose the following BPS conditions on the parameters of the background\\n\\n$$e_A Q_B = \\\\pm e_B Q_A, \\\\quad m = 0.$$  \\\\hspace{1cm} (A.15)\\n\\nWith this conditions it is possible to check that det( $\\\\Theta_{\\\\mu \\\\nu}$ ) = 0 is also satisfied. Solving these matrix equations shows that the spinor has 8 independent components. Then, solving for the gravitino variation shows that these components are not independents, and in fact, the total number of independent components its reduced to 4. The solution for the spinor is\\n\\n$$\\\\varepsilon_1 = \\\\tilde{\\\\varepsilon}.$$  \\\\hspace{1cm} (A.16)\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Demonstrations of inter-class similarity and intra-class variation. a) Images from the auditorium and movie theater classes have a high degree of similarity (inter-class similarity). b) Images of the office demonstrate a considerable degree of intra-class diversity, suggesting a wide spectrum of visual features within the category.\\n\\nscenes (though they have similar objects such as computers, chairs, or tables, as shown in Figure 1(b)), which may confuse the recognition system. Furthermore, constructing a scene representation that captures crucial semantic information reflecting the complexity of the data is challenging, particularly when dealing with a large number of categories.\\n\\nResearch in scene recognition frameworks can be categorized into ones that are based on hand-crafted engineering and methods that employ automatic feature extraction without human intervention. Hand-engineered features are based on manually designing and selecting features utilizing different techniques to capture spatial characteristics, local features, object-based concepts, and holistic representations of scenes [6, 7, 8, 9]. However, hand-crafted features require notable domain expertise and significant human effort, resulting in inefficiency. Consequently, Deep Convolutional Neural Networks (DCNNs) have largely replaced them due to their superior representation learning ability [10, 11, 12]. They have demonstrated that they attain superior classification performance when trained on extensive datasets. After AlexNet\u2019s [13] introduction, deep convolutional neural networks, such as VGG [14], Inception [15], ResNet [16], and DenseNet [17], impressively advanced the field of image classification due to their great ability of capturing locally correlated image values [18, 19, 20] and learning features that enhance efficiency compared to hand-crafted methods. Utilizing deep convolutional neural networks can enhance scene recognition performance, but it remains challenging due to the intricate spatial layout, intra-class variation, and inter-class similarity, which weaken discriminability among scenes [21, 22]. In addition, due to the rise of extensive scene-centric datasets, a simple CNN-generated representation model is inadequate to accurately discriminate large-scale scenes [23]. As a result, research has shifted its focus to developing more representative features by incorporating contextual information, such as objects, or proposing strategies to effectively extract features that facilitate decision-making boundaries. Furthermore, several studies have employed ensemble learning strategies to leverage the complementary strengths of multiple feature levels or recognition models [24, 25, 26].\\n\\nBesides improving accuracy, recognition systems based on deep neural networks face the challenge of ambiguity in the reasoning behind the model\u2019s predictions [27, 28, 29]. This boils down to the black-box nature of deep neural networks, which induces a lack of trust and ethical concerns regarding the model\u2019s decisions, especially in high-stakes applications such as medical analysis and self-driving vehicles [30, 31, 32]. Along this direction, numerous efforts have been made to help understand how a model solves a problem and makes decisions. Typically, an explanation should help us get the answers to the following questions: Why did the model predict this category? Is the prediction of the model reliable? Which parts of the input led\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Demonstrations of inter-class similarity and intra-class variation. a) Images from the auditorium and movie theater classes have a high degree of similarity (inter-class similarity). b) Images of the office demonstrate a considerable degree of intra-class diversity, suggesting a wide spectrum of visual features within the category.\\n\\nscenes (though they have similar objects such as computers, chairs, or tables, as shown in Figure 1(b)), which may confuse the recognition system. Furthermore, constructing a scene representation that captures crucial semantic information reflecting the complexity of the data is challenging, particularly when dealing with a large number of categories.\\n\\nResearch in scene recognition frameworks can be categorized into ones that are based on hand-crafted engineering and methods that employ automatic feature extraction without human intervention. Hand-engineered features are based on manually designing and selecting features utilizing different techniques to capture spatial characteristics, local features, object-based concepts, and holistic representations of scenes [6, 7, 8, 9]. However, hand-crafted features require notable domain expertise and significant human effort, resulting in inefficiency. Consequently, Deep Convolutional Neural Networks (DCNNs) have largely replaced them due to their superior representation learning ability [10, 11, 12]. They have demonstrated that they attain superior classification performance when trained on extensive datasets. After AlexNet\u2019s [13] introduction, deep convolutional neural networks, such as VGG [14], Inception [15], ResNet [16], and DenseNet [17], impressively advanced the field of image classification due to their great ability of capturing locally correlated image values [18, 19, 20] and learning features that enhance efficiency compared to hand-crafted methods. Utilizing deep convolutional neural networks can enhance scene recognition performance, but it remains challenging due to the intricate spatial layout, intra-class variation, and inter-class similarity, which weaken discriminability among scenes [21, 22]. In addition, due to the rise of extensive scene-centric datasets, a simple CNN-generated representation model is inadequate to accurately discriminate large-scale scenes [23]. As a result, research has shifted its focus to developing more representative features by incorporating contextual information, such as objects, or proposing strategies to effectively extract features that facilitate decision-making boundaries. Furthermore, several studies have employed ensemble learning strategies to leverage the complementary strengths of multiple feature levels or recognition models [24, 25, 26].\\n\\nBesides improving accuracy, recognition systems based on deep neural networks face the challenge of ambiguity in the reasoning behind the model\u2019s predictions [27, 28, 29]. This boils down to the black-box nature of deep neural networks, which induces a lack of trust and ethical concerns regarding the model\u2019s decisions, especially in high-stakes applications such as medical analysis and self-driving vehicles [30, 31, 32]. Along this direction, numerous efforts have been made to help understand how a model solves a problem and makes decisions. Typically, an explanation should help us get the answers to the following questions: Why did the model predict this category? Is the prediction of the model reliable? Which parts of the input led\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Database (NVD) by inferring new classes, enriching relations, and expanding conceptual coverage. The ontology is used to search for and query social media threads that contain cybersecurity-related information, and natural language processing techniques are used to relate unstructured information to concepts in the ontology. The paper highlights the advantages of Semantic Web technologies in integrating information from multiple and often heterogeneous sources, without human intervention. Rosa et.al. [8] presented a novel ontology-based approach to utilize ontology to identify and map threats to assets. With the support of formally sound approaches, this process can be streamlined and made more efficient. From an ontology perspective, the authors introduced ThreMA, an ontology-based approach for automating threat modeling in ICT infrastructures. ThreMA provides a standard meta-model that describes the infrastructure and a set of rules for threat modeling. The meta-model consists of three ontologies modules: ICT ontology for modeling the infrastructure, Data Flow ontology for representing data flow diagrams, and threat ontology for characterizing threats. The use of ontology and inference rules allows for a syntactical representation of the problem, mimicking expert thinking. This approach enhances extensibility, maintainability, and integration in a rapidly changing context. The paper emphasizes the importance of using ontologies to address the lack of context and low accuracy in threat modeling. Overall, ThreMA offers a comprehensive ontology-based solution for automating threat modeling in ICT infrastructures.\\n\\nIII. METHODOLOGY\\n\\nThis work presents an ontology for representing various data sources about cloud computing and security. This ontology enables a knowledge presentation framework for all cloud computing and its relationships. The ontology consists of several modules: Cloud Computing and services, Cloud Service underlying components, and CVE module.\\n\\nA. Cloud Computing Stack and Services Ontology Module\\n\\nThis section represents our proposed ontology module that covers the cloud computing stack and services. Our extension ontology is provided as a separate ontology, which is an important design criteria in ontology engine engineering [13, 14]. This ontology can be used to unify and provide a primary baseline for cloud computing stack, threat understanding, and system diagram presentations. Firstly, we start by creating the ontology of the cloud computing stack. Figure 2 depicts the ontology of the three cloud stack: Software as a Service (SaaS), Platform as a Service (PaaS), Infrastructure as a Service (IaaS), Function as a Service (FaaS), Communication as a service (CaaS), and Desktop as a service (DaaS). Figure 1 depicts the cloud service model. Cloud service has nine layers, green colored layers mean these layers are managed by the client while the other color refers to layers managed by the cloud providers.\\n\\n![Fig. 1. Cloud computing stack. The green color depicts the layers managed by the used user. While the orange-colored boxes represent the layers managed by the cloud provider.](image)\\n\\n![Fig. 2. Cloud computing stack ontology.](image)\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Database (NVD) by inferring new classes, enriching relations, and expanding conceptual coverage. The ontology is used to search for and query social media threads that contain cybersecurity-related information, and natural language processing techniques are used to relate unstructured information to concepts in the ontology. The paper highlights the advantages of Semantic Web technologies in integrating information from multiple and often heterogeneous sources, without human intervention. Rosa et.al. [8] presented a novel ontology-based approach to utilize ontology to identify and map threats to assets. With the support of formally sound approaches, this process can be streamlined and made more efficient. From an ontology perspective, the authors introduced ThreMA, an ontology-based approach for automating threat modeling in ICT infrastructures. ThreMA provides a standard metamodel that describes the infrastructure and a set of rules for threat modeling. The meta-model consists of three ontologies modules: ICT ontology for modeling the infrastructure, Data Flow ontology for representing data flow diagrams, and threat ontology for characterizing threats. The use of ontology and inference rules allows for a syntactical representation of the problem, mimicking expert thinking. This approach enhances extensibility, maintainability, and integration in a rapidly changing context. The paper emphasizes the importance of using ontologies to address the lack of context and low accuracy in threat modeling. Overall, ThreMA offers a comprehensive ontology-based solution for automating threat modeling in ICT infrastructures. \\n\\nIII. METHODOLOGY\\n\\nThis work presents an ontology for representing various data sources about cloud computing and security. This ontology enables a knowledge presentation framework for all cloud computing and its relationships. The ontology consists of several modules: Cloud Computing and services, Cloud Service underlying components, and CVE module. \\n\\nA. Cloud Computing Stack and Services Ontology Module\\n\\nThis section represents our proposed ontology module that covers the cloud computing stack and services. Our extension ontology is provided as a separate ontology, which is an important design criteria in ontology engine engineering [13, 14]. This ontology can be used to unify and provide a primary baseline for cloud computing stack, threat understanding, and system diagram presentations. Firstly, we start by creating the ontology of the cloud computing stack. Figure 2 depicts the ontology of the three cloud stack: Software as a Service (SaaS), Platform as a Service (PaaS), Infrastructure as a Service (IaaS), Function as a Service (FaaS), Communication as a service (CaaS), and Desktop as a service (DaaS). Figure 1 depicts the cloud service model. Cloud service has nine layers, green colored layers mean these layers are managed by the client while the other color refers to layers managed by the cloud providers. \\n\\n![Fig. 1. Cloud computing stack. The green color depicts the layers managed by the used user. While the orange-colored boxes represent the layers managed by the cloud provider.](image-url)\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"for the $q$-analogue to $\\\\lambda$, which reduces to\\n\\\\[ [n]_q = 1 + q + \\\\cdots + q^{n-1}, \\\\quad \\\\text{for } n \\\\in \\\\mathbb{N}^+. \\\\]\\nWe have that $[0]_q = 0$ and\\n\\\\[ [np]_q = \\\\frac{q^p - 1}{q - 1} \\\\frac{q^{np} - 1}{q^p - 1} = [p]_q \\\\cdot [n]_q^p, \\\\quad n, p \\\\in \\\\mathbb{N}^+. \\\\]\\nFor future use, we note that\\n\\\\[ |[n]_q| \\\\leq [n]_q, \\\\quad \\\\text{for } n \\\\in \\\\mathbb{N}, \\\\]\\nand also that\\n\\\\[ \\\\lim_{n \\\\to +\\\\infty} \\\\frac{[n]_q}{q^n} = \\\\frac{1}{q - 1}. \\\\]\\nThese constants appear naturally while considering Jackson\u2019s $q$-derivative of a function $f$, which is given by\\n\\\\[ d_q(f)(x) := \\\\frac{f(qx) - f(x)}{qx - x} = \\\\frac{\\\\sigma_q(f)(x) - f(x)}{qx - x}, \\\\]\\nwhenever the expression is defined. As before, $\\\\sigma_q(f)(x) := f(qx)$. For analytic functions $f \\\\in \\\\mathcal{O}(D_r)$, we see that\\n\\\\[ \\\\sigma_q(f), d_q(f) \\\\in \\\\mathcal{O}(D_{r/|q|}) \\\\]\\nand they can be computed term by term using its power series expansion according to the rules\\n\\\\[ d_q(x^n) = [n]_q x^{n-1}, \\\\quad \\\\sigma_q(x^n) = q^n x^n, \\\\quad n \\\\in \\\\mathbb{N}. \\\\]\\nOn the other hand, this formula allows to consider $d_q, \\\\sigma_d : \\\\mathbb{C}[[x]] \\\\to \\\\mathbb{C}[[x]]$, also defined term by term. In this setting, Leibniz rule is replaced by\\n\\\\[ d_q(fg)(x) = d_q(f)(x)g(x) + f(qx)d_q(g)(x). \\\\]\\nWe recall the coefficients\\n\\\\[ (a; q)_n = \\\\prod_{j=0}^{n-1} (1 - aq^j), \\\\quad (a; q^{-1})_\\\\infty = \\\\prod_{j=0}^{\\\\infty} (1 - aq^{-j}), \\\\quad a \\\\in \\\\mathbb{C}. \\\\]\\nThe second one is convergent as we can compare it with a geometric series. The $q$-factorial is defined accordingly as\\n\\\\[ [n]_q! = [1]_q [2]_q \\\\cdots [n]_q = \\\\frac{(q; q)_n}{(1 - q)_n}. \\\\]\\nIn general, for $|q| > 1$, since $\\\\lambda \\\\in \\\\mathbb{R} \\\\mapsto [\\\\lambda]_{|q|}$ is a strictly increasing function, the same holds for the map $n \\\\in \\\\mathbb{N} \\\\mapsto [n]_{|q|}!$. Therefore,\\n\\\\[ \\\\frac{[n - p]_{|q|}}{([n]_{|q|})^{1/p}} = \\\\frac{[n - p]_{|q|}}{([n]_{|q|} \\\\cdots [n - p + 1]_{|q|})^{1/p}} \\\\leq \\\\frac{[n - p]_{|q|}}{([n - p]_{|q|})^{1/p}}, \\\\]\\nand thus\\n\\\\[ \\\\frac{[n - p]_{|q|}}{([n]_{|q|})^{1/p}} \\\\leq \\\\frac{1}{([n - p]_{|q|})^{1/p}}, \\\\quad \\\\text{for integers } n > p > 0. \\\\]\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"for the $q$-analogue to $\\\\lambda$, which reduces to\\n\\\\[ [n]_q = 1 + q + \\\\cdots + q^{n-1}, \\\\quad \\\\text{for } n \\\\in \\\\mathbb{N}^+. \\\\]\\nWe have that $[0]_q = 0$ and\\n\\\\[ [np]_q = \\\\frac{q^p - 1}{q - 1} \\\\frac{q^{np} - 1}{q^p - 1} = [p]_q \\\\cdot [n]_q^p, \\\\quad n, p \\\\in \\\\mathbb{N}^+. \\\\]\\nFor future use, we note that\\n\\\\[ |[n]_q| \\\\leq [n]_q, \\\\quad \\\\text{for } n \\\\in \\\\mathbb{N}, \\\\]\\nand also that\\n\\\\[ \\\\lim_{n \\\\to +\\\\infty} \\\\frac{[n]_q}{q^n} = \\\\frac{1}{q - 1}. \\\\]\\nThese constants appear naturally while considering Jackson\u2019s $q$-derivative of a function $f$, which is given by\\n\\\\[ d_q(f)(x) := \\\\frac{f(qx) - f(x)}{qx - x} = \\\\frac{\\\\sigma_q(f)(x) - f(x)}{qx - x}, \\\\]\\nwhenever the expression is defined. As before, $\\\\sigma_q(f)(x) := f(qx)$. For analytic functions $f \\\\in \\\\mathcal{O}(D_r)$, we see that\\n\\\\[ \\\\sigma_q(f), d_q(f) \\\\in \\\\mathcal{O}(D_{r/|q|}) \\\\]\\nand they can be computed term by term using its power series expansion according to the rules\\n\\\\[ d_q(x^n) = [n]_q x^{n-1}, \\\\quad \\\\sigma_q(x^n) = q^n x^n, \\\\quad n \\\\in \\\\mathbb{N}. \\\\]\\nOn the other hand, this formula allows to consider $d_q, \\\\sigma_d : \\\\mathbb{C}[[x]] \\\\to \\\\mathbb{C}[[x]]$, also defined term by term. In this setting, Leibniz rule is replaced by\\n\\\\[ d_q(fg)(x) = d_q(f)(x)g(x) + f(qx)d_q(g)(x). \\\\]\\nWe recall the coefficients\\n\\\\[ (a; q)_n = \\\\prod_{j=0}^{n-1} (1 - aq^j), \\\\quad (a; q^{-1})_\\\\infty = \\\\prod_{j=0}^{\\\\infty} (1 - aq^{-j}), \\\\quad a \\\\in \\\\mathbb{C}. \\\\]\\nThe second one is convergent as we can compare it with a geometric series. The $q$-factorial is defined accordingly as\\n\\\\[ [n]_q! = [1]_q [2]_q \\\\cdots [n]_q = \\\\frac{(q; q)_n}{(1 - q)_n}. \\\\]\\nIn general, for $|q| > 1$, since $\\\\lambda \\\\in \\\\mathbb{R} \\\\mapsto [\\\\lambda]_{|q|}$ is a strictly increasing function, the same holds for the map $n \\\\in \\\\mathbb{N} \\\\mapsto [n]_{|q|}!$. Therefore,\\n\\\\[ \\\\frac{[n - p]_{|q|}}{([n]_{|q|})^{1/p}} = \\\\frac{[n - p]_{|q|}}{([n]_{|q|} \\\\cdots [n - p + 1]_{|q|})^{1/p}} \\\\leq \\\\frac{[n - p]_{|q|}}{[n - p + 1]_{|q|}} \\\\frac{1}{([n - p]_{|q|})^{1/p}}, \\\\]\\nand thus\\n\\\\[ \\\\frac{[n - p]_{|q|}}{([n]_{|q|})^{1/p}} \\\\leq \\\\frac{1}{([n - p]_{|q|})^{1/p}}, \\\\quad \\\\text{for integers } n > p > 0. \\\\]\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"metry breaking as reflected in $\\\\mu$ interpret as the reason for the intra-layer magnetic sym-\\nand outer Cr atoms of the bilayer system experience a\\nments as indicated in Fig. 10.\\n\\nTABLE I. CrSBr lattice structure details for undoped and\\ndoped bilayers including angles, distances, and magnetic mo-\\nments as indicated in Fig. 10.\\n\\n|                | undoped | doped  | difference |\\n|----------------|---------|--------|------------|\\n| lat. const. $a$ [\u00c5] | 3.483   | 3.496  | -0.013     |\\n| lat. const. $b$ [\u00c5] | 4.734   | 4.783  | -0.049     |\\n| $\\\\alpha$ [\u00b0]     | 95.84   | 94.77  | 1.07       |\\n| $\\\\beta$ [\u00b0]      | 92.85   | 93.34  | -0.50      |\\n| $\\\\gamma$ [\u00b0]     | 120.01  | 120.54 | -0.53      |\\n| $\\\\delta$ [\u00b0]     | 163.31  | 166.63 | -3.32      |\\n| $\\\\Delta_{\\\\text{intra}}$ [\u00c5] | 2.00    | 1.93   | 0.08       |\\n| $\\\\Delta_{\\\\text{inter}}$ [\u00c5] | 5.21    | 5.21   | 0.00       |\\n| $\\\\mu_1^B$        | 2.77    | 2.87   | -0.11      |\\n| $\\\\mu_2^B$        | 2.77    | 2.78   | -0.01      |\\n| $\\\\mu_3^B$        | -2.77   | -2.78  | 0.01       |\\n| $\\\\mu_4^B$        | -2.77   | -2.87  | 0.11       |\\n\\nN. D. Mermin and H. Wagner, Phys. Rev. Lett. 17, 1133 (1966).\\n\\nS. Chakravarty, B. I. Halperin, and D. R. Nelson, Phys. Rev. B 39, 2844 (1989).\\n\\nL. J. de Jongh, ed., Magnetic Properties of Layered Transition Metal Compounds (Springer, 1990).\\n\\nV. Y. Irkhin, A. A. Katanin, and M. I. Katsnelson, Phys. Rev. B 60, 1082 (1999).\\n\\nM. Gibertini, M. Koperski, A. F. Morpurgo, and K. S. Novoselov, Nature Nanotechnology 14, 408 (2019).\\n\\nK. Zollner, P. E. Faria Junior, and J. Fabian, Phys. Rev. B 100, 085128 (2019).\\n\\nB. Huang, G. Clark, E. Navarro-Moratalla, D. R. Klein, R. Cheng, K. L. Seyler, D. Zhong, E. Schmidgall, M. A. McGuire, D. H. Cobden, et al., Nature 546, 270 (2017).\\n\\nC. Gong, L. Li, Z. Li, H. Ji, A. Stern, Y. Xia, T. Cao, W. Bao, C. Wang, Y. Wang, et al., Nature 546, 265 (2017).\\n\\nY. Deng, Y. Yu, Y. Song, J. Zhang, N. Z. Wang, Z. Sun, Y. Yi, Y. Z. Wu, S. Wu, J. Zhu, et al., Nature 563, 94 (2018).\\n\\nO. G\u00f6ser, W. Paul, and H. Kahle, Journal of Magnetism and Magnetic Materials 92, 129 (1990).\\n\\nE. J. Telford, A. H. Dismukes, K. Lee, M. Cheng, A. Wieteska, A. K. Bartholomew, Y.-S. Chen, X. Xu, A. N. Pasupathy, X. Zhu, et al., Advanced Materials 32, 2003240 (2020).\\n\\nY. Guo, Y. Zhang, S. Yuan, B. Wang, and J. Wang, Nanoscale 10, 18036 (2018).\\n\\nZ. Jiang, P. Wang, J. Xing, X. Jiang, and J. Zhao, ACS Applied Materials and Interfaces 10, 39032 (2018).\\n\\nC. Wang, X. Zhou, L. Zhou, N.-H. Tong, Z.-Y. Lu, and W. Ji, Science Bulletin 64, 293 (2019).\\n\\nN. P. Wilson, K. Lee, J. Cenker, K. Xie, A. H. Dismukes, E. J. Telford, J. Fonseca, S. Sivakumar, C. Dean, T. Cao, et al., Nature Materials 20, 1657 (2021).\\n\\nJ. Klein, B. Pingault, M. Florian, M.-C. Hei\u00dfenb\u00fcttel, A. Steinhoff, Z. Song, K. Torres, F. Dirnberger, J. B. Curtis, M. Weile, et al., ACS Nano 17, 5316 (2023).\\n\\nM. Bianchi, S. Acharya, F. Dirnberger, J. Klein, D. Pashov, K. Mosina, Z. Sofer, A. N. Rudenko, M. I. Katsnelson, M. van Schilfgaarde, et al., Phys. Rev. B 107, 235107 (2023).\\n\\nA. N. Rudenko, M. R\u00f6sner, and M. I. Katsnelson, npj Computational Materials 9, 1 (2023).\\n\\nJ. Klein, T. Pham, J. D. Thomsen, J. B. Curtis, T. Denneulin, M. Lorke, M. Florian, A. Steinhoff, R. A. Wiscons, J. Luxa, et al., Nature Communications 13, 5420 (2022).\\n\\nE. J. Telford, A. H. Dismukes, R. L. Dudley, R. A. Wiscons, K. Lee, D. G. Chica, M. E. Ziebel, M.-G. Han, J. Yu, S. Shabani, et al., Nature Materials 21, 754 (2022).\\n\\nA. Grubi\u0161i\u0107-\u010cabo, M. Michiardi, C. E. Sanders, M. Bianchi, D. Curcio, D. Phuyal, M. H. Berntsen, Q. Guo, and M. Dendzik, Advanced Science (2023).\\n\\nJ. Strasdas, B. Pestka, M. Rybak, A. K. Budniak, N. Leuth, H. Boban, V. Feyer, I. Cojocariu, D. Baranowski, J. Klein, T. Pham, J. D. Thomsen, J. B. Curtis, T. Denneulin, M. Lorke, M. Florian, A. Steinhoff, R. A. Wiscons, J. Luxa, et al., arxiv.2211.05501.\\n\\nS. V. Hoffmann, C. Sondergaard, C. Schultz, Z. Li, and P. Hofmann, Nuclear Instruments and Methods in Physics Research, A 523, 441 (2004).\\n\\nA. I. Liechtenstein, V. I. Anisimov, and J. Zaanen, Physical Review B 52, R5467 (1995).\\n\\nG. Kresse and J. Furthm\u00fcller, Comp. Mat. Sci. 6, 15 (199 "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"metry breaking as reflected in $\\\\mu$ interpret as the reason for doping. As a result we find the largest angle change in $\\\\delta$. As the inter-layer distance stays the same, the inner and outer Cr atoms of the bilayer system experience a different change in their local environments, which we interpret as the reason for the intra-layer magnetic symmetry breaking as reflected in $\\\\mu_B^1 \\\\neq \\\\mu_B^2$ and $\\\\mu_B^3 \\\\neq \\\\mu_B^4$.\\n\\nN. D. Mermin and H. Wagner, Phys. Rev. Lett. 17, 1133 (1966). S. Chakravarty, B. I. Halperin, and D. R. Nelson, Phys. Rev. B 39, 2844 (1989). L. J. de Jongh, ed., Magnetic Properties of Layered Transition Metal Compounds (Springer, 1990). V. Y. Irkhin, A. A. Katanin, and M. I. Katsnelson, Phys. Rev. B 60, 1082 (1999). M. Gibertini, M. Koperski, A. F. Morpurgo, and K. S. Novoselov, Nature Nanotechnology 14, 408 (2019). K. Zollner, P. E. Faria Junior, and J. Fabian, Phys. Rev. B 100, 085128 (2019). B. Huang, G. Clark, E. Navarro-Moratalla, D. R. Klein, R. Cheng, K. L. Seyler, D. Zhong, E. Schmidgall, M. A. McGuire, D. H. Cobden, et al., Nature 546, 270 (2017). C. Gong, L. Li, Z. Li, H. Ji, A. Stern, Y. Xia, T. Cao, W. Bao, C. Wang, Y. Wang, et al., Nature 546, 265 (2017). Y. Deng, Y. Yu, Y. Song, J. Zhang, N. Z. Wang, Z. Sun, Y. Yi, Y. Z. Wu, S. Wu, J. Zhu, et al., Nature 563, 94 (2018). O. G\u00a8oser, W. Paul, and H. Kahle, Journal of Magnetism and Magnetic Materials 92, 129 (1990). E. J. Telford, A. H. Dismukes, K. Lee, M. Cheng, A. Wieteska, A. K. Bartholomew, Y.-S. Chen, X. Xu, A. N. Pasupathy, X. Zhu, et al., Advanced Materials 32, 2003240 (2020). Y. Guo, Y. Zhang, S. Yuan, B. Wang, and J. Wang, Nanoscale 10, 18036 (2018). Z. Jiang, P. Wang, J. Xing, X. Jiang, and J. Zhao, ACS Applied Materials and Interfaces 10, 39032 (2018). C. Wang, X. Zhou, L. Zhou, N.-H. Tong, Z.-Y. Lu, and W. Ji, Science Bulletin 64, 293 (2019). N. P. Wilson, K. Lee, J. Cenker, K. Xie, A. H. Dismukes, E. J. Telford, J. Fonseca, S. Sivakumar, C. Dean, T. Cao, et al., Nature Materials 20, 1657 (2021). J. Klein, B. Pingault, M. Florian, M.-C. Hei\u00dfenb\u00a8uttel, A. Steinhoff, Z. Song, K. Torres, F. Dirnberger, J. B. Curtis, M. Weile, et al., ACS Nano 17, 5316 (2023). M. Bianchi, S. Acharya, F. Dirnberger, J. Klein, D. Pashov, K. Mosina, Z. Sofer, A. N. Rudenko, M. I. Katsnelson, M. van Schilfgaarde, et al., Phys. Rev. B 107, 235107 (2023). A. N. Rudenko, M. R\u00a8osner, and M. I. Katsnelson, npj Computational Materials 9, 1 (2023). J. Klein, T. Pham, J. D. Thomsen, J. B. Curtis, T. Denneulin, M. Lorke, M. Florian, A. Steinhoff, R. A. Wiscons, J. Luxa, et al., Nature Communications 13, 5420 (2022). E. J. Telford, A. H. Dismukes, R. L. Dudley, R. A. Wiscons, K. Lee, D. G. Chica, M. E. Ziebel, M.-G. Han, J. Yu, S. Shabani, et al., Nature Materials 21, 754 (2022). A. Grubi\u02c7si\u00b4c-\u02c7Cabo, M. Michiardi, C. E. Sanders, M. Bianchi, D. Curcio, D. Phuyal, M. H. Berntsen, Q. Guo, and M. Dendzik, Advanced Science (2023). J. Strasdas, B. Pestka, M. Rybak, A. K. Budniak, N. Leuth, H. Boban, V. Feyer, I. Cojocariu, D. Baranowski, J. Avila, M. Dendzik, Advanced Science (2023). S. V. Hoffmann, C. Sondergaard, C. Schultz, Z. Li, and P. Hofmann, Nuclear Instruments and Methods in Physics Research, A 523, 441 (2004). A. I. Liechtenstein, V. I. Anisimov, and J. Zaanen, Physical Review B 52, R5467 (1995). G. Kresse and J. Furthm\u00a8uller, Comp. Mat. Sci. 6, 15 (1996).\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"describes the traveling-wave microwave photon transporting along the transmission line with \\\\( l = b, c \\\\) referring to its left, right side, and the relevant bosonic operators satisfy the communication relation: \\\\( \\\\{ l(\\\\omega), l'(\\\\omega') \\\\} = \\\\delta(\\\\omega - \\\\omega') \\\\). Also, the flux operator of the traveling-wave photon reads [14\u201316]:\\n\\n\\\\[\\n\\\\hat{\\\\phi}_l(x) = \\\\sqrt{\\\\frac{\\\\hbar Z_0}{4\\\\pi}} \\\\int_0^\\\\infty \\\\frac{d\\\\omega}{\\\\sqrt{\\\\omega}} \\\\left[ \\\\hat{l}(\\\\omega)e^{ikx} + \\\\hat{l}^\\\\dagger(\\\\omega)e^{-ikx} \\\\right],\\n\\\\]\\n\\n(7)\\n\\nwith \\\\( Z_0 \\\\) being the characteristic impedance of the transmission line, and thus\\n\\n\\\\[\\n\\\\hat{\\\\phi}_l(x) = (-i)\\\\sqrt{\\\\frac{\\\\hbar Z_0}{4\\\\pi}} \\\\int_0^\\\\infty d\\\\omega \\\\sqrt{\\\\omega} \\\\left[ \\\\hat{l}(\\\\omega)e^{ikx} - \\\\hat{l}^\\\\dagger(\\\\omega)e^{-ikx} \\\\right].\\n\\\\]\\n\\n(8)\\n\\nUnder the sufficiently low current bias, the CBJJ Hamiltonian reads: \\\\( \\\\hat{H}_{CBJJ} \\\\approx \\\\hat{H}_b \\\\), shown in Eq. 4. The physical boundary condition at \\\\( x = 0 \\\\), i.e., the location of the device, reads:\\n\\n\\\\[\\n\\\\hat{I}(0_b, t) = \\\\hat{I}(0_c, t), \\\\quad V_J = (\\\\Phi_0/2\\\\pi) \\\\delta + \\\\left[ \\\\hat{\\\\phi}(0_b) - \\\\hat{\\\\phi}(0_c) \\\\right].\\n\\\\]\\n\\nThus, under the low-excitation limit and rotating-wave approximation, i.e., the photon scattering is the desired elastic and any possibly created and annihilated of the photons at \\\\( x = 0 \\\\) is neglected, we have\\n\\n\\\\[\\n\\\\hat{H}_{CBJJ-B} = C_J \\\\hat{p}_b \\\\left[ \\\\hat{\\\\phi}(0_b) - \\\\hat{\\\\phi}(0_c) \\\\right] = i\\\\hbar \\\\sqrt{\\\\frac{\\\\kappa_1}{2\\\\pi}} \\\\int d\\\\omega \\\\left[ a^\\\\dagger l(\\\\omega) - l^\\\\dagger(\\\\omega)a \\\\right],\\n\\\\]\\n\\nwhere \\\\( \\\\kappa_l = Z_0/4Z_J (l = b, c) \\\\) describes the interaction between the CBJJ and the left/right traveling-wave photons, \\\\( Z_J = \\\\sqrt{L_J/C_J} \\\\) is the characteristic impedance of the Josephson junction. As a consequence, the Hamiltonian (with \\\\( \\\\hbar = 1 \\\\)) of the system [16\u201318]:\\n\\n\\\\[\\nH_B = \\\\left( \\\\omega_p - \\\\frac{i\\\\gamma}{2} \\\\right) a^\\\\dagger a + \\\\int d\\\\omega \\\\left[ \\\\omega b(\\\\omega)^\\\\dagger b(\\\\omega) + i \\\\sqrt{\\\\frac{\\\\kappa_1}{2\\\\pi}} (a^\\\\dagger b(\\\\omega) - ab(\\\\omega))^\\\\dagger \\\\right] + \\\\int d\\\\omega \\\\left[ \\\\omega c(\\\\omega)^\\\\dagger c(\\\\omega) + i \\\\sqrt{\\\\frac{\\\\kappa_2}{2\\\\pi}} (a^\\\\dagger c(\\\\omega) - ac(\\\\omega))^\\\\dagger \\\\right],\\n\\\\]\\n\\n(9)\\n\\nwhere \\\\( \\\\gamma \\\\) is decay rate of the cavity, \\\\( \\\\kappa_1 \\\\) and \\\\( \\\\kappa_2 \\\\) are the effective strengths of the boson coupled to the photons in the left and right sides of the transmission line, respectively. By using the standard input-output theory [18, 19], we get the relations:\\n\\n\\\\[\\n\\\\frac{da}{dt} = \\\\left( -i\\\\omega_p - \\\\frac{\\\\kappa + \\\\gamma}{2} \\\\right) a + \\\\sqrt{\\\\kappa_1} b_{in}(t) + \\\\sqrt{\\\\kappa_2} c_{in}(t),\\n\\\\]\\n\\n(10)\\n\\nand\\n\\n\\\\[\\n\\\\frac{da}{dt} = \\\\left( -i\\\\omega_p + \\\\frac{\\\\kappa - \\\\gamma}{2} \\\\right) a - \\\\sqrt{\\\\kappa_1} b_{out}(t) - \\\\sqrt{\\\\kappa_2} c_{out}(t),\\n\\\\]\\n\\n(11)\\n\\nwith \\\\( \\\\kappa = \\\\kappa_1 + \\\\kappa_2 \\\\),\\n\\n\\\\[\\n\\\\dot{b}_{in/out} = \\\\pm \\\\frac{1}{\\\\sqrt{2\\\\pi}} \\\\int d\\\\omega e^{-i\\\\omega(t-t')} b_0(\\\\omega),\\n\\\\]\\n\\nand\\n\\n\\\\[\\n\\\\dot{c}_{in/out} = \\\\pm \\\\frac{1}{\\\\sqrt{2\\\\pi}} \\\\int d\\\\omega e^{-i\\\\omega(t-t')} c_0(\\\\omega),\\n\\\\]\\n\\nare the input- and output fields, respectively. After the Fourier transformation: \\\\( x(t) = \\\\int_{-\\\\infty}^{\\\\infty} e^{-i\\\\omega(t-t_0)} x(\\\\omega) d\\\\omega/\\\\sqrt{2\\\\pi} \\\\), for \\\\( x(t) = a(t), b_{in}(t), c_{in}(t), b_{out}(t), \\\\) and \\\\( c_{out}(t) \\\\), respectively, we have\\n\\n\\\\[\\n-i\\\\omega a(\\\\omega) = \\\\left( -i\\\\omega_p - \\\\frac{\\\\kappa - \\\\gamma}{2} \\\\right) a(\\\\omega) + \\\\sqrt{\\\\kappa_1} b_{in}(\\\\omega) + \\\\sqrt{\\\\kappa_2} c_{in}(\\\\omega)\\n\\\\]\\n\\n(12)\\n\\nand\\n\\n\\\\[\\n-i\\\\omega a(\\\\omega) = \\\\left( -i\\\\omega_p + \\\\frac{\\\\kappa + \\\\gamma}{2} \\\\right) a(\\\\omega) - \\\\sqrt{\\\\kappa_1} b_{out}(\\\\omega) - \\\\sqrt{\\\\kappa_2} c_{out}(\\\\omega).\\n\\\\]\\n\\n(13)\\n\\nFor the configuration shown in Fig. 3, we can assume that \\\\( c_{in}(t) = 0 \\\\). Consequently, we have\\n\\n\\\\[\\nb_{in}(\\\\omega) + b_{out}(\\\\omega) = \\\\sqrt{\\\\kappa_1} a(\\\\omega)\\n\\\\]\\n\\n\\\\[\\nc_{in}(\\\\omega) + c_{out}(\\\\omega) = \\\\sqrt{\\\\kappa_2} a(\\\\omega).\\n\\\\]\\n\\n(14)\\n\\nTherefore, the measurable transmitted- and phase shift spectra of the traveling-wave photons can be calculated as\\n\\n\\\\[\\nT_B(\\\\omega) = \\\\left| \\\\frac{c_{out}(\\\\omega)}{b_{in}(\\\\omega)} \\\\right|^2 = \\\\frac{4\\\\kappa_1\\\\kappa_2}{4(\\\\omega - \\\\omega_p)^2 + (\\\\kappa + \\\\gamma)^2},\\n\\\\]\\n\\n(15)\\n\\nand\\n\\n\\\\[\\n\\\\phi_{TB}(\\\\omega) = \\\\arctan \\\\left[ -\\\\frac{2(\\\\omega - \\\\omega_p)}{\\\\kappa + \\\\gamma} \\\\right],\\n\\\\]\\n\\n(16)\\n\\nrespectively. In Fig. 4 we shows the spectra of the traveling-wave microwave photons scattered by a quantized CBJJ device with the typical parameters: \\\\( I_b = 0, I_c = 0.975 \\\\) pA, \\\\( C = 11.18 \\\\) pF, and thus \\\\( \\\\omega_p \\\\sim 2\\\\pi \\\\times 2.595 \\\\) GHz. It is seen clearly that, if the CBJJ device works as a boson, the peak of the photon transmission is located as the eigenfrequency of the\\n\\n![FIG. 4: The transmitted spectrum (a) and phase shift spectrum (b) of the traveling-wave scattered by the CBJJ. Here, the relevant parameters are set as: \\\\( \\\\kappa_1 = 0.004, \\\\kappa_2 = 0.008, \\\\gamma = 0.0008, \\\\) and \\\\( \\\\omega_p = 2\\\\pi \\\\times 2.5 "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"describes the traveling-wave microwave photon transporting along the transmission line with \\\\( l = b, c \\\\) referring to its left, right side, and the relevant bosonic operators satisfy the communication relation: \\\\( \\\\{ l(\\\\omega), l'(\\\\omega') \\\\} = \\\\delta(\\\\omega - \\\\omega') \\\\). Also, the flux operator of the traveling-wave photon reads [14\u201316]:\\n\\n\\\\[\\n\\\\hat{\\\\phi}_l(x) = \\\\sqrt{\\\\frac{\\\\hbar Z_0}{4\\\\pi}} \\\\int_0^\\\\infty d\\\\omega \\\\frac{1}{\\\\sqrt{\\\\omega}} \\\\left[ \\\\hat{l}(\\\\omega)e^{ikx} + \\\\hat{l}^\\\\dagger(\\\\omega)e^{-ikx} \\\\right],\\n\\\\]\\n\\n(7)\\n\\nwith \\\\( Z_0 \\\\) being the characteristic impedance of the transmission line, and thus\\n\\n\\\\[\\n\\\\hat{\\\\phi}_l(x) = (-i)\\\\sqrt{\\\\frac{\\\\hbar Z_0}{4\\\\pi}} \\\\int_0^\\\\infty d\\\\omega \\\\sqrt{\\\\omega} \\\\left[ \\\\hat{l}(\\\\omega)e^{ikx} - \\\\hat{l}^\\\\dagger(\\\\omega)e^{-ikx} \\\\right].\\n\\\\]\\n\\n(8)\\n\\nUnder the sufficiently low current bias, the CBJJ Hamiltonian reads: \\\\( \\\\hat{H}_{CBJJ} \\\\approx \\\\hat{H}_B \\\\), shown in Eq. 4. The physical boundary condition at \\\\( x = 0 \\\\), i.e., the location of the device, reads: \\\\( \\\\hat{I}(0_b, t) = \\\\hat{I}(0_c, t), V_J = (\\\\Phi_0/2\\\\pi) \\\\delta + \\\\left[ \\\\hat{\\\\phi}(0_b) - \\\\hat{\\\\phi}(0_c) \\\\right] \\\\). Thus, under the low-excitation limit and rotating-wave approximation, i.e., the photon scattering is the desired elastic and any possibly created and annihilated of the photons at \\\\( x = 0 \\\\) is neglected, we have\\n\\n\\\\[\\n\\\\hat{H}_{CBJJ-B} = C_J \\\\hat{p}_0 \\\\left[ \\\\hat{\\\\phi}(0_b) - \\\\hat{\\\\phi}(0_c) \\\\right] = i\\\\hbar \\\\sqrt{\\\\frac{\\\\kappa_1}{2\\\\pi}} \\\\int d\\\\omega \\\\left[ a^\\\\dagger l(\\\\omega) - l^\\\\dagger(\\\\omega)a \\\\right],\\n\\\\]\\n\\nwhere \\\\( \\\\kappa_l = Z_0/4Z_J (l = b, c) \\\\) describes the interaction between the CBJJ and the left/right traveling-wave photons, \\\\( Z_J = \\\\sqrt{L_J/C_J} \\\\) is the characteristic impedance of the Josephson junction. As a consequence, the Hamiltonian (with \\\\( \\\\hbar = 1 \\\\)) of the system [16\u201318]:\\n\\n\\\\[\\nH_B = \\\\left( \\\\omega_p - \\\\frac{i\\\\gamma}{2} \\\\right) a^\\\\dagger a + \\\\int d\\\\omega \\\\left[ \\\\omega b(\\\\omega)^\\\\dagger b(\\\\omega) + i \\\\sqrt{\\\\frac{\\\\kappa_1}{2\\\\pi}} (a^\\\\dagger b(\\\\omega) - ab(\\\\omega))^\\\\dagger \\\\right] + \\\\int d\\\\omega \\\\left[ \\\\omega c(\\\\omega)^\\\\dagger c(\\\\omega) + i \\\\sqrt{\\\\frac{\\\\kappa_2}{2\\\\pi}} (a^\\\\dagger c(\\\\omega) - ac(\\\\omega))^\\\\dagger \\\\right],\\n\\\\]\\n\\n(9)\\n\\nwhere \\\\( \\\\gamma \\\\) is decay rate of the cavity, \\\\( \\\\kappa_1 \\\\) and \\\\( \\\\kappa_2 \\\\) are the effective strengths of the boson coupled to the photons in the left and right sides of the transmission line, respectively. By using the standard input-output theory [18, 19], we get the relations: \\\\( \\\\frac{da}{dt} = \\\\left( -i\\\\omega_p - \\\\frac{\\\\kappa + \\\\gamma}{2} \\\\right) a + \\\\sqrt{\\\\kappa_1} b_{in}(t) + \\\\sqrt{\\\\kappa_2} c_{in}(t) \\\\),\\n\\n(10)\\n\\nand\\n\\n\\\\[\\n\\\\frac{da}{dt} = \\\\left( -i\\\\omega_p + \\\\frac{\\\\kappa - \\\\gamma}{2} \\\\right) a - \\\\sqrt{\\\\kappa_1} b_{out}(t) - \\\\sqrt{\\\\kappa_2} c_{out}(t),\\n\\\\]\\n\\n(11)\\n\\nwith \\\\( \\\\kappa = \\\\kappa_1 + \\\\kappa_2 \\\\),\\n\\n\\\\[\\n\\\\hat{b}_{in/out} = \\\\pm \\\\frac{1}{\\\\sqrt{2\\\\pi}} \\\\int d\\\\omega e^{-i\\\\omega(t-t')} b_0(\\\\omega),\\n\\\\]\\n\\nand\\n\\n\\\\[\\n\\\\hat{c}_{in/out} = \\\\pm \\\\frac{1}{\\\\sqrt{2\\\\pi}} \\\\int d\\\\omega e^{-i\\\\omega(t-t')} c_0(\\\\omega),\\n\\\\]\\n\\nare the input- and output fields, respectively. After the the Fourier transformation: \\\\( x(t) = \\\\int_{-\\\\infty}^{\\\\infty} e^{-i\\\\omega(t-t_0)} x(\\\\omega) d\\\\omega/\\\\sqrt{2\\\\pi} \\\\), for \\\\( x(t) = a(t), b_{in}(t), c_{in}(t), b_{out}(t), \\\\) and \\\\( c_{out}(t) \\\\), respectively, we have\\n\\n\\\\[\\n-i\\\\omega a(\\\\omega) = \\\\left( -i\\\\omega_p - \\\\frac{\\\\kappa - \\\\gamma}{2} \\\\right) a(\\\\omega) + \\\\sqrt{\\\\kappa_1} b_{in}(\\\\omega) + \\\\sqrt{\\\\kappa_2} c_{in}(\\\\omega)\\n\\\\]\\n\\n(12)\\n\\nand\\n\\n\\\\[\\n-i\\\\omega a(\\\\omega) = \\\\left( -i\\\\omega_p + \\\\frac{\\\\kappa + \\\\gamma}{2} \\\\right) a(\\\\omega) - \\\\sqrt{\\\\kappa_1} b_{out}(\\\\omega) - \\\\sqrt{\\\\kappa_2} c_{out}(\\\\omega).\\n\\\\]\\n\\n(13)\\n\\nFor the configuration shown in Fig. 3, we can assume that \\\\( c_{in}(t) = 0 \\\\). Consequently, we have\\n\\n\\\\[\\nb_{in}(\\\\omega) + b_{out}(\\\\omega) = \\\\sqrt{\\\\kappa_1} a(\\\\omega)\\n\\\\]\\n\\n\\\\[\\nc_{in}(\\\\omega) + c_{out}(\\\\omega) = \\\\sqrt{\\\\kappa_2} a(\\\\omega).\\n\\\\]\\n\\n(14)\\n\\nTherefore, the measurable transmitted- and phase shift spectra of the traveling-wave photons can be calculated as\\n\\n\\\\[\\nT_B(\\\\omega) = \\\\left| \\\\frac{b_{out}(\\\\omega)}{b_{in}(\\\\omega)} \\\\right|^2 = \\\\frac{4\\\\kappa_1\\\\kappa_2}{4(\\\\omega - \\\\omega_p)^2 + (\\\\kappa + \\\\gamma)^2},\\n\\\\]\\n\\n(15)\\n\\nand\\n\\n\\\\[\\n\\\\phi_{TB}(\\\\omega) = \\\\arctan \\\\left[ -\\\\frac{2(\\\\omega - \\\\omega_p)}{\\\\kappa + \\\\gamma} \\\\right],\\n\\\\]\\n\\n(16)\\n\\nrespectively. In Fig. 4 we shows the spectra of the traveling-wave microwave photons scattered by a quantized CBJJ device with the typical parameters: \\\\( I_b = 0, I_c = 0.975 \\\\) pA, \\\\( C = 11.18 \\\\) pF, and thus \\\\( \\\\omega_p \\\\sim 2\\\\pi \\\\times 2.595 \\\\) GHz. It is seen clearly that, if the CBJJ device works as a boson, the peak of the photon transmission is located as the eigenfrequency of the\\n\\n![FIG. 4: The transmitted spectrum (a) and phase shift spectrum (b) of the traveling-wave scattered by the CBJJ. Here, the relevant parameters are set as: \\\\( \\\\kappa_1 = 0.004, \\\\kappa_2 = 0.008, \\\\gamma = 0.0008, \\\\) and \\\\( \\\\omega_p = 2\\\\pi \\\\times 2.595 \\\\) GHz.]( "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1 Introduction\\n\\nVectorlike (VL) fermions are key ingredients in many new physics models beyond the Standard Model (SM) adopted to resolve both theoretical and experimental issues. Since chiral fermions in the fourth family are excluded experimentally [1, 2], these are considered to be vectorlike and their masses are given independently to the Higgs mechanism in the SM. The VL fermions are introduced in, for instance, supersymmetric models [3\u201310], gauge mediated supersymmetry breaking scenario [11\u201316], composite Higgs models [17\u201319], KSVZ axion models [20, 21], axion-like particle models [22\u201325], alternative solutions of the strong CP problem [26, 27], two Higgs doublet model augmented by VL fermions [28\u201338], and models for gauge coupling unification [39, 40].\\n\\nAmong the fourth family fermions, VL leptons (VLLs) with nonzero lepton number play unique roles in constructing lepton-philic dark matter (DM) models [41\u201346], mirror sector models [47\u201349], and explanations for the muon anomalies [50\u201365]. Interestingly, the lightest VLL is expected to be in the reach of the Large Hadron Collider (LHC) or High-Luminosity (HL)-LHC. Both ATLAS and CMS collaborations search for the pair production of VLLs each of which dominantly decays to a SM boson and a tau lepton [70\u201372]. For the doublet VLL, the ATLAS search excludes the mass range of 130 < m_{VLL} < 900 GeV [72], and the CMS search excludes the mass up to 1045 GeV [71]. The singlet VLL is less constrained and the limit is less than 150 GeV [71]. Prospects of such VLLs at the future colliders are discussed in Ref. [73]. The pair productions of the VLLs decaying to a SM boson and a muon (neutrino) are studied by the ATLAS [74] and the theorists [75, 76] using the Run-I data. The limit is obtained for m_{VLL} \u2272 500 GeV when the neutral component of the lightest doublet VLL dominantly decays to a W boson and a muon [75]. There are also studies for the VLL produced from cascade decays of extra neutral Higgs bosons [28\u201331, 35, 36], and signals from the VLLs decaying to a DM particle [77] or Z' boson [78].\\n\\nIn this paper, we study pair-productions of the VLLs, through the Drell-Yan process, decaying to the second generation lepton, namely muon-philic VLL. Such VLL is well motivated to explain the experimental anomalies in the muon g \u2212 2 [69, 79, 80] and the semi-leptonic B decays [81]. In this work, we obtain the current limits using the Run-2 data at \u221as = 13 TeV by simply recasting the ATLAS analyses searching for the triplet lepton in the type-III seesaw [84, 85]. We then estimate expected sensitivities at the HL-LHC using the same channels.\\n\\nThis paper is organized as follows. We briefly explain the VLLs in Sec. 2 and discuss the analysis strategy in Sec. 3. Our main results are shown in Sec. 4. Finally, we summarize the paper in Sec. 5.\\n\\n---\\n\\n1The latest experimental result [66] confirms the previous results [67, 68] which might deviate from the SM prediction [69].\\n\\n2Throughout this work, doublet (singlet) for VLL means iso-doublet (iso-singlet) under the SU(2)_L gauge symmetry.\\n\\n3The LHCb has recently announced the new result of R_{K^{(*)}} consistent with the SM expectation [82] which requires efforts such as separate measurements of the branching ratio at Belle-II [83].\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1 Introduction\\n\\nVectorlike (VL) fermions are key ingredients in many new physics models beyond the Standard Model (SM) adopted to resolve both theoretical and experimental issues. Since chiral fermions in the fourth family are excluded experimentally [1, 2], these are considered to be vectorlike and their masses are given independently to the Higgs mechanism in the SM. The VL fermions are introduced in, for instance, supersymmetric models [3\u201310], gauge mediated supersymmetry breaking scenario [11\u201316], composite Higgs models [17\u201319], KSVZ axion models [20,21], axionlike particle models [22\u201325], alternative solutions of the strong CP problem [26,27], two Higgs doublet model augmented by VL fermions [28\u201338], and models for gauge coupling unification [39,40].\\n\\nAmong the fourth family fermions, VL leptons (VLLs) with nonzero lepton number play unique roles in constructing lepton-philic dark matter (DM) models [41\u201346], mirror sector models [47\u201349], and explanations for the muon anomalies [50\u201365] 1 . Interestingly, the lightest VLL is expected to be in the reach of the Large Hadron Collider (LHC) or High-Luminosity (HL)-LHC The latest experimental result [66] confirms the previous results [67,68] which might deviate from the SM prediction [69].\\n\\nThroughout this work, doublet (singlet) for VLL means iso-doublet (iso-singlet) under the $SU(2)_L$ gauge symmetry.\\n\\nThe LHCb has recently announced the new result of $R_{K^{(*)}}$ consistent with the SM expectation [82] which requires efforts such as separate measurements of the branching ratio at Belle-II [83].\\n\\n1 The latest experimental result [66] confirms the previous results [67,68] which might deviate from the SM prediction [69].\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.1.2 Even $L$ with a fermion parity defect\\n\\nLet us introduce a $G$ defect. A concrete Hamiltonian to keep in mind is\\n\\n$$H_G = \\\\frac{i}{2} \\\\sum_{\\\\ell=1}^{L-1} \\\\chi_{\\\\ell+1} \\\\chi_{\\\\ell} - \\\\frac{i}{2} \\\\chi_1 \\\\chi_L,$$\\n\\n(3.21)\\n\\nwhere the defect is in the link connecting $(L, 1)$. We use the subscript $G$ for the Hamiltonian and symmetry operators in the system with a $G$ defect. However, we emphasize that for most of our discussion the particular form of the Hamiltonian will not matter. Note that the defect can be moved to other links, e.g., to the link $(1, 2)$, by conjugating $H_G$ by a local $G$ transformation, $\\\\chi_1$ or $g_1$.\\n\\nLet us determine the symmetry operators of the theory with the defect. We use the same fermion parity operator $G$ as in (3.10), because it commutes with $H_G$.\\\\(^{21}\\\\) On the other hand, instead of (3.5), the translation operator now acts on the fermion fields as\\n\\n$$T_G : \\\\chi_\\\\ell \\\\rightarrow T_G \\\\chi_\\\\ell T_G^{-1} = \\\\sum_{\\\\ell'} R(T_G)_{\\\\ell, \\\\ell'} \\\\chi_{\\\\ell'} = \\\\begin{cases} \\\\chi_{\\\\ell+1} & \\\\ell = 1, 2, \\\\ldots, L - 1 \\\\\\\\ -\\\\chi_1 & \\\\ell = L \\\\end{cases}$$\\n\\n(3.22)\\n\\nThe algebra satisfied by these operators is\\n\\n$$R(G)^2 = 1, \\\\quad R(T_G)^L = R(G), \\\\quad R(G) R(T_G) = R(T_G) R(G).$$\\n\\n(3.23)\\n\\nIn contrast to the case without the defect (3.7), now we have\\n\\n$$\\\\det R(T_G)_{\\\\ell, \\\\ell'} = +1,$$\\n\\n$$\\\\det R(G)_{\\\\ell, \\\\ell'} = +1.$$  \\n\\n(3.24)\\n\\nThis means that the twisted translation operator is an $SO(L)$ transformation and is constructed out of an even number of fermions, i.e., it is bosonic.\\n\\nLet us write $T_G$ in terms of the fermion fields. Again, (3.22) does not determine its phase normalization, and we will make an arbitrary choice below. Later, in Section 3.3, we will rescale it to $T_{\\\\text{NSNS}}$ and compare it to the continuum operators. Its action in (3.22) means that we should multiply $T$ by an operator that maps $\\\\chi_1 \\\\rightarrow -\\\\chi_1$ and leaves the other fermions unchanged, i.e., we should multiply it by $g_1 = -\\\\chi_2 \\\\chi_3 \\\\cdots \\\\chi_L$. Therefore, we take [81]\\\\(^{22}\\\\)\\n\\n$$T_G = (-1)^N g_1 T = \\\\frac{1}{2^{L-1}} (1 - \\\\chi_1 \\\\chi_2)(1 - \\\\chi_2 \\\\chi_3) \\\\cdots (1 - \\\\chi_{L-1} \\\\chi_L).$$\\n\\n(3.26)\\n\\n\\\\(^{21}\\\\)We do not write $G_G$ because it is the same as $G$.\\n\\n\\\\(^{22}\\\\)Alternatively, the translation operator for even $L$ with a defect can be written as\\n\\n$$T_G = \\\\frac{(-1)^N}{2^{L-1}} (\\\\chi_1 - \\\\chi_2)(\\\\chi_2 - \\\\chi_3) \\\\cdots (\\\\chi_{L-1} - \\\\chi_L) \\\\chi_L.$$  \\n\\n(3.25)\\n\\n(Note that in this forms, $T_G$ does not satisfy the locality condition (1.15).)\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.1.2 Even $L$ with a fermion parity defect\\n\\nLet us introduce a $G$ defect. A concrete Hamiltonian to keep in mind is\\n\\n$$H_G = \\\\frac{i}{2} \\\\sum_{\\\\ell=1}^{L-1} \\\\chi_{\\\\ell+1} \\\\chi_{\\\\ell} - \\\\frac{i}{2} \\\\chi_1 \\\\chi_L,$$\\n\\n(3.21)\\n\\nwhere the defect is in the link connecting $(L, 1)$. We use the subscript $G$ for the Hamiltonian and symmetry operators in the system with a $G$ defect. However, we emphasize that for most of our discussion the particular form of the Hamiltonian will not matter. Note that the defect can be moved to other links, e.g., to the link $(1, 2)$, by conjugating $H_G$ by a local $G$ transformation, $\\\\chi_1$ or $g_1$.\\n\\nLet us determine the symmetry operators of the theory with the defect. We use the same fermion parity operator $G$ as in ( 3.10 ), because it commutes with $H_G$.\\\\(^{21}\\\\) On the other hand, instead 21 of ( 3.5 ), the translation operator now acts on the fermion fields as\\n\\n$$T_G : \\\\chi_\\\\ell \\\\rightarrow T_G \\\\chi_\\\\ell T_G^{-1} = \\\\sum_{\\\\ell'} R(T_G)_{\\\\ell, \\\\ell'} \\\\chi_{\\\\ell'} = \\\\begin{cases} \\\\chi_{\\\\ell+1} & \\\\ell = 1, 2, \\\\ldots, L - 1 \\\\\\\\ -\\\\chi_1 & \\\\ell = L \\\\end{cases}$$\\n\\n(3.22)\\n\\nThe algebra satisfied by these operators is\\n\\n$$R(G)^2 = 1, \\\\quad R(T_G)^L = R(G), \\\\quad R(G) R(T_G) = R(T_G) R(G).$$\\n\\n(3.23)\\n\\nIn contrast to the case without the defect ( 3.7 ),now we have\\n\\n$$\\\\det R(T_G)_{\\\\ell, \\\\ell'} = +1,$$\\n\\n$$\\\\det R(G)_{\\\\ell, \\\\ell'} = +1.$$  \\n\\n(3.24)\\n\\nThis means that the twisted translation operator is an $SO(L)$ transformation and is constructed out of an even number of fermions, i.e., it is bosonic.\\\\(^{22}\\\\)\\n\\nLet us write $T_G$ in terms of the fermion fields. Again, ( 3.22 ) does not determine its phase normalization, and we will make an arbitrary choice below. Later, in Section 3.3 , we will rescale it to $T_{\\\\text{NSNS}}$ and compare it to the continuum operators. Its action in ( 3.22 ) means that we should multiply $T$ by an operator that maps $\\\\chi_1 \\\\rightarrow -\\\\chi_1$ and leaves the other fermions unchanged, i.e., we should multiply it by $g_1 = -\\\\chi_2 \\\\chi_3 \\\\cdots \\\\chi_L$. Therefore, we take [ 81 ] 22 22 Alternatively, the translation operator for even $L$ with a defect can be written as\\n\\n$$T_G = (-1)^N g_1 T = \\\\frac{1}{2^{L-1}} (1 - \\\\chi_1 \\\\chi_2) (1 - \\\\chi_2 \\\\chi_3) \\\\cdots (1 - \\\\chi_{L-1} \\\\chi_L).$$\\n\\n(3.26)\\n\\n(Note that in this forms, $T_G$ does not satisfy the locality condition ( 1.15 ).)\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"when measuring the correlation of cities. Although the correlation of cities can be measured from the aspect of POI distribution, the user behavioral transition pattern is a significant factor in the next POI recommendation task, we thus further explore such correlation from the angle of user sequential behaviors.\\n\\n**Correlation of Cities w.r.t Behavioral Patterns.** We examine the correlation of cities w.r.t. the categories of users\u2019 successive POI visits. In particular, given any two cities, $A_{\\\\text{cat}} = [A_{1}^{\\\\text{cat}}, A_{2}^{\\\\text{cat}} ... A_{|S|}^{\\\\text{cat}}]$ and $B_{\\\\text{cat}} = [B_{1}^{\\\\text{cat}}, B_{2}^{\\\\text{cat}} ... B_{|S|}^{\\\\text{cat}}]$ refer to the category transition distributions among $S$ transition types, e.g., $A_{1}^{\\\\text{cat}}$ denotes the ratio of transition type $FO \\\\rightarrow SS$ within city A. Analogously, the similarity among different cities can be calculated via the Pearson correlation coefficient, shown in Fig. 2(b). Interestingly, we observe that the correlation of cities w.r.t behavioral patterns is quite different from that w.r.t POI distribution. Specifically, PHO and CAL still keep higher similarity, whereas NYC shows comparably lower similarity with PHO and CAL. To further dig out how the four cities are correlated and different over the behavioral patterns, we compare the two most correlated cities (i.e., CAL and PHO) and the two least correlated cities (i.e., NYC and SIN). For ease of presentation, we select the 10 most frequent category transitions for comparison as shown in Fig. 2(c-d), where the $x$-axis denotes the category transitions, e.g., $AE \\\\rightarrow CU$ (AE2CU), and the $y$-axis shows the proportion of such a transition within a city. We find that the more correlated cities possess consistent distributions over the frequent category transitions and vice versa. The above observations depict the various correlations between cities, which inspire us to differentiate their influence when transferring knowledge from auxiliary cities to the target city.\\n\\n### 4 The Proposed MERec\\n\\nThis section presents the proposed MERec, which leverages the correlation of behavioral patterns when transferring knowledge from auxiliary cities to the target city, i.e., paying more attention to more correlated knowledge.\\n\\n**Problem Formulation.** Each city has its unique user set $\\\\mathcal{U}$ and POI set $\\\\mathcal{P}$ without sharing any common users and POIs. For user $u$, all his check-in records, i.e., $r = (p, c, g, t)$, are ordered by timestamps as in [22], where $p, c, g, t$ denote POI $p$, category $c$, coordinate $g$ (i.e., longitude and latitude) and timestamp $t$. We then split his historical records into sequences by day and obtain two\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"when measuring the correlation of cities. Although the correlation of cities can be measured from the aspect of POI distribution, the user behavioral transition pattern is a significant factor in the next POI recommendation task, we thus further explore such correlation from the angle of user sequential behaviors. \\n\\n**Correlation of Cities w.r.t Behavioral Patterns.** We examine the correlation of cities w.r.t. the categories of users\u2019 successive POI visits. In particular, given any two cities, $A_{\\\\text{cat}} = [A_{1}^{\\\\text{cat}}, A_{2}^{\\\\text{cat}} ... A_{|S|}^{\\\\text{cat}}]$ and $B_{\\\\text{cat}} = [B_{1}^{\\\\text{cat}}, B_{2}^{\\\\text{cat}} ... B_{|S|}^{\\\\text{cat}}]$ refer to the category transition distributions among $S$ transition types, e.g., $A_{1}^{\\\\text{cat}}$ denotes the ratio of transition type $FO \\\\rightarrow SS$ within city A. Analogously, the similarity among different cities can be calculated via the Pearson correlation coefficient, shown in Fig. 2(b). Interestingly, we observe that the correlation of cities w.r.t behavioral patterns is quite different from that w.r.t POI distribution. Specifically, PHO and CAL still keep higher similarity, whereas NYC shows comparably lower similarity with PHO and CAL. To further dig out how the four cities are correlated and different over the behavioral patterns, we compare the two most correlated cities (i.e., CAL and PHO) and the two least correlated cities (i.e., NYC and SIN). For ease of presentation, we select the 10 most frequent category transitions for comparison as shown in Fig. 2(c-d), where the $x$-axis denotes the category transitions, e.g., $AE \\\\rightarrow CU$ (AE2CU), and the $y$-axis shows the proportion of such a transition within a city. We find that the more correlated cities possess consistent distributions over the frequent category transitions and vice versa. The above observations depict the various correlations between cities, which inspire us to differentiate their influence when transferring knowledge from auxiliary cities to the target city. \\n\\n4 The Proposed MERec\\n\\nThis section presents the proposed MERec, which leverages the correlation of behavioral patterns when transferring knowledge from auxiliary cities to the target city, i.e., paying more attention to more correlated knowledge. \\n\\n**Problem Formulation.** Each city has its unique user set $U$ and POI set $P$ without sharing any common users and POIs. For user $u$, all his check-in records, i.e., $r = (p, c, g, t)$, are ordered by timestamps as in [22], where $p, c, g, t$ denote POI $p$, category $c$, coordinate $g$ (i.e., longitude and latitude) and timestamp $t$. We then split his historical records into sequences by day and obtain two\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.2. Symmetric and Schur-positive sets\\n\\nAs mentioned in Section 1, a set $\\\\mathcal{A}$ is symmetric with respect to a statistic function $D : \\\\mathcal{A} \\\\to 2^{[N-1]}$ if its generating function $Q_{N,D}(\\\\mathcal{A})$ is a symmetric function. Moreover, it is Schur-positive if all Schur coefficients are nonnegative integers.\\n\\nOne of the fundamental constructions of Schur-positive sets, regarding sets of standard Young tableaux (SYT), is due to Gessel [11]. Let SYT($\\\\lambda$) denote the set of standard Young tableaux of shape $\\\\lambda$. We draw tableaux in English notation, as in Figure 2. The descent set of $T \\\\in$ SYT($\\\\lambda$) is\\n\\n$$\\\\text{Des}(T) := \\\\{i \\\\in [N-1] \\\\mid i + 1 \\\\text{ appears in a lower row than } i \\\\text{ in } T\\\\}.$$ \\n\\nFor example, the descent set of the SYT in Figure 2 is $\\\\{2, 4, 7, 8\\\\}$.\\n\\n![Figure 2. A SYT of shape $\\\\lambda = (4, 2, 2, 1)$.](image)\\n\\nThe entry in row $i$ and column $j$ of a tableau $T \\\\in$ SYT($\\\\lambda$) is denoted as $T_{i,j}$. In addition, we define $\\\\text{row}_i(T) := \\\\{T_{i,j} \\\\mid 1 \\\\leq j \\\\leq \\\\lambda_i\\\\}$ as the set of entries in the $i$-th row of $T$. For example, if we consider the SYT shown in Figure 2, then $T_{3,2} = 8$ and $\\\\text{row}_3(T) = \\\\{5, 8\\\\}$.\\n\\n**Theorem 2.4** (Gessel [11]). For every $\\\\lambda \\\\vdash N$, the set SYT($\\\\lambda$) is Schur-positive with respect to Des. Moreover, $Q(\\\\text{SYT}(\\\\lambda)) = s_\\\\lambda$.\\n\\nIn 2015, Adin and Roichman proved the following criterion.\\n\\n**Theorem 2.5** ([2, Prop. 9.1]). A set $\\\\mathcal{A}$ is symmetric with respect to $D : S \\\\to 2^{[N-1]}$ if and only if\\n\\n$$\\\\sum_{a \\\\in \\\\mathcal{A}} t^{D(a)} = \\\\sum_{\\\\lambda \\\\vdash N} c_\\\\lambda \\\\sum_{T \\\\in \\\\text{SYT}(\\\\lambda)} t^{\\\\text{Des}(T)}$$\\n\\nfor some values $c_\\\\lambda$, where $t^J := \\\\prod_{j \\\\in J} t_j$ for $J \\\\subseteq [N-1]$. The coefficients $c_\\\\lambda$ are the Schur-coefficients of $\\\\mathcal{A}$. Moreover, $\\\\mathcal{A}$ is Schur-positive if and only if $c_\\\\lambda \\\\in \\\\mathbb{N}_0$ for all $\\\\lambda \\\\vdash N$.\\n\\nThis criterion implies that proving the Schur-positivity of a set is achievable by establishing a statistic-preserving bijection between the set and SYTs of shapes corresponding to a specific multiset.\\n\\nIn this paper, we will also apply a recently formulated criterion for symmetry [19].\\n\\n**Definition 2.6.** Let $\\\\mathcal{A}$ be a finite set with a statistic $D : \\\\mathcal{A} \\\\to 2^{[N-1]}$. The set of elements that respect a given composition $\\\\alpha \\\\vdash N$, denoted $\\\\mathcal{A}_D(\\\\alpha)$, consists of the elements $a \\\\in \\\\mathcal{A}$ such that $D(a) \\\\subseteq S_\\\\alpha$, where $S_\\\\alpha$ is the set corresponding to the composition $\\\\alpha$. When $D$ is clear from the context, we may write $\\\\mathcal{A}(\\\\alpha)$ instead.\\n\\n**Lemma 2.7.** A set $\\\\mathcal{A}$ is symmetric if and only if $|\\\\mathcal{A}(\\\\alpha)| = |\\\\mathcal{A}(\\\\beta)|$ for all $\\\\alpha \\\\sim \\\\beta \\\\vdash N$.\\n\\nNote that only sets of permutations are considered in [19]. However, Lemma 2.7 applies to other sets as well.\\n\\nAnother useful result about symmetric sets and symmetric functions is due to Bloom and Sagan:\\n\\n**Lemma 2.8** (Bloom and Sagan [6, Lemma 2.2]). For every set $S \\\\subseteq [N-1]$, the function $F_S$ is symmetric if and only if $S = [N-1]$ or $S = \\\\emptyset$. \"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.2. Symmetric and Schur SCHUR-POSITIVITY OF SHORT CHORDS IN MATCHINGS\\n\\nAs mentioned in Section 1 Symmetric and Schur-positive sets\\n\\nAs mentioned in Section 1, a set \\\\( \\\\mathcal{A} \\\\) is symmetric with respect to a statistic function \\\\( D : \\\\mathcal{A} \\\\to 2^{[N-1]} \\\\) if its generating function \\\\( Q_{N,D}(\\\\mathcal{A}) \\\\) is a symmetric function. Moreover, it is Schur-positive if all Schur coefficients are nonnegative integers. One of the fundamental constructions of Schur-positive sets, regarding sets of standard Young tableaux (SYT), is due to Gessel [11]. Let SYT(\\\\( \\\\lambda \\\\)) denote the set of standard Young tableaux of shape \\\\( \\\\lambda \\\\). We draw tableaux in English notation, as in Figure 2. The descent set of the SYT in Figure 2 is \\\\( \\\\{2, 4, 7, 8\\\\} \\\\).\\n\\nFor example, the descent set of the SYT in Figure 2 is \\\\( \\\\{2, 4, 7, 8\\\\} \\\\).\\n\\n\\\\[\\n\\\\begin{array}{cccc}\\n1 & 2 & 4 & 7 \\\\\\\\\\n3 & 6 & & \\\\\\\\\\n5 & 8 & & \\\\\\\\\\n9 & & & \\\\\\\\\\n\\\\end{array}\\n\\\\]\\n\\nFigure 2. A SYT of shape \\\\( \\\\lambda = (4, 2, 2, 1) \\\\).\\n\\nThe entry in row \\\\( i \\\\) and column \\\\( j \\\\) of a tableau \\\\( T \\\\in \\\\text{SYT}(\\\\lambda) \\\\) is denoted as \\\\( T_{i,j} \\\\). In addition, we define \\\\( \\\\text{row}_i(T) := \\\\{T_{i,j} \\\\mid 1 \\\\leq j \\\\leq \\\\lambda_i\\\\} \\\\) as the set of entries in the \\\\( i \\\\)-th row of \\\\( T \\\\). For example, if we consider the SYT shown in Figure 2, then \\\\( T_{3,2} = 8 \\\\) and \\\\( \\\\text{row}_3(T) = \\\\{5, 8\\\\} \\\\).\\n\\n**Theorem 2.4** (Gessel [11]) For every \\\\( \\\\lambda \\\\vdash N \\\\), the set \\\\( \\\\text{SYT}(\\\\lambda) \\\\) is Schur-positive with respect to \\\\( \\\\text{Des} \\\\). Moreover, \\\\( Q(\\\\text{SYT}(\\\\lambda)) = s_\\\\lambda \\\\).\\n\\nIn 2015, Adin and Roichman proved the following criterion. **Theorem 2.5** ([2, Prop. 9.1]) A set \\\\( \\\\mathcal{A} \\\\) is symmetric with respect to \\\\( D : S \\\\to 2^{[N-1]} \\\\) if and only if\\n\\n\\\\[\\n\\\\sum_{a \\\\in \\\\mathcal{A}} t^{D(a)} = \\\\sum_{\\\\lambda \\\\vdash N} c_\\\\lambda \\\\sum_{T \\\\in \\\\text{SYT}(\\\\lambda)} t^{\\\\text{Des}(T)}\\n\\\\]\\n\\nfor some values \\\\( c_\\\\lambda \\\\), where \\\\( t^J := \\\\prod_{j \\\\in J} t_j \\\\) for \\\\( J \\\\subseteq [N-1] \\\\). The coefficients \\\\( c_\\\\lambda \\\\) are the Schur-positivity of a set is achievable by establishing a statistic-preserving bijection between the set and SYTs of shapes corresponding to a specific multiset. In this paper, we will also apply a recently formulated criterion for symmetry [19].\\n\\n**Definition 2.6.** Let \\\\( \\\\mathcal{A} \\\\) be a finite set with a statistic \\\\( D : \\\\mathcal{A} \\\\to 2^{[N-1]} \\\\). The set of elements that respect a given composition \\\\( \\\\alpha \\\\vdash N \\\\), denoted \\\\( \\\\mathcal{A}_D(\\\\alpha) \\\\), consists of the elements \\\\( a \\\\in \\\\mathcal{A} \\\\) such that \\\\( D(a) \\\\subseteq S_\\\\alpha \\\\), where \\\\( S_\\\\alpha \\\\) is the set corresponding to the composition \\\\( \\\\alpha \\\\). When \\\\( D \\\\) is clear from the context, we may write \\\\( \\\\mathcal{A}(\\\\alpha) \\\\) instead.\\n\\n**Lemma 2.7.** A set \\\\( \\\\mathcal{A} \\\\) is symmetric if and only if \\\\( |\\\\mathcal{A}(\\\\alpha)| = |\\\\mathcal{A}(\\\\beta)| \\\\) for all \\\\( \\\\alpha \\\\sim \\\\beta \\\\vdash N \\\\).\\n\\nNote that only sets of permutations are considered in [19]. However, Lemma 2.7 applies to other sets as well. Another useful result about symmetric sets and symmetric functions is due to Bloom and Sagan: **Lemma 2.2])** For every set \\\\( S \\\\subseteq [N-1] \\\\), the function \\\\( F_S \\\\) is symmetric if and only if \\\\( S = [N-1] \\\\) or \\\\( S = \\\\emptyset \\\\).\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where $\\\\delta_{nm}$ is the Kronecker delta symbol. Such relation leads to the definition of the Laguerre transform of order $\\\\nu$:\\n\\n$$T^\\\\nu[f(x)] = \\\\left\\\\{ \\\\int_0^\\\\infty e^{-x} x^\\\\nu L_n^\\\\nu(x) f(x) dx \\\\right\\\\} = \\\\{c_n^\\\\nu\\\\},$$  \\\\hspace{1cm} (11)\\n\\nit must be emphasize that the Laguerre transform is a sequence of numbers in $\\\\mathbb{C}$. The inverse Laguerre transform is defined by\\n\\n$$f(x) = (T^\\\\nu)^{-1}[\\\\{c_n^\\\\nu\\\\}] = \\\\sum_{k=0}^\\\\infty c_n^\\\\nu L_n^\\\\nu(x).$$  \\\\hspace{1cm} (12)\\n\\n### 3 Examples of applications\\n\\n#### 3.1 Applications to the Schr\u00f6dinger equation\\n\\nIn this section is consider the equation\\n\\n$$-\\\\frac{1}{2} \\\\frac{d^2}{dr^2} \\\\psi(r) - (V(r) + E) \\\\psi(r) = 0$$  \\\\hspace{1cm} (13)\\n\\nwhich in appropriate units ($\\\\hbar=M=1$) is the steady state Schr\u00f6dinger equation defined in a one dimensional space, where $V(r)$ is a potential function and $E$ is the energy. Under the change of coordinates (see [0]), given by $\\\\lambda^{-1} \\\\xi(x) = dx/dr$, where $\\\\lambda \\\\geq 0$ has inverse length units, equation (13) becomes\\n\\n$$\\\\lambda^2 \\\\xi^2 \\\\left[ \\\\frac{d^2}{dx^2} \\\\psi(x) + \\\\frac{1}{\\\\xi} \\\\frac{d}{dx} \\\\psi(x) - \\\\frac{2}{\\\\lambda^2 \\\\xi^2} W(x) \\\\psi(x) \\\\right] = 0,$$  \\\\hspace{1cm} (14)\\n\\nwhere $W(x) = V(r) - E$. To obtain a Laguerre-type equation the change of coordinates must satisfy $x(r) \\\\geq 0$ and setting $\\\\frac{1}{\\\\xi} \\\\frac{d}{dx} = \\\\frac{a}{x}$, leads to $\\\\xi(x) = x^a e^{bx}$.\\n\\nIn this way, equation (14) becomes\\n\\n$$\\\\lambda^2 \\\\xi^2 \\\\left[ \\\\frac{d^2}{dx^2} \\\\psi(x) + \\\\left( \\\\frac{a}{x} + b \\\\right) \\\\frac{d}{dx} \\\\psi(x) + \\\\left( A_+ + \\\\frac{A_-}{x^2} - \\\\frac{A_0}{x} \\\\right) \\\\psi(x) \\\\right] = 0,$$  \\\\hspace{1cm} (15)\\n\\nwhere $A_\\\\pm, A_0, a, b$ are real parameters determined in terms of $V(r)$ and $E$.\\n\\nTo solve equation (15) it is proposed a solution of the form\\n\\n$$\\\\psi(x) = x^\\\\alpha e^{-\\\\beta x} y(x),$$  \\\\hspace{1cm} (16)\\n\\nwhere $y = \\\\sum_{k=0}^\\\\infty c_k L_k^\\\\nu(x)$, and $L_n^\\\\nu(x)$ are the Laguerre polynomials of order $\\\\nu$, and $\\\\alpha, \\\\beta, \\\\nu$ are dimensionless parameters, free for the moment, but to be determined according to the concrete examples to be solved below.\\n\\nTo solve (15) the use of the finite Laguerre transform is introduced. Many of the following transforms are known [0] or are obtained by direct calculation by using Laguerre polynomial properties found in [0] or in [0].\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where $\\\\delta_{nm}$ is the Kronecker delta symbol. Such relation leads to the definition of the Laguerre transform of order $\\\\nu$:\\n\\n$$T^\\\\nu[f(x)] = \\\\left\\\\{ \\\\int_0^\\\\infty e^{-x} x^\\\\nu L_n^\\\\nu(x) f(x) dx \\\\right\\\\} = \\\\{c_n^\\\\nu\\\\},$$  \\\\hspace{1cm} (11)\\n\\nit must be emphasize that the Laguerre transform is a sequence of numbers in $\\\\mathbb{C}$. The inverse Laguerre transform is defined by\\n\\n$$f(x) = (T^\\\\nu)^{-1}[\\\\{c_n^\\\\nu\\\\}] = \\\\sum_{k=0}^\\\\infty c_n^\\\\nu L_n^\\\\nu(x).$$  \\\\hspace{1cm} (12)\\n\\n### 3 Examples of applications\\n\\n#### 3.1 Applications to the Schr\u00a8odinger equation\\n\\nIn this section is consider the equation\\n\\n$$-\\\\frac{1}{2} \\\\frac{d^2}{dr^2} \\\\psi(r) - (V(r) + E) \\\\psi(r) = 0$$  \\\\hspace{1cm} (13)\\n\\nwhich in appropriate units ($\\\\hbar=M=1$) is the steady state Sch\u00a8odinger equation defined in a one dimensional space, where $V(r)$ is a potential function and $E$ is the energy. Under the change of coordinates (see [0]), given by $\\\\lambda^{-1} \\\\xi(x) = dx/dr$, where $\\\\lambda \\\\geq 0$ has inverse length units, equation (13) becomes\\n\\n$$\\\\lambda^2 \\\\xi^2 \\\\left[ \\\\frac{d^2}{dx^2} \\\\psi(x) + \\\\frac{1}{\\\\xi} \\\\frac{d}{dx} \\\\psi(x) - \\\\frac{2}{\\\\lambda^2 \\\\xi^2} W(x) \\\\psi(x) \\\\right] = 0,$$  \\\\hspace{1cm} (14)\\n\\nwhere $W(x) = V(r) - E$. To obtain a Laguerre-type equation the change of coordinates must satisfy $x(r) \\\\geq 0$ and setting $\\\\frac{1}{\\\\xi} \\\\frac{d}{dx} = \\\\frac{a}{x}$, leads to $\\\\xi(x) = x^a e^{bx}$.\\n\\nIn this way, equation (14) becomes\\n\\n$$\\\\lambda^2 \\\\xi^2 \\\\left[ \\\\frac{d^2}{dx^2} \\\\psi(x) + \\\\left( \\\\frac{a}{x} + b \\\\right) \\\\frac{d}{dx} \\\\psi(x) + \\\\left( A_+ + \\\\frac{A_-}{x^2} - \\\\frac{A_0}{x} \\\\right) \\\\psi(x) \\\\right] = 0,$$  \\\\hspace{1cm} (15)\\n\\nwhere $A_\\\\pm, A_0, a, b$ are real parameters determined in terms of $V(r)$ and $E$.\\n\\nTo solve equation (15) it is proposed a solution of the form\\n\\n$$\\\\psi(x) = x^\\\\alpha e^{-\\\\beta x} y(x),$$  \\\\hspace{1cm} (16)\\n\\nwhere $y = \\\\sum_{k=0}^\\\\infty c_k L_k^\\\\nu(x)$, and $L_n^\\\\nu(x)$ are the Laguerre polynomials of order $\\\\nu$, and $\\\\alpha, \\\\beta, \\\\nu$ are dimensionless parameters, free for the moment, but to be determined according to the concrete examples to be solved below. To solve (15) the use of the finite Laguerre transform is introduced. Many of the following transforms are known [0] or are obtained by direct calculation by using Laguerre polynomial properties found in [0] or in [0].\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Andrei Teimurazov\\\\textsuperscript{1}, Matthew McCormack\\\\textsuperscript{2,*}, Moritz Linkmann\\\\textsuperscript{2,\u2020} and Olga Shishkina\\\\textsuperscript{1,\u2021}\\n\\n\\\\textsuperscript{1}Max Planck Institute for Dynamics and Self-Organization, 37077 G\\\\textsuperscript{\\\\textregistered}ttingen, Germany\\n\\\\textsuperscript{2}School of Mathematics and Maxwell Institute for Mathematical Sciences, University of Edinburgh, UK\\n\\nAugust 3, 2023\\n\\nAbstract\\n\\nIn magnetoconvection, the flow of electromagnetically conductive fluid is driven by a combination of buoyancy forces, which create the fluid motion due to thermal expansion and contraction, and Lorentz forces, which distort the convective flow structure in the presence of a magnetic field. The differences in the global flow structures in the buoyancy-dominated and Lorentz-force-dominated regimes lead to different heat transport properties in these regimes, reflected in distinct dimensionless scaling relations of the global heat flux (Nusselt number $\\\\text{Nu}$) versus the strength of buoyancy (Rayleigh number $\\\\text{Ra}$) and electromagnetic forces (Hartmann number $\\\\text{Ha}$). Here, we propose a theoretical model for the transition between these two regimes for the case of a quasistatic vertical magnetic field applied to a convective fluid layer confined between two isothermal, a lower warmer and an upper colder, horizontal surfaces. The model suggests that the scaling exponents $\\\\gamma$ in the buoyancy-dominated regime, $\\\\text{Nu} \\\\sim \\\\text{Ra}^\\\\gamma$, and $\\\\xi$ in the Lorentz-force-dominated regime, $\\\\text{Nu} \\\\sim (\\\\text{Ha}^{-2}\\\\text{Ra})^\\\\xi$, are related as $\\\\xi = \\\\gamma/(1 - 2\\\\gamma)$, and the onset of the transition scales with $\\\\text{Ha}^{-1/\\\\gamma}\\\\text{Ra}$. These theoretical results are supported by our Direct Numerical Simulations for $10 \\\\leq \\\\text{Ha} \\\\leq 2000$, Prandtl number $\\\\text{Pr} = 0.025$ and $\\\\text{Ra}$ up to $10^9$ and data from the literature.\\n\\n1 Introduction\\n\\nMagnetoco\\\\textsuperscript{2}nvection (MC) governs most astro- and geophysical systems and is relevant to various engineering applications [25, 7]. The former include, for instance, outer layers of stars and liquid metal planetary cores [12], examples of the latter comprise liquid-metal batteries, induction heating, casting, liquid-metal cooling for nuclear fusion reactors and semiconductor crystal growth [6]. MC occurs in an electrically conducting fluid that is subjected both to a magnetic field and an imposed temperature gradient. The buoyancy forces induce convective fluid motion due to thermal expansion and contraction, while the magnetic field affects this motion and distorts the global flow structure through the Lorentz force, which eventually influences the heat transport in the system. The resulting main two control parameters, the strength of the imposed thermal driving and that of the external magnetic field, are encoded in independent dimensionless groups, the Rayleigh number $\\\\text{Ra}$ and Hartmann number $\\\\text{Ha}$, respectively.\\n\\nOne of the key objectives in MC research is to provide scaling relations for the heat transport through the system, represented in dimensionless form by the Nusselt number $\\\\text{Nu}$, as a function of $\\\\text{Ra}$ and $\\\\text{Ha}$. However, the heat transport scaling relations also depend on the flow configuration, including the angle between the magnetic field and gravity, the geometry of the container and the boundary conditions (BCs), and on whether the buoyancy forces dominate over the Lorentz forces in the system or vice versa. This inherent complexity results in the need, at least in principle, to derive separate heat transport scaling relations to describe each specific flow regime itself and transitions between distinct regimes. The considerable difficulty of doing so in a coherent manner is exacerbated by non-universal scaling relations even within specific regimes \u2013 the scaling relations in the buoyancy-dominated and Lorentz-force-dominated regimes themselves change with the control parameters, and transitions between the different regimes are also non-universal.\\n\\nThe objective of this paper is to offer a unifying heat transport model for the transition between the buoyancy-dominated and Lorentz-force-dominated regimes in quasistatic MC. We focus on Rayleigh\u2013B\u00e9nard\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Andrei Teimurazov\\\\textsuperscript{1}, Matthew McCormack\\\\textsuperscript{2,*}, Moritz Linkmann 2,\\\\textsuperscript{\u2020} and Olga Shishkina A. Teimurazov and M. McCormack contributed equally. 1,\\\\textsuperscript{\u2021}\\n\\n\\\\textsuperscript{1}Max Planck Institute for Dynamics and Self-Organization, 37077 G\u02dcA\\\\textsuperscript{ttingen, Germany}\\n\\\\textsuperscript{2}School of Mathematics and Maxwell Institute for Mathematical Sciences, University of Edinburgh, UK\\n\\nAugust 3, 2023\\n\\nAbstract\\n\\nIn magnetoconvection, the flow of electromagnetically conductive fluid is driven by a combination of \u2021 olga.shishkina@ds.mpg.de buoyancy forces, which create the fluid motion due to thermal expansion and contraction, and Lorentz forces, which distort the convective flow structure in the presence of a magnetic field. The differences in the global flow structures in the buoyancy-dominated and Lorentz-force-dominated regimes lead to different heat transport properties in these regimes, reflected in distinct dimensionless scaling relations of the global heat flux (Nusselt number Nu) versus the strength of buoyancy (Rayleigh number Ra) and electromagnetic forces (Hartmann number Ha). Here, we propose a theoretical model for the transition between these two regimes for the case of a quasistatic vertical magnetic field applied to a convective fluid layer confined between two isothermal, a lower warmer and an upper colder, horizontal surfaces. The model suggests that the scaling exponents $\\\\gamma$ in the buoyancy-dominated regime, $Nu \\\\sim Ra^\\\\gamma$, and $\\\\xi$ in the Lorentz-force-dominated regime, $Nu \\\\sim (Ha^{-2}Ra)^\\\\xi$, are related as $\\\\xi = \\\\gamma/(1 - 2\\\\gamma)$, and the onset of the transition scales with $Ha^{-1/\\\\gamma}Ra$. These theoretical results are supported by our Direct Numerical Simulations for $10 \\\\leq Ha \\\\leq 2000$, Prandtl number $Pr = 0.025$ and $Ra$ up to $10^9$ and data from the literature. \\n\\n1 Introduction\\n\\nMagnetoco\\\\textsuperscript{1}vection (MC) governs most astroand geophysical systems and is relevant to various engineering applications [25, 7]. The former include, for instance, outer layers of stars and liquid metal planetary cores [12], examples of the latter comprise liquid-metal batteries, induction heating, casting, liquid-metal cooling for nuclear fusion reactors and semiconductor crystal growth [6]. MC occurs in an electrically conducting fluid that is subjected both to a magnetic field and an imposed temperature gradient. The buoyancy forces induce convective fluid motion due to thermal expansion and contraction, while the magnetic field affects this motion and distorts the global flow structure through the Lorentz force, which eventually influences the heat transport in the system. The resulting main two control parameters, the strength of the imposed thermal driving and that of the external magnetic field, are encoded in independent dimensionless groups, the Rayleigh number $Ra$ and Hartmann number $Ha$, respectively.\\n\\nOne of the key objectives in MC research is to provide scaling relations for the heat transport through the system, represented in dimensionless form by the Nusselt number $Nu$, as a function of $Ra$ and $Ha$. However, the heat transport scaling relations also depend on the flow configuration, including the angle between the magnetic field and gravity, the geometry of the container and the boundary conditions (BCs), and on whether the buoyancy forces dominate over the Lorentz forces in the system or vice versa. This inherent complexity results in the need, at least in principle, to derive separate heat transport scaling relations to describe each specific flow regime itself and transitions between distinct regimes. The considerable difficulty of doing so in a coherent manner is exacerbated by non-universal scaling relations even within specific regimes \u2013 the scaling relations in the buoyancy-dominated and Lorentz-force-dominated regimes themselves change with the control parameters, and transitions between the different regimes are also non-universal. The objective of this paper is to offer a unifying heat transport model for the transition between the buoyancy-dominated and Lorentz-force-dominated regimes in quasistatic MC. We focus on Rayleigh\u2013B\u00b4enard\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B. Compared Methods\\n\\nThe experiment includes a comparison of different models:\\n\\n- **I) MHA-LSTM [4]**: This model only takes as inputs the past trajectories of the agents in the scene and outputs \\\\( L \\\\) trajectories with their associated probabilities (see the architecture in the red rectangle in Fig. 1). We use \\\\( L = 6 \\\\) attention heads.\\n\\n- **II) G-MHA-LSTM [17]**: We add to the previous model a radial grid representation from which we extract potential goals. We predict the goal and then the trajectories conditioned on the predicted goal. (see the architecture in the orange rectangle in Fig. 1).\\n\\n- **III) DCM-MHA-LSTM**: To predict the goal of the target agent, we combine the DCM and the neural network using the LMNL framework [15]. This model is described in Section III and the architecture is illustrated in the blue rectangle in Fig. 1.\\n\\n- **IV) ODCM-MHA-LSTM**: This model only uses the DCM to predict the goal of the target agent.\\n\\n**Goal set representations**: We also compare different types of radial grids. For the methods II), III) and IV), we compare our results for two types of radial grid: a **dynamic** grid (d) and a **fixed** one (f). Similar to [12], we build the dynamic grid by considering the target agent\u2019s current velocity \\\\( v_{t_{obs}} \\\\). If \\\\( v_{t_{obs}} = 0 \\\\), we replace it with an arbitrary value equals to 0.5 m.s\\\\(^{-1}\\\\). The fixed grid is built using the value \\\\( v = 5.83 m.s^{-1} \\\\), which corresponds to the mean of the velocities in the INTERACTION training set.\\n\\nC. Compared DCMs\\n\\nWe compare two types of DCMs for modelling the behavior of vehicle motion. For our case, the functions modelling vehicle motion phenomenon which we consider for goal selection in this work are:\\n\\n1) **occupancy**: directions containing neighbours in the vicinity are less desirable.\\n\\n2) **keep direction**: vehicles tend to maintain the same direction of motion.\\n\\n3) **collision avoidance**: when a neighbour vehicle\u2019s trajectory is head-on towards a potential goal, this goal becomes less desirable due to the chance of a collision.\\n\\n- **1) DCM 1**: For the first DCM configuration, we use a utility function defined as:\\n\\n\\\\[\\n    u_k(X) = \\\\beta_{dir} \\\\cdot dir_k + \\\\beta_{occ} \\\\cdot occ_k + \\\\beta_{col} \\\\cdot col_k\\n\\\\]\\n\\nWhere the functions \\\\( dir_k \\\\), \\\\( occ_k \\\\), and \\\\( col_k \\\\) correspond respectively to keep direction, occupancy and collision avoidance. These functions are defined in [2] and [6].\\n\\n- **2) DCM 2**: For the second DCM, the utility function is defined as:\\n\\n\\\\[\\n    u_k(X) = \\\\beta_{dir} \\\\cdot dir_k + \\\\beta_{occ} \\\\cdot occ_k\\n\\\\]\\n\\nWhere the function \\\\( dir_k \\\\) is the same as in (IV-C). For \\\\( occ_k \\\\), we use the same mathematical formula as the occupancy function in (IV-C), however, we don\u2019t consider the position of the neighbors at time \\\\( t_{obs} \\\\). Instead, we consider their predicted position at time \\\\( t_{obs} + t_f \\\\) using a Constant velocity model. We assume that before predicting his goal, the target agent first predicts the future positions of his surroundings according to their headings and current velocities, and then avoids the zones that are expected to be crowded. While training this model, we calculate the \\\\( occ_{k} \\\\) function using the growth truth positions of the neighbors.\\n\\nD. Implementation details\\n\\nWe use \\\\( K = 15 \\\\) number of potential goals. Similar to [8], our interaction space is 40 m ahead of the target vehicle, 10 m behind and 25 m on each side. We consider the neighbors situated in the interaction space at \\\\( t_{obs} \\\\). We also take into account the neighbors that are susceptible of being in this space from time \\\\( t_{obs} \\\\) to \\\\( t_f \\\\). To do so, we predict the trajectories of all of the neighbors in the scene using a Constant Velocity model and if they have a predicted position in the interaction space, we consider them in our model. We argue that this representation allows to consider neighbors that are not situated in the grid at \\\\( t_{obs} \\\\) but that can appear in the grid from time \\\\( t = t_{obs} + 1 \\\\) to \\\\( t = t_f \\\\). without having to create a bigger interaction space which can be more computationally expensive. We use \\\\( L + K = 6 + 15 \\\\) parallel attention operations. We use a batch size of 64 and Adam optimizer. The model is implemented using PyTorch [18].\\n\\nV. RESULTS\\n\\nA. Evaluation metrics\\n\\nOur method for trajectory forecasting is evaluated with the following three error metrics:\\n\\n- **Minimum Average Displacement Error over k** (\\\\( \\\\text{minADE}_k \\\\)): The average of pointwise L2 distances between the predicted trajectory and ground truth over the k most likely predictions.\\n\\n- **Minimum Final Displacement Error over k** (\\\\( \\\\text{minFDE}_k \\\\)): The final displacement error (FDE) is the L2 distance between the final points of the prediction and ground truth. We take the minimum FDE over the k most likely predictions and average over all agents.\\n\\n- **Collision II - Groundtruth collision (Col-II)** [19]: This metric calculates the percentage of collision between the primary vehicle\u2019s prediction and the neighbors in the groundtruth future scene.\\n\\nB. Comparison of Methods\\n\\nWe compare the methods described in Section IV-B. The results are reported in Table I. DCM\\\\(^1\\\\) and DCM\\\\(^2\\\\) refers to the first (resp the second) type of DCM described in IV-C. (f) and (d) correspond to respectively, the fixed and the dynamic radial grid representation for the extraction of potential goals. We can see that adding the DCM module decrease the percentage of collisions. We can see that the models using a fixed grid perform slightly better than when\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B. Compared Methods\\n\\nThe experiment includes a comparison of different models: \\n\\n- **I) MHA-LSTM [4]:** This model only takes as inputs the past trajectories of the agents in the scene and outputs \\\\( L \\\\) trajectories with their associated probabilities (see the architecture in the red rectangle in Fig. 1). We use \\\\( L = 6 \\\\) attention heads.\\n\\n- **II) G-MHA-LSTM [17]:** We add to the previous model a radial grid representation from which we extract potential goals. We predict the goal and then the trajectories conditioned on the predicted goal. (see the architecture in the orange rectangle in Fig. 1). \\n\\n- **III) DCM-MHA-LSTM :** To predict the goal of the target agent, we combine the DCM and the neural network using the LMNL framework [15]. This model is described in Section III and the architecture is illustrated in the blue rectangle in Fig. 1. \\n\\n- **IV) ODCM-MHA-LSTM :** This model only uses the DCM to predict the goal of the target agent. \\n\\n**Goal set representations :** We also compare different types of radial grids. For the methods II), III) and IV), we compare our results for two types of radial grid : a **dynamic** grid (d) and a **fixed** one (f). Similar to [12], we build the dynamic grid by considering the target agent\u2019s current velocity \\\\( v_{T}^{obs} \\\\). If \\\\( v_{T}^{obs} = 0 \\\\), we replace it with an arbitrary value equals to 0.5 \\\\( m.s^{-1} \\\\). The fixed grid is built using the value \\\\( v = 5.83 m.s^{-1} \\\\), which corresponds to the mean of the velocities in the INTERACTION training set. \\n\\nC. Compared DCMs\\n\\nWe compare two types of DCMs for modelling the behavior of vehicle motion. For our case, the functions modelling vehicle motion phenomenon which we consider for goal selection in this work are: \\n\\n1) **occupancy:** directions containing neighbours in the vicinity are less desirable. \\n2) **keep direction:** vehicles tend to maintain the same direction of motion. \\n3) **collision avoidance:** when a neighbour vehicle\u2019s trajectory is head-on towards a potential goal, this goal becomes less desirable due to the chance of a collision. \\n\\n- **1) DCM 1 :** For the first DCM configuration, we use a utility function defined as: \\n\\n\\\\[\\n u_k(X) = \\\\beta_{dir} \\\\cdot dir_k + \\\\beta_{occ} \\\\cdot occ_k + \\\\beta_{col} \\\\cdot col_k \\n\\\\]  \\n\\n(13)\\n\\nWhere the functions \\\\( dir_k \\\\), \\\\( occ_k \\\\), and \\\\( col_k \\\\) correspond respectively to keep direction, occupancy and collision avoidance. These functions are defined in [2] and [6]. \\n\\n- **2) DCM 2 :** For the second DCM, the utility function is defined as : \\n\\n\\\\[\\n u_k(X) = \\\\beta_{dir} \\\\cdot dir_k + \\\\beta_{occ} \\\\cdot occ_k \\n\\\\]  \\n\\n(14)\\n\\nWhere the function \\\\( dir_k \\\\) is the same as in (IV-C). For \\\\( occ_k \\\\), we use the same mathematical formula as the occupancy function in (IV-C), however, we don\u2019t consider the position of the neighbors at time \\\\( t_{obs} \\\\). Instead, we consider their predicted position at time \\\\( t_{obs} + t_f \\\\) using a Constant velocity model. We assume that before predicting his goal, the target agent first predicts the future positions of his surroundings according to their headings and current velocitites, and then avoids the zones that are expected to be crowded. While training this model, we calculate the \\\\( occ_k \\\\) function using the grouth truth positions of the neighbors. \\n\\nD. Implementation details\\n\\nWe use \\\\( K = 15 \\\\) number of potential goals. Similar to [8], our interaction space is 40 m ahead of the target vehicle, 10 m behind and 25 m on each side. We consider the neighbors situated in the interaction space at \\\\( t_{obs} \\\\). We also take into account the neighbors that are susceptible of being in this space from time \\\\( t_{obs} \\\\) to \\\\( t_f \\\\). To do so, we predict the trajectories of all of the neighbors in the scene using a Constant Velocity model and if they have a predicted position in the interaction space, we consider them in our model. We argue that this representation allows to consider neighbors that are not situated in the grid at \\\\( t_{obs} \\\\) but that can appear in the grid from time \\\\( t = t_{obs} + 1 \\\\) to \\\\( t = t_f \\\\). without having to create a bigger interaction space which can be more computationally expensive. We use \\\\( L + K = 6 + 15 \\\\) parallel attention operations. We use a batch size of 64 and Adam optimizer. The model is implemented using PyTorch [18].\\n\\nV. RESULTS\\n\\nA. Evaluation metrics\\n\\nOur method for trajectory forecasting is evaluated with the following three error metrics: \\n\\n- **Minimum Average Displacement Error over k (minADE_k) :** The average of pointwise L2 distances between the predicted trajectory and ground truth over the k most likely predictions. \\n\\n- **Minimum Final Displacement Error over k (minFDE_k) :** The final displacement error (FDE) is the L2 distance between the final points of the prediction and ground truth. We take the minimum FDE over the k most likely predictions and average over all agents. \\n\\n- **Collision II - Groundtruth collision (Col-II) [19]:** This metric calculates the percentage of collision between the primary vehicle\u2019s prediction and the neighbors in the groundtruth future scene. \\n\\nB. Comparison of Methods\\n\\nWe compare the methods described in Section IV-B. The results are reported in Table I. DCM\\\\(^1\\\\) and DCM\\\\(^2\\\\) refers to the first (resp the second) type of DCM described in IV-C. (f) and (d) correspond to respectively, the fixed and the dynamic radial grid representation for the extraction of potential goals. We can see that adding the DCM module decrease the percentage of collisions. We can see that the models using a fixed grid perform slightly better than when\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"that local interactions play a subordinate role and the hard-core constraint is largely inactive. As a result, the qualitative behavior of the system is dominated by the hopping processes, which naturally result in the formation of a Fermi surface similar to the one in graphene at high hole doping. The hard-core nature of the fermions and their density-density interactions then enter as corrections to this pure Fermi-gas behavior. By measuring the lattice Green\u2019s function $G_{\\\\alpha,j}^{\\\\alpha,\\\\beta} \\\\equiv \\\\langle c_{\\\\beta,j}^\\\\dagger c_{\\\\alpha,i} \\\\rangle$, we obtain the momentum-space occupation number $n(k)$ of the ground state wave function as\\n\\n$$n(k) = \\\\frac{1}{N} \\\\sum_{\\\\alpha=A,B} \\\\sum_{i,j} e^{i\\\\mathbf{k} \\\\cdot (\\\\mathbf{r}_i - \\\\mathbf{r}_j)} G_{\\\\alpha,j}^{\\\\alpha,\\\\alpha}.$$  \\n\\nFor non-interacting fermions on the honeycomb lattice, i.e. without density-density interactions or the hard-core constraint, $n(k)$ should exhibit a clear step (whose magnitude is the quasiparticle residue, $Z$) as a function of the graphene dispersion $\\\\epsilon_G(k)$. Fig. 3(a) clearly displays a step-like behavior for the occupation at lower fillings $\\\\nu \\\\lesssim 0.2$, consistent with the formation of a Fermi liquid. Increasing the fermion density leads to a gradual softening of the quasiparticle residue, so that the Fermi liquid appears to give way to a non-Fermi-liquid phase at $\\\\nu \\\\gtrsim 0.25$.\\n\\nInterestingly, we find that the Fermi liquid regime is interrupted by a charge ordered phase at filling $\\\\nu = 1/6$, which triples the unit cell and spontaneously polarizes into either the $A$ or $B$ sublattice. The expected six-fold ground state degeneracy in the many-body spectrum as well as the clear energetic preference of simulation clusters supporting this form of order strongly points at spontaneous symmetry breaking as the root of this incompressible phase. Additionally, the static structure factor $S_C(q)$ obtained from Fourier transformation of the measured density-density correlation function\\n\\n$$C_i^{\\\\alpha,\\\\beta}(a) \\\\equiv \\\\langle n_i^\\\\alpha n_{i+a}^\\\\beta \\\\rangle,$$\\n\\nshown in Fig. 3(b), is expected to exhibit pronounced peaks. Indeed, as can be seen in Fig. 3(c), the peak at $q = K$ extrapolates to finite values in the TDL while other signals vanish, indicative of long-range order.\\n\\nThese ED results are corroborated by our HF simulations, where the same type of symmetry breaking order prevails at $\\\\nu = 1/6$ and ground state energies per site are relatively close to the ones obtained from ED (cf. Fig. 1 for $\\\\nu \\\\lesssim 1/6$). HF also finds a charge density wave at $\\\\nu = 1/8$ consisting of a fermion delocalized around a honeycomb for every enlarged $2 \\\\times 2$ unit cell. This suggests a potentially more general instability towards charge order above some critical filling in the dilute fermion regime.\\n\\n### III. ZERO-ENERGY STATE WINDOW\\n\\n$1/4 \\\\leq \\\\nu \\\\lesssim 0.292$\\n\\nStarting from $\\\\nu = 1/4$, i.e. one fermion per two unit cells, the ground state of the SUSY model on the honeycomb lattice from Eq. (2) has exactly zero energy for a finite range of fillings. In fact, as illustrated in Fig. 4, we find robust zero-energy states for (almost) all rational fillings $1/4 \\\\leq \\\\nu \\\\lesssim 0.292$ within our finite-size simulations. This filling window extends beyond the homological predictions of Ref. [16].\\n\\nIn contrast to the finite-energy Fermi-liquid and charge ordered phases discussed in Sec. II, due to the supersymmetry\\n\\n---\\n\\n2 For the two rational fillings accessible to us in $\\\\nu \\\\in (0.286, 0.292)$, we suspect zero-energy states emerge for appropriate geometries but were unable to identify them in our study.\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"that local interactions play a subordinate role and the hard-core constraint is largely inactive. As a result, the qualitative behavior of the system is dominated by the hopping processes, which naturally result in the forma- tion of a Fermi surface similar to the one in graphene at high hole doping. The hard-core nature of the fermions and their density-density interactions then enter as corrections to this pure Fermi-gas behavior. By measuring the lattice Green\u2019s function $G_{\\\\alpha,j}^{\\\\alpha,\\\\beta} \\\\equiv \\\\langle c_{\\\\beta,j}^\\\\dagger c_{\\\\alpha,i} \\\\rangle$, we obtain the momentum-space occupation number $n(k)$ of the ground state wave function as\\n\\n$$n(k) = \\\\frac{1}{N} \\\\sum_{\\\\alpha=A,B} \\\\sum_{i,j} e^{i k \\\\cdot (r_i - r_j)} G_{i,j}^{\\\\alpha,\\\\alpha}.$$  \\n\\nFor non-interacting fermions on the honeycomb lattice, i.e. without density-density interactions or the hard-core constraint, $n(k)$ should exhibit a clear step (whose magnitude is the quasiparticle r esidue, $Z$) as a function of the graphene dispersion $\\\\epsilon_G(k)$. Fig. 3(a) clearly displays a step-like behavior for rly di\n    fferent fillings $\\\\nu \\\\lesssim 0.2$, consistent with the formation of a Fermi liquid. Increasing the fermion density leads to a gradual softening of the quasiparticle residue, so that the Fermi liquid regime is expected to give way to a non-Fermi-liquid phase at $\\\\nu \\\\gtrsim 0.25$.\\n\\nInterestingly, we find that the Fermi liquid regime is interrupted by a charge ordered phase at filling \u03bd = / nt 1/6, which triples the unit cell and spontaneously polarizes into either the A or B sublattice. The expected six-fold ground state degeneracy in the many-body spectrum as g\nwell as t he clear energetic preference of simulation clusters supporting this form of order strongly points at spontaneous symmetry breaking as the root of this incompressible phase. Additionally, the static structure factor $S_C(q)$ obtained from Fourier transformation of the measured density-density correlation function\\n\\n$$C_i^{\\\\alpha,\\\\beta}(a) \\\\equiv \\\\langle n_i^\\\\alpha n_{i+a}^\\\\beta \\\\rangle,$$  \\n\\nshown in Fig. (b), is expected to exhibit pronounced peaks. Indeed, as can be seen in Fig. 3 (c), the peak at $q = K$ extrapolates to finite values in the TDL while other signals vanish, indicative of long-range order. These ED results are corroborated by our HF simulations,\n         i.e. one fermion per two unit cells, the ground state of the SUSY model on the honeycomb lattice from Eq. (2) has exactly zero energy for a finite range of fillings. In fact, as illustrated in Fig. 4, we find robust zero-energy states for (almost) all rational fillings $1/4 \\\\leq \\\\nu \\\\lesssim 0.292$ within our finite-size simulations. This filling window extends beyond the homological predictions of Ref. [16].\\n\\nIn contrast to the finite-energy Fermi-liquid and charge ordered phases discussed in Sec. II, due to the supersym-\\n\\nIII. ZERO-ENERGY STATE WINDOW\\n\\n$1/4 \\\\leq \\\\nu \\\\lesssim 0.292$\\n\\nStarting from $\\\\nu = 1/6  d     d  t t           i          it preva\n   l ti  e  s  e  t  a  s  e  s  f  o  r  (a  l  m  o  s  t)  a  l  r  a  t  i  o  n  f  i  l  l  i  n  g s  1/4 \\\\leq \\\\nu \\\\lesssim 0.292$ within our finite-size simulations. This filling window extends beyond the homological predictions of Ref. [16].\\n\\nIn contrast to the finite-energy Fermi-liquid and charge ordered phases discussed in Sec. II, due to the supersym-\\n\\n2 For the two rational fillings accessible to us in $\\\\nu \\\\in (0.286, 0.292)$, we suspect zero-energy states emerge for appropriate geometries but were unable to identify them in our study.\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"University, Thailand. We acknowledge the supporting computing infrastructure provided by NSTDA, CU, CUAASC, NSRF via PMUB [B05F650021, B37G660013] (Thailand). URL:www.e-science.in.th. The Computational Materials Physics (CMP) Project, SLRI, Thailand, is acknowledged for providing computational resource.\\n\\n[1] R. J. Needs and C. J. Pickard, Perspective: Role of structure prediction in materials discovery and design, APL Materials 4, 053210 (2016).\\n[2] W. Kohn and L. J. Sham, Phys. Rev. 140, A1133 (1965).\\n[3] A. R. Oganov and C. W. Glass, Crystal structure prediction using ab initio evolutionary techniques: Principles and applications, The Journal of Chemical Physics 124, 244704 (2006).\\n[4] Y. Wang, J. Lv, L. Zhu, and Y. Ma, Crystal structure prediction via particle-swarm optimization, Phys. Rev. B 82, 094116 (2010).\\n[5] C. J. Pickard and R. J. Needs, Ab initio random structure searching, Journal of Physics: Condensed Matter 23, 053201 (2011).\\n[6] A. R. Oganov, C. J. Pickard, Q. Zhu, and R. J. Needs, Structure prediction drives materials discovery, Nature Reviews Materials 4, 331 (2019).\\n[7] J. C. Sch\u00f6n, K. Doll, and M. Jansen, Predicting solid compounds via global exploration of the energy landscape of solids on the ab initio level without recourse to experimental information, physica status solidi (b) 247, 23 (2010).\\n[8] T. Xie, X. Fu, O.-E. Ganea, R. Barzilay, and T. S. Jaakkola, Crystal diffusion variational autoencoder for periodic material generation, in International Conference on Learning Representations (2022).\\n[9] C. Shi, S. Luo, M. Xu, and J. Tang, Learning gradient fields for molecular conformation generation, in Proceedings of the 38th International Conference on Machine Learning, Proceedings of Machine Learning Research, Vol. 139, edited by M. Meila and T. Zhang (PMLR, 2021) pp. 9558\u20139568.\\n[10] M. Xu, L. Yu, Y. Song, C. Shi, S. Ermon, and J. Tang, Geodiff: A geometric diffusion model for molecular conformation generation, in International Conference on Learning Representations (2022).\\n[11] J. Guan, W. W. Qian, X. Peng, Y. Su, J. Peng, and J. Ma, 3d equivariant diffusion for target-aware molecule generation and affinity prediction, in The Eleventh International Conference on Learning Representations (2023).\\n[12] S. Kang and K. Cho, Conditional molecular design with deep generative models, Journal of Chemical Information and Modeling 59, 43 (2019), pMID: 30016587.\\n[13] J. Lim, S. Ryu, J. W. Kim, and W. Y. Kim, Molecular generative model based on conditional variational autoencoder for de novo molecular design, Journal of Cheminformatics 10, 31 (2018).\\n[14] Y. Song, L. Shen, L. Xing, and S. Ermon, Solving inverse problems in medical imaging with score-based generative models, in International Conference on Learning Representations (2022).\\n[15] A. Cui, K. Jiang, M. Jiang, L. Shang, L. Zhu, Z. Hu, G. Xu, and J. Chu, Decoding phases of matter by machine-learning raman spectroscopy, Phys. Rev. Appl. 12, 054049 (2019).\\n[16] M. R. Carbone, M. Topsakal, D. Lu, and S. Yoo, Machine-learning x-ray absorption spectra to quantitative accuracy, Phys. Rev. Lett. 124, 156401 (2020).\\n[17] Z. Liang, M. R. Carbone, W. Chen, F. Meng, E. Stavitski, D. Lu, M. S. Hybertsen, and X. Qu, Decoding structure-spectrum relationships with physically organized latent spaces, Phys. Rev. Mater. 7, 053802 (2023).\\n[18] Y. Song and S. Ermon, Generative modeling by estimating gradients of the data distribution, in Advances in Neural Information Processing Systems, Vol. 32, edited by H. Wallach, H. Larochelle, A. Beygelzimer, F. d\u2019Alch\u00e9-Buc, E. Fox, and R. Garnett (Curran Associates, Inc., 2019).\\n[19] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole, Score-based generative modeling through stochastic differential equations, in International Conference on Learning Representations (2021).\\n[20] J. Ho, A. Jain, and P. Abbeel, Denoising diffusion probabilistic models, in Advances in Neural Information Processing Systems, Vol. 33, edited by H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin (Curran Associates, Inc., 2020) pp. 6840\u20136851.\\n[21] M. M. Bronstein, J. Bruna, T. Cohen, and P. Velickovic, Geometric deep learning: Grids, groups, graphs, geodesics, and gauges, CoRR abs/2104.13478 (2021), 2104.13478.\\n[22] T. S. Cohen, M. Geiger, J. K\u00f6hler, and M. Welling, Spherical CNNs, in International Conference on Learning Representations (2018).\\n[23] N. Thomas, T. Smidt, S. Kearnes, L. Yang, L. Li, K. Kohlhoff, and P. Riley, Tensor field networks: Rotation- and translation-equivariant neural networks for 3d point clouds (2018).\\n[24] D. P. Kingma and M. Welling, Auto-encoding variational bayes, in International Conference on Learning Representations (2014).\\n[25] R. Jiao, W. Huang, P. Lin, J. Han, P. Chen, Y. Lu, and Y. Liu, Crystal structure prediction by joint equivariant diffusion on lattices and fractional coordinates, in Workshop on \u201cMachine Learning for Materials\u201d ICLR 2023 (2023).\\n[26] A. Okhotin, D. Molchanov, V. Arkhipkin, G. Bar-tosh, A. Alanov, and D. Vetrov, Star-shaped denoising diffusion probabilistic models (2023), arXiv:2302.05259 [stat.ML].\\n[27] J. Gasteiger, S. Giri, J. T. Margraf, and S. G\u00fcnemann, Fast and uncertainty-aware directional message passing for non-equilibrium molecules (2022), arXiv:2011.14115 [cs.LG].\\n[28] W. Hu*, B. Liu*, J. Gomes, M. Zitnik, P. Liang, V. Pande, and J. Leskovec, Strategies for pre-training graph neural networks, in International Conference on Learning Representations (2020).\\n[29] K. Sch\u00fctt, P.-J. Kindermans, H. E. Sauceda Felix, S. Chmiela, A. Tkatchenko, and K.-R. M\u00fcller, Schnet: A continuous-filter convolutional neural network for modeling quantum interactions, in Advances in Neural Infor-\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"University, Thailand. We acknowledge the supporting computing infrastructure provided by NSTDA, CU, CUAASC, NSRF via PMUB [B05F650021, B37G660013] (Thailand). URL:www.e-science.in.th. The Computational Materials Physics (CMP) Project, SLRI, Thailand, is acknowledged for providing computational resource. \\n\\n[1] R. J. Needs and C. J. Pickard, Perspective: Role of structure prediction in materials discovery and design, APL Materials 4, 053210 (2016). [2] W. Kohn and L. J. Sham, Phys. Rev. 140, A1133 (1965). [3] A. R. Oganov and C. W. Glass, Crystal structure prediction using ab initio evolutionary techniques: Principles and applications, The Journal of Chemical Physics 124, 244704 (2006). [4] Y. Wang, J. Lv, L. Zhu, and Y. Ma, Crystal structure prediction via particle-swarm optimization, Phys. Rev. B 82, 094116 (2010). [5] C. J. Pickard and R. J. Needs, Ab initio random structure searching, Journal of Physics: Condensed Matter 23, 053201 (2011). [6] A. R. Oganov, C. J. Pickard, Q. Zhu, and R. J. Needs, Structure prediction drives materials discovery, Nature Reviews Materials 4, 331 (2019). [7] J. C. Sch\u00a8on, K. Doll, and M. Jansen, Predicting solid compounds via global exploration of the energy landscape of solids on the ab initio level without recourse to experimental information, physica status solidi (b) 247, 23 (2010). [8] T. Xie, X. Fu, O.-E. Ganea, R. Barzilay, and T. S. Jaakkola, Crystal diffusion variational autoencoder for periodic material generation, in International Conference on Learning Representations (2022). [9] C. Shi, S. Luo, M. Xu, and J. Tang, Learning gradient fields for molecular conformation generation, in Proceedings of the 38th International Conference on Machine Learning, Proceedings of Machine Learning Research, Vol. 139, edited by M. Meila and T. Zhang (PMLR, 2021) pp. 9558\u20139568. [10] M. Xu, L. Yu, Y. Song, C. Shi, S. Ermon, and J. Tang, Geodiff: A geometric diffusion model for molecular conformation generation, in International Conference on Learning Representations (2022). [11] J. Guan, W. W. Qian, X. Peng, Y. Su, J. Peng, and J. Ma, 3d equivariant diffusion for target-aware molecule generation and affinity prediction, in The Eleventh International Conference on Learning Representations (2023). [12] S. Kang and K. Cho, Conditional molecular design with deep generative models, Journal of Chemical Information and Modeling 59, 43 (2019), pMID: 30016587. [13] J. Lim, S. Ryu, J. W. Kim, and W. Y. Kim, Molecular generative model based on conditional variational autoencoder for de novo molecular design, Journal of Cheminformatics 10, 31 (2018). [14] Y. Song, L. Shen, L. Xing, and S. Ermon, Solving inverse problems in medical imaging with score-based generative models, in International Conference on Learning Repre- sentations (2022). [15] A. Cui, K. Jiang, M. Jiang, L. Shang, L. Zhu, Z. Hu, G. Xu, and J. Chu, Decoding phases of matter by machine-learning raman spectroscopy, Phys. Rev. Appl. 12, 054049 (2019). [16] M. R. Carbone, M. Topsakal, D. Lu, and S. Yoo, Machine-learning x-ray absorption spectra to quantitative accuracy, Phys. Rev. Lett. 124, 156401 (2020). [17] Z. Liang, M. R. Carbone, W. Chen, F. Meng, E. Stavitski, D. Lu, M. S. Hybertsen, and X. Qu, Decoding structure-spectrum relationships with physically organized latent spaces, Phys. Rev. Mater. 7, 053802 (2023). [18] Y. Song and S. Ermon, Generative modeling by estimating gradients of the data distribution, in Advances in Neural Information Processing Systems, Vol. 32, edited by H. Wallach, H. Larochelle, A. Beygelzimer, F. d\u2019Alch\u00e9-Buc, E. Fox, and R. Garnett (Curran Associates, Inc., 2019). [19] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole, Score-based generative modeling through stochastic differential equations, in International Conference on Learning Representations (2021). [20] J. Ho, A. Jain, and P. Abbeel, Denoising diffusion probabilistic models, in Advances in Neural Information Processing Systems, Vol. 33, edited by H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin (Curran Associates, Inc., 2020) pp. 6840\u20136851. [21] M. M. Bronstein, J. Bruna, T. Cohen, and P. Velickovic, Geometric deep learning: Grids, groups, graphs, geodesics, and gauges, CoRR abs/2104.13478 (2021), 2104.13478. [22] T. S. Cohen, M. Geiger, J. K\u00f6hler, and M. Welling, Spherical CNNs, in International Conference on Learning Representations (2018). [23] N. Thomas, T. Smidt, S. Kearnes, L. Yang, L. Li, K. Kohlhoff, and P. Riley, Tensor field networks: Rotation- and translation-equivariant neural networks for 3d point clouds (2018). [24] D. P. Kingma and M. Welling, Auto-encoding variational bayes, in International Conference on Learning Representations (2014). [25] R. Jiao, W. Huang, P. Lin, J. Han, P. Chen, Y. Lu, and Y. Liu, Crystal structure prediction by joint equivariant diffusion on lattices and fractional coordinates, in Workshop on \u201cMachine Learning for Materials\u201d ICLR 2023 (2023). [26] A. Okhotin, D. Molchanov, V. Arkhipkin, G. Bar- tosh, A. Alanov, and D. Vetrov, Star-shaped denoising diffusion probabilistic models (2023), arXiv:2302.05259 [stat.ML]. [27] J. Gasteiger, S. Giri, J. T. Margraf, and S. G\u00fcnemann, Fast and uncertainty-aware directional message passing for non-equilibrium molecules (2022), arXiv:2011.14115 [cs.LG]. [28] W. Hu*, B. Liu*, J. Gomes, M. Zitnik, P. Liang, V. Pande, and J. Leskovec, Strategies for pre-training graph neural networks, in International Conference on Learning Representations (2020). [29] K. Sch\u00fctt, P.-J. Kindermans, H. E. Sauceda Felix, S. Chmiela, A. Tkatchenko, and K.-R. M\u00fcller, Schnet: A continuous-filter convolutional neural network for modeling quantum interactions, in Advances in Neural Infor-\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"which is precisely (1.3) for \\\\( p = 1 \\\\) due to (3.12); the non-normalized case follows in a standard way.\\n\\nSince the Sobolev inequality (1.3) for \\\\( p = 1 \\\\) is equivalent to the isoperimetric inequality\\n\\n\\\\[\\n\\\\frac{n}{(\\\\omega_n \\\\text{AVR}_g)^{\\\\frac{1}{n}}} \\\\text{Vol}_g(\\\\Omega)^{\\\\frac{n-1}{n}} \\\\leq \\\\mathcal{P}_g(\\\\partial \\\\Omega)\\n\\\\]  \\n\\n(3.15)\\n\\nfor every bounded open domain \\\\( \\\\Omega \\\\subset M \\\\) with smooth boundary (\\\\( \\\\mathcal{P}_g \\\\) being the perimeter), and (3.15) is sharp, see Balogh and Krist\u00e1ly [2] and Brendle [4], it turns out that (1.3) is also sharp. \\\\( \\\\square \\\\)\\n\\n4. Proof of the sharp \\\\( L^p \\\\)-logarithmic Sobolev inequality (Theorem 1.2)\\n\\n4.1. The case \\\\( p > 1 \\\\). Let \\\\( p > 1 \\\\) and fix \\\\( f \\\\in C_0^\\\\infty(M) \\\\) arbitrarily; we may assume that \\\\( f \\\\) is nonnegative and\\n\\n\\\\[\\n\\\\int_M f^p \\\\text{d}v_g = 1.\\n\\\\]\\n\\nAs before, let \\\\( \\\\Omega = \\\\{ x \\\\in M : f(x) > 0 \\\\} \\\\); since \\\\( f \\\\in C_0^\\\\infty(M) \\\\), then \\\\( \\\\overline{\\\\Omega} \\\\) is compact.\\n\\nLet \\\\( x_0 \\\\in \\\\Omega \\\\). For every \\\\( \\\\lambda > 0 \\\\) and \\\\( k \\\\in \\\\mathbb{N} \\\\), we introduce the truncated Gaussian bubble \\\\( G_{\\\\lambda,k} : M \\\\to \\\\mathbb{R} \\\\) given by\\n\\n\\\\[\\nG_{\\\\lambda,k}(x) = P_k(d_g(x_0,x))e^{-\\\\lambda d_g^2(x_0,x)},\\n\\\\]\\n\\nwhere \\\\( P_k \\\\) is defined in (3.2). We observe that the support of \\\\( G_{\\\\lambda,k} \\\\) is the ball \\\\( B_{x_0}(k+1) \\\\). Let\\n\\n\\\\[\\n\\\\mathcal{J}_{\\\\lambda,k} = \\\\int_M G_{\\\\lambda,k}(y) \\\\text{d}v_g(y);\\n\\\\]\\n\\nclearly, \\\\( 0 < \\\\mathcal{J}_{\\\\lambda,k} < \\\\infty \\\\) for every \\\\( \\\\lambda > 0 \\\\) and \\\\( k \\\\in \\\\mathbb{N} \\\\).\\n\\nLet \\\\( \\\\text{d}\\\\mu(x) = f^p(x) \\\\text{d}v_g(x) \\\\) and \\\\( \\\\text{d}\\\\nu(y) = \\\\frac{G_{\\\\lambda,k}(y)}{\\\\mathcal{J}_{\\\\lambda,k}} \\\\text{d}v_g(y) \\\\) be two probability measures on \\\\((M, g)\\\\) with compact supports; by the theory of OMT one can find a unique map \\\\( T : \\\\overline{\\\\Omega} \\\\to B_{x_0}(k+1) \\\\subset M \\\\) pushing \\\\( \\\\mu \\\\) forward to \\\\( \\\\nu \\\\) having the form \\\\( T(x) = \\\\exp_x(-\\\\nabla_g u(x)) \\\\) for a.e. \\\\( x \\\\in \\\\Omega \\\\), for some \\\\( c = d_g^2/2 \\\\)-concave function \\\\( u : \\\\overline{\\\\Omega} \\\\to \\\\mathbb{R} \\\\). The associated Monge-Amp\u00e8re equation is\\n\\n\\\\[\\nf^p(x) = \\\\frac{G_{\\\\lambda,k}(T(x))}{\\\\mathcal{J}_{\\\\lambda,k}} \\\\det DT(x) \\\\text{ for a.e. } x \\\\in \\\\Omega.\\n\\\\]  \\n\\n(4.1)\\n\\nAccordingly, by (4.1), a change of variables, Jensen\u2019s inequality and Propositions 2.1 and 2.2, we have\\n\\n\\\\[\\n\\\\int_M f^p \\\\log f^p \\\\text{d}v_g = \\\\int_\\\\Omega f^p(x) \\\\log \\\\left( \\\\frac{G_{\\\\lambda,k}(T(x))}{\\\\mathcal{J}_{\\\\lambda,k}} \\\\det DT(x) \\\\right) \\\\text{d}v_g(x)\\n\\\\]\\n\\n\\\\[\\n= \\\\int_\\\\Omega f^p(x) \\\\log (G_{\\\\lambda,k}(T(x))) \\\\text{d}v_g(x) + n \\\\int_\\\\Omega f^p(x) \\\\log \\\\left( \\\\frac{\\\\det \\\\frac{1}{n} DT(x)}{\\\\mathcal{J}_{\\\\lambda,k}^\\\\frac{1}{n}} \\\\right) \\\\text{d}v_g(x)\\n\\\\]\\n\\n\\\\[\\n\\\\leq \\\\int_\\\\Omega \\\\frac{G_{\\\\lambda,k}(T(x))}{\\\\mathcal{J}_{\\\\lambda,k}} \\\\log (G_{\\\\lambda,k}(T(x))) \\\\det DT(x) \\\\text{d}v_g(x)\\n\\\\]\\n\\n\\\\[\\n+ n \\\\log \\\\left( \\\\int_\\\\Omega f^p(x) \\\\frac{\\\\det \\\\frac{1}{n} DT(x)}{\\\\mathcal{J}_{\\\\lambda,k}^\\\\frac{1}{n}} \\\\text{d}v_g(x) \\\\right)\\n\\\\]\\n\\n\\\\[\\n\\\\leq \\\\frac{1}{\\\\mathcal{J}_{\\\\lambda,k}} \\\\int_M G_{\\\\lambda,k}(y) \\\\log (G_{\\\\lambda,k}(y)) \\\\text{d}v_g(y)\\n\\\\]\\n\\n\\\\[\\n+ n \\\\log \\\\left( \\\\int_\\\\Omega \\\\frac{f^p(x)}{\\\\mathcal{J}_{\\\\lambda,k}^\\\\frac{1}{n}} \\\\left( 1 - \\\\frac{\\\\Delta_g u(x)}{n} \\\\right) \\\\text{d}v_g(x) \\\\right)\\n\\\\]\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"which is precisely (1.3) for \\\\( p = 1 \\\\) due to (3.12); the non-normalized case follows in a standard way.\\n\\nSince the Sobolev inequality (1.3) for \\\\( p = 1 \\\\) is equivalent to the isoperimetric inequality\\n\\n\\\\[\\n\\\\frac{1}{n} \\\\log \\\\left( \\\\frac{\\\\text{Vol}_g(\\\\Omega)}{\\\\text{Vol}_g(\\\\partial \\\\Omega)} \\\\right) \\\\leq \\\\frac{1}{n} \\\\log \\\\left( \\\\frac{\\\\text{Vol}_g(\\\\Omega)}{\\\\text{Vol}_g(\\\\partial \\\\Omega)} \\\\right)\\n\\\\]\\n\\nfor every bounded open domain \\\\( \\\\Omega \\\\subset M \\\\) with smooth boundary (\\\\( \\\\mathcal{P}_g \\\\) being the perimeter), and (3.15) is sharp, see Balogh and Krist\u00e1ly [2] and Brendle [4], it turns out that (1.3) is also sharp. \\\\( \\\\square \\\\)\\n\\n4. Proof of the sharp \\\\( L^p \\\\)-logarithmic Sobolev inequality (Theorem 1.2)\\n\\n4.1. The case \\\\( p > 1 \\\\). Let \\\\( p > 1 \\\\) and fix \\\\( f \\\\in C_0^\\\\infty(M) \\\\) arbitrarily; we may assume that \\\\( f \\\\) is nonnegative and\\n\\n\\\\[\\n\\\\int_M f^p \\\\, dv_g = 1.\\n\\\\]\\n\\nAs before, let \\\\( \\\\Omega = \\\\{ x \\\\in M : f(x) > 0 \\\\} \\\\); since \\\\( f \\\\in C_0^\\\\infty(M) \\\\), then \\\\( \\\\overline{\\\\Omega} \\\\) is compact.\\n\\nLet \\\\( x_0 \\\\in \\\\Omega \\\\). For every \\\\( \\\\lambda > 0 \\\\) and \\\\( k \\\\in \\\\mathbb{N} \\\\), we introduce the truncated Gaussian bubble \\\\( G_{\\\\lambda,k} : M \\\\to \\\\mathbb{R} \\\\) given by\\n\\n\\\\[\\nG_{\\\\lambda,k}(x) = P_k(d_g(x_0, x)) e^{-\\\\lambda d_g^2(x_0, x)},\\n\\\\]\\n\\nwhere \\\\( P_k \\\\) is defined in (3.2). We observe that the support of \\\\( G_{\\\\lambda,k} \\\\) is the ball \\\\( B_{x_0}(k+1) \\\\). Let\\n\\n\\\\[\\n\\\\mathcal{J}_{\\\\lambda,k} = \\\\int_M G_{\\\\lambda,k}(y) \\\\, dv_g(y);\\n\\\\]\\n\\nclearly, \\\\( 0 < \\\\mathcal{J}_{\\\\lambda,k} < \\\\infty \\\\) for every \\\\( \\\\lambda > 0 \\\\) and \\\\( k \\\\in \\\\mathbb{N} \\\\).\\n\\nLet \\\\( d\\\\mu(x) = f^p(x) \\\\, dv_g(x) \\\\) and \\\\( d\\\\nu(y) = \\\\frac{G_{\\\\lambda,k}(y)}{\\\\mathcal{J}_{\\\\lambda,k}} \\\\, dv_g(y) \\\\) be two probability measures on \\\\((M, g)\\\\) with compact supports; by the theory of OMT one can find a unique map \\\\( T : \\\\overline{\\\\Omega} \\\\to B_{x_0}(k+1) \\\\subset M \\\\) pushing \\\\( \\\\mu \\\\) forward to \\\\( \\\\nu \\\\) having the form \\\\( T(x) = \\\\exp_x(-\\\\nabla_g u(x)) \\\\) for a.e. \\\\( x \\\\in \\\\Omega \\\\), for some \\\\( c = d_g^2/2 \\\\)-concave function \\\\( u : \\\\overline{\\\\Omega} \\\\to \\\\mathbb{R} \\\\). The associated Monge-Amp\u00e8re equation is\\n\\n\\\\[\\nf^p(x) = \\\\frac{G_{\\\\lambda,k}(T(x))}{\\\\mathcal{J}_{\\\\lambda,k}} \\\\det DT(x) \\\\text{ for a.e. } x \\\\in \\\\Omega.\\n\\\\]\\n\\nAccordingly, by (4.1), a change of variables, Jensen\u2019s inequality and Propositions 2.1 and 2.2, we have\\n\\n\\\\[\\n\\\\int_M f^p \\\\log f^p \\\\, dv_g = \\\\int_\\\\Omega f^p(x) \\\\log \\\\left( \\\\frac{G_{\\\\lambda,k}(T(x))}{\\\\mathcal{J}_{\\\\lambda,k}} \\\\det DT(x) \\\\right) \\\\, dv_g(x)\\n\\\\]\\n\\n\\\\[\\n= \\\\int_\\\\Omega f^p(x) \\\\log \\\\left( G_{\\\\lambda,k}(T(x)) \\\\right) \\\\, dv_g(x) + n \\\\int_\\\\Omega f^p(x) \\\\log \\\\left( \\\\frac{\\\\det \\\\frac{1}{n} DT(x)}{\\\\mathcal{J}_{\\\\lambda,k}^\\\\frac{1}{n}} \\\\right) \\\\, dv_g(x)\\n\\\\]\\n\\n\\\\[\\n\\\\leq \\\\int_\\\\Omega \\\\frac{G_{\\\\lambda,k}(T(x))}{\\\\mathcal{J}_{\\\\lambda,k}} \\\\log \\\\left( G_{\\\\lambda,k}(T(x)) \\\\right) \\\\, dv_g(x)\\n\\\\]\\n\\n\\\\[\\n+ n \\\\log \\\\left( \\\\int_\\\\Omega f^p(x) \\\\frac{\\\\det \\\\frac{1}{n} DT(x)}{\\\\mathcal{J}_{\\\\lambda,k}^\\\\frac{1}{n}} \\\\, dv_g(x) \\\\right)\\n\\\\]\\n\\n\\\\[\\n\\\\leq \\\\frac{1}{\\\\mathcal{J}_{\\\\lambda,k}} \\\\int_M G_{\\\\lambda,k}(y) \\\\log \\\\left( G_{\\\\lambda,k}(y) \\\\right) \\\\, dv_g(y)\\n\\\\]\\n\\n\\\\[\\n+ n \\\\log \\\\left( \\\\int_\\\\Omega f^p(x) \\\\frac{1}{\\\\mathcal{J}_{\\\\lambda,k}^\\\\frac{1}{n}} \\\\left( 1 - \\\\frac{\\\\Delta_g u(x)}{n} \\\\right) \\\\, dv_g(x) \\\\right)\\n\\\\]\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"we confirm the previous detection of He\\\\textsubscript{1} and, by analysing this He\\\\textsubscript{2}(S) signal, we study the hydrodynamical escape of this planet and derive the temperature and mass-loss rate of its upper atmosphere.\\n\\n### Table 1: Stellar parameters of 23.\\n\\n| Parameter | Value | Reference |\\n|-----------|-------|-----------|\\n| Name      | HD 235088 | HD |\\n|           | TIC 293954617 | TESS |\\n|           | TOI-1430 | TOI |\\n|           | HIP 98688 | HIP |\\n\\n### Coordinates and spectral types\\n\\n| Parameter | Value | Reference |\\n|-----------|-------|-----------|\\n| $\\\\alpha$ (J2000) | $20^h 02^m 27^s.4$ | Gaia EDR3 |\\n| $\\\\delta$ (J2000) | $+53^\\\\circ 22' 36''5$ | Gaia EDR3 |\\n| Spectral type | K2 V | Sect. 3.1 |\\n\\n### Parallaxes and kinematics\\n\\n| Parameter | Value | Reference |\\n|-----------|-------|-----------|\\n| $\\\\pi$ [mas] | 24.25 \u00b1 0.01 | TYC |\\n| $d$ [pc] | 41.24 \u00b1 0.02 | Gaia EDR3 |\\n| $\\\\mu_\\\\alpha \\\\cos \\\\delta$ [mas yr\\\\textsuperscript{-1}] | 165.05 \u00b1 0.02 | Gaia EDR3 |\\n| $\\\\mu_\\\\delta$ [mas yr\\\\textsuperscript{-1}] | 145.17 \u00b1 0.02 | Gaia EDR3 |\\n| $\\\\gamma$ [km s\\\\textsuperscript{-1}] | $-27.370 \\\\pm 0.002$ | Gaia DR2 |\\n| $U$ [km s\\\\textsuperscript{-1}] | $-41.75 \\\\pm 0.02$ | This work |\\n| $V$ [km s\\\\textsuperscript{-1}] | $-22.16 \\\\pm 0.01$ | This work |\\n| $W$ [km s\\\\textsuperscript{-1}] | $-19.03 \\\\pm 0.02$ | This work |\\n| RUWE | 0.966 | Gaia DR3 |\\n\\n### Magnitudes\\n\\n| Parameter | Value | Reference |\\n|-----------|-------|-----------|\\n| $B$ [mag] | 10.129 \u00b1 0.038 | 2MASS |\\n| $V$ [mag] | 9.19 \u00b1 0.03 | HIP |\\n| $J$ [mag] | 7.646 \u00b1 0.03 | 2MASS |\\n\\n### Stellar parameters\\n\\n| Parameter | Value | Reference |\\n|-----------|-------|-----------|\\n| $L_X$ [\u00d710\\\\textsuperscript{28} erg s\\\\textsuperscript{-1}] | 1.89 \u00b1 0.07 | Sect. 2.2 |\\n| $L_\\\\star$ [L\\\\textsubscript{\\\\odot}] | 0.3609 \u00b1 0.0052 | Sect. 3.1 |\\n| $T_{\\\\text{eff}}$ [K] | 5037\u00b114 | Sect. 3.1 |\\n| log($g$ [cm s\\\\textsuperscript{-2}]) | 4.63 \u00b1 0.02 | Sect. 3.1 |\\n| [Fe/H] | $-0.01 \\\\pm 0.02$ | Sect. 3.1 |\\n| $\\\\pi$ | $-0.08 \\\\pm 0.13$ | B18 |\\n| $R_\\\\star$ [R\\\\textsubscript{\\\\odot}] | 0.789\\\\textsuperscript{+0.022}_{-0.021} | Sect. 3.1 |\\n| $M_\\\\star$ [M\\\\textsubscript{\\\\odot}] | 0.843\\\\textsuperscript{+0.033}_{-0.056} | Sect. 3.1 |\\n| $V_{\\\\text{broad}}$ [km s\\\\textsuperscript{-1}] | 2.89 \u00b1 0.03 | Sect. 3.1 |\\n| $\\\\nu$ sin $i$ [km s\\\\textsuperscript{-1}] | $<2.90$ | Sect. 3.1 |\\n| $P_{\\\\text{rot}}$ [d] | 12.0 \u00b1 0.4 | Sect. 5.1 |\\n| log($R_{\\\\text{HK}}$) | $-4.242 \\\\pm 0.016$ | M22 |\\n| Age [Myr] | 600\u2013800 | Sect. 3.2 |\\n\\n### References\\n\\nHD: Cannon & Pickering (1993); TESS: TESS Input Catalog v8.2 (Stassun et al. 2018); Gaia EDR3: Gaia Collaboration 2020; Gaia DR2: Soubiran et al. 2018; TOI: Guerrero et al. (2021); HIP: van Leeuwen (2007); TYC: H\u00f8g et al. (2000a); 2MASS: Cutri et al.\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"we confirm the previous detection of He\\\\textsubscript{1} and, by analysing this He\\\\textsubscript{2}(S) signal, we study the hydrodynamical escape of this planet and derive the temperature and mass-loss rate of its upper atmosphere.\\n\\n### Table 1: Stellar parameters of 23.\\n\\n| Parameter | Value | Reference |\\n|-----------|-------|-----------|\\n| Name      | HD 235088 | HD |\\n|           | TIC 293954617 | TESS |\\n|           | TOI-1430 | TOI |\\n|           | HIP 98688 | HIP |\\n\\n### Coordinates and spectral types\\n\\n| Parameter | Value | Reference |\\n|-----------|-------|-----------|\\n| $\\\\alpha$ (J2000) | $20^h 02^m 27^s.4$ | Gaia EDR3 |\\n| $\\\\delta$ (J2000) | $+53^\\\\circ 22' 36''5$ | Gaia EDR3 |\\n| Spectral type | K2 V | Sect. 3.1 |\\n\\n### Parallaxes and kinematics\\n\\n| Parameter | Value | Reference |\\n|-----------|-------|-----------|\\n| $\\\\pi$ [mas] | 24.25 $\\\\pm$ 0.01 | TYC |\\n| $d$ [pc] | 41.24 $\\\\pm$ 0.02 | Gaia EDR3 |\\n| $\\\\mu_\\\\alpha \\\\cos \\\\delta$ [mas yr\\\\textsuperscript{-1}] | 165.05 $\\\\pm$ 0.02 | Gaia EDR3 |\\n| $\\\\mu_\\\\delta$ [mas yr\\\\textsuperscript{-1}] | 145.17 $\\\\pm$ 0.02 | Gaia EDR3 |\\n| $\\\\gamma$ [km s\\\\textsuperscript{-1}] | $-27.370 \\\\pm 0.002$ | Gaia DR2 |\\n| $U$ [km s\\\\textsuperscript{-1}] | $-41.75 \\\\pm 0.02$ | This work |\\n| $V$ [km s\\\\textsuperscript{-1}] | $-22.16 \\\\pm 0.01$ | This work |\\n| $W$ [km s\\\\textsuperscript{-1}] | $-19.03 \\\\pm 0.02$ | This work |\\n| RUWE | 0.966 | Gaia DR3 |\\n\\n### Magnitudes\\n\\n| Parameter | Value | Reference |\\n|-----------|-------|-----------|\\n| $B$ [mag] | 10.129 $\\\\pm$ 0.038 | 2MASS |\\n| $V$ [mag] | 9.19 $\\\\pm$ 0.03 | HIP |\\n| $J$ [mag] | 7.646 $\\\\pm$ 0.03 | 2MASS |\\n\\n### Stellar parameters\\n\\n| Parameter | Value | Reference |\\n|-----------|-------|-----------|\\n| $L_X$ [$\\\\times 10^{38}$ erg s\\\\textsuperscript{-1}] | 1.89 $\\\\pm$ 0.07 | Sect. 2.2 |\\n| $L_\\\\star$ [$L_\\\\odot$] | 0.3609 $\\\\pm$ 0.0052 | Sect. 3.1 |\\n| $T_{\\\\text{eff}}$ [K] | 5037 $\\\\pm$ 14 | Sect. 3.1 |\\n| log($g$ [cm s\\\\textsuperscript{-2}]) | 4.63 $\\\\pm$ 0.02 | Sect. 3.1 |\\n| [Fe/H] | $-0.01 \\\\pm 0.02$ | Sect. 3.1 |\\n| $\\\\pi$ | $-0.08 \\\\pm 0.13$ | B18 |\\n| $R_\\\\star$ [$R_\\\\odot$] | 0.789 $\\\\pm$ 0.022 | Sect. 3.1 |\\n| $M_\\\\star$ [$M_\\\\odot$] | 0.843 $\\\\pm$ 0.053 | Sect. 3.1 |\\n| $V_{\\\\text{broad}}$ [km s\\\\textsuperscript{-1}] | 2.89 $\\\\pm$ 0.03 | Sect. 3.1 |\\n| $\\\\nu$ sin $i$ [km s\\\\textsuperscript{-1}] | $<2.90$ | Sect. 3.1 |\\n| $P_{\\\\text{rot}}$ [d] | 12.0 $\\\\pm$ 0.4 | Sect. 5.1 |\\n| log($R_{\\\\text{HK}}$) | $-4.24 \\\\pm 0.016$ | M22 |\\n| Age [Myr] | 600\u2013800 | Sect. 3.2 |\\n\\n### References\\n\\nHD: Cannon & Pickering (1993); TESS: TESS Input Catalog v8.2 (Stassun et al. 2018); Gaia EDR3: Gaia Collaboration 2020; Gaia DR2: Soubiran et al. 2018; TOI: Guerrero et al. (2021); HIP: van Leeuwen (2007); TYC: H\u00f8g et al. (2000a); 2MASS: Cutri et al. (2003); EDR3: Gaia Collaboration "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"9. Zeros\\n\\nIt is well known that the zeros of orthogonal polynomials with respect to a positive definite linear functional are real, simple, and located in the interior of the convex hull of the support \\\\([15, 20]\\\\). With this in mind, let \\\\(\\\\{x_{n,k}(z)\\\\}_{k=1}^{n}\\\\) be the the zeros of \\\\(P_n(x)\\\\) in an increasing order, i.e.,\\n\\n\\\\[\\nP_n(x_{n,k}(z), z) = 0\\n\\\\]\\n\\nwith\\n\\n\\\\[\\nx_{n,1}(z) < x_{n,2}(z) < \\\\cdots < x_{n,n}(z).\\n\\\\]\\n\\nNext we will focus our attention on an electrostatic interpretation of the zeros of the polynomial \\\\(P_n(x; z)\\\\) in terms of the energy associated with a logarithmic potential and next we will study the dynamics of them in terms of the parameter \\\\(z\\\\).\\n\\n9.1. Electrostatic interpretation. Evaluating the operator \\\\(D_{n+1}\\\\) defined in (5.6) at \\\\(x = x_{n+1,k}\\\\), we get\\n\\n\\\\[\\n\\\\frac{\\\\partial^2 P_{n+1}}{\\\\partial x P_{n+1}} \\\\bigg|_{x=x_{n+1,k}} = -\\\\frac{2x_{n+1,k} - z}{\\\\phi(x_{n+1,k})} + \\\\frac{a_{n+1}}{C_n(x_{n+1,k})} + \\\\frac{(x_{n,k} - b_n)C_{n-1}(x_{n,k}; z)}{a_n\\\\phi(x_{n,k}; z)} + \\\\frac{\\\\delta_n(x_{n,k}; z) + \\\\delta_{n-1}(x_{n,k}; z)}{\\\\phi(x_{n,k}; z)},\\n\\\\]\\n\\nwhere \\\\(C_n(x; z)\\\\) and \\\\(\\\\delta_n(x; z)\\\\) were defined in Proposition 5.4. Taking into account (5.4) the above expression reads\\n\\n\\\\[\\n\\\\frac{\\\\partial^2 P_{n+1}}{\\\\partial x P_{n+1}} \\\\bigg|_{x=x_{n+1,k}} = -\\\\frac{1}{x_{n+1,k} - z} - \\\\frac{1}{x_{n+1,k} - \\\\beta_n} + 1 - \\\\frac{\\\\alpha}{x_{n+1,k}},\\n\\\\]\\n\\nwhere \\\\(\\\\beta_n = -b_{n+1} + (2n + \\\\alpha + z + 3)\\\\). Observe that the weight function associated with \\\\(\\\\ell\\\\) is \\\\(w(x) = x^\\\\alpha e^{-x} = e^{-x + \\\\alpha \\\\ln x}\\\\). If we define \\\\(v(x) = x - \\\\alpha \\\\ln x\\\\), (the external potential [22, Section 3.5]), then\\n\\n\\\\[\\n\\\\frac{\\\\partial^2 P_{n+1}}{\\\\partial x P_{n+1}} \\\\bigg|_{x=x_{n+1,k}} = -\\\\frac{1}{x_{n+1,k} - z} - \\\\frac{1}{x_{n+1,k} - \\\\beta_n} + 1 - \\\\frac{\\\\alpha + 1}{x_{n+1,k}},\\n\\\\]\\n\\nRemark 9.1. If \\\\(\\\\{\\\\tilde{x}_{n+1,k}(z)\\\\}_{k=1}^{n+1}\\\\) are the zeros of \\\\(Q_{n+1}(x)\\\\) in an increasing order, i.e. \\\\(\\\\tilde{x}_{n+1,1} < \\\\tilde{x}_{n+1,2} < \\\\cdots < \\\\tilde{x}_{n+1,n+1}\\\\), then we have\\n\\n\\\\[\\n\\\\frac{\\\\partial^2 Q_{n+1}}{\\\\partial x Q_{n+1}} \\\\bigg|_{x=\\\\tilde{x}_{n+1,k}} = -\\\\frac{1}{\\\\tilde{x}_{n+1,k} - z} - \\\\frac{1}{\\\\tilde{x}_{n+1,k} - \\\\tilde{\\\\beta}_n} + 1 - \\\\frac{\\\\alpha + 1}{\\\\tilde{x}_{n+1,k}},\\n\\\\]\\n\\nwhere \\\\(\\\\tilde{\\\\beta}_n = -d_{n+1} + (2n + \\\\alpha + z + 4)\\\\).\\n\\nTheorem 9.2. The zeros of \\\\(P_{n+1}(x; z)\\\\) are the equilibrium points of \\\\(n + 1\\\\) unit charged particles located in the interval \\\\((0, z)\\\\) under the influence of the external potential\\n\\n\\\\[\\nV_P(x) = \\\\ln \\\\frac{1}{|x - z|} + \\\\ln \\\\frac{1}{|x|} - \\\\ln \\\\frac{1}{|x - \\\\beta_n|} + v(x), \\\\quad 0 < x < z.\\n\\\\]\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"9. Zeros\\n\\nIt is well known that the zeros of orthogonal polynomials with respect to a positive definite linear functional are real, simple, and located in the interior of the convex hull of the support [ 15 , 20 ]. With this in mind, let \\\\( \\\\{ x_{n,k}(z) \\\\}_{k=1}^{n} \\\\) be the the zeros of \\\\( P_{n}(x) \\\\) in an increasing order, i. e.,\\n\\n\\\\[\\nP_{n}(x_{n,k}(z), z) = 0\\n\\\\]\\n\\nwith\\n\\n\\\\[\\nx_{n,1}(z) < x_{n,2}(z) < \\\\cdots < x_{n,n}(z).\\n\\\\]\\n\\nNext we will focus our attention on an electrostatic interpretation of the zeros of the polynomial \\\\( P_{n}(x; z) \\\\) in terms of the energy associated with a logarithmic potential and next we will study the dynamics of them in terms of the parameter \\\\( z \\\\).\\n\\n9.1. Electrostatic interpretation. Evaluating the operator \\\\( D_{n+1} \\\\) defined in ( 5.6 ) at \\\\( x = x_{n+1,k} \\\\), we get\\n\\n\\\\[\\n\\\\frac{\\\\partial^{2} P_{n+1}}{\\\\partial x P_{n+1}} \\\\bigg|_{x=x_{n+1,k}} = -\\\\frac{2x_{n+1,k} - z}{\\\\phi(x_{n+1,k})} + \\\\frac{a_{n+1}}{C_{n}(x_{n+1,k})} + \\\\frac{(x_{n,k} - b_{n})C_{n-1}(x_{n,k}; z)}{a_{n}\\\\phi(x_{n,k}; z)} + \\\\frac{\\\\delta_{n}(x_{n,k}; z) + \\\\delta_{n-1}(x_{n,k}; z)}{\\\\phi(x_{n,k}; z)},\\n\\\\]\\n\\nwhere \\\\( C_{n}(x; z) \\\\) and \\\\( \\\\delta_{n}(x; z) \\\\) were defined in Proposition 5.4 . Taking into account ( 5.4 ) the above expression reads\\n\\n\\\\[\\n\\\\frac{\\\\partial^{2} P_{n+1}}{\\\\partial x P_{n+1}} \\\\bigg|_{x=x_{n+1,k}} = -\\\\frac{1}{x_{n+1,k} - z} - \\\\frac{1}{x_{n+1,k} - \\\\beta_{n}} + 1 - \\\\frac{\\\\alpha}{x_{n+1,k}},\\n\\\\]\\n\\nwhere \\\\( \\\\beta_{n} = -b_{n+1} + (2n + \\\\alpha + z + 3) \\\\). Observe that the weight function associated with \\\\( \\\\ell \\\\) is \\\\( w(x) = x^{\\\\alpha}e^{-x} = e^{-x + \\\\alpha \\\\ln x} \\\\). If we define \\\\( v(x) = x - \\\\alpha \\\\ln x \\\\), (the external potential [ 22 , Section 3.5]), then\\n\\n\\\\[\\n\\\\frac{\\\\partial^{2} P_{n+1}}{\\\\partial x P_{n+1}} \\\\bigg|_{x=x_{n+1,k}} = -\\\\frac{1}{x_{n+1,k} - z} - \\\\frac{1}{x_{n+1,k} - \\\\beta_{n}} + 1 - \\\\frac{\\\\alpha + 1}{x_{n+1,k}},\\n\\\\]\\n\\nRemark 9.1. If \\\\( \\\\{ \\\\tilde{x}_{n+1,k}(z) \\\\}_{k=1}^{n+1} \\\\) are the zeros of \\\\( Q_{n+1}(x) \\\\) in an increasing order, i.e. \\\\( \\\\tilde{x}_{n+1,1} < \\\\tilde{x}_{n+1,2} < \\\\cdots < \\\\tilde{x}_{n+1,n+1} \\\\), then we have\\n\\n\\\\[\\n\\\\frac{\\\\partial^{2} Q_{n+1}}{\\\\partial x Q_{n+1}} \\\\bigg|_{x=\\\\tilde{x}_{n+1,k}} = -\\\\frac{1}{\\\\tilde{x}_{n+1,k} - z} - \\\\frac{1}{\\\\tilde{x}_{n+1,k} - \\\\tilde{\\\\beta}_{n}} + 1 - \\\\frac{\\\\alpha + 1}{\\\\tilde{x}_{n+1,k}},\\n\\\\]\\n\\nwhere \\\\( \\\\tilde{\\\\beta}_{n} = -d_{n+1} + (2n + \\\\alpha + z + 4) \\\\).\\n\\nTheorem 9.2. The zeros of \\\\( P_{n+1}(x; z) \\\\) are the equilibrium points of \\\\( n + 1 \\\\) unit charged particles located in the interval \\\\( (0, z) \\\\) under the influence of the external potential\\n\\n\\\\[\\nV_{P}(x) = \\\\ln \\\\frac{1}{|x - z|} + \\\\ln \\\\frac{1}{|x|} - \\\\ln \\\\frac{1}{|x - \\\\beta_{n}|} + v(x), \\\\quad 0 < x < z.\\n\\\\]\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"for all $t \\\\in [0, \\\\delta)$ almost surely. Furthermore, for $p \\\\geq 4$, using Ito\u2019s formula for $\\\\|u\\\\|_{1,2}^p$, we have\\n\\n$$\\n\\\\|w^\\\\varepsilon(t)\\\\|_{1,2}^p - \\\\|w_0\\\\|_{1,2}^p = p \\\\sum_k \\\\lambda_k \\\\int_0^t \\\\|w^\\\\varepsilon(s)\\\\|_{1,2}^{p-2} (\\\\partial_x (\\\\Psi_k w^\\\\varepsilon(s)), w^\\\\varepsilon(s))_{1,2} d\\\\beta^k(s)\\n$$\\n\\n$$\\n+ \\\\frac{p}{2} \\\\int_0^t \\\\|w^\\\\varepsilon(s)\\\\|_{1,2}^{p-2} (2 \\\\langle A^\\\\varepsilon w^\\\\varepsilon(s), w^\\\\varepsilon(s) \\\\rangle) + \\\\|B(w^\\\\varepsilon(s))\\\\|_{L_2}^2 ds\\n$$\\n\\n$$\\n+ \\\\frac{p(p-2)}{2} \\\\sum_k \\\\lambda_k^2 \\\\int_0^t \\\\|w^\\\\varepsilon(s)\\\\|_{1,2}^{p-4} (\\\\partial_x (\\\\Psi_k w^\\\\varepsilon(s)), w^\\\\varepsilon(s))^2_{1,2} ds\\n$$\\n\\n$$\\n+ \\\\frac{p(p-2)}{2} \\\\sum_k \\\\lambda_k^2 \\\\int_0^t \\\\|w^\\\\varepsilon(s)\\\\|_{1,2}^{p-4} (\\\\Psi_k f(w^\\\\varepsilon(s)), w^\\\\varepsilon(s))^2_{1,2} ds\\n$$\\n\\n(4.25)\\n\\nwhere we used the definition of the norm $\\\\|B(u)\\\\|_{L_2}$. Next, using Burkholder-Davis-Gundy inequality:\\n\\n$$\\n\\\\mathbb{E} \\\\sup_{t \\\\in [0,t]} \\\\left| \\\\int_0^t \\\\|w^\\\\varepsilon(s)\\\\|_{1,2}^{p-2} (\\\\partial_x (\\\\Psi_k w^\\\\varepsilon(s)), w^\\\\varepsilon(s))_{1,2} d\\\\beta^k(s) \\\\right|\\n$$\\n\\n$$\\n\\\\leq 3 \\\\mathbb{E} \\\\left( \\\\int_0^t \\\\|w^\\\\varepsilon(s)\\\\|_{1,2}^{2p-4} \\\\partial_x (\\\\Psi_k w^\\\\varepsilon(s), w^\\\\varepsilon(s))^2_{1,2} ds \\\\right)^{\\\\frac{1}{2}}\\n$$\\n\\n$$\\n\\\\leq C \\\\mathbb{E} \\\\left( \\\\int_0^t \\\\|w^\\\\varepsilon(s)\\\\|_{1,2}^{2p-4} (\\\\Psi_k f(w^\\\\varepsilon(s)), w^\\\\varepsilon(s))^2_{1,2} ds \\\\right)^{\\\\frac{1}{2}}\\n$$\\n\\n$$\\n\\\\leq \\\\nu \\\\mathbb{E} \\\\sup_{s \\\\in [0,t]} \\\\|w^\\\\varepsilon(s)\\\\|_{1,2}^p + \\\\frac{C}{\\\\nu} \\\\mathbb{E} \\\\int_0^t \\\\|w^\\\\varepsilon(s)\\\\|_{1,2}^p ds,\\n$$\\n\\n(4.26)\\n\\nwhere $\\\\nu > 0$ can be chosen arbitrarily small, and $C$ is independent of $\\\\nu$. Note that the expected values in (4.26) are finite due to (4.8).\\n\\nIn a similar way, we get\\n\\n$$\\n\\\\mathbb{E} \\\\sup_{t \\\\in [0,t]} \\\\left| \\\\int_0^t \\\\|w^\\\\varepsilon(s)\\\\|_{1,2}^{p-2} (\\\\Psi_k f(w^\\\\varepsilon(s)), w^\\\\varepsilon(s))_{1,2} d\\\\beta^k(s) \\\\right|\\n$$\\n\\n$$\\n\\\\leq 3 \\\\mathbb{E} \\\\left( \\\\int_0^t \\\\|w^\\\\varepsilon(s)\\\\|_{1,2}^{2p-4} (\\\\Psi_k f(w^\\\\varepsilon(s)), w^\\\\varepsilon(s))^2_{1,2} ds \\\\right)^{\\\\frac{1}{2}}\\n$$\\n\\n(4.27)\\n\\nBut\\n\\n$$\\n(\\\\Psi_k f(w^\\\\varepsilon(s)), w^\\\\varepsilon(s))_{1,2} = \\\\int_0^L \\\\Psi_k f(w^\\\\varepsilon(s)) w^\\\\varepsilon(s) ds\\n$$\\n\\n$$\\n+ \\\\int_0^L \\\\partial_x (\\\\Psi_k f(w^\\\\varepsilon(s))) \\\\partial_x w^\\\\varepsilon(s) ds := I_1 + I_2.\\n$$\\n\\n(4.28)\\n\\nThe estimate for $I_1$ follows from the conditions imposed on $f$, namely\\n\\n$$\\n|I_1| \\\\leq C \\\\|w^\\\\varepsilon(s)\\\\|_{1,2}^2.\\n$$\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"for all $t \\\\in [0, \\\\delta)$ almost surely. Furthermore, for $p \\\\geq 4$, using Ito\u2019s formula for $\\\\|u\\\\|_{1,2}^p$, we have\\n\\n$$\\n\\\\|w^\\\\varepsilon(t)\\\\|_{1,2}^p - \\\\|w_0\\\\|_{1,2}^p = p \\\\sum_k \\\\lambda_k \\\\int_0^t \\\\|w^\\\\varepsilon(s)\\\\|_{1,2}^{p-2} (\\\\partial_x (\\\\Psi_k w^\\\\varepsilon(s)), w^\\\\varepsilon(s))_{1,2} d\\\\beta^k(s)\\n$$\\n\\n$$\\n+ \\\\frac{p}{2} \\\\int_0^t \\\\|w^\\\\varepsilon(s)\\\\|_{1,2}^{p-2} (2 \\\\langle A^\\\\varepsilon w^\\\\varepsilon(s), w^\\\\varepsilon(s) \\\\rangle) + \\\\|B(w^\\\\varepsilon(s))\\\\|_{L^2}^2 ds\\n$$\\n\\n$$\\n+ \\\\frac{p(p-2)}{2} \\\\sum_k \\\\lambda_k^2 \\\\int_0^t \\\\|w^\\\\varepsilon(s)\\\\|_{1,2}^{p-4} (\\\\partial_x (\\\\Psi_k w^\\\\varepsilon(s)), w^\\\\varepsilon(s))^2_{1,2} ds\\n$$\\n\\n$$\\n+ \\\\frac{p(p-2)}{2} \\\\sum_k \\\\lambda_k^2 \\\\int_0^t \\\\|w^\\\\varepsilon(s)\\\\|_{1,2}^{p-4} (\\\\Psi_k f(w^\\\\varepsilon(s)), w^\\\\varepsilon(s))^2_{1,2} ds\\n$$\\n\\n(4.25)\\n\\nwhere we used the definition of the norm $\\\\|B(u)\\\\|_{L^2}$. Next, using Burkholder-Davis-Gundy inequality: \\n\\n$$\\n\\\\mathbb{E} \\\\sup_{t \\\\in [0, t]} \\\\left| \\\\int_0^t \\\\|w^\\\\varepsilon(s)\\\\|_{1,2}^{p-2} (\\\\partial_x (\\\\Psi_k w^\\\\varepsilon(s)), w^\\\\varepsilon(s))_{1,2} d\\\\beta^k(s) \\\\right|\\n$$\\n\\n$$\\n\\\\leq 3 \\\\mathbb{E} \\\\left( \\\\int_0^t \\\\|w^\\\\varepsilon(s)\\\\|_{1,2}^{2p-4} (\\\\partial_x (\\\\Psi_k w^\\\\varepsilon(s)), w^\\\\varepsilon(s))^2_{1,2} ds \\\\right)^{\\\\frac{1}{2}}\\n$$\\n\\n$$\\n\\\\leq C \\\\mathbb{E} \\\\left( \\\\int_0^t \\\\|w^\\\\varepsilon(s)\\\\|_{1,2}^{2p-4} (\\\\partial_x (\\\\Psi_k w^\\\\varepsilon(s)), w^\\\\varepsilon(s))^2_{1,2} ds \\\\right)^{\\\\frac{1}{2}}\\n$$\\n\\n$$\\n\\\\leq \\\\nu \\\\mathbb{E} \\\\sup_{s \\\\in [0, t]} \\\\|w^\\\\varepsilon(s)\\\\|_{1,2}^p + \\\\frac{C}{\\\\nu} \\\\mathbb{E} \\\\int_0^t \\\\|w^\\\\varepsilon(s)\\\\|_{1,2}^p ds,\\n$$\\n\\n(4.26)\\n\\nwhere $\\\\nu > 0$ can be chosen arbitrarily small, and $C$ is independent of $\\\\nu$. Note that the expected values in (4.26) are finite due to (4.8). \\n\\nIn a similar way, we get\\n\\n$$\\n\\\\mathbb{E} \\\\sup_{t \\\\in [0, t]} \\\\left| \\\\int_0^t \\\\|w^\\\\varepsilon(s)\\\\|_{1,2}^{p-2} (\\\\Psi_k f(w^\\\\varepsilon(s)), w^\\\\varepsilon(s))_{1,2} d\\\\beta^k(s) \\\\right|\\n$$\\n\\n$$\\n\\\\leq 3 \\\\mathbb{E} \\\\left( \\\\int_0^t \\\\|w^\\\\varepsilon(s)\\\\|_{1,2}^{2p-4} (\\\\Psi_k f(w^\\\\varepsilon(s)), w^\\\\varepsilon(s))^2_{1,2} ds \\\\right)^{\\\\frac{1}{2}}\\n$$\\n\\n(4.27)\\n\\nBut\\n\\n$$\\n(\\\\Psi_k f(w^\\\\varepsilon(s)), w^\\\\varepsilon(s))_{1,2} = \\\\int_0^L \\\\Psi_k f(w^\\\\varepsilon(s)) w^\\\\varepsilon(s) ds\\n$$\\n\\n$$\\n+ \\\\int_0^L \\\\partial_x (\\\\Psi_k f(w^\\\\varepsilon(s))) \\\\partial_x w^\\\\varepsilon(s) ds := I_1 + I_2.\\n$$\\n\\n(4.28)\\n\\nThe estimate for $I_1$ follows from the conditions imposed on $f$, namely\\n\\n$$\\n|I_1| \\\\leq C \\\\|w^\\\\varepsilon(s)\\\\|_{1,2}^2.\\n$$\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To simplify the analysis, a narrow frequency band was selected between 24 and 61 Hz, with the fourth and fifth bending modes of the blades dominating the response in this band. Although other modes appear to have a small influence in this band, a 2DOF assumption was imposed. (This assumption results in smoothing of the FRF over the band, and might result in some loss of interpretability, but is acceptable for these preliminary analyses). The real part was modelled as a probabilistic FRF, using the FRF estimate from Eq. (13) as the mean of the likelihood function, as described in Case 1, presented in Section 6 of this paper. The real parts of the averaged FRFs for each blade, at the second accelerometer from the blade root (corresponding to the drive-point location), are shown in Figures 5a and 5b. Figure 5a shows the full measured bandwidth, and Figure 5b shows the FRF in the bandwidth of interest, between 24 and 61 Hz.\\n\\nFigures 5a and 5b show increasing variability with respect to frequency, which is an expected result, given that higher-frequency modes are more sensitive to small physical changes than lower-frequency modes. For modes less than 80 Hz, the maximum frequency difference among the blades was approximately 2.5 Hz; for modes greater than 80 Hz, the maximum frequency difference was approximately 6.3 Hz. Note the grouping visible at several of the peaks, where Blades 1 and 2 appear closely aligned in frequency while Blades 3 and 4 appear closely aligned. These results are quite relevant for PBSHM. All of the helicopter blades are healthy, and represent a normal-condition state of the population. Consider a situation where only FRFs from one of the groupings are available for training a model (or FRFs from the other groups are missing data). The normal condition could be heavily biased towards the training set, and incoming FRFs could be flagged as damaged, even if they are healthy. Further details regarding the data collection and processing for these tests can be found in [27].\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To simplify the analysis, a narrow frequency band was selected between 24 and 61 Hz, with the fourth and fifth bending modes of the blades dominating the response in this band. Although other modes appear to have a small influence in this band, a 2DOF assumption was imposed. (This assumption results in smoothing of the FRF over the band, and might result in some loss of interpretability, but is acceptable for these preliminary analyses). The real part was modelled as a probabilistic FRF, using the FRF estimate from Eq. (13) as the mean of the likelihood function, as described in Case 1, presented in Section 6 of this paper. The real parts of the averaged FRFs for each blade, at the second accelerometer from the blade root (corresponding to the drive-point location), are shown in Figures 5a and 5b. Figure 5a shows the full measured bandwidth, and Figure 5b shows the FRF in the bandwidth of interest, between 24 and 61 Hz. \\n\\nFigures 5a and 5b show increasing variability with respect to frequency, which is an expected result, given that higher-frequency modes are more sensitive to small physical changes than lower-frequency modes. For modes less than 80 Hz, the maximum frequency difference among the blades was approximately 2.5 Hz; for modes greater than 80 Hz, the maximum frequency difference was approximately 6.3 Hz. Note the grouping visible at several of the peaks, where Blades 1 and 2 appear closely aligned in frequency while Blades 3 and 4 appear closely aligned. These results are quite relevant for PBSHM. All of the helicopter blades are healthy, and represent a normal-condition state of the population. Consider a situation where only FRFs from one of the groupings are available for training a model (or FRFs from the other groups are missing data). The normal condition could be heavily biased towards the training set, and incoming FRFs could be flagged as damaged, even if they are healthy. Further details regarding the data collection and processing for these tests can be found in [27].\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Adaptive multi-stage integration schemes for Hamiltonian Monte Carlo\\n\\nLorenzo Nagar\\\\textsuperscript{a,}\\\\textsuperscript{*}, Mario Fern\u00e1ndez-Pend\u00e1s\\\\textsuperscript{b}, Jes\u00fas Mar\u00eda Sanz-Serna\\\\textsuperscript{c}, Elena Akhmatskaya\\\\textsuperscript{a,d}\\n\\n\\\\textsuperscript{a}BCAM - Basque Center for Applied Mathematics, Alameda de Mazarredo 14, 48009 Bilbao, Spain\\n\\\\textsuperscript{b}DIPC, Donostia International Physics Center, Manuel Lardizabal Ibilbidea 4, 20018 Donostia, Spain\\n\\\\textsuperscript{c}Departamento de Matem\u00e1ticas, Universidad Carlos III de Madrid, Avenida Universidad 30, 28911 Legan\u00e9s, Spain\\n\\\\textsuperscript{d}Ikerbasque - Basque Foundation for Science, Euskadi Plaza 5, 48009 Bilbao, Spain\\n\\nAbstract\\n\\nHamiltonian Monte Carlo (HMC) is a powerful tool for Bayesian statistical inference due to its potential to rapidly explore high dimensional state space, avoiding the random walk behavior typical of many Markov Chain Monte Carlo samplers. The proper choice of the integrator of the Hamiltonian dynamics is key to the efficiency of HMC. It is becoming increasingly clear that multi-stage splitting integrators are a good alternative to the Verlet method, traditionally used in HMC. Here we propose a principled way of finding optimal, problem-specific integration schemes (in terms of the best conservation of energy for harmonic forces/Gaussian targets) within the families of 2- and 3-stage splitting integrators. The method, which we call Adaptive Integration Approach for statistics, or s-AIA, uses a multivariate Gaussian model and simulation data obtained at the HMC burn-in stage to identify a system-specific dimensional stability interval and assigns the most appropriate 2-/3-stage integrator for any user-chosen simulation step size within that interval. s-AIA has been implemented in the in-house software package HaiCS without introducing computational overheads in the simulations. The efficiency of the s-AIA integrators and their impact on the HMC accuracy, sampling performance and convergence are discussed in compari-\\n\\n\\\\textsuperscript{*}Corresponding author:\\nEmail address: lnagar@bcamath.org (Lorenzo Nagar)\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Adaptive multi-stage integration schemes for Hamiltonian Monte Carlo\\n\\nLorenzo Nagar\\\\textsuperscript{a,}\\\\textsuperscript{*}, Mario Fern\u00e1ndez-Pend\u00e1s\\\\textsuperscript{b}, Jes\u00fas Mar\u00eda Sanz-Serna\\\\textsuperscript{c}, Elena Akhmatskaya\\\\textsuperscript{a,d}\\n\\n\\\\textsuperscript{a}BCAM - Basque Center for Applied Mathematics, Alameda de Mazarredo 14, 48009 Bilbao, Spain\\n\\\\textsuperscript{b}DIPC, Donostia International Physics Center, Manuel Lardizabal Ibilbidea 4, 20018 Donostia, Spain\\n\\\\textsuperscript{c}Departamento de Matem\u00e1ticas, Universidad Carlos III de Madrid, Avenida Universidad 30, 28911 Legan\u00e9s, Spain\\n\\\\textsuperscript{d}Ikerbasque Basque Foundation for Science, Euskadi Plaza 5, 48009 Bilbao, Spain\\n\\nAbstract\\n\\nHamiltonian Monte Carlo (HMC) is a powerful tool for Bayesian statistical inference due to its potential to rapidly explore high dimensional state space, avoiding the random walk behavior typical of many Markov Chain Monte Carlo samplers. The proper choice of the integrator of the Hamiltonian dynamics is key to the efficiency of HMC. It is becoming increasingly clear that multi-stage splitting integrators are a good alternative to the Verlet method, traditionally used in HMC. Here we propose a principled way of finding optimal, problem-specific integration schemes (in terms of the best conservation of energy for harmonic forces/Gaussian targets) within the families of 2- and 3-stage splitting integrators. The method, which we call Adaptive Integration Approach for statistics, or s-AIA, uses a multivariate Gaussian model and simulation data obtained at the HMC burn-in stage to identify a system-specific dimensional stability interval and assigns the most appropriate 2-/3-stage integrator for any user-chosen simulation step size within that interval. s-AIA has been implemented in the in-house software package HaiCS without introducing computational overheads in the simula-\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and the references there for some of the algorithmic developments. A particular feature of numerical approximations of PDE solutions based on DNNs as approximation architectures that was observed in practice was the apparent insensitivity of the DNN approximation quality to the so-called \u201ccurse of dimensionality\u201d (CoD for short). This is particularly relevant for approximating maps\\n\\n\\\\[ \\\\mathcal{G} : \\\\mathcal{X} \\\\to \\\\mathcal{Y} \\\\]  \\n\\nbetween (in general, infinite-dimensional) separable Hilbert spaces \\\\( \\\\mathcal{X} \\\\) and \\\\( \\\\mathcal{Y} \\\\). Operators \\\\( \\\\mathcal{G} \\\\) as in (1) emerge for example as parameter-to-solution mappings for parametric PDEs within the field of Uncertainty Quantification (see, e.g., [48] and the references there), or in so-called digital twins of complex, physical systems governed by partial differential equations (PDEs) (see [32] and the references there). Owing to the infinite dimension of \\\\( \\\\mathcal{X} \\\\) and \\\\( \\\\mathcal{Y} \\\\) in (1), efficient numerical approximations of maps \\\\( \\\\mathcal{G} \\\\) are to overcome the CoD.\\n\\nSeveral (intrinsically different) mechanisms for overcoming the CoD in DNN emulations have been identified and mathematically justified recently. This includes the seminal work of A. Barron [3], Monte-Carlo path simulation type arguments (e.g. [20, 29] and the references there), and the emulation of sparse (generalized) polynomial chaos expansions (e.g. [2, 17]) by DNNs (e.g. [56, 51, 57]).\\n\\nSpecifically, in [56, 51, 57], a parametric representation of inputs \\\\( x \\\\in \\\\mathcal{X} \\\\) of \\\\( \\\\mathcal{G} \\\\) was used to prove DNN emulation rates for approximating \\\\( \\\\mathcal{G} \\\\). The construction used DNNs whose depth scales polylogarithmic in the parameter dimension, and polynomially in the DNN expression accuracy (i.e., emulation fidelity). Key in the proofs of these results is the holomorphic dependence of \\\\( \\\\mathcal{G}(x) \\\\) on the input \\\\( x \\\\). The related DNN emulation results were obtained with sparsely connected, deep feedforward NNs with ReLU or smooth (e.g. sigmoidal or \\\\( \\\\tanh(\\\\cdot) \\\\)) activation. DNN emulation rate results that are free from the CoD for low regularity maps \\\\( \\\\mathcal{G} \\\\) between function spaces were obtained e.g. using the so-called Feynman-Kac representation of solutions of Kolmogorov PDEs in (jump-)diffusion models. These results used ReLU DNNs of moderate depth [20, 29], but the error bounds hold in a mean-square sense or only with high probability.\\n\\nWhile quantified, parametric holomorphy of solution families of parametric PDEs has been verified in many settings (particularly in elliptic and parabolic PDEs, e.g. [27, 66, 31, 12, 23]), there are broad classes of applications where relevant maps are H\u00f6lder or Lipschitz, but not holomorphic. One purpose of the present paper is to obtain mean-square DNN expression rate bounds for Operator Network (ONet) emulations with architecture (2) below, of Lipschitz (and, more generally, H\u00f6lder smooth) maps \\\\( \\\\mathcal{G} \\\\) between separable Hilbert spaces.\\n\\n### 1.1 Previous work for operator networks\\n\\nA rather recent line of research uses so-called Operator Networks to emulate the possibly nonlinear input-output map \\\\( \\\\mathcal{G} \\\\), such as for example the coefficient-to-solution map in linear, elliptic divergence form PDEs of second order. A variety of DNN architectures has been put forward recently with the aim of efficient operator emulation, with distinct architectures tailored to the emulation of particular operators. A number of acronyms labelling these DNN classes has been coined (\u201cdeepONets\u201d [45], Fourier Neural Operators \u201cFNOs\u201d [35, 41], UNet architectures combined with FNOs \u201cU-FNOs\u201d [62], encoders based on transformers, etc.). We refer to [35, 38, 21, 49, 42, 6] and the references there.\\n\\nIn this paper, we discuss an architecture that belongs to the same general category as those proposed in, for instance, [25, 45, 24]. It reduces the task of approximating \\\\( \\\\mathcal{G} \\\\) to that of emulating (components of) countably-parametric maps \\\\( \\\\mathcal{G} : \\\\ell^2(\\\\mathbb{N}) \\\\to \\\\ell^2(\\\\mathbb{N}) \\\\) with DNNs: using an appropriate encoder \\\\( \\\\mathcal{E}_X : \\\\mathcal{X} \\\\to \\\\ell^2(\\\\mathbb{N}) \\\\) and decoder \\\\( \\\\mathcal{D}_Y : \\\\ell^2(\\\\mathbb{N}) \\\\to \\\\mathcal{Y} \\\\), the map \\\\( \\\\mathcal{G} \\\\) in (1) allows the structural representation\\n\\n\\\\[ \\\\mathcal{G} = \\\\mathcal{D}_Y \\\\circ \\\\mathcal{G} \\\\circ \\\\mathcal{E}_X. \\\\]  \\n\\n\\\\(^1\\\\)More generally, separably-valued maps \\\\( \\\\mathcal{G} \\\\) into an otherwise nonseparable target space \\\\( \\\\mathcal{Y} \\\\) may be considered. In [35, Section 9, App. B] additional conditions on separable Banach spaces \\\\( \\\\mathcal{X} \\\\) and \\\\( \\\\mathcal{Y} \\\\) necessary to extend the present arguments to this more general setting are discussed.\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and the references there for some of the algorithmic developments. A particular feature of numerical approximations of PDE solutions based on DNNs as approximation architectures that was observed in practice was the apparent insensitivity of the DNN approximation quality to the so-called \u201ccurse of dimensionality\u201d (CoD for short). This is particularly relevant for approximating maps\\n\\n\\\\[ \\\\mathcal{G} : \\\\mathcal{X} \\\\to \\\\mathcal{Y} \\\\]  \\n\\nbetween (in general, infinite-dimensional) separable Hilbert spaces \\\\( \\\\mathcal{X} \\\\) and \\\\( \\\\mathcal{Y} \\\\). Operators \\\\( \\\\mathcal{G} \\\\) as in (1) emerge for example as parameter More generally, separably-valued maps \\\\( \\\\mathcal{G} \\\\) into an otherwise nonse emerge for example as parameter-to-solution mappings for parametric PDEs within the field of Uncertainty Quantification (see, e.g., [48] and the references there), or in so-called digital twins of complex, physical systems governed by partial differential equations (PDEs) (see [32] and the references there). Owing to the infinite dimension of \\\\( \\\\mathcal{X} \\\\) and \\\\( \\\\mathcal{Y} \\\\) in (1), efficient numerical approximations of maps \\\\( \\\\mathcal{G} \\\\) are to overcome the CoD.\\n\\nSeveral (intrinsically different) mechanisms for overcoming the CoD in DNN emulations have been identified and mathematically justified recently. This includes the seminal work of A. Barron [3], Monte-Carlo path simulation type arguments (e.g. [20, 29] and the references there), and the emulation of sparse (generalized) polynomial chaos expansions (e.g. [2, 17]) by DNNs (e.g. [56, 51, 57]).\\n\\nSpecifically, in [56, 51, 57], a parametric representation of inputs \\\\( x \\\\in \\\\mathcal{X} \\\\) of \\\\( \\\\mathcal{G} \\\\) was used to prove DNN emulation rates for approximating \\\\( \\\\mathcal{G} \\\\). The construction used DNNs whose depth scales polylogarithmic in the parameter dimension, and polynomially in the DNN expression accuracy (i.e., emulation fidelity). Key in the proofs of these results is the holomorphic dependence of \\\\( \\\\mathcal{G}(x) \\\\) on the input \\\\( x \\\\). The related DNN emulation results were obtained with sparsely connected, deep feedforward NNs with ReLU or smooth (e.g. sigmoidal or \\\\( \\\\tanh(\\\\cdot) \\\\)) activation. DNN emulation rate results that are free from the CoD for low regularity maps \\\\( \\\\mathcal{G} \\\\) between function spaces were obtained e.g. using the so-called Feynman-Kac representation of solutions of Kolmogorov PDEs in (jump-)diffusion models. These results used ReLU DNNs of moderate depth [20, 29], but the error bounds hold in a mean-square sense or only with high probability.\\n\\nWhile quantified, parametric holomorphy of solution families of parametric PDEs has been verified in many settings (particularly in elliptic and parabolic PDEs, e.g. [27, 66, 31, 12, 23]), there are broad classes of applications where relevant maps are H\u00f6lder or Lipschitz, but not holomorphic. One purpose of the present paper is to obtain mean-square DNN expression rate bounds for Operator Network (ONet) emulations with architecture (2) below, of Lipschitz (and, more generally, H\u00f6lder smooth) maps \\\\( \\\\mathcal{G} \\\\) between separable Hilbert spaces.\\n\\n### 1.1 Previous work for operator networks\\n\\nA rather recent line of research uses so-called Operator Networks to emulate the possibly nonlinear input-output map \\\\( \\\\mathcal{G} \\\\), such as for example the coefficient-to-solution map in linear, elliptic divergence form PDEs of second order. A variety of DNN architectures has been put forward recently with the aim of efficient operator emulation, with distinct architectures tailored to the emulation of particular operators. A number of acronyms labelling these DNN classes has been coined (\u201cdeepONets\u201d [45], Fourier Neural Operators \u201cFNOs\u201d [35, 41], UNet architectures combined with FNOs \u201cU-FNOs\u201d [62], encoders based on transformers, etc.). We refer to [35, 38, 21, 49, 42, 6] and the references there.\\n\\nIn this paper, we discuss an architecture that belongs to the same general category as those proposed in, for instance, [25, 45, 24]. It reduces the task of approximating \\\\( \\\\mathcal{G} \\\\) to that of emulating (components of) countably-parametric maps \\\\( \\\\mathcal{G} : \\\\ell^2(\\\\mathbb{N}) \\\\to \\\\ell^2(\\\\mathbb{N}) \\\\) with DNNs: using an appropriate encoder \\\\( \\\\mathcal{E}_X : \\\\mathcal{X} \\\\to \\\\ell^2(\\\\mathbb{N}) \\\\) and decoder \\\\( \\\\mathcal{D}_Y : \\\\ell^2(\\\\mathbb{N}) \\\\to \\\\mathcal{Y} \\\\), the map \\\\( \\\\mathcal{G} \\\\) in (1) allows the structural representation\\n\\n\\\\[ \\\\mathcal{G} = \\\\mathcal{D}_Y \\\\circ \\\\mathcal{G} \\\\circ \\\\mathcal{E}_X. \\\\]\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Let \\\\( y \\\\in W_4 \\\\). Then \\\\( y = [b_k, d_0, d_1, d_2] \\\\) for \\\\( k = 0 \\\\) or \\\\( 1 \\\\). The corresponding equation in (23) reads\\n\\n\\\\[\\nt_0^{b_k, d_0, d_1, d_2} a_{[b_k, d_0]} a_{[d_1, d_2]} + t_1^{b_k, d_0, d_1, d_2} a_{[b_k, d_1]} a_{[d_0, d_2]} + t_2^{b_k, d_0, d_1, d_2} a_{[b_k, d_2]} a_{[d_0, d_1]} = 0\\n\\\\]\\n\\n\\\\[\\nt_0 = \\\\begin{cases} \\n-1 & \\\\text{if } d_0 < d_1 < b_k < d_2 \\\\\\\\\\n+1 & \\\\text{otherwise}\\n\\\\end{cases}\\n\\\\]\\n\\n\\\\[\\nt_1 = \\\\begin{cases} \\n-1 & \\\\text{if } b_k < d_0 < d_1 < d_2 \\\\text{ or } d_0 < d_1 < d_2 < b_k \\\\\\\\\\n+1 & \\\\text{otherwise}\\n\\\\end{cases}\\n\\\\]\\n\\n\\\\[\\nt_2 = \\\\begin{cases} \\n-1 & \\\\text{if } d_0 < b_k < d_1 < d_2 \\\\\\\\\\n+1 & \\\\text{otherwise}\\n\\\\end{cases}\\n\\\\]\\n\\nEach of \\\\( a_{[d_1, d_2]}, a_{[d_0, d_2]} \\\\) & \\\\( a_{[d_0, d_1]} \\\\) are fixed by Equation (24). Substituting in, (multiplying out \\\\( a_{[b_0, b_1]} \\\\)), the equation now reads\\n\\n\\\\[\\nt_0^{b_k, d_0, d_1, d_2} a_{[b_k, d_0]} (s_0^{b_0, b_1, d_1, d_2} a_{[b_0, d_1]} a_{[b_1, d_2]} + s_1^{b_0, b_1, d_1, d_2} a_{[b_0, d_2]} a_{[b_1, d_1]}) \\\\\\\\\\n+ t_1^{b_k, d_0, d_1, d_2} a_{[b_k, d_1]} (s_0^{b_0, b_1, d_0, d_2} a_{[b_0, d_0]} a_{[b_1, d_2]} + s_1^{b_0, b_1, d_0, d_2} a_{[b_0, d_2]} a_{[b_1, d_0]}) \\\\\\\\\\n+ t_2^{b_k, d_0, d_1, d_2} a_{[b_k, d_2]} (s_0^{b_0, b_1, d_0, d_1} a_{[b_0, d_0]} a_{[b_1, d_1]} + s_1^{b_0, b_1, d_0, d_1} a_{[b_0, d_1]} a_{[b_1, d_0]}) = 0 \\\\tag{25}\\n\\\\]\\n\\nLabelling the terms of Equation (25) in order 1 to 6, we can pair them off as follows. If \\\\( k = 0 \\\\), pair (1, 3), (2, 5) & (4, 6). If \\\\( k = 1 \\\\), instead pair (1, 6), (2, 4) & (3, 5). Then it can be shown that each pair sums to 0. For example, for \\\\( k = 0 \\\\), we have:\\n\\n\\\\[\\n(1) + (3) \\\\propto t_0^{b_0, d_0, d_1, d_2} s_0^{b_0, b_1, d_1, d_2} s_0^{b_0, b_1, d_1, d_2} + t_1^{b_0, d_0, d_1, d_2} s_0^{b_0, b_1, d_1, d_2} s_0^{b_0, b_1, d_1, d_2}\\n\\\\]\\n\\n\\\\[\\n(2) + (5) \\\\propto t_0^{b_0, d_0, d_1, d_2} s_0^{b_0, b_1, d_1, d_2} s_0^{b_0, b_1, d_1, d_2} + t_2^{b_0, d_0, d_1, d_2} s_0^{b_0, b_1, d_1, d_2} s_0^{b_0, b_1, d_1, d_2}\\n\\\\]\\n\\n\\\\[\\n(4) + (6) \\\\propto t_1^{b_0, d_0, d_1, d_2} s_1^{b_0, b_1, d_1, d_2} s_1^{b_0, b_1, d_1, d_2} + t_2^{b_0, d_0, d_1, d_2} s_1^{b_0, b_1, d_1, d_2} s_1^{b_0, b_1, d_1, d_2}\\n\\\\]\\n\\nFor example, note that \\\\( t_0 \\\\) and \\\\( t_1 \\\\) have the same sign only if \\\\( d_0 < b_0 < d_1 < d_2 \\\\). If this is the case, then \\\\( s_0^{b_0, b_1, d_1, d_2} \\\\) and \\\\( s_0^{b_0, b_1, d_1, d_2} \\\\) have different signs, as can be seen from checking the 3 possible relative locations of \\\\( b_1 \\\\). Thus \\\\((1) + (3) = 0\\\\). Similarly for the other pairs. The remaining cases can all be checked similarly.\\n\\nA similar argument holds for \\\\( y = [d_0, d_1, d_2, d_3] \\\\in W_6 \\\\).\\n\\n\\\\[\\\\square\\\\]\\n\\n## B An \\\\( \\\\epsilon \\\\)-net for the Gaussian states\\n\\nFor each \\\\( j \\\\in \\\\mathcal{A}_n \\\\), define the regions\\n\\n\\\\[\\nS_j = \\\\left\\\\{ |\\\\psi\\\\rangle = \\\\sum_{j' \\\\in \\\\mathcal{A}_n} c_{j'} |j'\\\\rangle \\\\in G : |c_j| \\\\geq |c_{j'}| \\\\quad \\\\forall j' \\\\in \\\\mathcal{A}_n \\\\right\\\\}\\n\\\\]\\n\\nClearly, the union over \\\\( j \\\\) gives all of \\\\( G \\\\). In each region \\\\( S_j \\\\), Gaussian states may be defined by the set of amplitudes with labels within distance 2 of \\\\( j \\\\), given by the set \\\\( C = \\\\{ c_k : d(j, k) \\\\leq 2 \\\\} \\\\). For these amplitudes, we have\\n\\n\\\\[\\n|c_j| \\\\in [2^{\\\\frac{1-n}{2}}, 1] \\\\\\\\\\n|c_k| \\\\in [0, |c_j|] \\\\quad \\\\forall c_k \\\\in C \\\\setminus \\\\{ c_j \\\\}\\n\\\\]\\n\\nLet \\\\( \\\\mathcal{N}_j = \\\\{ |\\\\psi_i\\\\rangle \\\\} \\\\subset S_j \\\\) be a maximal set of pure states satisfying\\n\\n\\\\[\\n\\\\forall |\\\\psi_i\\\\rangle, |\\\\psi_i'\\\\rangle \\\\in \\\\mathcal{N}_j, \\\\exists x \\\\in \\\\mathcal{A}_n \\\\text{ s.t. } d(j, x) \\\\leq 2 \\\\quad |\\\\langle x|\\\\psi_i\\\\rangle - \\\\langle x|\\\\psi_i'\\\\rangle| \\\\geq \\\\eta\\n\\\\]\\n\\nLet \\\\( \\\\mathcal{N} = \\\\cup_j \\\\mathcal{N}_j \\\\). We wish to find a suitable \\\\( \\\\eta \\\\) so that \\\\( \\\\mathcal{N} \\\\) is an \\\\( \\\\ "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Let $y \\\\in W_4$. Then $y = [b_k, d_0, d_1, d_2]$ for $k = 0$ or 1. The corresponding equation in (23) reads\\n\\n$$t_0 b_k, d_0, d_1, d_2 a_{[b_k, d_0]} a_{[d_1, d_2]} + t_1 b_k, d_0, d_1, d_2 a_{[b_k, d_1]} a_{[d_0, d_2]} + t_2 b_k, d_0, d_1, d_2 a_{[b_k, d_2]} a_{[d_0, d_1]} = 0$$\\n\\n$$t_0 = \\\\begin{cases} -1 & \\\\text{if } d_0 < d_1 < b_k < d_2 \\\\\\\\ +1 & \\\\text{otherwise} \\\\end{cases}$$\\n\\n$$t_1 = \\\\begin{cases} -1 & \\\\text{if } b_k < d_0 < d_1 < d_2 \\\\text{ or } d_0 < d_1 < d_2 < b_k \\\\\\\\ +1 & \\\\text{otherwise} \\\\end{cases}$$\\n\\n$$t_2 = \\\\begin{cases} -1 & \\\\text{if } d_0 < b_k < d_1 < d_2 \\\\\\\\ +1 & \\\\text{otherwise} \\\\end{cases}$$\\n\\nEach of $a_{[d_1, d_2]}, a_{[d_0, d_2]}$ & $a_{[d_0, d_1]}$ are fixed by Equation (24). Substituting in, (multiplying out $a_{[b_0, b_1]}$), the equation now reads\\n\\n$$t_0 b_k, d_0, d_1, d_2 a_{[b_k, d_0]} (s_0 b_k, b_1, d_1, d_2 a_{[b_0, d_1]} a_{[b_1, d_2]} + s_1 b_k, b_1, d_1, d_2 a_{[b_0, d_2]} a_{[b_1, d_1]})$$\\n\\n$$+ t_1 b_k, d_0, d_1, d_2 a_{[b_k, d_1]} (s_0 b_k, b_1, d_0, d_2 a_{[b_0, d_0]} a_{[b_1, d_2]} + s_1 b_k, b_1, d_0, d_2 a_{[b_0, d_2]} a_{[b_1, d_0]})$$\\n\\n$$+ t_2 b_k, d_0, d_1, d_2 a_{[b_k, d_2]} (s_0 b_k, b_1, d_0, d_1 a_{[b_0, d_0]} a_{[b_1, d_1]} + s_1 b_k, b_1, d_0, d_1 a_{[b_0, d_1]} a_{[b_1, d_0]}) = 0 \\\\quad (25)$$\\n\\nLabelling the terms of Equation ( 25 ) in order 1 to 6, we can pair them off as follows. If $k = 0$, pair (1, 3), (2, 5) & (4, 6). If $k = 1$, instead pair (1, 6), (2, 4) & (3, 5). Then it can be shown that each pair sums to 0. For example, for $k = 0$, we have:\\n\\n$$\\\\begin{align*}\\n(1) + (3) & \\\\propto t_0 b_0, d_0, d_1, d_2 s_0 b_0, b_1, d_1, d_2 + t_1 b_0, d_0, d_1, d_2 s_0 b_0, b_1, d_0, d_2 \\\\\\\\\\n(2) + (5) & \\\\propto t_0 b_0, d_0, d_1, d_2 s_0 b_0, b_1, d_1, d_2 + t_2 b_0, d_0, d_1, d_2 s_0 b_0, b_1, d_0, d_1 \\\\\\\\\\n(4) + (6) & \\\\propto t_1 b_0, d_0, d_1, d_2 s_1 b_0, b_1, d_0, d_2 + t_2 b_0, d_0, d_1, d_2 s_1 b_0, b_1, d_1, d_1\\n\\\\end{align*}$$\\n\\nFor example, note that $t_0$ and $t_1$ have the same sign only if $d_0 < b_0 < d_1 < d_2$. If this is the case, then $s_0 b_0, b_1, d_1, d_2$ and $s_0 b_0, b_1, d_0, d_2$ have different signs, as can be seen from checking the 3 possible relative locations of $b_1$. Thus $(1) + (3) = 0$. Similarly for the other pairs. The remaining cases can A similar argument holds for $y = [d_0, d_1, d_2, d_3] \\\\in W_6$.\\n\\n\\\\[ \\\\square \\\\]\\n\\nB An $\\\\epsilon$-net for the Gaussian states\\n\\nFor each $j \\\\in A_n$, define the regions\\n\\n$$S_j = \\\\left\\\\{ |\\\\psi\\\\rangle = \\\\sum_{j' \\\\in A_n} c_{j'} |j'\\\\rangle \\\\in G : |c_j| \\\\geq |c_{j'}| \\\\quad \\\\forall j' \\\\in A_n \\\\right\\\\}$$\\n\\nClearly, the union over $j$ gives all of $G$. In each region $S_j$, Gaussian states may be defined by the set of amplitudes with labels within distance 2 of $j$, given by the set $C = \\\\{ c_k : d(j, k) \\\\leq 2 \\\\}$. For these amplitudes, we have\\n\\n$$|c_j| \\\\in [2^{1-n}, 1]$$\\n\\n$$|c_k| \\\\in [0, |c_j|] \\\\quad \\\\forall c_k \\\\in C \\\\setminus \\\\{ c_j \\\\}$$\\n\\nLet $N_j = \\\\{ |\\\\psi_i\\\\rangle \\\\} \\\\subset S_j$ be a maximal set of pure states satisfying\\n\\n$$\\\\forall |\\\\psi_i\\\\rangle, |\\\\psi_i'\\\\rangle \\\\in N_j, \\\\exists x \\\\in A_n \\\\text{ s.t. } d(j, x) \\\\leq 2 \\\\quad |\\\\langle x|\\\\psi_i\\\\rangle - \\\\langle x|\\\\psi_i'\\\\rangle| \\\\geq \\\\eta$$\\n\\nLet $N = \\\\cup_j N_j$. We wish to find a suitable $\\\\eta$ so that $N$ is an $\\\\epsilon$-net. We require the 2 following lemmas:\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"one obtains from (3.75)\\n\\n\\\\[ y_0^{(2)} = 1 - k z_0(v^{(2)}) = 1 - \\\\frac{1}{2} k^{1/2} + \\\\frac{1}{4} k \\\\ln k^{-1} + O(k) \\\\]  \\n\\n(3.94)\\n\\nand due to (3.79)\\n\\n\\\\[ y_1^{(2)} = -k z_1(v^{(2)}) = -A(\\\\zeta_0) \\\\frac{k^{1/2}}{2 J_0} \\\\left( 1 + O(k^{1/2}) \\\\right) + O(k^{-5/2}). \\\\]  \\n\\n(3.95)\\n\\nThen, using the Taylor formula with respect to \\\\( \\\\mu \\\\), we shift initial conditions (3.92) to the point \\\\( y = y_0^{(2)} \\\\) as follows:\\n\\n\\\\[ v_0(y_0^{(2)}) = v_0^{(2)}, \\\\quad v_1(y_0^{(2)}) = -\\\\frac{dv_0}{dy}(y_0^{(2)}) \\\\cdot y_1^{(2)}, \\\\]\\n\\n\\\\[ \\\\zeta_1(y_0^{(2)}) = \\\\zeta_1^{(2)}, \\\\quad \\\\zeta_2(y_0^{(2)}) = \\\\zeta_2^{(2)} - \\\\frac{d\\\\zeta_1}{dy}(y_0^{(2)}) \\\\cdot y_1^{(2)}, \\\\]  \\n\\n(3.96)\\n\\n\\\\[ J_1(y_0^{(2)}) = J_1^{(2)}, \\\\quad J_2(y_0^{(2)}) = J_2^{(2)} - \\\\frac{dJ_1}{dy}(y_0^{(2)}) \\\\cdot y_1^{(2)}. \\\\]\\n\\nSubstituting (3.93) into (3.91) and expanding with respect to \\\\( \\\\mu \\\\), one obtains equations for the \\\\( i \\\\)-th approximations \\\\( \\\\zeta_i \\\\) and \\\\( J_i \\\\):\\n\\n\\\\[ \\\\frac{d\\\\zeta_i}{dy} = \\\\mathcal{R}_i^{(c)}(y), \\\\]  \\n\\n(3.97)\\n\\n\\\\[ \\\\frac{dJ_i}{dy} = \\\\mathcal{R}_i^{(j)}(y). \\\\]\\n\\nThe solution of this system is\\n\\n\\\\[ \\\\zeta_i(y) = \\\\zeta_i(y_0^{(2)}) + \\\\int_{y_0^{(2)}}^{y} \\\\mathcal{R}_i^{(c)}(s) \\\\, ds, \\\\]  \\n\\n(3.98)\\n\\n\\\\[ J_i(y) = J_i(y_0^{(2)}) + \\\\int_{y_0^{(2)}}^{y} \\\\mathcal{R}_i^{(j)}(s) \\\\, ds. \\\\]\\n\\nFor \\\\( i = 1 \\\\) one has\\n\\n\\\\[ \\\\mathcal{R}_1^{(c)}(y) = \\\\frac{f_3}{y^2 - 1 - k(v_0(y) - 1)}, \\\\]  \\n\\n(3.99)\\n\\n\\\\[ \\\\mathcal{R}_1^{(j)}(y) = J_0 k^{1/2} \\\\frac{f_4}{y^2 - 1 - k(v_0(y) - 1)} e^{-v_0(y)}. \\\\]\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"one obtains from (3.75)  \\n\\\\[ y_0^{(2)} = 1 - k z_0(v^{(2)}) = 1 - \\\\frac{1}{2} k^{1/2} + \\\\frac{1}{4} k \\\\ln k^{-1} + O(k) \\\\]  \\n(3.94)\\n\\nand due to (3.79)  \\n\\\\[ y_1^{(2)} = -k z_1(v^{(2)}) = -A(\\\\zeta_0) \\\\frac{k^{1/2}}{2 J_0} \\\\left( 1 + O(k^{1/2}) \\\\right) + O(k^{-5/2}). \\\\]  \\n(3.95)\\n\\nThen, using the Taylor formula with respect to \\\\( \\\\mu \\\\), we shift initial conditions (3.92) to the point \\\\( y = y_0^{(2)} \\\\) as follows:\\n\\n\\\\[ v_0(y_0^{(2)}) = v_0^{(2)}, \\\\quad v_1(y_0^{(2)}) = -\\\\frac{dv_0}{dy}(y_0^{(2)}) \\\\cdot y_1^{(2)}, \\\\]\\n\\\\[ \\\\zeta_1(y_0^{(2)}) = \\\\zeta_1^{(2)}, \\\\quad \\\\zeta_2(y_0^{(2)}) = \\\\zeta_2^{(2)} - \\\\frac{d\\\\zeta_1}{dy}(y_0^{(2)}) \\\\cdot y_1^{(2)}, \\\\]\\n\\\\[ J_1(y_0^{(2)}) = J_1^{(2)}, \\\\quad J_2(y_0^{(2)}) = J_2^{(2)} - \\\\frac{dJ_1}{dy}(y_0^{(2)}) \\\\cdot y_1^{(2)}. \\\\]  \\n(3.93) into (3.91) and expanding with respect to \\\\( \\\\mu \\\\), one obtains equations for the \\\\( i \\\\)-th approximations \\\\( \\\\zeta_i \\\\) and \\\\( J_i \\\\):\\n\\n\\\\[ \\\\frac{d\\\\zeta_i}{dy} = \\\\mathcal{R}_i^{(c)}(y), \\\\]\\n\\\\[ \\\\frac{dJ_i}{dy} = \\\\mathcal{R}_i^{(j)}(y). \\\\]  \\n(3.97)\\n\\nThe solution of this system is\\n\\n\\\\[ \\\\zeta_i(y) = \\\\zeta_i(y_0^{(2)}) + \\\\int_{y_0^{(2)}}^{y} \\\\mathcal{R}_i^{(c)}(s) \\\\, ds, \\\\]\\n\\\\[ J_i(y) = J_i(y_0^{(2)}) + \\\\int_{y_0^{(2)}}^{y} \\\\mathcal{R}_i^{(j)}(s) \\\\, ds. \\\\]  \\n(3.98)\\n\\nFor \\\\( i = 1 \\\\) one has\\n\\n\\\\[ \\\\mathcal{R}_1^{(c)}(y) = \\\\frac{f_3}{y^2 - 1 - k(v_0(y) - 1)}, \\\\]\\n\\\\[ \\\\mathcal{R}_1^{(j)}(y) = J_0 k^{1/2} \\\\frac{f_4}{y^2 - 1 - k(v_0(y) - 1)} e^{-v_0(y)}. \\\\]  \\n(3.99)\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Notice that at the optimal dual solution $\\\\lambda^{\\\\text{opt}}$ and $\\\\{\\\\mu_k^{\\\\text{opt}}\\\\}$, it must follow that $\\\\lambda^{\\\\text{opt}} > 0$ and $A(\\\\lambda^{\\\\text{opt}}, \\\\{\\\\mu_k^{\\\\text{opt}}\\\\})$ is of full rank (i.e., $\\\\text{rank}(A(\\\\lambda^{\\\\text{opt}}, \\\\{\\\\mu_k^{\\\\text{opt}}\\\\})) = N_t$), since otherwise, the maximum transmit power constraint in (22b) cannot be satisfied. Then, Proposition 1 follows directly from Lemma 3. This completes the proof.\\n\\n**APPENDIX B**\\n\\n**PROOF OF LEMMA 3**\\n\\nFirst, we have $\\\\text{tr}(A(\\\\lambda, \\\\{\\\\mu_k\\\\})S_x) = \\\\text{tr}(AU^H S_x U)$. Let $\\\\tilde{S}_x = U^H S_x U$. It is easy to figure out that $\\\\text{tr}(S_x^{-1}) = \\\\text{tr}(\\\\tilde{S}_x^{-1})$. Recall that $S_x$ is positive semi-definite. We denote $(\\\\tau_1, \\\\ldots, \\\\tau_{N_t})$ as the diagonal entries of $\\\\tilde{S}_x$ to be determined. Note that $\\\\text{tr}(A(\\\\lambda, \\\\{\\\\mu_k\\\\})S_x) = \\\\sum_{i=1}^{N} \\\\alpha_i \\\\tau_i$. Here, we introduce the following lemma to find the minimum of $\\\\text{tr}(\\\\tilde{S}_x^{-1})$ w.r.t. $(\\\\tau_1, \\\\ldots, \\\\tau_{N_t})$, for which the proof can be found in [43, Appendix A] and thus is omitted.\\n\\n**Lemma 5.** [43] For a positive semi-definite matrix $B_0 \\\\in \\\\mathbb{C}^{M \\\\times M}$, with $(m, n)$-th entry $a(m, n)$, it holds that\\n\\n$$\\\\text{tr}(B_0^{-1}) \\\\geq \\\\sum_{i=1}^{M} \\\\frac{1}{a(i, i)},$$\\n\\nwhere the equality holds if and only if $B_0$ is diagonal.\\n\\nHence, $\\\\tilde{S}_x$ must be diagonal and we obtain\\n\\n$$\\\\text{tr}(A(\\\\lambda, \\\\{\\\\mu_k\\\\})S_x) + \\\\text{tr}(S_x^{-1}) = \\\\text{tr}(\\\\Lambda \\\\tilde{S}_x) + \\\\text{tr}(\\\\tilde{S}_x^{-1}) = \\\\sum_{i=1}^{N} \\\\alpha_i \\\\tau_i + \\\\sum_{i=1}^{N_t} \\\\frac{1}{\\\\tau_i}.$$  \\\\hspace{1cm} (61)\\n\\nIn this case, when $N < N_t$, for $i > N$, $\\\\tau_i$ must approach infinity to achieve the minimum value 0, i.e., for $i > N$, $\\\\tau_i \\\\to +\\\\infty$. As for $i \\\\leq N$, by checking the first order differentiation, we have $\\\\tau_i = \\\\alpha_i^{-1/2}$. Then, we obtain $S_x^*$ as\\n\\n$$S_x^* = U \\\\Sigma U^H,$$\\n\\nwhere $\\\\Sigma = \\\\text{diag}(\\\\tau_1, \\\\ldots, \\\\tau_{N_t})$ with $\\\\tau_i \\\\to +\\\\infty$, for $i > N$, and $\\\\tau_i = \\\\alpha_i^{-1/2}$, for $i \\\\leq N$.\\n\\nWhen $A(\\\\lambda, \\\\{\\\\mu_k\\\\})$ is full-rank, we have\\n\\n$$\\\\text{tr}(A(\\\\lambda, \\\\{\\\\mu_k\\\\})S_x) + \\\\text{tr}(S_x^{-1}) = \\\\text{tr}(\\\\Lambda \\\\tilde{S}_x) + \\\\text{tr}(\\\\tilde{S}_x^{-1}) = \\\\sum_{i=1}^{N_t} \\\\alpha_i \\\\tau_i + \\\\sum_{i=1}^{N_t} \\\\frac{1}{\\\\tau_i}.$$  \\\\hspace{1cm} (63)\\n\\nFor any $i \\\\leq N_t$, we need to set $\\\\tau_i = \\\\alpha_i^{-1/2}$ to achieve the minimum value. Then, the $S_x^*$ is given by $S_x^* = U^H \\\\Sigma U$, where $\\\\Lambda = \\\\text{diag}(\\\\tau_1, \\\\ldots, \\\\tau_{N_t})$ with $\\\\tau_i = \\\\alpha_i^{-1/2}$, for $i \\\\leq N_t$. \\n\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Notice that at the optimal dual solution $\\\\lambda^{\\\\text{opt}}$ and $\\\\{\\\\mu_k^{\\\\text{opt}}\\\\}$, it must follow that $\\\\lambda^{\\\\text{opt}} > 0$ and $A(\\\\lambda^{\\\\text{opt}}, \\\\{\\\\mu_k^{\\\\text{opt}}\\\\})$ is of full rank (i.e., $\\\\text{rank}(A(\\\\lambda^{\\\\text{opt}}, \\\\{\\\\mu_k^{\\\\text{opt}}\\\\})) = N_t$), since otherwise, the maximum transmit power constraint in (22b) cannot be satisfied. Then, Proposition 1 follows directly from Lemma 3. This completes the proof. A PPENDIX B P ROOF OF L EMMA 3\\n\\nFirst, we have $\\\\text{tr}(A(\\\\lambda, \\\\{\\\\mu_k\\\\})S_x) = \\\\text{tr}(AU^H S_x U)$. Let $\\\\tilde{S}_x = U^H S_x U$. It is easy to figure out that $\\\\text{tr}(S_x^{-1}) = \\\\text{tr}(\\\\tilde{S}_x^{-1})$. Recall that $S_x$ is positive semi-definite. We denote $(\\\\tau_1, \\\\ldots, \\\\tau_{N_t})$ as the diagonal entries of $\\\\tilde{S}_x$ to be determined. Note that $\\\\text{tr}(A(\\\\lambda, \\\\{\\\\mu_k\\\\})S_x) = \\\\sum_{i=1}^{N} \\\\alpha_i \\\\tau_i$. Here, we introduce the following lemma to find the minimum of $\\\\text{tr}(\\\\tilde{S}_x^{-1})$ w.r.t. $(\\\\tau_1, \\\\ldots, \\\\tau_{N_t})$, for which the proof can be found in [43, Appendix A] and thus is omitted. \\n\\n**Lemma 5.** [43] For a positive semi-definite matrix $B_0 \\\\in \\\\mathbb{C}^{M \\\\times M}$, with $(m, n)$-th entry $a(m, n)$, it holds that\\n\\n$$\\\\text{tr}(B_0^{-1}) \\\\geq \\\\sum_{i=1}^{M} \\\\frac{1}{a(i, i)},$$\\n\\nwhere the equality holds if and only if $B_0$ is diagonal.\\n\\nHence, $\\\\tilde{S}_x$ must be diagonal and we obtain\\n\\n$$\\\\text{tr}(A(\\\\lambda, \\\\{\\\\mu_k\\\\})S_x) + \\\\text{tr}(S_x^{-1}) = \\\\text{tr}(\\\\Lambda \\\\tilde{S}_x) + \\\\text{tr}(\\\\tilde{S}_x^{-1}) = \\\\sum_{i=1}^{N} \\\\alpha_i \\\\tau_i + \\\\sum_{i=1}^{N_t} \\\\frac{1}{\\\\tau_i}.$$  \\\\hspace{1cm} (61)\\n\\nIn this case, when $N < N_t$, for $i > N$, $\\\\tau_i$ must approach infinity to achieve the minimum value 0, i.e., for $i > N$, $\\\\tau_i \\\\to +\\\\infty$. As for $i \\\\leq N$, by checking the first order differentiation, we have $\\\\tau_i = \\\\alpha_i^{-1/2}$. Then, we obtain $S_x^*$ as\\n\\n$$S_x^* = U \\\\Sigma U^H,$$\\n\\nwhere $\\\\Sigma = \\\\text{diag}(\\\\tau_1, \\\\ldots, \\\\tau_{N_t})$ with $\\\\tau_i \\\\to +\\\\infty$, for $i > N$, and $\\\\tau_i = \\\\alpha_i^{-1/2}$, for $i \\\\leq N$.\\n\\nWhen $A(\\\\lambda, \\\\{\\\\mu_k\\\\})$ is full-rank, we have\\n\\n$$\\\\text{tr}(A(\\\\lambda, \\\\{\\\\mu_k\\\\})S_x) + \\\\text{tr}(S_x^{-1}) = \\\\text{tr}(\\\\Lambda \\\\tilde{S}_x) + \\\\text{tr}(\\\\tilde{S}_x^{-1}) = \\\\sum_{i=1}^{N_t} \\\\alpha_i \\\\tau_i + \\\\sum_{i=1}^{N_t} \\\\frac{1}{\\\\tau_i}.$$  \\\\hspace{1cm} (63)\\n\\nFor any $i \\\\leq N_t$, we need to set $\\\\tau_i = \\\\alpha_i^{-1/2}$ to achieve the minimum value. Then, the $S_x^*$ is given by $S_x^* = U^H \\\\Sigma U$, where $\\\\Lambda = \\\\text{diag}(\\\\tau_1, \\\\ldots, \\\\tau_{N_t})$ with $\\\\tau_i = \\\\alpha_i^{-1/2}$, for $i \\\\leq N_t$. \\n\\n\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Proof. The proof is an adaptation of the proof of [Kye08, Lemma 4.6]. Since \\\\( \\\\omega_H^g(\\\\chi(u)) = \\\\tau_H^g(u) \\\\), \\\\( \\\\lambda_H^g \\\\circ \\\\chi \\\\) extends to an isometric embedding\\n\\n\\\\[\\nL^2(\\\\text{Rep}(G), \\\\tau_H^g) \\\\to l^2_g(\\\\hat{H}) \\\\cong L^2(\\\\mathcal{O}(G), \\\\omega_H^g).\\n\\\\]\\n\\nThe image \\\\( \\\\lambda_H^g \\\\circ \\\\chi(\\\\mathcal{O}(G)) \\\\) is a \\\\(*\\\\)-algebra that maps \\\\( L^2(\\\\text{Rep}(G), \\\\tau_H^g) \\\\) into itself and hence maps \\\\( L^2(\\\\text{Rep}(G), \\\\tau_H^g)^\\\\perp \\\\) into itself. Therefore \\\\( \\\\lambda_H^g(\\\\chi(u)) \\\\) has the form\\n\\n\\\\[\\n\\\\begin{bmatrix}\\n\\\\lambda_H^g(\\\\chi(u))|_{L^2(\\\\text{Rep}(G), \\\\tau_H^g)} & 0 \\\\\\\\\\n0 & \\\\lambda_H^g(\\\\chi(u))|_{L^2(\\\\text{Rep}(G), \\\\tau_H^g)^\\\\perp}\\n\\\\end{bmatrix}.\\n\\\\]\\n\\nHence\\n\\n\\\\[\\n||\\\\lambda_H^g(\\\\chi(u))|| = \\\\max\\\\{||\\\\lambda_H^g(\\\\chi(u))|_{L^2(\\\\text{Rep}(G), \\\\tau_H^g)}||, ||\\\\lambda_H^g(\\\\chi(u))|_{L^2(\\\\text{Rep}(G), \\\\tau_H^g)^\\\\perp}||\\\\}\\n\\\\geq ||\\\\lambda_H^g(\\\\chi(u))|_{L^2(\\\\text{Rep}(G), \\\\tau_H^g)}||\\n= ||\\\\pi_{\\\\tau_H^g}(u)||.\\n\\\\]\\n\\nThis proves that the map\\n\\n\\\\[\\n\\\\kappa : \\\\lambda_H^g \\\\circ \\\\chi(\\\\mathcal{O}(G)) \\\\to \\\\pi_{\\\\tau_H^g}(\\\\mathbb{C}[\\\\text{Rep}(G)]), \\\\lambda_H^g(\\\\chi(u)) \\\\mapsto \\\\pi_{\\\\tau_H^g}(u)\\n\\\\]\\n\\nis bounded and therefore extends to a contractive \\\\(*\\\\)-homomorphism\\n\\n\\\\[\\n\\\\kappa : \\\\overline{\\\\lambda_H^g \\\\circ \\\\chi(\\\\mathcal{O}(G))} \\\\to C_{\\\\tau_H^g}^*(\\\\text{Rep}(G)).\\n\\\\]\\n\\nTo finish the proof, we claim \\\\( \\\\kappa \\\\) is injective. This easily follows from the observation\\n\\n\\\\[\\n\\\\omega_H^g(\\\\chi(u)^* \\\\chi(v)) = \\\\omega_H^g(\\\\chi(\\\\bar{u} \\\\cdot v)) = \\\\tau_H^g(\\\\bar{u} \\\\cdot v).\\n\\\\]\\n\\n\\\\( \\\\square \\\\)\\n\\nFor \\\\( \\\\alpha \\\\in \\\\text{Irr}(H) \\\\) let \\\\( P_\\\\alpha \\\\in l^\\\\infty(\\\\hat{H}) \\\\) denote the orthogonal projection onto \\\\( H_\\\\alpha \\\\) which is nothing more than the identity operator in \\\\( M_{n_\\\\alpha} \\\\subset l^\\\\infty(\\\\hat{H}) \\\\). It is easily observed that the map \\\\( \\\\alpha \\\\mapsto P_\\\\alpha \\\\) induces a unitary isomorphism\\n\\n\\\\[\\nl^2(\\\\text{Irr}(H)) \\\\cong \\\\text{span}\\\\{\\\\eta_H(P_\\\\alpha) : \\\\alpha \\\\in \\\\text{Irr}(H)\\\\} \\\\subset l^2_g(\\\\hat{H}).\\n\\\\]\\n\\nFrom this and the duality it is also easy to check that\\n\\n\\\\[\\nL^2(\\\\mathcal{O}(G), \\\\omega_H^g) = \\\\text{span}\\\\{\\\\eta_H(q_H(\\\\chi(u))) : u \\\\in \\\\text{Rep}(G)\\\\} \\\\subset l^2_g(\\\\text{Irr}(H))\\n\\\\]\\n\\n27\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Proof. The proof is an adaptation of the proof of [ Kye08 , Lemma 4.6]. Since \\\\( \\\\omega_H^g(\\\\chi(u)) = \\\\tau_H^g(u) \\\\), \\\\( \\\\lambda_H^g \\\\circ \\\\chi \\\\) extends to an isometric embedding\\n\\n\\\\[\\nL^2(\\\\text{Rep}(G), \\\\tau_H^g) \\\\to l^2_g(\\\\hat{H}) \\\\cong L^2(\\\\mathcal{O}(G), \\\\omega_H^g).\\n\\\\]\\n\\nThe image \\\\( \\\\lambda_H^g \\\\circ \\\\chi(\\\\mathcal{O}(G)) \\\\) is a \\\\(*\\\\)-algebra that maps \\\\( L^2(\\\\text{Rep}(G), \\\\tau_H^g) \\\\) into itself and hence maps \\\\( L^2(\\\\text{Rep}(G), \\\\tau_H^g)^\\\\perp \\\\) into itself. Therefore \\\\( \\\\lambda_H^g(\\\\chi(u)) \\\\) has the form\\n\\n\\\\[\\n\\\\begin{bmatrix}\\n\\\\lambda_H^g(\\\\chi(u))|_{L^2(\\\\text{Rep}(G), \\\\tau_H^g)} & 0 \\\\\\\\\\n0 & \\\\lambda_H^g(\\\\chi(u))|_{L^2(\\\\text{Rep}(G), \\\\tau_H^g)^\\\\perp}\\n\\\\end{bmatrix}.\\n\\\\]\\n\\nHence\\n\\n\\\\[\\n||\\\\lambda_H^g(\\\\chi(u))|| = \\\\max\\\\{||\\\\lambda_H^g(\\\\chi(u))|_{L^2(\\\\text{Rep}(G), \\\\tau_H^g)}||, ||\\\\lambda_H^g(\\\\chi(u))|_{L^2(\\\\text{Rep}(G), \\\\tau_H^g)^\\\\perp}||\\\\}\\n\\\\geq ||\\\\lambda_H^g(\\\\chi(u))|_{L^2(\\\\text{Rep}(G), \\\\tau_H^g)}||\\n= ||\\\\pi_{\\\\tau_H^g}(u)||.\\n\\\\]\\n\\nThis proves that the map\\n\\n\\\\[\\n\\\\kappa : \\\\lambda_H^g \\\\circ \\\\chi(\\\\mathcal{O}(G)) \\\\to \\\\pi_{\\\\tau_H^g}(\\\\mathbb{C}[\\\\text{Rep}(G)]), \\\\lambda_H^g(\\\\chi(u)) \\\\mapsto \\\\pi_{\\\\tau_H^g}(u)\\n\\\\]\\n\\nis bounded and therefore extends to a contractive \\\\(*\\\\)-homomorphism\\n\\n\\\\[\\n\\\\kappa : \\\\overline{\\\\lambda_H^g \\\\circ \\\\chi(\\\\mathcal{O}(G))} \\\\to C_{\\\\tau_H^g}^*(\\\\text{Rep}(G)).\\n\\\\]\\n\\nTo finish the proof, we claim \\\\( \\\\kappa \\\\) is injective. This easily follows from the observation\\n\\n\\\\[\\n\\\\omega_H^g(\\\\chi(u)^* \\\\chi(v)) = \\\\omega_H^g(\\\\chi(\\\\bar{u} \\\\cdot v)) = \\\\tau_H^g(\\\\bar{u} \\\\cdot v).\\n\\\\]\\n\\n\\\\( \\\\square \\\\)\\n\\nFor \\\\( \\\\alpha \\\\in \\\\text{Irr}(H) \\\\) let \\\\( P_\\\\alpha \\\\in l^\\\\infty(\\\\hat{H}) \\\\) denote the orthogonal projection onto \\\\( H_\\\\alpha \\\\) which is nothing more than the identity operator in \\\\( M_{n_\\\\alpha} \\\\subset l^\\\\infty(\\\\hat{H}) \\\\). It is easily observed that the map \\\\( \\\\alpha \\\\mapsto P_\\\\alpha \\\\) induces a unitary isomorphism\\n\\n\\\\[\\nl^2(\\\\text{Irr}(H)) \\\\cong \\\\text{span}\\\\{\\\\eta_H(P_\\\\alpha) : \\\\alpha \\\\in \\\\text{Irr}(H)\\\\} \\\\subset l^2_g(\\\\hat{H}).\\n\\\\]\\n\\nFrom this and the duality it is also easy to check that\\n\\n\\\\[\\nL^2(\\\\mathcal{O}(G), \\\\omega_H^g) = \\\\text{span}\\\\{\\\\eta_H(q_H(\\\\chi(u))) : u \\\\in \\\\text{Rep}(G)\\\\} \\\\subset l^2_g(\\\\text{Irr}(H))\\n\\\\]\\n\\n27\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Quantitative analysis of optimal Sobolev-Lorentz embeddings with \\\\( \\\\alpha \\\\)-homogeneous weights\\n\\nPETR GURKA, JAN LANG, ZDEN\u011aK MIHULA\\n\\nAbstract. This paper quantitatively investigates the structure of non-compactness of the optimal weighted Sobolev-Lorentz embedding with homogeneous weights in an open convex cone. We prove the optimal embedding in question and obtain the exact values of all injective strict \\\\( s \\\\)-numbers (in particular, the Bernstein numbers) of the embedding. Opposite to the earlier results in this direction, the non-compactness in this case does not occur uniformly over all sub-domains of the underlying domain. Despite that, we find an infinitely dimensional subspace restricted onto which the embedding is isomorphic, proving that the embedding is not strictly singular.\\n\\n1. Introduction\\n\\nIt is a truth generally acknowledged that Sobolev embeddings hold a prominent position in various areas of mathematics, making comprehensive understanding of their internal structure and behavior essential. One of their oft-studied aspects is their compactness and its quality. Quite often the quality of compactness is analyzed through the decay rate of different \\\\( s \\\\)-numbers. Various \\\\( s \\\\)-numbers are closely related to the spectral theory of the corresponding differential operators associated with Sobolev-type embeddings and provide estimates for the growth of their eigenvalues (see [14]). There is quite extensive literature in which the quality of compactness of Sobolev embeddings is investigated. However, significantly less attention has been devoted to studying the structure of non-compact Sobolev embeddings, where the measure of non-compactness may be related to the shape of the essential spectrum (see [12]).\\n\\nNaturally, there are several ways which Sobolev embeddings can become non-compact, such as:\\n\\n(a) when the underlying domain is unbounded (see [1], cf. [15]);\\n(b) when the boundary of the underlying domain is excessively irregular (see [20, 21, 25, 26]);\\n(c) when the target function norm is overly strong\u2014in other words, the target function space is too close to the optimal one (see [19, 24] and references therein).\\n\\nAmong these possibilities, the last one is particularly intriguing because it has not been explored quantitatively nearly as much as the others, despite the interest in optimal Sobolev embeddings (e.g., see [10] and references therein). Previous works investigating the case (c) (see [4, 18, 22, 23]) dealt with Sobolev embeddings that are non-compact.\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Quantitative analysis of optimal Sobolev-Lorentz embeddings with \\\\( \\\\alpha \\\\)-homogeneous weights\\n\\nPETR GURKA, JAN LANG, ZDEN\u011aK MIHULA\\n\\nAbstract. This paper quantitatively investigates the structure of non-compactness of the optimal weighted Sob Sobolev spaces, Sobolev-Lorentz embeddings, homogeneous weights, compactness, Bernstein numbers, measure of non-compactness. This research was supported by of the optimal weighted Sobolev-Lorentz embedding with homogeneous weights in an open convex cone. We prove the optimal embedding in question and obtain the exact values of all injective strict \\\\( s \\\\)-numbers (in particular, the Bernstein numbers) of the embedding. Opposite to the earlier results in this direction, the non-compactness in this case does not occur uniformly over all sub-domains of the underlying domain. Despite that, we find an infinitely dimensional subspace restricted onto which the embedding is isomorphic, proving that the embedding is not strictly singular. \\n\\n1. Introduction\\n\\nIt is a truth generally acknowledged that Sobolev embeddings hold a prominent position in various areas of mathematics, making comprehensive understanding of their internal structure and behavior essential. One of their oft-studied aspects is their compactness and its quality. Quite often the quality of compactness is analyzed through the decay rate of different \\\\( s \\\\)-numbers. Various \\\\( s \\\\)-numbers are closely related to the spectral theory of the corresponding differential operators associated with Sobolev-type embeddings and provide estimates for the growth of their eigenvalues (see [ 14 ]). There is quite extensive literature in which the quality of compactness of Sobolev embeddings is investigated. However, significantly less attention has been devoted to studying the structure of noncompact Sobolev embeddings, where the measure of non-compactness may be related to the shape of the essential spectrum (see [12]). Naturally, there are several ways which Sobolev embeddings can become non-compact, such as: (a) when the underlying domain is unbounded (see [1], cf. [15]); (b) when the boundary of the underlying domain is excessively irregular (see [ 20 21 25 26]; (c) when the target function norm is overly strong\u2014in other words, the target function space is too close to the optimal one (see [19, 24] and references therein). Among these possibilities, the last one is particularly intriguing because it has not been explored quantitatively nearly as much as the others, despite the interest in optimal Sobolev embeddings (e.g., see [ 10 ] and references therein). Previous works investigating the case (c) (see [ 4 18 22 23 ]) dealt with Sobolev embeddings that are non-compact.\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"which we call the limit equations of (1.2). Here unknown functions are the tangential velocity field \\\\( v \\\\) and the pressure \\\\( q \\\\). Also, \\\\( f \\\\) is a given external force. We write \\\\( P, \\\\nabla_T, \\\\text{div}_T, D_T(v), \\\\) and \\\\( \\\\nabla_T v \\\\) for the orthogonal projection onto the tangent plane of \\\\( \\\\Gamma \\\\), the tangential gradient, the surface divergence, the surface strain rate tensor, and the covariant derivative of \\\\( v \\\\) along itself, respectively. Also, \\\\( \\\\gamma^0 \\\\) and \\\\( \\\\gamma^1 \\\\) are nonnegative constants which stands for the friction coefficients. For details of notations, see Section 2. Note that, when \\\\( g \\\\equiv 1 \\\\) on \\\\( \\\\Gamma \\\\) and \\\\( \\\\gamma^0 = \\\\gamma^1 = 0 \\\\), the limit equations (1.4) reduce to the surface Navier\u2013Stokes equations with Boussinesq\u2013Scriven surface stress tensor (see [4, 52, 2])\\n\\n\\\\[\\n\\\\begin{align*}\\n-2\\\\nu P \\\\text{div}_T[D_T(v)] + \\\\nabla_T v + \\\\nabla_T q &= f & \\\\text{on } \\\\Gamma, \\\\\\\\\\n\\\\text{div}_T v &= 0 & \\\\text{on } \\\\Gamma.\\n\\\\end{align*}\\n\\\\]\\n\\nMoreover, the equations (1.5) are equivalent to the Navier\u2013Stokes equations on an abstract Riemannian manifold (see [14, 56, 10])\\n\\n\\\\[\\n\\\\begin{align*}\\n-\\\\nu \\\\{\\\\Delta_B v + \\\\text{Ric}(v)\\\\} + \\\\nabla_T v + \\\\nabla_T q &= f & \\\\text{on } \\\\Gamma, \\\\\\\\\\n\\\\text{div}_T v &= 0 & \\\\text{on } \\\\Gamma,\\n\\\\end{align*}\\n\\\\]\\n\\nwhere \\\\( \\\\Delta_B \\\\) is the Bochner Laplacian on \\\\( \\\\Gamma \\\\) and \\\\( \\\\text{Ric} \\\\) is the Ricci curvature of \\\\( \\\\Gamma \\\\) (see e.g. [34, Lemma C.11] for the equivalence of the above equations).\\n\\nIn the nonstationary setting, we rigorously derived the limit equations (1.4) from the bulk equations (1.2) by the thin-film limit in our previous work [34]. There we proved under suitable assumptions that, for an \\\\( L^2 \\\\)-strong solution \\\\( u^\\\\varepsilon \\\\) to the nonstationary Navier\u2013Stokes equations in \\\\( \\\\Omega_\\\\varepsilon \\\\), its average\\n\\n\\\\[\\nM u^\\\\varepsilon(y) = \\\\frac{1}{\\\\varepsilon g(y)} \\\\int_{\\\\varepsilon g_0(y)}^{\\\\varepsilon g_1(y)} u^\\\\varepsilon(y + r n(y)) \\\\, dr, \\\\quad y \\\\in \\\\Gamma\\n\\\\]\\n\\nconverges weakly to a tangential vector field \\\\( v \\\\) on \\\\( \\\\Gamma \\\\) in an appropriate function space as \\\\( \\\\varepsilon \\\\to 0 \\\\), and derived the nonstationary limit equations on \\\\( \\\\Gamma \\\\) by characterizing \\\\( v \\\\) as a unique \\\\( L^2 \\\\)-weak solution to the limit equations. We also obtained some estimates for the difference of \\\\( u^\\\\varepsilon \\\\) and \\\\( v \\\\) which show that \\\\( v \\\\) approximates \\\\( u^\\\\varepsilon \\\\) in the \\\\( L^2 \\\\) sense when \\\\( \\\\varepsilon \\\\) is small.\\n\\nAs in the nonstationary case [34], we can derive (1.4) from (1.2) by means of convergence of a solution and characterization of the limit, but the procedure is the same so we omit it here. In this paper, we focus on difference estimates for the solutions \\\\( u^\\\\varepsilon \\\\) to (1.2) and \\\\( v \\\\) to (1.4). Let us fix some notations and formally state our main results (see Section 2 for details). Let \\\\( \\\\mathcal{P}_\\\\varepsilon \\\\) be the orthogonal projection from \\\\( L^2(\\\\Omega_\\\\varepsilon)^3 \\\\) onto a function space \\\\( \\\\mathcal{H}_\\\\varepsilon \\\\) given in (2.14), which is the standard \\\\( L^2 \\\\) solenoidal space on \\\\( \\\\Omega_\\\\varepsilon \\\\) or its subspace. For a vector field \\\\( u \\\\) on \\\\( \\\\Omega_\\\\varepsilon \\\\), let \\\\( M_\\\\varepsilon u \\\\) be the tangential component of the average \\\\( Mu \\\\) on \\\\( \\\\Gamma \\\\). Let\\n\\n\\\\[\\nH^1(\\\\Gamma, TT) = \\\\{ v \\\\in H^1(\\\\Gamma)^3 \\\\mid v \\\\cdot n = 0 \\\\text{ on } \\\\Gamma \\\\}\\n\\\\]\\n\\nand \\\\( H^{-1}(\\\\Gamma, TT) \\\\) be the dual space of \\\\( H^1(\\\\Gamma, TT) \\\\). Formally speaking, our main results are as follows (see Theorems 2.5 and 2.6 for the precise statements).\\n\\n**Theorem 1.1.** Let \\\\( f^\\\\varepsilon \\\\in L^2(\\\\Omega_\\\\varepsilon)^3 \\\\), \\\\( f \\\\in H^{-1}(\\\\Gamma, TT) \\\\), and \\\\( u^\\\\varepsilon \\\\) and \\\\( v \\\\) be weak solutions to (1.2) and (1.4), respectively. Under suitable assumptions, suppose that there exist \\\\( c_1, c_2 > 0 \\\\) and \\\\( \\\\alpha \\\\in (0, 1] \\\\) independent of \\\\( \\\\varepsilon \\\\) such that\\n\\n\\\\[\\n\\\\| \\\\mathcal{P}_\\\\varepsilon f^\\\\varepsilon \\\\|_{L^2(\\\\Omega_\\\\varepsilon)} \\\\leq c_1 \\\\varepsilon^{-1+\\\\alpha}, \\\\quad \\\\| M_\\\\varepsilon \\\\mathcal{P}_\\\\varepsilon f^\\\\varepsilon \\\\|_{H^{-1}(\\\\Gamma, TT)} \\\\leq c_2\\n\\\\]\\n\\nfor all \\\\( \\\\varepsilon > 0 \\\\) sufficiently small. Then there exist \\\\( c, \\\\rho > 0 \\\\) independent of \\\\( \\\\varepsilon \\\\) such that\\n\\n\\\\[\\n\\\\| M_\\\\varepsilon u^\\\\varepsilon - v \\\\|_{H^1(\\\\Gamma)} \\\\leq c \\\\left( \\\\delta(\\\\varepsilon) + \\\\| M_\\\\varepsilon \\\\mathcal{P}_\\\\varepsilon f^\\\\varepsilon - f \\\\|_{H^{-1}(\\\\Gamma, TT)} \\\\right)\\n\\\\]\\n\\nfor all \\\\( \\\\varepsilon > 0 \\\\) sufficiently small provided that \\\\( \\\\| v \\\\|_{H^1(\\\\Gamma)} \\\\leq \\\\rho \\\\), where\\n\\n\\\\[\\n\\\\delta(\\\\varepsilon) = \\\\varepsilon^{\\\\alpha/4} + \\\\sum_{i=0}^{\\\\alpha} \\\\frac{\\\\gamma_i^4}{\\\\varepsilon} - \\\\gamma_i^4.\\n\\\\]\\n\\nMoreover, we have the following difference estimate in \\\\( \\\\Omega_\\\\varepsilon \\\\):\\n\\n\\\\[\\n\\\\varepsilon^{-1/2} \\\\| u^\\\\varepsilon - \\\\bar{v} \\\\|_{L^2(\\\\Omega_\\\\varepsilon)} \\\\leq \\\\varepsilon \\\\left( \\\\delta(\\\\varepsilon) + \\\\| M_\\\\varepsilon \\\\mathcal{P}_\\\\varepsilon f^\\\\varepsilon - f \\\\|_{H^{-1}(\\\\Gamma, TT)} \\\\right).\\n\\\\]\\n\\nHere \\\\( \\\\bar{v} \\\\) is the constant extension of \\\\( v \\\\) in the normal direction of \\\\( \\\\Gamma \\\\).\\n\\nNote that the left-hand side of (1.9) is divided by \\\\( \\\\varepsilon^{1/2} \\\\) since the \\\\( L^2(\\\\Omega_\\\\varepsilon) \\\\)-norm involves the square root of the thickness of \\\\( \\\\Omega_\\\\varepsilon \\\\). By (1.9), we can say that the solution \\\\( v \\\\) to the limit equations (1.4) approximates the solution \\\\( u^\\\\varepsilon \\\\) to the bulk equations (1.2) when \\\\( \\\\varepsilon \\\\) is small. We also have an approximation result for \\\\( \\\\nabla u^\\\\varepsilon \\\\) "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"which we call the limit equations of ( 1.2 ). Here unknown functions are the tangen\\ntional velocity field \\\\( v \\\\) and the pressure \\\\( q \\\\). Also, \\\\( f \\\\) is a given external force. We write \\\\( P, \\\\nabla_T, \\\\text{div}_T, D_T(v), \\\\) and \\\\( \\\\nabla_T v \\\\) for the orthogonal projection onto the tangent plane of \\\\( \\\\Gamma \\\\), the tangential gradient, the surface divergence, the surface strain rate tensor, and the covariant derivative of \\\\( v \\\\) along itself, respectively. Also, \\\\( \\\\gamma^0 \\\\) and \\\\( \\\\gamma^1 \\\\) are nonnegative constants which stands for the friction coefficients. For details of notations, see Section 2 . Note that, when \\\\( g \\\\equiv 1 \\\\) on \\\\( \\\\Gamma \\\\) and \\\\( \\\\gamma^0 = \\\\gamma^1 = 0 \\\\), the limit equations ( 1.4 ) reduce to the surface Navier\u2013Stokes equations with Boussinesq\u2013Scriven surface stress tensor (see [ 4 , 52 , 2 ]) \\n\\n\\\\[\\n\\\\begin{align*}\\n-2\\\\nu P \\\\text{div}_T[D_T(v)] + \\\\nabla_T v + \\\\nabla_T q &= f & \\\\text{on } \\\\Gamma, \\\\\\\\\\n\\\\text{div}_T v &= 0 & \\\\text{on } \\\\Gamma.\\n\\\\end{align*}\\n\\\\]\\n\\nMoreover, the equations ( 1.5 ) are equivalent to the Navier\u2013Stokes equations on an abstract Riemannian manifold (see [ 14 , 56 , 10 ]) \\n\\n\\\\[\\n\\\\begin{align*}\\n-\\\\nu \\\\{ \\\\Delta_B v + \\\\text{Ric}(v) \\\\} + \\\\nabla_T v + \\\\nabla_T q &= f & \\\\text{on } \\\\Gamma, \\\\\\\\\\n\\\\text{div}_T v &= 0 & \\\\text{on } \\\\Gamma,\\n\\\\end{align*}\\n\\\\]\\n\\nwhere \\\\( \\\\Delta_B \\\\) is the Bochner Laplacian on \\\\( \\\\Gamma \\\\) and \\\\( \\\\text{Ric} \\\\) is the Ricci curvature of \\\\( \\\\Gamma \\\\) (see e.g.  [ 34 , Lemma C.11] for the equivalence of the above equations). \\n\\nIn the nonstationary setting, we rigorously derived the limit equations ( 1.4 ) from the bulk equations ( 1.2 ) by the thin-film limit in our previous work [ 34 ]. There we proved under suitable assumptions that, for an \\\\( L^2 \\\\)-strong solution \\\\( u^\\\\varepsilon \\\\) to the nonstationary Navier\u2013Stokes equations in \\\\( \\\\Omega_\\\\varepsilon \\\\), its average \\n\\n\\\\[\\nM u^\\\\varepsilon(y) = \\\\frac{1}{\\\\varepsilon g(y)} \\\\int_{\\\\varepsilon g_0(y)}^{\\\\varepsilon g_1(y)} u^\\\\varepsilon(y + r n(y)) \\\\, dr, \\\\quad y \\\\in \\\\Gamma\\n\\\\]\\n\\nconverges weakly to a tangential vector field \\\\( v \\\\) on \\\\( \\\\Gamma \\\\) in an appropriate function space as \\\\( \\\\varepsilon \\\\to 0 \\\\), and derived the nonstationary limit equations on \\\\( \\\\Gamma \\\\) by characterizing \\\\( v \\\\) as a unique \\\\( L^2 \\\\)-weak solution to the limit equations. We also obtained some estimates for the difference of \\\\( u^\\\\varepsilon \\\\) and \\\\( v \\\\) which show that \\\\( v \\\\) approximates \\\\( u^\\\\varepsilon \\\\) in the \\\\( L^2 \\\\) sense when \\\\( \\\\varepsilon \\\\) is small. \\n\\nAs in the nonstationary case [ 34 ], we can derive ( 1.4 ) from ( 1.2 ) by means of convergence of a solution and characterization of the limit, but the procedure is the same so we omit it here. In this paper, we focus on difference estimates for the solutions \\\\( u^\\\\varepsilon \\\\) to ( 1.2 ) and \\\\( v \\\\) to ( 1.4 ). Let us fix some notations and formally state our main results (see Section 2 for details). Let \\\\( P_\\\\varepsilon \\\\) be the orthogonal projection from \\\\( L^2(\\\\Omega_\\\\varepsilon)^3 \\\\) onto a function space \\\\( \\\\mathcal{H}_\\\\varepsilon \\\\) given in ( 2.14 ), which is the standard \\\\( L^2 \\\\) solenoidal space on \\\\( \\\\Omega_\\\\varepsilon \\\\) or its subspace. For a vector field \\\\( u \\\\) on \\\\( \\\\Omega_\\\\varepsilon \\\\), let \\\\( M_\\\\varepsilon u \\\\) be the tangential component of the average \\\\( Mu \\\\) on \\\\( \\\\Gamma \\\\). Let \\n\\n\\\\[\\nH^1(\\\\Gamma, TT) = \\\\{ v \\\\in H^1(\\\\Gamma)^3 \\\\mid v \\\\cdot n = 0 \\\\text{ on } \\\\Gamma \\\\}\\n\\\\]\\n\\nand \\\\( H^{-1}(\\\\Gamma, TT) \\\\) be the dual space of \\\\( H^1(\\\\Gamma, TT) \\\\). Formally speaking, our main results are as follows (see Theorems 2.5 and 2.6 for the precise statements). \\n\\n**Theorem 1.1.** Let \\\\( f^\\\\varepsilon \\\\in L^2(\\\\Omega_\\\\varepsilon)^3 \\\\), \\\\( f \\\\in H^{-1}(\\\\Gamma, TT) \\\\), and \\\\( u^\\\\varepsilon \\\\) and \\\\( v \\\\) be weak solutions to ( 1.2 ) and ( 1.4 ), respectively. Under suitable assumptions, suppose that there exist \\\\( c_1, c_2 > 0 \\\\) and \\\\( \\\\alpha \\\\in (0, 1] \\\\) independent of \\\\( \\\\varepsilon \\\\) such that \\n\\n\\\\[\\n\\\\| P_\\\\varepsilon f^\\\\varepsilon \\\\|_{L^2(\\\\Omega_\\\\varepsilon)}^2 \\\\leq c_1 \\\\varepsilon^{-1+\\\\alpha}, \\\\quad \\\\| M_\\\\varepsilon P_\\\\varepsilon f^\\\\varepsilon \\\\|_{H^{-1}(\\\\Gamma, TT)}^2 \\\\leq c_2\\n\\\\]\\n\\nfor all \\\\( \\\\varepsilon > 0 \\\\) sufficiently small. Then there exist \\\\( c, \\\\rho > 0 \\\\) independent of \\\\( \\\\varepsilon \\\\) such that \\n\\n\\\\[\\n\\\\| M_\\\\varepsilon u^\\\\varepsilon - v \\\\|_{H^1(\\\\Gamma)} \\\\leq c \\\\left( \\\\delta(\\\\varepsilon) + \\\\| M_\\\\varepsilon P_\\\\varepsilon f^\\\\varepsilon - f \\\\|_{H^{-1}(\\\\Gamma, TT)} \\\\right)\\n\\\\]\\n\\nfor all \\\\( \\\\varepsilon > 0 \\\\) sufficiently small provided that \\\\( \\\\| v \\\\|_{H^1(\\\\Gamma)} \\\\leq \\\\rho \\\\), where \\n\\n\\\\[\\n\\\\delta(\\\\varepsilon) = \\\\varepsilon^{\\\\alpha/4} + \\\\sum_{i=0}^{\\\\alpha} \\\\frac{\\\\gamma_i^4}{\\\\varepsilon} - \\\\gamma_i^4.\\n\\\\]\\n\\nMoreover, we have the following difference estimate in \\\\( \\\\Omega_\\\\varepsilon \\\\): \\n\\n\\\\[\\n\\\\varepsilon^{-1/2} \\\\| u^\\\\varepsilon - \\\\bar{v} \\\\|_{L^2(\\\\Omega_\\\\varepsilon)} \\\\leq \\\\varepsilon \\\\left( \\\\delta(\\\\varepsilon) + \\\\| M_\\\\varepsilon P_\\\\varepsilon f^\\\\varepsilon - f \\\\|_{H^{-1}(\\\\Gamma, TT)} \\\\right).\\n\\\\]\\n\\nHere \\\\( \\\\bar{v} \\\\) is the constant extension of \\\\( v \\\\) in the normal direction of \\\\( \\\\Gamma \\\\). \\n\\nNote that the left-hand side of ( 1.9 ) is divided by \\\\( \\\\varepsilon^{1/2} \\\\) since the \\\\( L^2(\\\\Omega_\\\\varepsilon) \\\\)-norm involves the square root of the thickness of \\\\( \\\\Omega_\\\\varepsilon \\\\). By ( 1.9 ), we can say that the solution \\\\( v \\\\) to the limit equations ( 1.4 ) approximates the solution \\\\( u^\\\\varepsilon \\\\) to the bulk equations ( 1.2 ) when \\\\( \\\\varepsilon \\\\) is small. We also have "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"for all $\\\\tau \\\\in S_0$, where $E[\\\\cdot]$ denotes the expectation. In this context, the functions $\\\\tau \\\\in S_0$ are named weakly convex. Moreover, [TV97, Lemma 6] gives a weaker version of Theorem 3: Let $\\\\tau \\\\in S_0$. For $a, b \\\\in [0, \\\\infty)$ with $a \\\\geq b$, it was shown that $\\\\tau(a + b) + \\\\tau(a - b) \\\\leq 2\\\\tau(a) + 2\\\\tau(b)$.\\n\\n1.3.5 Statistics\\n\\nTheorem 1 can be applied to prove rates of convergence for certain kinds of means [Sch19]: We may want to calculate a mean value of some sample points in a metric spaces. One candidate for this is the Fr\u00e9chet mean [Fr\u00e948], also called barycenter. It is the (set of) minimizer(s) of the squared distance to the sample points. If $Y$ is a random variable with values in a metric space $(Q, d)$, the Fr\u00e9chet mean is $\\\\arg\\\\min_{q \\\\in Q} E[Y, q^2]$, where we assume $E[Y, q^2] < \\\\infty$ for all $q \\\\in Q$. Similarly, one can define the Fr\u00e9chet median [FVJ09] as $\\\\arg\\\\min_{q \\\\in Q} E[Y, q]$, or a more general $\\\\tau$-Fr\u00e9chet mean [Sch22] as $\\\\arg\\\\min_{q \\\\in Q} E[\\\\tau(Y, q)]$ for functions $\\\\tau : [0, \\\\infty) \\\\to \\\\mathbb{R}$. Given a sequence of independent random variables $Y_1, Y_2, \\\\ldots$ with the same distribution as $Y$, a standard task in statistics is to bound the distance between the sample statistics and its corresponding population version. In our case, assume the $\\\\tau$-Fr\u00e9chet mean is unique and define\\n\\n$$m := \\\\arg\\\\min_{q \\\\in Q} E[\\\\tau(Y, q)], \\\\quad \\\\hat{m}_n := \\\\arg\\\\min_{q \\\\in Q} \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\tau(Y_i, q).$$\\n\\nWe want to bound $\\\\overline{\\\\hat{m}_n}, m$ depending on $n$. One can employ quadruple inequalities such as (3) to obtain a suitable upper bound [Sch19, Theorem 1]. This approach is particularly useful, if we do not want to make the assumption that the diameter of the metric space $\\\\sup_{q, p \\\\in Q} d(q, p)$ is finite. With Theorem 1, one can obtain such a bound for $\\\\tau$-Fr\u00e9chet means with $\\\\tau \\\\in S$ (under some conditions). We emphasize that this is only possible with (3) and not with (8). Noteworthy examples of $\\\\tau \\\\in S$ in this context, aside from $\\\\tau = \\\\tau_\\\\alpha$, are the Huber loss $\\\\tau_{H, \\\\delta}$ [Hub64] and the Pseudo-Huber loss $\\\\tau_{pH, \\\\delta}$ [Cha+94] for $\\\\delta \\\\in (0, \\\\infty)$,\\n\\n$$\\\\tau_{H, \\\\delta}(x) := \\\\begin{cases} \\\\frac{1}{2}x^2 & \\\\text{for } x \\\\leq \\\\delta, \\\\\\\\ \\\\delta(x - \\\\frac{1}{2}\\\\delta) & \\\\text{for } x > \\\\delta, \\\\end{cases} \\\\quad \\\\tau_{pH, \\\\delta}(x) := \\\\delta^2 \\\\left( \\\\sqrt{1 + \\\\frac{x^2}{\\\\delta^2}} - 1 \\\\right),$$\\n\\nas well as $x \\\\mapsto \\\\ln(\\\\cosh(x))$ [Gre90]. These functions are of great interest in robust statistics and image processing as their respective minimizers combine properties of the classical mean ($\\\\tau_2$-Fr\u00e9chet mean) and the median ($\\\\tau_1$-Fr\u00e9chet mean).\\n\\n1.4 Outline\\n\\nIn the remaining sections, we first discuss the set $T$, i.e., the set of quadruple transformations, see section 2. We continue with a discussion of the set $S$, i.e., nondecreasing, convex functions with concave derivative, in section 3. Thereafter, we prove our main result, i.e., $S \\\\subseteq T$. The basic ideas of the proof and variations of the main result are presented in section 4. The technical details can be found in appendix B and C. The proof of Theorem 3 can be found in appendix A. In section 5 we discuss implications of the main results and open questions.\\n\\n2 Quadruple Transformations\\n\\nWe explore some properties of quadruple functions $\\\\tau \\\\in T$ and their quadruple constant $L^*_\\\\tau$.\\n\\n2.1 Properties\\n\\nLemma 4 (Constant functions).\\n\\n(i) For $c \\\\in \\\\mathbb{R}$, let $\\\\tau_c := (x \\\\mapsto c)$. Then $\\\\tau_c \\\\in T$ with $L^*_\\\\tau = 0$. \\n\\n6\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"for all $\\\\tau \\\\in S_0$, where $E[\\\\cdot]$ denotes the expectation. In this context, the functions $\\\\tau \\\\in S_0$ are named weakly convex. Moreover, [TV97, Lemma 6] gives a weaker version of Theorem 3: Let $\\\\tau \\\\in S_0$. For $a, b \\\\in [0, \\\\infty)$ with $a \\\\geq b$, it was shown that $\\\\tau(a + b) + \\\\tau(a - b) \\\\leq 2\\\\tau(a) + 2\\\\tau(b)$.\\n\\n1.3.5 Statistics\\n\\nTheorem 1 can be applied to prove rates of convergence for certain kinds of means [Sch19]: We may want to calculate a mean value of some sample points in a metric spaces. One candidate for this is the Fr\u00e9chet mean [Fr\u00b4e48], also called barycenter. It is the (set of) minimizer(s) of the squared distance to the sample points. If $Y$ is a random variable with values in a metric space $(Q, d)$, the Fr\u00e9chet mean is $\\\\arg\\\\min_{q \\\\in Q} E[Y, q^2]$, where we assume $E[Y, q^2] < \\\\infty$ for all $q \\\\in Q$. Similarly, one can define Fr\u00b4echet mean is unique and define\\n\\n$$m := \\\\arg\\\\min_{q \\\\in Q} E[Y, q], \\\\quad \\\\hat{m}_n := \\\\arg\\\\min_{q \\\\in Q} \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\tau(Y_i, q).$$\\n\\nWe want to bound $\\\\hat{m}_n, m$ depending on $n$. One can employ quadruple inequalities such as (3) to obtain a suitable upper bound [Sch19, Theorem 1]. This approach is particularly useful, if we do not want to make the assumption that the diameter of the metric space $\\\\sup_{q, p \\\\in Q} d(q, p)$ is finite. With Theorem 1, one can define the Fr\u00b4echet median [FVJ09] as $\\\\arg\\\\min_{q \\\\in Q} E[Y, q]$, or a more general $\\\\tau$-Fr\u00e9chet mean [Sch22] as $\\\\arg\\\\min_{q \\\\in Q} E[\\\\tau(Y, q)]$ for functions $\\\\tau: [0, \\\\infty) \\\\to \\\\mathbb{R}$. Given a sequence of independent random variables $Y_1, Y_2, \\\\ldots$ with the same distribution as $Y$, a standard task in statistics is to bound the distance between the sample statistics and its corresponding population version. In our case, assume the $\\\\tau$-Fr\u00e9chet mean is unique and define\\n\\n$$\\\\tau_{H, \\\\delta}(x) := \\\\begin{cases} \\\\frac{1}{2}x^2 & \\\\text{for } x \\\\leq \\\\delta, \\\\\\\\ \\\\delta(x - \\\\frac{1}{2}\\\\delta) & \\\\text{for } x > \\\\delta, \\\\end{cases} \\\\quad \\\\tau_{pH, \\\\delta}(x) := \\\\delta^2 \\\\left( \\\\sqrt{1 + \\\\frac{x^2}{\\\\delta^2}} - 1 \\\\right),$$\\n\\nas well as $x \\\\mapsto \\\\ln(\\\\cosh(x))$ [Gre90]. These functions are of great interest in robust statistics and image processing as their respective minimizers combine properties of the classical mean ($\\\\tau_2$-Fr\u00e9chet mean) and the median ($\\\\tau_1$-Fr\u00e9chet mean).\\n\\n1.4 Outline\\n\\nIn the remaining sections, we first discuss the set $T$, i.e., the set of quadruple transformations, see section 2. We continue with a discussion of the set $S$, i.e., nondecreasing, convex functions with concave derivative, in section 3. Thereafter, we prove our main result, i.e., $S \\\\subseteq T$. The basic ideas of the proof and variations of the main result are presented in section 4. The technical details can be found in appendix B and C. The proof of Theorem 1]. This approach is particularly useful, if we do not want to make the assumption that the diameter of the metric space $\\\\sup_{q, p \\\\in Q} d(q, p)$ is finite. With Theorem 1, one can obtain such a bound for $\\\\tau$-Fr\u00e9chet means with $\\\\tau \\\\in S$ (under some conditions). -Fr\u00b4echet means with $\\\\tau \\\\in S$ (under some conditions). We emphasize that this is only possible with (3) and not with (8). Noteworthy examples of $\\\\tau \\\\in S$ in this context, aside from $\\\\tau = \\\\tau_\\\\alpha$, are the Huber loss $\\\\tau_{H, \\\\delta}$ [Hub64] and the Pseudo-Huber loss $\\\\tau_{pH, \\\\delta}$ [Cha+94] for $\\\\delta \\\\in (0, \\\\infty)$,\\n\\n$$\\\\tau_{H, \\\\delta}(x) := \\\\begin{cases} \\\\frac{1}{2}x^2 & \\\\text{for } x \\\\leq \\\\delta, \\\\\\\\ \\\\delta(x - \\\\frac{1}{2}\\\\delta) & \\\\text{for } x > \\\\delta, \\\\end{cases} \\\\quad \\\\tau_{pH, \\\\delta}(x) := \\\\delta^2 \\\\left( \\\\sqrt{1 + \\\\frac{x^2}{\\\\delta^2}} - 1 \\\\right),$$\\n\\nas well as $x \\\\mapsto \\\\ln(\\\\cosh(x))$ [Gre90]. These functions are of great interest in robust statistics and image processing as their respective minimizers combine properties of the classical mean (\\\\tau_2-Fr\u00e9chet mean) and the median (\\\\tau_1-Fr\u00e9chet mean).\\n\\n2 Quadruple Transformations\\n\\nWe explore some properties of quadruple functions $\\\\tau \\\\in T$ and their quadruple constant $L^*_\\\\tau$.\\n\\n2.1 Properties\\n\\nLemma 4 (Constant functions).\\n\\n(i) For $c \\\\in \\\\mathbb{R}$, let $\\\\tau_c := (x \\\\mapsto c)$. Then $\\\\tau_c \\\\in T$ with $L^*_\\\\tau = 0$. \\n\\n6\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The above assumptions are fundamental to our approximate calculation, and a further mild technical assumption allows us to avoid tedious consideration of uninteresting cases:\\n\\n- The time scales of relaxation of flagellar force and orientation are comparable:\\n\\n\\\\[ \\\\gamma_F / \\\\gamma_\\\\Theta \\\\sim \\\\text{ord}(1). \\\\]  \\n\\n(25)\\n\\nIndeed, we find this condition satisfied by the parameters we inferred from experimental observations in Table 1.\\n\\nUnder the assumptions described above, we obtain the following approximation for the translational diffusivity:\\n\\n\\\\[\\n\\\\lim_{t \\\\to \\\\infty} \\\\frac{\\\\langle X^{(c)}(t) \\\\circ X^{(c)}(t) \\\\rangle}{2t} = D_t^*,\\n\\\\]\\n\\n\\\\[\\nD_t^* \\\\equiv D_t + \\\\frac{V^{*2}}{2} \\\\frac{D_r^*}{D_t^* + \\\\Omega_t^*} + \\\\tilde{D}_t\\n\\\\]\\n\\n(26)\\n\\nwhere \\\\( V^{*2} \\\\) is the mean-square velocity coarse-grained over the flagellar time scale (19), and\\n\\n\\\\[\\n\\\\tilde{D}_t \\\\equiv \\\\frac{\\\\alpha \\\\Omega_t^*}{2 \\\\gamma_t^2 \\\\gamma_t (\\\\Omega_t^* + D_t^*)^2} \\\\left[ \\\\frac{2 \\\\sigma_F^2}{\\\\gamma_F} \\\\sum_{j,j'=1}^N F_j^{(0)} F_{j'}^{(0)} \\\\cos(\\\\Theta_j^{(0)} - \\\\Theta_{j'}^{(0)}) \\\\cos(\\\\ "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The above assumptions are fundamental to our approximate calculation, and a further mild technical assumption allows us to avoid tedious consideration of uninteresting cases: \\n\\n- The time scales of relaxation of flagellar force and orientation are comparable: \\n\\n\\\\[ \\\\gamma_F / \\\\gamma_\\\\Theta \\\\sim \\\\text{ord}(1). \\\\]  \\n\\n(25)\\n\\nIndeed, we find this condition satisfied by the parameters we inferred from experimental observations in Table 1. \\n\\nUnder the assumptions described above, we obtain the following approximation for the translational diffusivity: \\n\\n\\\\[\\n\\\\lim_{t \\\\to \\\\infty} \\\\frac{\\\\langle X^{(c)}(t) \\\\otimes X^{(c)}(t) \\\\rangle}{2t} = D_t^*,\\n\\\\]\\n\\n\\\\[\\nD_t^* \\\\equiv D_t + \\\\frac{V^{*2}}{2} \\\\frac{D_r^*}{D_t^* + \\\\Omega_t^*} + \\\\tilde{D}_t\\n\\\\]\\n\\n(26)\\n\\nwhere \\\\( V^{*2} \\\\) is the mean-square velocity coarse-grained over the flagellar time scale (19), and \\n\\n\\\\[\\n\\\\tilde{D}_t \\\\equiv \\\\frac{\\\\alpha \\\\Omega_t^*}{2 \\\\gamma_t^2 \\\\gamma_\\\\Theta (\\\\Omega_t^* + D_t^*)^2} \\\\left[ \\\\frac{2 \\\\sigma_F^2}{\\\\gamma_F} \\\\sum_{j,j'=1}^N F_j^{(0)} F_{j'}^{(0)} \\\\cos(\\\\Theta_j^{(0)} - \\\\Theta_{j'}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"QUENCHED DECAY OF CORRELATIONS FOR RANDOM CONTRACTING LORENZ MAPS\\n\\nANDREW LARKIN AND MARKS RUZIBOEV\\n\\nAbstract. In this work we consider i.i.d. random perturbations of contracting Lorenz maps sufficiently close to a Rovella parameter. We prove that the quenched correlations of the random dynamical system decays exponentially.\\n\\n1. Introduction\\n\\nThe Lorenz system was introduced in [30] as a simplified model for atmospheric convection. Numerical simulations have shown that the Lorenz system admits a strange attractor, called the Lorenz attractor, which became one of the most iconic examples in the field.\\n\\nA rigorous mathematical approach was developed with the introduction of the so called geometric Lorenz flow by [1, 26], which mimicks simulation of the dynamics of the Lorenz flow, and which has a robust strange attractor under $C^1$ perturbations. Later in [37, 38] it was shown that the actual Lorenz attractor is indeed a singular hyperbolic attractor, further showing that the geometric Lorenz attractor represents the Lorenz attractor well. Moreover, it admits the so called Sinai-Ruelle-Bowen (SRB) measure, which is ergodic [11]. Its statistical properties, such as mixing rates, limit theorems and their stability under various perturbations, were studied intensively (see for example, [31, 10, 9, 7, 14, 13, 24]).\\n\\nAnother class of systems with similar properties was introduced in [35] called the contracting Lorenz flow. A fundamental difference between these is that the attractor of the system introduced by Rovella is not robust under perturbations, but still abundant in a measure theoretic sense. The set of measures for which the system is chaotic is called Rovella parameters and satisfies strong chaotic properties [8]; moreover, restricted to this set the system is stochastically stable [32, 33]. In [34] the authors addressed thermodynamic formalism for it. Up to now, the contracting Lorenz flow and one dimensional maps with critical points remain a profound example of a truly nonuniformly hyperbolic systems, which is studied via construction of induced schemes. We refer to [3] for a comprehensive account of these constructions.\\n\\nRecently, there has been increased interest in studying statistical properties of random dynamical systems, especially quenched (path-wise) properties. When the system has good uniformly hyperbolic properties, spectral techniques are still applicable and imply strong statistical properties; we refer to [19, 20, 21, 23, 22] and references therein for results on quenched decay of correlations, limit theorems and stability results in this case.\\n\\nFor the non-uniformly expanding (or non-uniformly hyperbolic) case, spectral techniques are not applicable directly. In this regards, it is customary to employ inducing techniques, in particular randomised version of Young Tower [39] construction called random Young towers. This was first carried out in [15] for random\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"QUENCHED DECAY OF CORRELATIONS FOR RANDOM CONTRACTING LORENZ MAPS\\n\\nANDREW LARKIN AND MARKS RUZIBOEV\\n\\nAbstract. In this work we consider i.i.d. random perturbations of contracting Lorenz maps sufficiently close to a Rovella parameter. We prove that the quenched correlations of the random dynamical system decays exponentially. \\n\\n1. Introduction\\n\\nThe Lorenz system was introduced in [30] as a simplified model for atmospheric convection. Numerical simulations have shown that the Lorenz system admits a strange attractor, called the Lorenz attractor, which became one of the most iconic examples in the field. A rigorous mathematical approach was developed with the introduction of the so called geometric Lorenz flow by [1, 26], which mimicks simulation of the dynamics of the Lorenz flow, and which has a robust strange attractor under $C^1$ perturbations. Later in [37, 38] it was shown that the actual Lorenz attractor is indeed a singular hyperbolic attractor, further showing that the geometric Lorenz attractor represents the Lorenz attractor well. Moreover, it admits the so called Sinai-Ruelle-Bowen (SRB) measure, which is ergodic [11]. Its statistical properties, such as mixing rates, limit theorems and their stability under various perturbations, were studied intensively (see for example, [31, 10, 9, 7, 14, 13, 24]). Another class of systems with similar properties was introduced in [35] called the contracting Lorenz flow. A fundamental difference between these is that the attractor of the system introduced by Rovella is not robust under perturbations, but still abundant in a measure theoretic sense. The set of measures for which the system is chaotic is called Rovella parameters and satisfies strong chaotic properties [8]; moreover, restricted to this set the system is stochastically stable [32, 33]. In [34] the authors addressed thermodynamic formalism for it. Up to now, the contracting Lorenz flow and one dimensional maps with critical points remain a profound example of a truly nonuniformly hyperbolic systems, which is studied via construction of induced schemes. We refer to [3] for a comprehensive account of these constructions. Recently, there has been increased interest in studying statistical properties of random dynamical systems, especially quenched (path-wise) properties. When the system has good uniformly hyperbolic properties, spectral techniques are still applicable and imply strong statistical properties; we refer to [19, 20, 21, 23, 22] and references therein for results on quenched decay of correlations, limit theorems and stability results in this case. For the non-uniformly expanding (or non-uniformly hyperbolic) case, spectral techniques are not applicable directly. In this regards, it is customary to employ inducing techniques, in particular randomised version of Young Tower [39] construction called random Young towers. This was first carried out in [15] for random\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"though our model exhibits a larger variance in the estimated elasticity values compared to the BLP model.\\n\\n(a) Own-Elasticity Estimation (Our Model vs. BLP Model)  \\n(b) Cross-Elasticity Estimation (Our Model vs. BLP Model)\\n\\nFigure 4: Elasticity Estimation Comparison\\nNote: Figure 4 illustrates the distributions of the estimated own- and cross-elasticities obtained from our model and the BLP model. The filled areas in the violin plots represent the complete range of the elasticities, while the text labels indicate the mean values.\\n\\nWe further estimate the average own-elasticity for high-priced, medium-priced, and low-priced cars and construct a confidence interval for each category using our inference procedure. We present our result in Table 14.\\n\\n6 Conclusion\\n\\nChoice models are fundamental in understanding consumer behavior and informing business decisions. Over the years, various methods, both parametric and non-parametric, have been developed to represent consumer behavior. While parametric methods, such as logit or probit-based models, are favored for their simplicity and interpretability, their restrictive assumptions can limit their ability to fully capture consumer preferences\u2019 intricacies. On the other hand, non-parametric methods offer a more flexible approach, but they often suffer from the \u201ccurse of dimensionality\u201d, where the complexity of estimating choice functions escalates exponentially with an increase in the number of products.\\n\\nIn this paper, we propose a fundamental characterization of choice models that combines the tractability of traditional choice models and the flexibility of non-parametric estimators. This characterization specifically tackles the challenge of high dimensionality in choice systems and facilitates flexible estimation of choice functions. Through extensive simulations, we validate the efficacy of our model, demonstrating its superior ability to capture a range of consumer behaviors that traditional choice models fail to capture. We also show how to\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"though our model exhibits a larger variance in the estimated elasticity values compared to the BLP model. \\n\\n![Own-Elasticity Estimation on Automobile Data](image1)\\n\\n(a) Own-Elasticity Estimation (Our Model vs. BLP Model)\\n\\n![Cross-Elasticity Estimation on Automobile Data](image2)\\n\\n(b) Cross-Elasticity Estimation (Our Model vs. BLP Model)\\n\\nFigure 4: Elasticity Estimation Comparison\\n\\nNote: Figure 4 illustrates the distributions of the estimated own- and cross-elasticities obtained from our model and the BLP model. The filled areas in the violin plots represent the complete range of the elasticities, while the text labels indicate the mean values.\\n\\nWe further estimate the average own-elasticity for high-priced, medium-priced, and lowpriced cars and construct a confidence interval for each category using our inference procedure. We present our result in Table 14 .\\n\\n6 Conclusion\\n\\nChoice models are fundamental in understanding consumer behavior and informing business decisions. Over the years, various methods, both parametric and non-parametric, have been developed to represent consumer behavior. While parametric methods, such as logit or probit-based models, are favored for their simplicity and interpretability, their restrictive assumptions can limit their ability to fully capture consumer preferences\u2019 intricacies. On the other hand, non-parametric methods offer a more flexible approach, but they often suffer from the \u201ccurse of dimensionality\u201d, where the complexity of estimating choice functions escalates exponentially with an increase in the number of products. \\n\\nIn this paper, we propose a fundamental characterization of choice models that combines the tractability of traditional choice models and the flexibility of non-parametric estimators. This characterization specifically tackles the challenge of high dimensionality in choice systems and facilitates flexible estimation of choice functions. Through extensive simulations, we validate the efficacy of our model, demonstrating its superior ability to capture a range of consumer behaviors that traditional choice models fail to capture. We also show how to\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Data Availability\\n\\nThe original data analysed in this work are part of the Guaranteed Time Observation (GTO) program 1282 (PI: Th. Henning) with number 66 and will become public on 2 August 2023 on the MAST database ([https://mast.stsci.edu](https://mast.stsci.edu)). The portion of the spectrum presented in Fig. 3 is available on Zenodo at [https://zenodo.org/record/7991022](https://zenodo.org/record/7991022). The spectroscopic data for water can be downloaded from the HITRAN database ([https://hitran.org](https://hitran.org)). The Spitzer-IRS spectrum plotted in Fig. 1 is part of the Spitzer-IRS GTO program 40679 (PI: G. Rieke). The spectrum was extracted and calibrated using private codes56,64 and is available on Zenodo at [https://zenodo.org/record/7991022](https://zenodo.org/record/7991022). The optical constants of the dust species considered in the fitting procedure for the dust continuum can be downloaded from the HJPDOC database ([https://www2.mpia-hd.mpg.de/HJPDOC](https://www2.mpia-hd.mpg.de/HJPDOC)).\\n\\nCode Availability\\n\\nThe slab model used in this work is a private code developed by B.T. and collaborators. It can be obtained from B.T. upon request. The synthetic spectra presented in this work can be reproduced using the slabspec code, which can be found at [https://doi.org/10.5281/zenodo.4037306](https://doi.org/10.5281/zenodo.4037306). The fitting procedure for the dust continuum uses the publicly available MultiNest Bayesian fitting algorithm ([https://github.com/JohannesBuchner/MultiNest](https://github.com/JohannesBuchner/MultiNest)) and the PyMultiNest package ([https://github.com/JohannesBuchner/PyMultiNest](https://github.com/JohannesBuchner/PyMultiNest)). Figures were made with Matplotlib version 3.5.1. under the Matplotlib license at [https://matplotlib.org/](https://matplotlib.org/).\\n\\nAcknowledgements\\n\\nThe MINDS team would like to thank the entire MIRI European and US instrument team. Support from STScI is also appreciated. The following National and International Funding Agencies funded and supported the MIRI development: NASA; ESA; Belgian Science Policy Office (BELSPO); Centre Nationale d\u2019Etudes Spatiales (CNES); Danish National Space Centre; Deutsches Zentrum fur L\u00fcft- und Raumfahrt (DLR); Enterprise Ireland; Ministerio De Econom\u00eda y Competitividad; Netherlands Research School for Astronomy (NOVA); Netherlands Organisation for Scientific Research (NWO); Science and Technology Facilities Council; Swiss Space Office; Swedish National Space Agency; and UK Space Agency. G.P. would like to thank B. Bitsch and E. Gaidos for fruitful discussions and P. Hausschildt for kindly providing the model atmosphere. V.C. and O.A. acknowledge funding from the Belgian F.R.S.-FNRS. Th.H., R.F. and K.S. acknowledge support from the European Research Council under the Horizon 2020 Framework Program via the ERC Advanced Grant Origins 83 24 28. B.T. is a Laureate of the Paris Region fellowship program, which is supported by the Ile-de-France Region and has received funding under the Horizon 2020 innovation framework program and Marie Sklodowska-Curie grant agreement No. 945298. B.T. acknowledges support from the Programme National \u2018Physique et Chimie du Milieu Interstellaire\u2019 (PCMI) of CNRS/INSU with INC/INP cofunded by CNES. D.G. would like to thank the Research Foundation Flanders for co-financing the present research (grant number V435622N). D.G. and I.A. thank the European Space Agency (ESA) and the Belgian Federal Science Policy Office (BELSPO) for their support in the framework of the PRODEX Programme. I.K., A.M.A., and E.v.D. acknowledge support from grant TOP-1614.001.751 from the Dutch Research Council (NWO). I.K. and J.K. acknowledge funding from H2020-MSCA-ITN-2019, grant no. 860470 (CHAMELEON). E.F.v.D. acknowledges support from the ERC grant 101019751 MOLDISK and the Danish National Research Foundation through the Center of Excellence \u201cInterCat\u201d (DNRF150). T.P.R acknowledges support from ERC grant 743029 EASY. D.B. has been funded by Spanish MCIN/AEI/10.13039/501100011033 grants PID2019-\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Data Availability\\n\\nThe original data analysed in this work are part of the Guaranteed Time Observation (GTO) program 1282 (PI: Th. Henning) with number 66 and will become public on 2 August 2023 on the MAST database ( [https://mast.stsci.edu](https://mast.stsci.edu) ). The portion of the spectrum presented in Fig. 3 is available on Zenodo at [https://zenodo.org/record/7991022](https://zenodo.org/record/7991022) . The spectroscopic data for water can be downloaded from the HITRAN database ( [https://hitran.org](https://hitran.org) ). The Spitzer-IRS spectrum plotted in Fig. 1 is part of the Spitzer-IRS GTO program 40679 (PI: G. Rieke). The spectrum was extracted and calibrated using private codes , and is available on Zenodo at [https://zenodo.org/record/7991022](https://zenodo.org/record/7991022) . The optical constants of the dust species considered in the fitting procedure for the dust continuum can be downloaded from the HJPDOC database ( [https://www2.mpia-hd.mpg.de/HJPDOC](https://www2.mpia-hd.mpg.de/HJPDOC) ).\\n\\nCode Availability\\n\\nThe slab model used in this work is a private code developed by B.T. and collaborators. It can be obtained from B.T. upon request. The synthetic spectra presented in this work can be reproduced using the slabspec code, which can be found at [https://doi.org/10.5281/zenodo.4037306](https://doi.org/10.5281/zenodo.4037306) . The fitting procedure for the dust continuum uses the publicly available MultiNest Bayesian fitting algorithm ( [https://github.com/JohannesBuchner/MultiNest](https://github.com/JohannesBuchner/MultiNest) ) and the PyMultiNest package ( [https://github.com/JohannesBuchner/PyMultiNest](https://github.com/JohannesBuchner/PyMultiNest) ). Figures were made with Matplotlib version 3.5.1. under the Matplotlib license at [https://matplotlib.org/](https://matplotlib.org/) .\\n\\nAcknowledgements\\n\\nThe MINDS team would like to thank the entire MIRI European and US instrument team. Support from STScI is also appreciated. The following National and International Funding Agencies funded and supported the MIRI development: NASA; ESA; Belgian Science Policy Office (BELSPO); Centre Nationale d\u2019Etudes Spatiales (CNES); Danish National Space Centre; Deutsches Zentrum fur L\u00fcftund Raumfahrt (DLR); Enterprise Ireland; Ministerio De Economi\u00e1 y Competividad; Netherlands Research School for Astronomy (NOVA); Netherlands Organisation for Scientific Research (NWO); Science and Technology Facilities Council; Swiss Space Office; Swedish National Space Agency; and UK Space Agency. G.P. would like to thank B. Bitsch and E. Gaidos for fruitful discussions and P. Hausschildt for kindly providing the model atmosphere. V.C. and O.A. acknowledge funding from the Belgian F.R.S.FNRS. Th.H., R.F. and K.S. acknowledge support from the European Research Council under the Horizon 2020 Framework Program via the ERC Advanced Grant Origins 83 24 28. B.T. is a Laureate of the Paris Region fellowship program, which is supported by the Ile-de-France Region and has received funding under the Horizon 2020 innovation framework program and Marie Sklodowska-Curie grant agreement No. 945298. B.T. acknowledges support from the Programme National \u2018Physique et Chimie du Milieu Interstellaire\u2019 (PCMI) of CNRS/INSU with INC/INP cofunded by CNES. D.G. would like to thank the Research Foundation Flanders for co-financing the present research (grant number V435622N). D.G. and I.A. thank the European Space Agency (ESA) and the Belgian Federal Science Policy Office (BELSPO) for their support in the framework of the PRODEX Programme. I.K., A.M.A., and E.v.D. acknowledge support from grant TOP-1614.001.751 from the Dutch Research Council (NWO). I.K. and J.K. acknowledge funding from H2020-MSCA-ITN-2019, grant no. 860470 (CHAMELEON). E.F.v.D. acknowledges support from the ERC grant 101019751 MOLDISK and the Danish National Research Foundation through the Center of Excellence \u201cInterCat\u201d (DNRF150). T.P.R acknowledges support from ERC grant 743029 EASY. D.B. has been funded by Spanish MCIN/AEI/10.13039/501100011033 grants PID2019-\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"coupled WGM resonators to achieve multi-band UR of photons through modulation of the intermode backscatterings of resonators[41]. In the reciprocal system that the equivalent transmission in both directions was exhibited. Obviously, these systems previously mentioned only investigated the transmission or reflection characteristics, without considering achieving complete non-reciprocity in both channels, whereas directional transport in both channels is vital to enhance the controllability of photons.\\n\\nTo this end, we propose a non-reciprocal system consisting of two WGM resonators that are individually embedded with a Zeeman split quantum dot (QD)[42\u201344] and indirectly coupled through an optical fiber. By optimizing some system parameters, we demonstrate the simultaneous realization of UR and unidirectional transmissionlessness (UT). Moreover, the conversion between UR and UT can be achieved by adjusting the coupling strength between WGM resonators and optical fiber. Additionally, a one-to-one correspondence is established between the resonant frequencies of QDs energy levels and the positions of UR and UT peaks.\\n\\n1 Model and calculations\\n\\nThe schematic of system is shown in Fig.1 (a) and energy levels of QD is shown in Fig.1 (b). Assuming that the WGM resonators and QDs have the same loss rates $\\\\gamma$, then the Hamiltonian of the system can be written as (assuming $\\\\hbar = 1$)\\n\\n$$H = \\\\int dx \\\\{ -i v_g C_R^\\\\dagger(x) \\\\frac{\\\\partial}{\\\\partial x} C_R(x) + i v_g C_L^\\\\dagger(x) \\\\frac{\\\\partial}{\\\\partial x} C_L(x) \\\\}$$\\n\\n$$+ \\\\sum_{j=1,2} G_j \\\\delta[x - (j - 1)d] \\\\{ C_R^\\\\dagger C_{aj} + C_L^\\\\dagger C_{bj} + \\\\text{H.c.} \\\\}$$\\n\\n$$+ \\\\sum_{j=1,2} \\\\{ [g_j (C_{aj} \\\\sigma_{Rj}^\\\\dagger + C_{bj} \\\\sigma_{Lj}^\\\\dagger) + h_j C_{aj} C_{bj} + \\\\text{H.c.}]$$\\n\\n$$+ (\\\\omega_0 - \\\\omega_j - i\\\\gamma) \\\\sigma_{Rj}^\\\\dagger \\\\sigma_{Rj} + (\\\\omega_0 + \\\\omega_j - i\\\\gamma) \\\\sigma_{Lj}^\\\\dagger \\\\sigma_{Lj}$$\\n\\n$$+ (\\\\omega_{aj} - i\\\\gamma) C_{aj}^\\\\dagger C_{aj} + (\\\\omega_{bj} - i\\\\gamma) C_{bj}^\\\\dagger C_{bj} \\\\},$$\\n\\n(1)\\n\\nwhere $C_R^\\\\dagger(x)$ ($C_R(x)$) and $C_L^\\\\dagger(x)$ ($C_L(x)$) are creation (annihilation) operators at $x$ for forward and backward propagating photon along fiber, respectively. $C_{bj}^\\\\dagger$ ($C_{bj}$) and $C_{aj}^\\\\dagger$ ($C_{aj}$) are creation (annihilation) operators of CW mode $b_j$ and CCW mode $a_j$ with resonance frequencies $\\\\omega_{bj}$ and $\\\\omega_{aj}$, respectively. $\\\\sigma_{Lj}^\\\\dagger$ ($\\\\sigma_{Lj}$) and $\\\\sigma_{Rj}^\\\\dagger$ ($\\\\sigma_{Rj}$) are transition operators which mean the transitions from states $|g_j\\\\rangle$ ($|e_{Lj}\\\\rangle$) to $|e_{Lj}\\\\rangle$ ($|g_j\\\\rangle$) and $|g_j\\\\rangle$ ($|e_{Rj}\\\\rangle$) to $|e_{Rj}\\\\rangle$ ($|g_j\\\\rangle$), respectively. $v_g$ is group velocity of photon. $G_j$ ($g_j$) is coupling strength between the $j$th WGM resonator and fiber ($j$th QD), and $h_j$ is transition rate between $b_j$ and $a_j$. We set up $\\\\omega_{a1} = \\\\omega_{a2} = \\\\omega_a$, $\\\\omega_{b1} = \\\\omega_{b2} = \\\\omega_b$, $G_1 = G_2 = G$, $g_1 = g_2 = g$ and $h_1 = h_2 = h$ for the sake of simplicity in the following discussions.\\n\\nAssuming that photon with energy $E_k = \\\\omega = v_g k$ is incident along the forward direction, where $\\\\omega$ and $k$ are frequency and wave vector of the incident photon, and state of system is (assume $\\\\omega_a = \\\\omega_b = \\\\omega$ and system is originally prepared in\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"coupled WGM resonators to achieve multi-band UR of photons through modulation of the intermode backscatterings of resonators[ 41 ]. In the reciprocal system that the equivalent transmission in both directions was exhibited. Obviously, these systems previously mentioned only investigated the transmission or reflection characteristics, without considering achieving complete nonreciprocity in both channels, whereas directional transport in both channels is vital to enhance the controllability of photons. \\n\\nTo this end, we propose a non-reciprocal system consisting of two WGM resonators that are individually embedded with a Zeeman split quantum dot (QD)[ 42 \u2013 44 ] and indirectly coupled through an optical fiber. By optimizing some sys- tem parameters, we demonstrate the simultaneous realization of UR and unidirectional transmissionlessness (UT). Moreover, the conversion between UR and UT can be achieved by adjusting the coupling strength between WGM resonators and optical fiber. Additionally, a one-to-one correspondence is established between the resonant frequencies of QDs energy levels and the positions of UR and UT peaks. \\n\\n1 Model and calculations\\n\\nThe schematic of system is shown in Fig. 1 (a) and energy levels of QD is shown in Fig. 1 (b). Assuming that the WGM resonators and QDs have the same loss rates \u03b3, then the Hamiltonian of the system can be written as (assuming $\\\\hbar = 1$)\\n\\n$$H = \\\\int dx \\\\{ -i v_g C_R^\\\\dagger(x) \\\\frac{\\\\partial}{\\\\partial x} C_R(x) + i v_g C_L^\\\\dagger(x) \\\\frac{\\\\partial}{\\\\partial x} C_L(x) \\\\}$$\\n\\n$$+ \\\\sum_{j=1,2} G_j \\\\delta[x - (j - 1)d] \\\\{ C_R^\\\\dagger C_{aj} + C_L^\\\\dagger C_{bj} + \\\\text{H.c.} \\\\}$$\\n\\n$$+ \\\\sum_{j=1,2} \\\\{ [g_j (C_{aj} \\\\sigma_{Rj}^\\\\dagger + C_{bj} \\\\sigma_{Lj}^\\\\dagger) + h_j C_{aj} C_{bj} + \\\\text{H.c.}]$$\\n\\n$$+ (\\\\omega_0 - \\\\omega_j - i \\\\gamma) \\\\sigma_{Rj}^\\\\dagger \\\\sigma_{Rj} + (\\\\omega_0 + \\\\omega_j - i \\\\gamma) \\\\sigma_{Lj}^\\\\dagger \\\\sigma_{Lj}$$\\n\\n$$+ (\\\\omega_{aj} - i \\\\gamma) C_{aj}^\\\\dagger C_{aj} + (\\\\omega_{bj} - i \\\\gamma) C_{bj}^\\\\dagger C_{bj} \\\\},$$\\n\\n(1)\\n\\nwhere $C_R^\\\\dagger(x)$ ($C_R(x)$) and $C_L^\\\\dagger(x)$ ($C_L(x)$) are creation (annihilation) operators at $x$ for forward and backward propagating photon along fiber, respectively. $C_{bj}^\\\\dagger$ ($C_{bj}$) and $C_{aj}^\\\\dagger$ ($C_{aj}$) are creation (annihilation) operators of CW mode $b_j$ and CCW mode $a_j$ with resonance frequencies $\\\\omega_{bj}$ and $\\\\omega_{aj}$, respectively. $\\\\sigma_{Lj}^\\\\dagger$ ($\\\\sigma_{Lj}$) and $\\\\sigma_{Rj}^\\\\dagger$ ($\\\\sigma_{Rj}$) are transition operators which mean the transitions from states $|g_j\\\\rangle$ ($|e_{Lj}\\\\rangle$) to $|e_{Lj}\\\\rangle$ ($|g_j\\\\rangle$) and $|g_j\\\\rangle$ ($|e_{Rj}\\\\rangle$) to $|e_{Rj}\\\\rangle$ ($|g_j\\\\rangle$), respectively. $v_g$ is group velocity of photon. $G_j$ ($g_j$) is coupling strength between the $j$th WGM resonator and fiber (jth QD), and $h_j$ is transition rate between $b_j$ and $a_j$. We set up $\\\\omega_{a1} = \\\\omega_{a2} = \\\\omega_a$, $\\\\omega_{b1} = \\\\omega_{b2} = \\\\omega_b$, $G_1 = G_2 = G$, $g_1 = g_2 = g$ and $h_1 = h_2 = h$ for the sake of simplicity in the following discussions. \\n\\nAssuming that photon with energy $E_k = \\\\omega = v_g k$ is incident along the forward direction, where $\\\\omega$ and $k$ are frequency and wave vector of the incident photon, and state of system is (assume $\\\\omega_a = \\\\omega_b = \\\\omega$ and system is originally prepared in Accepted in Quantum 2017-05-09, click title to verify. Published under CC-BY 4.0.\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"with a step size of 0.01. The final weights are used in our OWAF technique. Table 3 illustrates the effects of various fusion strategies. For fare comparison, we use both MRI and DTI data in all cases. The experimental results in the table 3 clearly reveal that the proposed OWAF outperforms other fusion strategies.\\n\\nTable 4: Comparisons of the proposed method with State-of-the-art Approaches\\n\\n| Approach          | MODALITY | PD vs HC | PD vs. SWEDD | HC vs. SWEDD | PD vs. HC vs SWEDD |\\n|-------------------|----------|----------|--------------|--------------|-------------------|\\n|                   |          | Ac       | Pr           | Re           | Ac               | Ac               | Ac               |\\n| Adeli 2016 [1]    | ML       | MRI      | 81.9         | -            | -                | -                | -                |\\n| Cigdem 2018 [6]   | ML       | MRI      | 93.7         | 95           | -                | -                | -                |\\n| Prashanth 2018 [20]| ML       | SPECT    | 95           | 96.7         | -                | -                | -                |\\n| Singh 2018 [2]    | ML       | MRI      | 95.37        | -            | 96.04            | 93.03            | -                |\\n| Gabriel 2021 [21] | ML       | MRI      | 99.01(M)     | 100(M)       | 99.3(M)          | 100(F)           | -                |\\n|                   |          |          | 87.10(F)     | 97.2(F)      | 93.05 (A)        | -                | -                |\\n| Li 2019 [10]      | DL (AE)  | MRI + DTI| 85.24        | 95.8         | 68.1             | -                | 89.67            |\\n| Tremblay 2020 [22]| DL       | MRI      | 88.3         | 88.2         | 88.4             | -                | -                |\\n| Chakraborty 2020 [7]| DL      | MRI      | 95.3         | 92.7         | 91.4             | -                | -                |\\n| Sivaranjini 2020 [23]| DL     | MRI      | 88.9         | -            | 89.3             | -                | -                |\\n| Rajanbabu 2022 [8]| DL (EL)  | MRI      | 97.5         | 97.9         | 97.1             | -                | -                |\\n| Proposed method   | DL       | MRI + DTI| 97.8         | 97.2         | 97.6             | 94.5             | 95.7             |\\n\\nAc, Pr, Re, M, F, A, AE, EL, - Indicates Accuracy, Precision, Recall, Male, Female, Average, Auto Encoder, Ensemble Learning & data not available respectively. All the values are in %.\\n\\n3.3 Comparisons with State-of-the-art Approaches\\n\\nWe compare our method with ten state-of-the-art approaches. There are no results available for a direct 3-class PD classification. So, we compare our results with those papers that have addressed the PD classification on the PPMI database using single or multiple modalities and with three or fewer two-class classifications. The results of comparisons are shown in Table 4. Out of the ten methods we have considered, five are based on machine learning (ML) and the rest five are based on deep learning (DL). Further, in four out of five DL based approaches, only a single modality, namely, MRI is used for classification. Also note that eight of these ten techniques have only addressed a single two-class classification problem between PD and HC and did not consider the challenging SWEDD class at all. The remaining two approaches did consider SWEDD as a third class but have divided the three-class classification problem into multiple binary classes [2,10]. However, Li at al. [10] did not report the classification results for PD vs. SWEDD in their paper. In order to have fair comparisons, we have also included three binary classifications as obtained from our method in this table. Our direct three-class classification accuracy turns out to be superior than two-class classification accuracy of at least eight out of 10 methods. It is also higher than two out of three binary classification accuracy of [2]. Note that in [2], the authors used a somewhat different experimental protocol by considering two publicly available databases of ADNI and PPMI. In our work, we explicitly consider data with both MRI and DTI for the same individual\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"with a step size of 0.01. The final weights are used in our OWAF technique. Table 3 illustrates the effects of various fusion strategies. For fare comparison, we use both MRI and DTI data in all cases. The experimental results in the table 3 clearly reveal that the proposed OWAF outperforms other fusion strategies. \\n\\nTable 4: Comparisons of the proposed method with State-of-the-art Ac, Pr, Re, M, F, A, AE, EL, Indicates Accuracy, Precision, Recall, Male, Female, Average, Auto Encoder, Ensemble Learning & data not available respectively. All the values are in %.\\n\\n| Approach          | MODALITY | PD vs HC | PD vs. SWED Comparisons with State-of-the-art Approaches |\\n|-------------------|----------|----------|--------------------------------------------------------|\\n| Adeli 2016 [1]    | ML       | MRI      | 81.9 - "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We then define the damped mode:\\n\\n\\\\begin{equation}\\n\\\\tilde{W}^\\\\varepsilon := \\\\frac{\\\\nabla (P(\\\\tilde{\\\\varrho}^\\\\varepsilon))}{\\\\tilde{\\\\varrho}^\\\\varepsilon} + \\\\tilde{v}^\\\\varepsilon + \\\\nabla (-\\\\Delta)^{-1}(\\\\tilde{\\\\varrho}^\\\\varepsilon - \\\\bar{\\\\varrho}).\\n\\\\end{equation}\\n\\nAs the first equation of (1.3) can be rewritten as\\n\\n\\\\[ \\\\partial_t \\\\tilde{\\\\varrho}^\\\\varepsilon - \\\\Delta (P(\\\\tilde{\\\\varrho}^\\\\varepsilon)) - \\\\text{div} \\\\left( \\\\tilde{\\\\varrho}^\\\\varepsilon \\\\nabla (-\\\\Delta)^{-1}(\\\\tilde{\\\\varrho}^\\\\varepsilon - \\\\bar{\\\\varrho}) \\\\right) = \\\\text{div}(\\\\tilde{\\\\varrho}^\\\\varepsilon \\\\tilde{W}^\\\\varepsilon), \\\\]\\n\\nwe expect the limit density \\\\( N \\\\) to satisfy the following parabolic-elliptic Keller-Segel system:\\n\\n\\\\begin{equation}\\n\\\\begin{cases}\\n\\\\partial_t N - \\\\Delta (P(N)) = \\\\text{div} (N \\\\nabla V) \\\\\\\\\\n-\\\\Delta V = N - \\\\bar{\\\\varrho}\\n\\\\end{cases}\\n\\\\end{equation}\\n\\nsupplemented with the initial data \\\\( \\\\lim_{\\\\varepsilon \\\\to 0} \\\\tilde{\\\\varrho}^\\\\varepsilon \\\\).\\n\\nOur second aim is to justify the passage to the limit when \\\\( \\\\varepsilon \\\\to 0 \\\\) of the Euler-Poisson system towards the parabolic-elliptic Keller-Segel system.\\n\\nRecall that (1.5) is a model for describing the evolution of density \\\\( N = N(t, x) \\\\in \\\\mathbb{R}_+ \\\\) of a biological population under the influence of a chemical agent with concentration \\\\( V = V(t, x) \\\\in \\\\mathbb{R}^d \\\\). Chemotaxis are an important means of cell communication. How cells are arranged and organized is determined by communication by chemical signals. Studying such a biological process is important because it has repercussions in many branches of medicine such as cancer [0], [0], embryonic development [0] or vascular networks [0], [0]. The previous system is famous in biology and comes from E.F Keller and L.A Segel in [0]. This basic model was used to describe the collective movement of bacteria possibly leading to cell aggregation by chemotactic effect. We refer to the articles [0] and [0] for more details and information about the different Keller-Segel models studied since the 1970s.\\n\\nOur aim here is to demonstrate that (1.5) may be obtained from the Euler-Poisson system with damping when the parameter \\\\( \\\\varepsilon \\\\) tends to 0. This question has been addressed in [0] on the torus case and Sobolev spaces in a situation where the potential satisfies a less singular equation: the author justifies the passage to the limit for regular periodic solutions. A lot of articles justify another limit: the passage from the parabolic-parabolic Keller-Segel system to the parabolic-elliptic Keller-Segel system (see e.g. the paper [0] by P-G. Lemari\u00e9-Rieusset for the case of Morrey spaces).\\n\\nIn the same spirit as this article, T. Crin-Barat, Q. He and L. Shou in [0] justified the high relaxation asymptotics for the (less singular) parabolic-parabolic Keller-Segel system (the potential satisfies the equation \\\\( -\\\\Delta V + bV = aN \\\\) with \\\\( a, b > 0 \\\\)): this other system comes from the system (HPC) (hyperbolic-parabolic-chemotaxis) which is a damped isentropic compressible Euler system with a potential satisfying an elliptical equation. In comparison with what is done here, T. Crin-Barat et al used a parabolic approach to justify their passage to the limit. Here, we have to handle the more singular case where the limit system is parabolic-elliptic.\\n\\n2. Main results and sketch of the proof\\n\\nIn this section, we will first present and motivate the functional spaces used. Secondly we will state the results and the sketch of the proofs about the well-posedness behavior of Euler-Poisson system and the justification of the passage to the limit to parabolic-elliptic Keller-Segel system.\\n\\n2.1. Functional spaces.\\n\\nBefore describing the main results of this article, we introduce the different notations and definitions used throughout this document. We will designate by \\\\( C > 0 \\\\) an independent constant of \\\\( \\\\varepsilon \\\\) and time, and \\\\( f \\\\lesssim g \\\\) will mean \\\\( f \\\\leq Cg \\\\). For any Banach space \\\\( X \\\\) and all functions \\\\( f, g \\\\in X \\\\), we denote \\\\( \\\\| (f, g) \\\\|_X := \\\\| f \\\\|_X + \\\\| g \\\\|_X \\\\).\\n\\nWe designate by \\\\( L^2(\\\\mathbb{R}_+; X) \\\\) the set of measurable functions \\\\( f : [0, +\\\\infty[ \\\\to X \\\\) such that \\\\( t \\\\mapsto \\\\| f(t) \\\\|_X \\\\) belongs to \\\\( L^2(\\\\mathbb{R}_+) \\\\) and write \\\\( \\\\| \\\\cdot \\\\|_{L^2(\\\\mathbb{R}_+; X)} := \\\\| \\\\cdot \\\\|_{L^2(\\\\mathbb{R}_+)} \\\\).\\n\\nIn this article we will use a decomposition in Fourier space, called the homogeneous Littlewood-Paley decomposition. To this end, we introduce a regular non-negative function \\\\( \\\\varphi \\\\) on \\\\( \\\\mathbb{R}^d \\\\) with support in the\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We then define the damped mode: \\n\\n\\\\begin{equation}\\n\\\\tilde{W}^\\\\varepsilon := \\\\frac{\\\\nabla (P(\\\\tilde{\\\\varrho}^\\\\varepsilon))}{\\\\tilde{\\\\varrho}^\\\\varepsilon} + \\\\tilde{v}^\\\\varepsilon + \\\\nabla (-\\\\Delta)^{-1}(\\\\tilde{\\\\varrho}^\\\\varepsilon - \\\\bar{\\\\varrho}).\\n\\\\end{equation}\\n\\nAs the first equation of (1.3) can be rewritten as \\n\\n\\\\[ \\\\partial_t \\\\tilde{\\\\varrho}^\\\\varepsilon - \\\\Delta (P(\\\\tilde{\\\\varrho}^\\\\varepsilon)) - \\\\text{div} \\\\left( \\\\tilde{\\\\varrho}^\\\\varepsilon \\\\nabla (-\\\\Delta)^{-1}(\\\\tilde{\\\\varrho}^\\\\varepsilon - \\\\bar{\\\\varrho}) \\\\right) = \\\\text{div}(\\\\tilde{\\\\varrho}^\\\\varepsilon \\\\tilde{W}^\\\\varepsilon), \\\\]\\n\\nwe expect the limit density \\\\( N \\\\) to satisfy the following parabolic-elliptic Keller-Segel system:\\n\\n\\\\begin{equation}\\n\\\\begin{cases}\\n\\\\partial_t N - \\\\Delta (P(N)) = \\\\text{div} (N \\\\nabla V) \\\\\\\\\\n-\\\\Delta V = N - \\\\bar{\\\\varrho}\\n\\\\end{cases}\\n\\\\end{equation}\\n\\nsupplemented with the initial data \\\\( \\\\lim_{\\\\varepsilon \\\\to 0} \\\\tilde{\\\\varrho}^\\\\varepsilon \\\\).\\n\\nOur second aim is to justify the passage to the limit when \\\\( \\\\varepsilon \\\\to 0 \\\\) of the Euler-Poisson system towards the parabolic-elliptic Keller-Segel system. Recall that (1.5) is a model for describing the evolution of density \\\\( N = N(t, x) \\\\in \\\\mathbb{R}_+ \\\\) of a biological population under the influence of a chemical agent with concentration \\\\( V = V(t, x) \\\\in \\\\mathbb{R}^d \\\\). Chemotaxis are an important means of cell communication. How cells are arranged and organized is determined by communication by chemical signals. Studying such a biological process is important because it has repercussions in many branches of medicine such as cancer [0], [0], embryonic development [0] or vascular networks [0], [0]. The previous system is famous in biology and comes from E.F Keller and L.A Segel in [0]. This basic model was used to describe the collective movement of bacteria possibly leading to cell aggregation by chemotactic effect. We refer to the articles [0] and [0] for more details and information about the different Keller-Segel models studied since the 1970s. Our aim here is to demonstrate that (1.5) may be obtained from the Euler-Poisson system with damping when the parameter \\\\( \\\\varepsilon \\\\) tends to 0. This question has been addressed in [0] on the torus case and Sobolev spaces in a situation where the potential satisfies a less singular equation : the author justifies the passage to the limit for regular periodic solutions. A lot of articles justify another limit: the passage from the parabolic-parabolic Keller-Segel system to the parabolic-elliptic Keller-Segel system (see e.g. the paper [0] by P-G. Lemari\u00e9-Rieusset for the case of Morrey spaces). In the same spirit as this article, T. Crin-Barat, Q. He and L. Shou in [0] justified the high relaxation asymptotics for the (less singular) parabolic-parabolic Keller-Segel system (the potential satisfies the equation \\\\( -\\\\Delta V + bV = aN \\\\) with \\\\( a, b > 0 \\\\) : this other system comes from the system (HPC) (hyperbolic-parabolicchemotaxis) which is a damped isentropic compressible Euler system with a potential satisfying an elliptical equation. In comparison with what is done here, T. Crin-Barat et al used a parabolic approach to justify their passage to the limit. Here, we have to handle the more singular case where the limit system is parabolicelliptic. \\n\\n2. Main results and sketch of the proof\\n\\nIn this section, we will first present and motivate the functional spaces used. Secondly we will state the results and the sketch of the proofs about the well-posedness behavior of Euler-Poisson system and the justification of the passage to the limit to parabolic-elliptic Keller-Segel system. \\n\\n2.1. Functional spaces.\\n\\nBefore describing the main results of this article, we introduce the different notations and definitions used throughout this document. We will designate by \\\\( C > 0 \\\\) an independent constant of \\\\( \\\\varepsilon \\\\) and time, and \\\\( f \\\\lesssim g \\\\) will mean \\\\( f \\\\leq Cg \\\\). For any Banach space \\\\( X \\\\) and all functions \\\\( f, g \\\\in X \\\\), we denote \\\\( \\\\| (f, g) \\\\|_X := \\\\| f \\\\|_X + \\\\| g \\\\|_X \\\\). We designate by \\\\( L^2(\\\\mathbb{R}_+; X) \\\\) the set of measurable functions \\\\( f : [0, +\\\\infty[ \\\\to X \\\\) such that \\\\( t \\\\mapsto \\\\| f(t) \\\\|_X \\\\) belongs to \\\\( L^2(\\\\mathbb{R}_+) \\\\) and write \\\\( \\\\| \\\\cdot \\\\|_{L^2(\\\\mathbb{R}_+; X)} := \\\\| \\\\cdot \\\\|_{L^2(\\\\mathbb{R}_+)} \\\\).\\n\\nIn this article we will use a decomposition in Fourier space, called the homogeneous Littlewood-Paley decomposition. To this end, we introduce a regular non-negative function \\\\( \\\\varphi \\\\) on \\\\( \\\\mathbb{R}^d \\\\) with support in the\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.5. Corollary. Let $P$ be a polyhedron of dimension $n$ with abelian $\\\\pi_1(P)$ and finitely generated $H_i(\\\\tilde{P})$, for $i \\\\geq 2$. Then $D(P) \\\\leq \\\\sum_{i=1}^{n} n_i$, where $n_1$ and $n_i$ ($i \\\\geq 2$) are the number of nonzero direct summands in the canonical form of $\\\\pi_1(P)$ and $H_i(\\\\tilde{P})$, respectively.\\n\\n4.6. Corollary. Let $P$ be a polyhedron of dimension $n$ with free $\\\\pi_1(P)$ and finitely generated $H_i(\\\\tilde{P})$, for $i \\\\geq 2$. Then $D(P) \\\\leq \\\\text{rank}(\\\\pi_1(P)) + \\\\sum_{i=2}^{n} n_i$, where $n_i$ is the number of nonzero direct summands in the canonical form of $H_i(\\\\tilde{P})$.\\n\\n4.7. Corollary. Let $P$ be a polyhedron of dimension $n$ with elementary amenable $\\\\pi_1(P)$ of finite cohomological dimension and finitely generated $H_i(\\\\tilde{P})$, for $i \\\\geq 2$. Then $D(P) \\\\leq h(\\\\pi_1(P)) + \\\\sum_{i=2}^{n} n_i$, where $h(\\\\pi_1(P))$ is the Hirsch length of $\\\\pi_1(P)$ and $n_i$ is the number of nonzero direct summands in the canonical form of $H_i(\\\\tilde{P})$.\\n\\nRecently, in [14], Ko\u0142odziejczyk proved that 2-dimensional polyhedra whose fundamental groups are elementary amenable with finite cohomological dimension have finite depth. In the sequel, we are going to present upper bounds for such polyhedra.\\n\\nRecall that if $P$ is a polyhedron of dimension $n$, then $H_n(P)$ is free abelian (see, for example, [18, Theorem 7.24]). Now we state our second main result.\\n\\n4.8. Theorem. If $P$ is a 2-dimensional polyhedron and $\\\\text{sl}(\\\\pi_1(P)) < \\\\infty$, then $D(P) \\\\leq \\\\text{sl}(\\\\pi_1(P)) + \\\\text{rank}(H_2(P))$.\\n\\nProof. Consider the following chain of CW-complexes:\\n\\n$$\\\\cdots < X_{i+1} < X_i < \\\\cdots < X_3 < X_2 < X_1 < X_0 = P.$$ \\n\\nLet $d_{X_{i+1}} : X_i \\\\to X_{i+1}$ and $u_{X_{i+1}} : X_{i+1} \\\\to X_i$ be the domination of $X_i$ over $X_{i+1}$ and the converse map, i.e., $d_{X_{i+1}} u_{X_{i+1}} \\\\simeq \\\\text{id}_{X_{i+1}}$. Then $\\\\pi_1(d_{X_{i+1}}) \\\\pi_1(u_{X_{i+1}}) = \\\\text{id}_{\\\\pi_1(X_{i+1})}$ and $H_2(d_{X_{i+1}}) H_2(u_{X_{i+1}}) = \\\\text{id}_{H_2(X_{i+1})}$. As a result, $\\\\text{im}(\\\\pi_1(u_{X_{i+1}}))$ and $\\\\text{im}H_2(u_{X_{i+1}})$ are retracts of $\\\\pi_1(X_i)$ and $H_2(X_i)$.\\n\\nAssume that $\\\\text{im}(\\\\pi_1(u_{X_{i+1}})) = \\\\pi_1(X_i)$ and $\\\\text{im}(H_2(u_{X_{i+1}})) = H_2(X_i)$. Then $\\\\pi_1(u_{X_{i+1}})$ and $H_2(u_{X_{i+1}})$ are isomorphisms. Accordingly, $\\\\pi_1(d_{X_{i+1}})$ is an isomorphism and $H_2(X_i) \\\\cong H_2(X_{i+1})$. Since $\\\\pi_1(X_i) \\\\cong \\\\pi_1(X_{i+1})$, we have $H_1(X_i) \\\\cong H_1(X_{i+1})$ by the Hurewicz Theorem (see [18, Theorem 4.29]). This fact and $H_2(X_i) \\\\cong H_2(X_{i+1})$ imply that $\\\\chi(X_i) = \\\\chi(X_{i+1})$, where $\\\\chi(X)$ denotes the Euler-Poincare characteristic of polyhedron $X$ of dimension $m$. Now since $d_{X_{i+1}} : X_i \\\\to X_{i+1}$ is a homotopy domination between $X_i$ and $X_{i+1}$ that induces an isomorphism on the fundamental groups, it is a homotopy equivalence (by [14, Theorems 2]) which contradicts $X_{i+1} < X_i$. Therefore, either $\\\\text{im}(\\\\pi_1(u_{X_{i+1}}))$ is a proper retract of $\\\\pi_1(X_i)$ or $\\\\text{im}(H_2(u_{X_{i+1}}))$ is a proper retract of $H_2(X_i)$.\\n\\nSo by Lemma 3.3, $\\\\text{sl}(\\\\text{im}(\\\\pi_1(u_{X_{i+1}}))) < \\\\text{sl}(\\\\pi_1(X_i))$ or $\\\\text{rank}(\\\\text{im}(H_2(u_{X_{i+1}}))) < \\\\text{rank}(H_2(X_i))$. Since $\\\\pi_1(X_{i+1}) \\\\cong \\\\text{im}(\\\\pi_1(u_{X_{i+1}}))$ and $H_2(X_{i+1}) \\\\cong \\\\text{im}(H_2(u_{X_{i+1}}))$, we have by Lemma 3.4 that $\\\\text{sl}(\\\\pi_1(X_{i+1})) < \\\\text{sl}(\\\\pi_1(X_i))$ or $\\\\text{rank}(H_2(X_{i+1})) < \\\\text{rank}(H_2(X_i))$. Thus for $i_0 := \\\\text{sl}(\\\\pi_1(P)) + \\\\text{rank}(H_2(P))$, we have $\\\\text{sl}(\\\\pi_1(X_{i_0})) = 0$ and $\\\\text{rank}(H_2(X_{i_0})) = 0$ which means by Lemma 3.4 that $\\\\pi_1(X_{i_0}) = 1$ and $H_2(X_{i_0}) = 0$. Hence, $X_{i_0}$ is homotopically trivial and so the proof is finished. \\\\qed\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.5. Corollary. Let $P$ be a polyhedron of dimension $n$ with abelian $\\\\pi_1(P)$ and finitely generated $H_i(\\\\tilde{P})$, for $i \\\\geq 2$. Then $D(P) \\\\leq \\\\sum_{i=1}^{n} n_i$, where $n_1$ and $n_i$ ($i \\\\geq 2$) are the number of nonzero direct summands in the canonical form of $\\\\pi_1(P)$ and $H_i(\\\\tilde{P})$, respectively.\\n\\n4.6. Corollary. Let $P$ be a polyhedron of dimension $n$ with free $\\\\pi_1(P)$ and finitely generated $H_i(\\\\tilde{P})$, for $i \\\\geq 2$. Then $D(P) \\\\leq \\\\text{rank}(\\\\pi_1(P)) + \\\\sum_{i=2}^{n} n_i$, where $n_i$ is the number of nonzero direct summands in the canonical form of $H_i(\\\\tilde{P})$.\\n\\n4.7. Corollary. Let $P$ be a polyhedron of dimension $n$ with elementary amenable $\\\\pi_1(P)$ of finite cohomological dimension and finitely generated $H_i(\\\\tilde{P})$, for $i \\\\geq 2$. Then $D(P) \\\\leq h(\\\\pi_1(P)) + \\\\sum_{i=2}^{n} n_i$, where $h(\\\\pi_1(P))$ is the Hirsch length of $\\\\pi_1(P)$ and $n_i$ is the number of nonzero direct summands in the canonical form of $H_i(\\\\tilde{P})$.\\n\\nRecently, in [14], Ko lodziejczyk proved that 2-dimensional polyhedra whose fundamental groups are elementary amenable with finite cohomological dimension have finite depth. In the sequel, we are going to present upper bounds for such polyhedra. Recall that if $P$ is a polyhedron of dimension $n$, then $H_n(P)$ is free abelian (see, for example, [18, Theorem 7.24]). Now we state our second main result.  \\n\\n4.8. Theorem. If $P$ is a 2-dimensional polyhedron and $\\\\text{sl}(\\\\pi_1(P)) < \\\\infty$, then $D(P) \\\\leq \\\\text{sl}(\\\\pi_1(P)) + \\\\text{rank}(H_2(P))$.\\n\\nProof. Consider the following chain of CW-complexes: \\n\\n$$\\n\\\\cdots < X_{i+1} < X_i < \\\\cdots < X_3 < X_2 < X_1 < X_0 = P.\\n$$\\n\\nLet $d_{X_{i+1}} : X_i \\\\to X_{i+1}$ and $u_{X_{i+1}} : X_{i+1} \\\\to X_i$ be the domination of $X_i$ over $X_{i+1}$ and the converse map, i.e., $d_{X_{i+1}} u_{X_{i+1}} \\\\simeq \\\\text{id}_{X_{i+1}}$. Then $\\\\pi_1(d_{X_{i+1}}) \\\\pi_1(u_{X_{i+1}}) = \\\\text{id}_{\\\\pi_1(X_{i+1})}$ and $H_2(d_{X_{i+1}}) H_2(u_{X_{i+1}}) = \\\\text{id}_{H_2(X_{i+1})}$. As a result, $\\\\text{im}(\\\\pi_1(u_{X_{i+1}}))$ and $\\\\text{im}H_2(u_{X_{i+1}})$ are retracts of $\\\\pi_1(X_i)$ and $H_2(X_i)$.\\n\\nAssume that $\\\\text{im}(\\\\pi_1(u_{X_{i+1}})) = \\\\pi_1(X_i)$ and $\\\\text{im}(H_2(u_{X_{i+1}})) = H_2(X_i)$. Then $\\\\pi_1(u_{X_{i+1}})$ and $H_2(u_{X_{i+1}})$ are isomorphisms. Accordingly, $\\\\pi_1(d_{X_{i+1}})$ is an isomorphism and $H_2(X_i) \\\\cong H_2(X_{i+1})$. Since $\\\\pi_1(X_i) \\\\cong \\\\pi_1(X_{i+1})$, we have $H_1(X_i) \\\\cong H_1(X_{i+1})$ by the Hurewicz Theorem (see [18, Theorem 4.29]). This fact and $H_2(X_i) \\\\cong H_2(X_{i+1})$ imply that $\\\\chi(X_i) = \\\\chi(X_{i+1})$, where $\\\\chi(X)$ denotes the EulerPoincare characteristic of polyhedron $X$ of dimension $m$. Now since $d_{X_{i+1}} : X_i \\\\to X_{i+1}$ is a homotopy domination between $X_i$ and $X_{i+1}$ that induces an isomorphism on the fundamental groups, it is a homotopy equivalence (by [14, Theorems 2]) which contradicts $X_{i+1} < X_i$. Therefore, either $\\\\text{im}(\\\\pi_1(u_{X_{i+1}}))$ is a proper retract of $\\\\pi_1(X_i)$ or $\\\\text{im}(H_2(u_{X_{i+1}}))$ is a proper retract of $H_2(X_i)$.\\n\\nSo by Lemma 3.3, $\\\\text{sl}(\\\\text{im}(\\\\pi_1(u_{X_{i+1}}))) < \\\\text{sl}(\\\\pi_1(X_i))$ or $\\\\text{rank}(\\\\text{im}(H_2(u_{X_{i+1}}))) < \\\\text{rank}(H_2(X_i))$. Since $\\\\pi_1(X_{i+1}) \\\\cong \\\\text{im}(\\\\pi_1(u_{X_{i+1}}))$ and $H_2(X_{i+1}) \\\\cong \\\\text{im}(H_2(u_{X_{i+1}}))$, we have by Lemma 3.4 that $\\\\text{sl}(\\\\pi_1(X_{i+1})) < \\\\text{sl}(\\\\pi_1(X_i))$ or $\\\\text{rank}(H_2(X_{i+1})) < \\\\text{rank}(H_2(X_i))$. Thus for $i_0 := \\\\text{sl}(\\\\pi_1(P)) + \\\\text{rank}(H_2(P))$, we have $\\\\text{sl}(\\\\pi_1(X_{i_0})) = 0$ and $\\\\text{rank}(H_2(X_{i_0})) = 0$ which means by Lemma 3.4 that $\\\\pi_1(X_{i_0}) = 1$ and $H_2(X_{i_0}) = 0$. Hence, $X_{i_0}$ is homotopically trivial and so the proof is finished. \\\\qed\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Outline. We use Fenichel\u2019s reduction to regularize the singular perturbation in existence and eigenvalue problem in Section 2. In Section 4, we study the resulting regularized traveling wave problem using functional-analytic methods, using methods developed in [2, 4, 3] to find pulled and pushed front profiles as well as the transition curve. Section 5 establishes marginal spectral stability of these fronts thus justifying the pushed and pulled terminology. In Section 6, we briefly compare the expansions obtained in Theorem 1.2 to those obtained using numerical continuation. The appendix contains the construction and properties of traveling fronts at $\\\\delta = 0$.\\n\\n2 Regularization via geometric singular perturbation theory\\n\\n2.1 Reduction of existence problem\\n\\nWe express (1.5) as a dynamical system in the variable $x$ by choosing coordinates $U, W = U', H = \\\\frac{V-U}{\\\\delta^2}, Z = \\\\delta H'$, obtaining\\n\\n\\\\[\\n\\\\begin{align*}\\nU' &= W \\\\\\\\\\nW' &= -\\\\frac{1}{d_1} \\\\Gamma(U, W, H, Z) \\\\\\\\\\n\\\\delta H' &= Z \\\\\\\\\\n\\\\delta Z' &= H + \\\\frac{1}{d_1} \\\\Gamma(U, W, H, Z),\\n\\\\end{align*}\\n\\\\]\\n\\n(2.1)\\n\\nwhere\\n\\n\\\\[\\n\\\\Gamma(U, W, H, Z) = cW + W^2 + \\\\delta WZ + UH + U(1 - U).\\n\\\\]\\n\\n(2.2)\\n\\nWhen $\\\\delta = 0$ the system (2.1) reduces to two algebraic equations coupled to two differential equations. One identifies the following reduced slow manifold comprised of solutions of the algebraic of equations in the singular limit $\\\\delta = 0$,\\n\\n\\\\[\\n\\\\mathcal{M}_0 = \\\\left\\\\{ (U, W, H, Z) \\\\mid Z = 0, \\\\ H = -\\\\frac{cW + W^2 + U - U^2}{d_1 + U} \\\\right\\\\}.\\n\\\\]\\n\\n(2.3)\\n\\nThe linearization of (2.1) at any such fixed point has two zero eigenvalues and two hyperbolic eigenvalues $\\\\pm \\\\sqrt{1 + \\\\frac{U}{d_1}}$ for $U \\\\geq 0$. The eigenspaces of the non-zero eigenvalues are traverse to $\\\\mathcal{M}_0$ and therefore the reduced manifold is normally hyperbolic. Fenichel\u2019s Persistence Theorem [5] implies that $\\\\mathcal{M}_0$ persists as an invariant manifold $\\\\mathcal{M}_\\\\delta$ with the following properties.\\n\\nProposition 2.1 (Reduction for existence problem). Fix $0 < \\\\underline{d} < \\\\bar{d}$, $M > 1$, and an integer $k \\\\geq 2$. There exists a $\\\\bar{\\\\delta} > 0$ such that all trajectories of (2.1) with $|\\\\delta| < \\\\bar{\\\\delta}$, $\\\\underline{d} < d_1 < \\\\bar{d}$ satisfying $-\\\\frac{d_1}{2} < U < M$ and $|W| \\\\leq M$ lie in a slow manifold $\\\\mathcal{M}_\\\\delta$, which is normally hyperbolic and invariant under the flow of (2.1), and may be written as a graph $\\\\mathcal{M}_\\\\delta = \\\\{(U, W, H, Z) : H = \\\\psi_H(U, W; \\\\delta), Z = \\\\psi_Z(U, W; \\\\delta)\\\\}$, where\\n\\n\\\\[\\n\\\\begin{align*}\\nH &= \\\\psi_H(U, W; \\\\delta) = \\\\psi_H^0(U, W) + \\\\delta \\\\psi_H^1(U, W) + \\\\delta^2 \\\\psi_H^2(U, W) + O(\\\\delta^4), \\\\\\\\\\nZ &= \\\\psi_Z(U, W; \\\\delta) = \\\\delta \\\\psi_Z^1(U, W) + O(\\\\delta^2),\\n\\\\end{align*}\\n\\\\]\\n\\n(2.4)\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Outline. We use Fenichel\u2019s reduction to regularize the singular perturbation in existence and eigenvalue problem in Section 2. In Section 4, we study the resulting regularized traveling wave problem using functional-analytic methods, using methods developed in [2, 4, 3] to find pulled and pushed front profiles as well as the transition curve. Section 5 establishes marginal spectral stability of these fronts thus justifying the pushed and pulled terminology. In Section 6, we briefly compare the expansions obtained in Theorem 1.2 to those obtained using numerical continuation. The appendix contains the construction and properties of traveling fronts at $\\\\delta = 0$.\\n\\n2 Regularization via geometric singular perturbation theory\\n\\n2.1 Reduction of existence problem\\n\\nWe express (1.5) as a dynamical system in the variable $x$ by choosing coordinates $U, W = U', H = \\\\frac{V - U}{\\\\delta^2}, Z = \\\\delta H'$, obtaining\\n\\n\\\\[\\n\\\\begin{align*}\\nU' &= W \\\\\\\\\\nW' &= -\\\\frac{1}{d_1} \\\\Gamma(U, W, H, Z) \\\\\\\\\\n\\\\delta H' &= Z \\\\\\\\\\n\\\\delta Z' &= H + \\\\frac{1}{d_1} \\\\Gamma(U, W, H, Z),\\n\\\\end{align*}\\n\\\\]\\n\\n(2.1) reduces to two algebraic equations coupled to two differential equations. One identifies the following reduced slow manifold comprised of solutions of the algebraic of equations in the singular limit $\\\\delta = 0$,\\n\\n\\\\[\\n\\\\mathcal{M}_0 = \\\\left\\\\{ (U, W, H, Z) \\\\mid Z = 0, \\\\ H = -\\\\frac{cW + W^2 + U - U^2}{d_1 + U} \\\\right\\\\}.\\n\\\\]\\n\\n(2.3)\\n\\nThe linearization of (2.1) at any such fixed point has two zero eigenvalues and two hyperbolic eigenvalues $\\\\pm \\\\sqrt{1 + \\\\frac{U}{d_1}}$ for $U \\\\geq 0$. The eigenspaces of the non-zero eigenvalues are traverse to $\\\\mathcal{M}_0$ and therefore the reduced manifold is normally hyperbolic. Fenichel\u2019s Persistence Theorem [5] implies that $\\\\mathcal{M}_0$ persists as an invariant manifold $\\\\mathcal{M}_\\\\delta$ with the following properties.\\n\\nProposition 2.1 (Reduction for existence problem) Fix $0 < \\\\underline{d} < \\\\bar{d}$, $M > 1$, and an integer $k \\\\geq 2$. There exists a $\\\\bar{\\\\delta} > 0$ such that all trajectories of (2.1) with $|\\\\delta| < \\\\bar{\\\\delta}$, $\\\\underline{d} < d_1 < \\\\bar{d}$ satisfying $-\\\\frac{d_1}{2} < U < M$ and $|W| \\\\leq M$ lie in a slow manifold $\\\\mathcal{M}_\\\\delta$, which is normally hyperbolic and invariant under the flow of (2.1), and may be written as a graph $\\\\mathcal{M}_\\\\delta = \\\\{(U, W, H, Z) : H = \\\\psi_H(U, W; \\\\delta), Z = \\\\psi_Z(U, W; \\\\delta)\\\\}$, where\\n\\n\\\\[\\n\\\\begin{align*}\\nH &= \\\\psi_H(U, W; \\\\delta) = \\\\psi_H^0(U, W) + \\\\delta \\\\psi_H^1(U, W) + \\\\delta^2 \\\\psi_H^2(U, W) + O(\\\\delta^4), \\\\\\\\\\nZ &= \\\\psi_Z(U, W; \\\\delta) = \\\\delta \\\\psi_Z^1(U, W) + O(\\\\delta^2),\\n\\\\end{align*}\\n\\\\]\\n\\n(2.4)\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"the input data dimension. Figure 4a shows the memory size (in MB) of the 4 shallow classifiers considered in our experiments, based on the input size in terms of the percentage of PCA explained variance. As expected, the size of most of the classifiers increases according to the input dimension, except for Random Forest (RF), whose size remains nearly constant at around 1 MB (between 0.98 and 1.11). Logistic Regression (LR), the simplest classifier, is also the one with the lowest memory footprint in all the experiments, starting from less than 1 KB (i.e., 922 Bytes), up to 3.64 KB with 99% of PCA explained variance. On the other hand, AdaBoost (AB) results to be the most demanding model in terms of memory, with an overall size that ranges from just 5.74 MB with 60% of PCA, up to 185.05 MB with the full dimension of the input. Finally, SVM has an intermediate memory footprint among the other classifiers, ranging from 42.6 KB up to 1.88 MB.\\n\\nOn the other hand, when the deep audio models are fine-tuned, the size of the additional fully-connected layers should be considered to estimate the overall memory footprint. Figure 4b shows the average size of the fine-tuned models, highlighting both the size of the original pre-trained models, and the size of the additional layers for classification. We can note that, in general,\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the input data dimension. Figure 4a shows the memory size (in MB) of the 4 shallow classifiers considered in our experiments, based on the input size in terms of the percentage of PCA explained variance. As expected, the size of most of the classifiers increases according to the input dimension, except for Random Forest (RF), whose size remains nearly constant at around 1 MB (between 0.98 and 1.11). Logistic Regression (LR), the simplest classifier, is also the one with the lowest memory footprint in all the experiments, starting from less than 1 KB (i.e., 922 Bytes), up to 3.64 KB with 99% of PCA explained variance. On the other hand, AdaBoost (AB) results to be the most demanding model in terms of memory, with an overall size that ranges from just 5.74 MB with 60% of PCA, up to 185.05 MB with the full dimension of the input. Finally, SVM has an intermediate memory footprint among the other classifiers, ranging from 42.6 KB up to 1.88 MB. \\n\\nOn the other hand, when the deep audio models are fine-tuned, the size of the additional fully-connected layers should be considered to estimate the overall memory footprint. Figure 4b shows the average size of the fine-tuned models, highlighting both the size of the original pre-trained models, and the size of the additional layers for classification. We can note that, in general,\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"be effective for support vector machines (SVM) [79] and large language models [25]. Another line of work called transductive learning uses test data to add constraints to the margin of SVMs [31, 11, 66]. The principle of transduction, as stated by Vapnik, also emphasizes locality [18, 67]: \\\"Try to get the answer that you really need but not a more general one.\\\"\\n\\nIn computer vision, the idea of training at test time has been well explored for specific applications [30, 57, 46, 73], especially depth estimation [62, 63, 82, 84, 43]. Our paper extends TTT-MAE [19], detailed in Section 3. TTT-MAE, in turn, is inspired by the work Sun et al. [61], which proposed the general framework for test-time training with self-supervision, regardless of application. The particular self-supervised task used in [61] is rotation prediction [21]. Many other papers have followed this framework since then [24, 60, 40, 77], including [69] on videos discussed in Section 1, and [5] which we discuss next.\\n\\nIn [5], each video is treated as a dataset of unordered frames instead of a stream. In particular, there is no concept of past vs. future frames. The same model is used on the entire video. In contrast, our paper emphasizes locality. We have access to only the current and past frames, and our model keeps learning over time. In addition, all of our results are on real world videos, while [5] experiment on videos with artificial corruptions. These corruptions are also i.i.d. across frames.\\n\\nOur paper is very much inspired by [45]. To make video segmentation more efficient, [45] makes predictions frame-by-frame using a small student model. If the student is not confident, it queries an expensive teacher model, and then trains the student to fit the prediction from the teacher online. Thanks to temporal smoothness, the student can generalize confidently across many frames without querying the teacher, so learning and predicting combined is still faster than naively using the teacher at every frame. Our method only consists of one model, which learns from a self-supervised task instead of a teacher model. Rather than focusing on computational efficiency as in [45], the main goal of our paper is to improve inference quality. Behind their particular algorithm, however, we see the shared idea of locality, regardless of the form of supervision.\\n\\n3 Background: TTT-MAE\\n\\nOur paper extends the work of Test-Time Training with Masked Autoencoders (TTT-MAE) [19], and uses TTT-MAE as the inner loop when updating the model for each frame. This section briefly describes TTT-MAE, as background for our extension. Figure 3 illustrates the process of TTT-MAE.\\n\\nThe architecture for TTT with self-supervision [61] is Y-shaped with a stem and two heads: a prediction head $g$ for the self-supervised task, a prediction head $h$ for the main task, and a feature extractor $f$ as the stem. The output features of $f$ are shared between $g$ and $h$ as input. For TTT-MAE, the self-supervised task is masked image reconstruction [27]. Following standard terminology for autoencoders, $f$ is also called the encoder, and $g$ the decoder.\\n\\nEach input image $x$ is first split into many non-overlapping patches. To produce the autoencoder input $\\\\tilde{x}$, we mask out majority, e.g. 80%, of the patches in $x$ at random. The self-supervised objective $\\\\ell_s(g \\\\circ f(\\\\tilde{x}), x)$ compares the reconstructed patches from $g \\\\circ f(\\\\tilde{x})$ to the masked patches in $x$, and computes the pixel-wise mean squared error. For the main task, e.g. segmentation, all patches in the original $x$ are given as input to $h \\\\circ f$, during both training and testing.\\n\\n3.1 Training-Time Training\\n\\nThere are three widely accepted ways to optimize the model components ($f$, $g$, $h$) at training time: joint training, probing, and fine-tuning. Fine-tuning is unsuitable for TTT, because it makes $h$ rely too much on features that are used by the main task. Our paper uses joint training, described in Section 4. In contrast, [19] uses probing, which we describe next for completeness.\\n\\nTo prepare for probing, the common practice is to first train $f$ and $g$ with $\\\\ell_s$ on the training set without ground truth. This preparation stage is also called self-supervised pre-training. TTT-\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"be effective for support vector machines (SVM) [ 79 ] and large language models [ 25 ]. Another line of work called transductive learning uses test data to add constraints to the margin of SVMs [ 31 , 11 , 66 ]. The principle of transduction, as stated by Vapnik, also emphasizes locality [ 18 , 67 ]: \"Try to get the answer that you really need but not a more general one.\" \\n\\nIn computer vision, the idea of training at test time has been well explored for specific applications [ 30 , 57 , 46 , 73 ], especially depth estimation [ 62 , 63 , 82 , 84 , 43 ]. Our paper extends TTT-MAE [ 19 ], detailed in Section 3 . TTT-MAE, in turn, is inspired by the work Sun et al. [ 61 ], which proposed the general framework for test-time training with self-supervision, regardless of application. The particular self-supervised task used in [ 61 ] is rotation prediction [ 21 ]. Many other papers have followed this framework since then [ 24 , 60 , 40 , 77 ], including [ 69 ] on videos discussed in Section 1 , and [ 5 ] which we discuss next. \\n\\nIn [ 5 ] experiment on videos with artificial corruptions. These corruptions are also i.i.d. across frames. \\n\\nOur paper is very much inspired by [ 45 ]. To make video segmentation more e ffi cient, [ 45 ] makes predictions frame-by-frame using a small student model. If the student is not confident, it queries an expensive teacher model, and then trains the student to fit the prediction from the teacher online. Thanks to temporal smoothness, the student can generalize confidently across many frames without querying the teacher, so learning and predicting combined is still faster than naively using the teacher at every frame. Our method only consists of one model, which learns from a self-supervised task instead of a teacher model. Rather than focusing on computational e ffi ciency as in [ 45 ], the main goal of our paper is to improve inference quality. Behind their particular algorithm, however, we see the shared idea of locality, regardless of the form of supervision. \\n\\n3 Background: TTT-MAE\\n\\nOur paper extends the work of Test-Time Training with Masked Autoencoders (TTT-MAE) [ 19 ], and uses TTT-MAE as the inner loop when updating the model for each frame. This section briefly describes TTT-MAE, as background for our extension. Figure 3 illustrates the process of TTT-MAE. \\n\\nThe architecture for TTT with self-supervision [ 61 ] is Y-shaped with a stem and two heads: a prediction head g for the self-supervised task, a prediction head h for the main task, and a feature extractor f as the stem. The output features of f are shared between g and h as input. For TTT-MAE, the self-supervised task is masked image reconstruction [ 27 ]. Following standard terminology for autoencoders, f is also called the encoder, and g the decoder. \\n\\nEach input image x is first split into many non-overlapping patches. To produce the autoencoder input \u02dc x, we mask out majority, e.g. 80%, of the patches in x at random. The self-supervised objective \u2113 s ( g \u25e6 f ( \u02dc x ), x ) compares the reconstructed patches from g \u25e6 f ( \u02dc x ) to the masked patches in x, and computes the pixel-wise mean squared error. For the main task, e.g. segmentation, all patches in the original x are given as input to h \u25e6 f , during both training and testing. \\n\\n3.1 Training-Time Training\\n\\nThere are three widely accepted ways to optimize the model components ( f , g , h ) at training time: joint training, probing, and fine-tuning. Fine-tuning is unsuitable for TTT, because it makes h rely too much on features that are used by the main task. Our paper uses joint training, described in Section 4 . In contrast, [ 19 ] uses probing, which we describe next for completeness. \\n\\nTo prepare for probing, the common practice is to first train f and g with \u2113 s on the training set without ground truth. This preparation stage is also called self-supervised pre-training. TTT-\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the temperature feature to evaluate its impact on the clustering algorithm\u2019s efficiency in identifying locations of interest.\\n\\n3.3.1 Obtaining Historical Temperature Data. However, such temperature analysis is only feasible when temperature data is available. Certain studies concerning elephant movement (Tsalyuk et al. [23]; Wall et al. [24]) lack a temperature feature. Therefore, we explored methods to approximate temperature data from other data sources. Using the meteostat python package and API, we identified weather stations proximate to the study site. The historical data was queried and appended to the study data, enabling calculation of Temperature-influenced centroids that would have been impossible to calculate otherwise.\\n\\nThe procedure entailed three key steps: (1) Identifying a nearby weather station, (2) Matching timestamps with the queried data, and (3) Evaluating the capability of the appended historical temperature data in calculating temperature-influenced centroids.\\n\\nFor the first step, the median latitude and longitude of the given elephant\u2019s movement data was computed, which was then used to query a nearby station. The second step involved normalizing and interpolating the time series data from the station, provided by meteostat, to ensure a higher temporal granularity that matches the given data. In the third step, the correlation between the historical station data and Kruger temperature data was evaluated using the coefficient of determination, R-squared.\\n\\nOur results indicate a moderate correlation between the study data and the station data. This correlation, combined with the performance of the Temperature-influenced centroids with the weather data, gives us confidence to extend this technique to datasets that lack temperature data. Based on our experiment with elephant AM306 (See Figure 1) from the Kruger dataset, we found that the Temperature-influenced feature space aided in revealing more nuanced locations of interest within the larger clusters identified by the Without Temperature influence feature space.\\n\\n3.3.2 Fuzzy Timestamp Matching. Fuzzy timestamp matching is an advanced data processing technique that matches timestamps not based on exact equality but within a certain tolerance level. This tolerance level, or fuzzy threshold, is usually calculated by taking half of the median of the difference of timestamps in the dataset. The mathematical representation of the fuzzy timestamp matching process could be described as follows:\\n\\nGiven two timestamps, \\\\( t_1 \\\\) and \\\\( t_2 \\\\), and a tolerance level \\\\( \\\\delta \\\\), the timestamps \\\\( t_1 \\\\) and \\\\( t_2 \\\\) are said to match if:\\n\\n\\\\[\\n|t_1 - t_2| \\\\leq \\\\delta\\n\\\\]\\n\\nwhere \\\\( |t_1 - t_2| \\\\) denotes the absolute difference between the timestamps \\\\( t_1 \\\\) and \\\\( t_2 \\\\). In this case, \\\\( \\\\delta \\\\) is calculated as:\\n\\n\\\\[\\n\\\\delta = 0.5 \\\\times \\\\text{median}(|t[i+1] - t[i]|), \\\\forall i \\\\text{ to } N - 1\\n\\\\]\\n\\nwhere \\\\( N \\\\) is the total number of timestamps, and \\\\( t[i] \\\\) represents the \\\\( i \\\\)th timestamp in the ordered sequence. This fuzzy matching approach increases the likelihood of matches and can help to mitigate data loss when aligning data from different sources or with different temporal resolutions. However, it is important to note that this technique may also introduce some uncertainty into the analysis due to the mismatched timestamps. Hence, an appropriate balance between data retention and accuracy should be maintained while deciding the value of \\\\( \\\\delta \\\\).\\n\\nThe integration of weather station temperature data with animal movement datasets presented a significant challenge due to the relatively low percentage of matching timestamps. For instance, in the case of AM189 from Etosha, a mere 19.662% of timestamps corresponded. This limited overlap signifies a considerable loss of data, which undermines the analysis. To address this issue, we utilized \u201cfuzzy\u201d timestamp matching. This method extends the criteria of a match beyond exact timestamp equality, incorporating a pre-defined threshold for the discrepancy between two timestamps that still qualifies them as a match. The mathematical formulation of this concept is as follows: Given two timestamps \\\\( t_1 \\\\) and \\\\( t_2 \\\\), and a tolerance level (or fuzzy threshold) \\\\( \\\\delta \\\\), the timestamps \\\\( t_1 \\\\) and \\\\( t_2 \\\\) are said to match if the absolute difference between them, denoted as \\\\( |t_1 - t_2| \\\\), does not exceed \\\\( \\\\delta \\\\). The fuzzy threshold \\\\( \\\\delta \\\\) is calculated as half the median of the differences between all sequential pairs of timestamps in the dataset.\\n\\nBy employing fuzzy timestamp matching, the percentage of matched data can be substantially increased. For example, in the case of AG191 from Etosha, conventional timestamp matching resulted in a match percentage of 41.85%. With the application of fuzzy matching, this percentage rose to 74.50%. Notably, these additional matches, achieved through fuzzy matching, are proximal\\n\\n| Metric                          | Value                  |\\n|---------------------------------|------------------------|\\n| R-squared (zero-centered)       | 0.6871044690549571     |\\n| Offset (study \u2013 station)        | 9.84010669689293       |\\n| % of timestamps found           | 61.6%                  |\\n\\nTable 1: Statistics for Figure 2\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the temperature feature to evaluate its impact on the clustering algorithm\u2019s efficiency in identifying locations of interest.  \\n\\n3.3.1 Obtaining Historical Temperature Data. However, such temperature analysis is only feasible when temperature data is available. Certain studies concerning elephant movement (Tsalyuk et al. [ 23 ]; Wall et al. [ 24 ]) lack a temperature feature. Therefore, we explored methods to approximate temperature data from other data sources. Using the meteostat python package and API, we identified weather stations proximate to the study site. The historical data was queried and appended to the study data, enabling calculation of Temperature-influenced centroids that would have been impossible to calculate otherwise.  \\n\\nThe procedure entailed three key steps: (1) Identifying a nearby weather station, (2) Matching timestamps with the queried data, and (3) Evaluating the capability of the appended historical temperature data in calculating temperature-influenced centroids.  \\n\\nFor the first step, the median latitude and longitude of the given elephant\u2019s movement data was computed, which was then used to query a nearby station. The second step involved normalizing and interpolating the time series data from the station, provided by meteostat, to ensure a higher temporal granularity that matches the given data. In the third step, the correlation between the historical station data and Kruger temperature data was evaluated using the coefficient of determination, R-squared.  \\n\\nOur results indicate a moderate correlation between the study data and the station data. This correlation, combined with the performance of the Temperature-influenced centroids with the weather data, gives us confidence to extend this technique to datasets that lack temperature data. Based on our experiment with elephant AM306 (See Figure 1) from the Kruger dataset, we found that the Temperature-influenced feature space aided in revealing more nuanced locations of interest within the larger clusters identified by the Without Temperature influence feature space.  \\n\\n3.3.2 Fuzzy Timestamp Matching. Fuzzy timestamp matching is an advanced data processing technique that matches timestamps not based on exact equality but within a certain tolerance level. This tolerance level, or fuzzy threshold, is usually calculated by taking half of the median of the difference of timestamps in the dataset. The mathematical representation of the fuzzy timestamp matching process could be described as follows: Given two timestamps, \\\\( t_1 \\\\) and \\\\( t_2 \\\\), and a tolerance level \\\\( \\\\delta \\\\), the timestamps \\\\( t_1 \\\\) and \\\\( t_2 \\\\) are said to match if:  \\n\\n\\\\[\\n|t_1 - t_2| \\\\leq \\\\delta\\n\\\\]  \\n\\nwhere \\\\( |t_1 - t_2| \\\\) denotes the absolute difference between the timestamps \\\\( t_1 \\\\) and \\\\( t_2 \\\\). In this case, \\\\( \\\\delta \\\\) is calculated as:  \\n\\n\\\\[\\n\\\\delta = 0.5 \\\\times \\\\text{median}(|t[i+1] - t[i]|), \\\\forall i \\\\text{ to } N - 1\\n\\\\]  \\n\\nwhere \\\\( N \\\\) is the total number of timestamps, and t[i] represents the ith timestamp in the ordered sequence. This fuzzy matching approach increases the likelihood of matches and can help to mitigate data loss when aligning data from different sources or with different temporal resolutions. However, it is important to note that this technique may also introduce some uncertainty into the analysis due to the mismatched timestamps. Hence, an appropriate balance between data retention and accuracy should be maintained while deciding the value of \\\\( \\\\delta \\\\).  \\n\\nThe integration of weather station temperature data with animal movement datasets presented a significant challenge due to the relatively low percentage of matching timestamps. For instance, in the case of AM189 from Etosha, a mere 19.662% of timestamps corresponded. This limited overlap signifies a considerable loss of data, which undermines the analysis. To address this issue, we utilized \"fuzzy\" timestamp matching. This method extends the criteria of a match beyond exact timestamp equality, incorporating a pre-defined threshold for the discrepancy between two timestamps that still qualifies them as a match. The mathematical formulation of this concept is as follows: Given two timestamps \\\\( t_1 \\\\) and \\\\( t_2 \\\\), and a tolerance level (or fuzzy threshold) \\\\( \\\\delta \\\\), the timestamps \\\\( t_1 \\\\) and \\\\( t_2 \\\\) are said to match if the absolute difference between them, denoted as \\\\( |t_1 - t_2| \\\\), does not exceed \\\\( \\\\delta \\\\). The fuzzy threshold \\\\( \\\\delta \\\\) is calculated as half the median of the differences between all sequential pairs of timestamps in the dataset.  \\n\\nBy employing fuzzy timestamp matching, the percentage of matched data can be substantially increased. For example, in the case of AG191 from Etosha, conventional timestamp matching resulted in a match percentage of 41.85%. With the application of fuzzy matching, this percentage rose to 74.50%. Notably, these additional matches, achieved through fuzzy matching, are proximal\\n\\n| Metric                          | Value                  |\\n|--------------------------------|------------------------|\\n| R-squared (zero-centered)      | 0.6871044690549571     |\\n| Offset (study \u2013 station)       | 9.84010669689293       |\\n| % of timestamps found          | 61.6%                  |\\n\\nTable 1: Statistics for Figure 2\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"marking a quantity evaluated at the stationary expansion point \\\\((U^\\\\circ, \\\\vec{r}^\\\\circ)\\\\) as \\\\(X^\\\\circ\\\\). We will use this notation throughout this work.\\\\(^c\\\\) Equation 7 yields an alternative formulation of Equation 4 using the exchanged second mixed derivative (Eq. 1).\\n\\n**Grand canonical energy of a stationary point** We can now insert the just derived potential-dependent geometric shift of a stationary point \\\\(\\\\Delta r^\\\\circ(\\\\Delta U)\\\\) (Eq. 7) into the second-order expansion of the gePES (Eq. 6) around a known stationary point \\\\((U^\\\\circ, \\\\vec{r}^\\\\circ)\\\\), eliminating the spatial dependence and returning the potential-dependent grand canonical energy \\\\(\\\\mathcal{E}^\\\\circ(U)\\\\) of a stationary point accurate to second order:\\n\\n\\\\[\\n\\\\mathcal{E}^\\\\circ(U) = \\\\mathcal{E}(U^\\\\circ, \\\\vec{r}^\\\\circ) - q^\\\\circ \\\\Delta U \\\\\\\\\\n+ \\\\frac{1}{2} \\\\left( \\\\sum_{i,j,k,l} (\\\\mathcal{H}_{j,k}^{-1}(\\\\frac{\\\\partial q}{\\\\partial r_k})^\\\\circ \\\\Delta U) \\\\mathcal{H}_{i,j}^{-1}(\\\\mathcal{H}_{i,l}^{-1}(\\\\frac{\\\\partial q}{\\\\partial r_l})^\\\\circ \\\\Delta U) \\\\right) \\\\\\\\\\n- 2 \\\\sum_{i,j} (\\\\frac{\\\\partial q}{\\\\partial r_i})^\\\\circ (\\\\mathcal{H}_{i,j}^{-1}(\\\\frac{\\\\partial q}{\\\\partial r_j})^\\\\circ \\\\Delta U) \\\\Delta U - C_{\\\\text{el}}^\\\\circ \\\\Delta U^2 + \\\\mathcal{O}(\\\\Delta U^3) ,\\n\\\\]\\n\\nwhere we dropped the force-contribution, since \\\\(\\\\mathcal{F}_i^\\\\circ = 0\\\\). Rearranging and using \\\\(\\\\sum_i \\\\mathcal{H}_{k,i}^{-1} \\\\mathcal{H}_{i,j}^\\\\circ = \\\\delta_{k,j}\\\\) then yields the energy \\\\(\\\\mathcal{E}^\\\\circ\\\\) of the stationary point at potential \\\\(U = U^\\\\circ + \\\\Delta U\\\\):\\n\\n\\\\[\\n\\\\mathcal{E}^\\\\circ(U) = \\\\mathcal{E}^\\\\circ - q^\\\\circ \\\\Delta U - \\\\frac{1}{2} \\\\left( C_{\\\\text{geom}}^\\\\circ + \\\\sum_{i,j} \\\\mathcal{H}_{i,j}^{-1}(\\\\frac{\\\\partial q}{\\\\partial r_i})^\\\\circ (\\\\frac{\\\\partial q}{\\\\partial r_j})^\\\\circ \\\\Delta U^2 + \\\\mathcal{O}(\\\\Delta U^3) \\\\right) .\\n\\\\]\\n\\nThis expression is essentially the (double-)integrated form of Equation 5. We could equally obtain it by integrating Equation 5 twice from the potential \\\\(U^\\\\circ\\\\) where all required properties are known\\n\\n\\\\(^c\\\\)As a further clarification on the difference between \\\\(X^\\\\circ\\\\) and \\\\(X^\\\\circ\\\\): \\\\(X^\\\\circ(U)\\\\) essentially is a shortcut for writing \\\\(X(U, \\\\vec{r}^\\\\circ(U))\\\\), i.e. for denoting quantities evaluated at the stationary point at the desired potential. The additional circle in \\\\(X^\\\\circ\\\\) indicates a property evaluated at a stationary expansion point. Essentially, we derive quantities \\\\(Y^\\\\circ\\\\) at a desired potential \\\\(U\\\\) based on quantities \\\\(X^\\\\circ\\\\) evaluated at a stationary expansion point \\\\((U^\\\\circ, \\\\vec{r}^\\\\circ(U^\\\\circ))\\\\), i.e. at a different potential \\\\(U^\\\\circ\\\\).\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"marking a quantity evaluated at the stationary expansion point \\\\((U^\\\\circ, \\\\vec{r}^\\\\circ)\\\\) as \\\\(X^\\\\circ\\\\). We will use this notation throughout this work.\\\\(^c\\\\) Equation 7 yields an alternative formulation of Equation 4 using the exchanged second mixed derivative (Eq. 1).\\n\\n**Grand canonical energy of a stationary point** We can now insert the just derived potential-dependent geometric shift of a stationary point \\\\(\\\\Delta r^\\\\circ(\\\\Delta U)\\\\) (Eq. 7) into the second-order expansion of the gePES (Eq. 6) around a known stationary point \\\\((U^\\\\circ, \\\\vec{r}^\\\\circ)\\\\), eliminating the spatial dependence and returning the potential-dependent grand canonical energy \\\\(\\\\mathcal{E}^\\\\circ(U)\\\\) of a stationary point accurate to second order:\\n\\n\\\\[\\n\\\\mathcal{E}^\\\\circ(U) = \\\\mathcal{E}(U^\\\\circ, \\\\vec{r}^\\\\circ) - q^\\\\circ \\\\Delta U \\\\\\\\\\n+ \\\\frac{1}{2} \\\\left( \\\\sum_{i,j,k,l} (\\\\mathcal{H}^\\\\circ_{j,k} - 1) \\\\left( \\\\frac{\\\\partial q}{\\\\partial r_k} \\\\right)^\\\\circ \\\\Delta U \\\\right) \\\\mathcal{H}^\\\\circ_{i,j} \\\\left( \\\\mathcal{H}^\\\\circ_{i,l} - 1 \\\\right) \\\\left( \\\\frac{\\\\partial q}{\\\\partial r_l} \\\\right)^\\\\circ \\\\Delta U \\\\\\\\\\n- 2 \\\\sum_{i,j} \\\\left( \\\\frac{\\\\partial q}{\\\\partial r_i} \\\\right)^\\\\circ \\\\left( \\\\mathcal{H}^\\\\circ_{i,j} - 1 \\\\right) \\\\left( \\\\frac{\\\\partial q}{\\\\partial r_j} \\\\right)^\\\\circ \\\\Delta U - C_{\\\\text{el}}^\\\\circ \\\\Delta U^2 \\\\right) + \\\\mathcal{O}(\\\\Delta U^3) ,\\n\\\\]\\n\\nwhere we dropped the force-contribution, since \\\\(\\\\mathcal{F}^\\\\circ_i = 0\\\\). Rearranging and using \\\\(\\\\sum_i \\\\mathcal{H}^\\\\circ_{k,i} = \\\\delta_{k,j}\\\\) then yields the energy \\\\(\\\\mathcal{E}^\\\\circ\\\\) of the stationary point at potential \\\\(U = U^\\\\circ + \\\\Delta U\\\\):\\n\\n\\\\[\\n\\\\mathcal{E}^\\\\circ(U) = \\\\mathcal{E}^\\\\circ - q^\\\\circ \\\\Delta U - \\\\frac{1}{2} \\\\left( C_{\\\\text{el}}^\\\\circ + \\\\sum_{i,j} \\\\mathcal{H}^\\\\circ_{i,j} - 1 \\\\left( \\\\frac{\\\\partial q}{\\\\partial r_i} \\\\right)^\\\\circ \\\\left( \\\\frac{\\\\partial q}{\\\\partial r_j} \\\\right)^\\\\circ \\\\right) \\\\Delta U^2 + \\\\mathcal{O}(\\\\Delta U^3) .\\n\\\\]\\n\\nThis expression is essentially the (double-)integrated form of Equation 5. We could equally obtain it by integrating Equation 5 twice from the potential \\\\(U^\\\\circ\\\\) where all required properties are known\\n\\n\\\\(^c\\\\)As a further clarification on the difference between \\\\(X^\\\\circ\\\\) and \\\\(X^\\\\circ\\\\): \\\\(X^\\\\circ(U)\\\\) essentially is a shortcut for writing \\\\(X(U, \\\\vec{r}^\\\\circ(U))\\\\), i.e. for denoting quantities evaluated at the stationary point at the desired potential. The additional circle in \\\\(X^\\\\circ\\\\) indicates a property evaluated at a stationary expansion point. Essentially, we derive quantities \\\\(Y^\\\\circ\\\\) at a desired potential \\\\(U\\\\) based on quantities \\\\(X^\\\\circ\\\\) evaluated at a stationary expansion point \\\\((U^\\\\circ, \\\\vec{r}^\\\\circ(U^\\\\circ))\\\\), i.e. at a different potential \\\\(U^\\\\circ\\\\).\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and scales like $m_\\\\nu^4$ such that the dynamical friction effect is dominated by the most massive neutrino eigenstate. We may then assume a single neutrino species for simplicity, or explicitly write the total dynamical friction force as a sum of individual contribution from different eigenstates.\\n\\nIn order to connect Eq. (3.19) with the results of future sections, it is convenient to rewrite it in terms of a quantity with dimension of inverse time. Since $F = Mdv_H/dt$, we can define:\\n\\n$$\\\\tau^{-1} = -\\\\frac{\\\\vec{F} \\\\cdot \\\\vec{v}_H}{Mv_H^2} = \\\\frac{2}{3\\\\pi} \\\\log AG^2 M m_\\\\nu^4 = 3.4 \\\\times 10^{-5} \\\\frac{\\\\log \\\\Lambda}{\\\\log 100} \\\\frac{M}{10^{13} M_\\\\odot} \\\\left(\\\\frac{m_\\\\nu}{0.1 \\\\text{eV}}\\\\right)^4 H_0,$$\\n\\nwhich is the characteristic time scale for an order one fractional decrease in the halo velocity due to the dynamical friction effect. Note that $1/\\\\tau H_0 = \\\\Delta v/v$ is the overall relative decrease in the halo velocity over the age of the Universe $t \\\\sim 1/H_0$. We obtain a numerical value of $\\\\Delta v/v = 3.4 \\\\times 10^{-5}$ for a halo mass $M = 10^{13} M_\\\\odot$ and individual neutrino mass $m_\\\\nu = 0.1 \\\\text{eV}$, when also assuming $\\\\Lambda = 100$. This already suggests that the dynamical friction effect is quite small, although it can pick up some significant contributions from the clustering of nearby halos as we will see in Sec. 5.\\n\\n### 3.3 Limitations to the 1-halo approach\\n\\nThus far we have determined the anisotropic clustering of massive neutrinos behind moving point mass halos and the corresponding dynamical friction force. A more realistic calculation would have to account for both the finite extent of the halo and the presence of large-scale structure. Indeed, the Eq. (3.20) involves an unknown Coulomb logarithm, $\\\\log \\\\Lambda$, where in typical applications of the dynamical friction formula the cutoff $\\\\Lambda$ can be estimated as the ratio of maximum and minimum impact parameters, $\\\\Lambda \\\\sim b_{\\\\text{max}}/b_{\\\\text{min}}$ [51]. Here $b_{\\\\text{min}} \\\\sim R_{\\\\text{halo}}$ is the halo radius, and $b_{\\\\text{max}} \\\\sim \\\\lambda_{\\\\text{coh}} \\\\sim 0.1 \\\\text{Mpc}^{-1}$ is the CDM velocity coherence scale. The CDM bulk flow is only coherent over sufficiently small scales and hence our analysis based on a single moving halo is expected to break down at scales $\\\\lambda \\\\gtrsim \\\\lambda_{\\\\text{coh}}$.\\\\(^3\\\\) This point will be made more clear in the next section, where we also provide a precise definition for the velocity coherence scale.\\n\\n\\\\(^3\\\\)We should also impose a cutoff corresponding to the distance traveled by free-streaming neutrinos, which sets the scale where neutrino inhomogeneities are coherent with CDM. As we shall see the neutrino free-streaming scale\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and scales like $m_\\\\nu^4$ such that the dynamical friction effect is dominated by the most massive neutrino eigenstate. We may then assume a single neutrino species for simplicity, or explicitly write the total dynamical friction force as a sum of individual contribution from different eigenstates. Thus far we have determined the anisotropic clustering of massive neutrinos behind moving point mass halos and the corresponding dynamical friction force. A more realistic calculation would have to account for both the finite extent of the halo and the presence of large-scale structure. Indeed, this already suggests that the dynamical friction effect In order to connect Eq. ( 3.19 ) with the results of future sections, it is convenient to rewrite it in terms of a quantity with dimension of inverse time. Since $F = M dv_H / dt$, we can define:\\n\\n$$\\n\\\\tau^{-1} = - \\\\frac{\\\\vec{F} \\\\cdot \\\\vec{v}_H}{M v_H^2} = \\\\frac{2}{3\\\\pi} \\\\log A G^2 M m_\\\\nu^4 = 3.4 \\\\times 10^{-5} \\\\frac{\\\\log \\\\Lambda}{\\\\log 100} \\\\frac{M}{10^{13} M_\\\\odot} \\\\left( \\\\frac{m_\\\\nu}{0.1 \\\\text{eV}} \\\\right)^4 H_0 ,\\n$$\\n\\nwhich is the characteristic time scale for an order one fractional decrease in the halo velocity due to the dynamical friction effect. Note that $1/\\\\tau H_0 = \\\\Delta v / v$ is the overall relative decrease in the halo velocity over the age of the Universe $t \\\\sim 1 / H_0$. We obtain a numerical value of $\\\\Delta v / v = 3.4 \\\\times 10^{-5}$ for a halo mass $M = 10^{13} M_\\\\odot$ and individual neutrino mass $m_\\\\nu = 0.1 \\\\text{eV}$, when also assuming $\\\\Lambda = 100$. This already suggests that the dynamical friction effect is quite small, although it can pick up some significant contributions from the clustering of nearby halos as we will see in Sec. 5 .\\n\\n### 3.3 Limitations to the 1-halo approach\\n\\nThus far we have determined the anisotropic clustering of massive neutrinos behind moving point mass halos and the corresponding dynamical friction force. A more realistic calculation would have to account for both the finite extent of the halo and the presence of large-scale structure. Indeed, the Eq. ( 3.20 ) involves an unknown Coulomb logarithm, $\\\\log \\\\Lambda$, where in typical applications of the dynamical friction formula the cutoff $\\\\Lambda$ can be estimated as the ratio of maximum and minimum impact parameters, $\\\\Lambda \\\\sim b_{\\\\text{max}} / b_{\\\\text{min}}$ [51]. Here $b_{\\\\text{min}} \\\\sim R_{\\\\text{halo}}$ is the halo radius, and $b_{\\\\text{max}} \\\\sim \\\\lambda_{\\\\text{coh}} \\\\sim 0.1 \\\\text{Mpc}^{-1}$ is the CDM velocity coherence scale. The CDM bulk flow is only coherent over sufficiently small scales and hence our analysis based on a single moving halo is expected to break down at scales $\\\\lambda \\\\gtrsim \\\\lambda_{\\\\text{coh}}$. This point will be made We should also impose a cutoff corresponding to the distance traveled by free-streaming neutrinos, which sets the scale where neutrino inhomogeneities are coherent with CDM. As we shall see the neutrino free-streaming scale\\n\\n![Figure 2](image-url)\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Branches Mutual Promotion for End-to-End Weakly Supervised Semantic Segmentation\\n\\nLei Zhu, Hangzhou He, Xinliang Zhang, Qian Chen, Shuang Zeng, Qiushi Ren, Yanye Lu*\\n\\nAbstract\u2014End-to-end weakly supervised semantic segmentation aims at optimizing a segmentation model in a single-stage training process based on only image annotations. Existing methods adopt an online-trained classification branch to provide pseudo annotations for supervising the segmentation branch. However, this strategy makes the classification branch dominate the whole concurrent training process, hindering these two branches from assisting each other. In our work, we treat these two branches equally by viewing them as diverse ways to generate the segmentation map, and add interactions on both their supervision and operation to achieve mutual promotion. For this purpose, a bidirectional supervision mechanism is elaborated to force the consistency between the outputs of these two branches. Thus, the segmentation branch can also give feedback to the classification branch to enhance the quality of localization seeds. Moreover, our method also designs interaction operations between these two branches to exchange their knowledge to assist each other. Experiments indicate our work outperforms existing end-to-end weakly supervised segmentation methods.\\n\\nIndex Terms\u2014Weakly Supervised Learning, Image Segmentation, Object Localization\\n\\nI. INTRODUCTION\\n\\nSEMANTIC segmentation is a primary vision task, aiming to annotate pixels in an image as target objects or backgrounds. However, training a segmentation model in a fully-supervised manner requires annotating all pixels in training images, costing extensive human resources. To solve this problem, weakly supervised semantic segmentation (WSSS) appears and attracts extensive attention, which adopts only image-level annotation for the training process. However, as shown in Fig. 1 A, WSSS methods usually require multiple training stages, e.g., tuning a classification network with image annotations to produce localization seeds [1]\u2013[3], deriving pseudo annotations after refining the seeds [4]\u2013[6], and finally training the segmentation network with the pseudo annotations [7], [8].\\n\\nRecently, some end-to-end weakly supervised semantic segmentation (E2E-WSSS) methods arose to simplify the heavy multi-stage training process into a single stage [9]\u2013[12]. As shown in Fig. 1 B, these methods train a two-branch network in only a single stage, where the classification branch supervised by image-level annotation can online provide pseudo annotations for the segmentation branch. Compared with multi-stage WSSS, the concurrently-trained classification branch cannot stably provide seed to derive accurate pseudo annotations for supervising the segmentation branch. So, existing E2E-WSSS methods focus on improving the classification branch to provide better supervision by refining the localization seed with online spatial propagation [11]\u2013[13] or determining reliable regions on the pseudo annotations [9], [10].\\n\\nIn our work, we argue that current E2E-WSSS methods may fall into a trap, following the multi-stage WSSS to unidirectionally supervise the segmentation branch based on the prediction of the classification branch, without considering the feedback of the segmentation branch. In this way, the classification branch will dominate the whole training process, even if it may perform worse than the segmentation branch, as visualized in Fig. 2. Thus, the classification branch will converge to a similar optimum as the offline trained classification network but cannot stably provide pseudo annotations for the segmentation branch, which causes the large performance gap between current E2E-WSSS and multi-stage WSSS methods.\\n\\nActually, in the E2E-WSSS setting, these two branches are basically at equal status because they are concurrently optimized during training. From another perspective, the segmentation branch can also assist the concurrently-trained classification branch in generating better localization seeds, which is a crucial trait of E2E-WSSS and yet to be explored by existing methods. Based on this perspective, our work treats these two branches equally by viewing them as diverse ways to achieve the same goal, generating the segmentation map of input images. Thus, as shown in Fig. 1 C, interactions are\\n\\nFig. 1. Comparison of WSSS strategies: A. Multi-stage WSSS contains multiple training stages. B. Existing E2E-WSSS unidirectionally supervises the segmentation branch with pseudo annotations online provided by the classification branch. C. Our proposed E2E-WSSS strategy interacts both the supervision and operation between these two branches to achieve mutual promotion.\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Branches Mutual Promotion for End-to-End Weakly Supervised Semantic Segmentation\\n\\nLei Zhu, Hangzhou He, Xinliang Zhang, Qian Chen, Shuang Zeng, Qiushi Ren, Yanye Lu*  \\n\\nAbstract\u2014End-to-end weakly supervised semantic segmen- \\ntation aims at optimizing a segmentation model in a single- \\nstage training process based on only image annotations. Existing \\nmethods adopt an online-trained classification branch to provide \\npseudo annotations for supervising the segmentation branch. \\nHowever, this strategy makes the classification branch dominate \\nthe whole concurrent training process, hindering these two \\nbranches from assisting each other. In our work, we treat \\nthese two branches equally by viewing them as diverse ways \\nto generate the segmentation map, and add interactions on both \\ntheir supervision and operation to achieve mutual promotion. For \\nthis purpose, a bidirectional supervision mechanism is elaborated \\nto force the consistency between the outputs of these two \\nbranches. Thus, the segmentation branch can also give feedback \\nto the classification branch to enhance the quality of localization \\nseeds. Moreover, our method also designs interaction operations \\nbetween these two branches to exchange their knowledge to assist \\neach other. Experiments indicate our work outperforms existing \\nend-to-end weakly supervised segmentation methods. \\n\\nIndex Terms\u2014Weakly Supervised Learning, Image Segmenta- \\ntion, Object Localization\\n\\nI. INTRODUCTION\\n\\nSEMANTIC segmentation is a primary vision task, aiming \\nto annotate pixels in an image as target objects or backgrounds. However, training a segmentation model in a fullysupervised manner requires annotating all pixels in training images, costing extensive human resources. To solve this problem, weakly supervised semantic segmentation (WSSS) appears and attracts extensive attention, which adopts only image-level annotation for the training process. However, as shown in Fig. 1 A, WSSS methods usually require multiple training stages, e.g., tuning a classification network with image annotations to produce localization seeds [1]\u2013[3], deriving pseudo annotations after refining the seeds [4]\u2013[6], and finally training the segmentation network with the pseudo annotations [7], [8].\\n\\nRecently, some end-to-end weakly supervised semantic segmentation (E2E-WSSS) methods arose to simplify the heavy multi-stage training process into a single stage [9]\u2013[12]. As shown in Fig. 1 B, these methods train a two-branch network in only a single stage, where the classification branch supervised by image-level annotation can online provide pseudo annotations for supervising the segmentation branch. Compared with multi-stage WSSS strategies: the concurrently-trained classification branch cannot stably provide seed to derive accurate pseudo annotations for supervising the segmentation branch. So, existing E2E-WSS . Multi-stage WSSS contains multiple training stages. B. Existing E2E-WSSS unidirectionally supervises the segmentation branch with pseudo annotations online provided by the classification branch. C. Our proposed E2E-WSSS) methods arose to simplify the heavy multi-stage training process into a single stage [9]\u2013 [12]. As shown in Fig. 1 B, these methods train a twobranch network in only a single stage, where the classification branch supervised by image-level annotation can online provide pseudo annotations for the segmentation branch. Compared with multi-stage WSSS, the concurrently-trained classification branch cannot stably provide seed to derive accurate pseudo annotations for supervising the segmentation branch. So, existing E2E-WSSS methods focus on improving the classification branch to provide better supervision by refining the localization seed with online spatial propagation [11]\u2013[13] or determining reliable regions on the pseudo annotations [9], [10]. In our work, we argue that current E2E-WSSS methods may fall into a trap, following the multi-stage WSSS to unidirectionally supervise the segmentation branch based on the prediction of the classification branch, without considering the feedback of the segmentation branch. In this way, the classification branch will dominate the whole training process, even if it may perform worse than the segmentation branch, as visualized in Fig. 2. Thus, the classification branch will converge to a similar optimum as the offline trained classification network but cannot stably provide pseudo annotations for the segmentation branch, which causes the large performance gap between current E2E-WSSS and multi-stage WSSS methods. Actually, in the E2E-WSSS setting, these two branches are basically at equal status because they are concurrently optimized during training. From another perspective, the segmentation branch can also assist the concurrently-trained classification branch in generating better localization seeds, which is a crucial trait of E2E-WSSS and yet to be explored by existing methods. Based on this perspective, our work treats these two branches equally by viewing them as diverse ways to achieve the same goal, generating the segmentation map of input images. Thus, as shown in Fig. 1 C, interactions are\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"shown that the dependence of mining rewards on propagation latency is more intricate than this [35]. Specifically, an honest miner that is well connected with other miners inadvertently creates efficient, low latency paths for other miners by acting as a centrally located bridge between the miners. However, to maximize the marginal gains in reward due to the network, it is important for a miner to have paths to other miners that are, on average, of a lower delay relative to the delays of paths between other miners. For example, if miners are arranged as a star topology with links of unit delay and uniform compute power across nodes, the central node receives a higher reward compared to the leaf nodes by including more blocks on the blockchain. On the other hand, on a complete graph topology with unit delay links and uniform compute power as before, all nodes receive the same reward. A node identically connected to other nodes in the two cases (i.e., the central node in the star topology and any arbitrary node in the complete graph topology: both have direct links to all other nodes) receives different rewards, as rewards depend not only on the node\u2019s own connections but also on how other nodes\u2019 connections. Thus, there is an inherent tension for a miner in increasing her own connectivity to the rest of the network while simultaneously ensuring that the connectivity between other miners do not significantly increase. A systematic research of this tension, and efficient connection policies to maximize marginal mining reward gain due to the network, have not been done to our best knowledge.\\n\\nIn this work, we formalize the p2p topology construction problem as a game between miners and present Cobalt, a decentralized policy for optimizing reward. We consider a simplified setting where only a single node chooses its connections, while the rest of the network\u2019s topology is fixed. We assume that the global topology of the p2p network is unknown to miners. We thus model the problem of optimizing rewards by the connections-deciding miner node as a Markov decision process (MDP) with no state and an action set with a combinatorial number of actions.\\n\\nWe derive the optimal neighbor selection policy using a combinatorial multi-armed bandit (MAB) approach [14]. In the MAB algorithm, the agent (miner) explores various candidate connection configurations, and gradually adapts its connections based on past experience to gain the most mining rewards. A key contribution of our work is a network coordinates based model for efficiently learning the MAB environment [19]. In this model, miners are assigned real-valued vectors from an Euclidean space, which capture the relative location of miners with respect to each other in the network. The coordinates are continuously updated based upon the reward feedback the agent receives from the environment. Thus, despite not having global knowledge of the network initially, we show that it is possible for an agent to learn about the network by just using the observed reward information.\\n\\nTo enable the deployment of MAB algorithm, we have built a simulator. To simplify the reward computation in the simulator, rather than simulating the actual mining process at each step of the MDP, we consider a computationally easier function that only depends on the pairwise shortest path lengths between miners. Importantly, our MDP reward function captures the property that a miner\u2019s mining gains depends on how small the shortest path lengths between the agent and other miners are relative to the shortest path lengths between other miners. Experimentally we show Cobalt outperforms or matches heuristics on diverse network settings.\\n\\nII. RELATED WORK\\n\\nP2P network design for optimizing mining rewards has remained a relatively under-explored topic in the community. The work that is closest to our is Perigee [34] which proposes an adaptive peer-selection algorithm for minimizing block propagation latency in the network. However, Perigee does not model the game-theoretic competition between miners. Subsequent works [11], [43] consider optimizing the network to maximize extractable value (MEV) from transactions. A number of prior works have exposed the impact of the network on mining [12], [26], [28], [37], [40], [47], [48]. While these works generally suggest that better network connectivity translates to higher mining rewards earned, the competitive effects of network connectivity and methods to optimize them have not been discussed. Other related works include KadCast [38] which proposes a Kademlia-based structured overlay for efficient block broadcast, and relay networks such as BloXroute [29] for transports blocks quickly across vast geographic distances.\\n\\nThe idea of network coordinates for p2p networks has been prominently explored in the network systems literature since the turn of the millenium, including distributed approaches to learn them [19], [32], [36]. More recently, a number of theoretical works have studied using low-distortion embeddings in finite metrics (i.e., over finite graphs) for various applications, e.g., sparse spanner construction [10], [13], [16], [21].\\n\\nGame theory of blockchains, especially at the consensus layer, has received considerable attention. For example, Lewenberg et al. [33] use game theory to study how mining rewards can be shared across members of a mining pool. On the other hand, prior works have considered various network games outside the context of blockchains [24], [39]. Our work is the first (to our best knowledge) to consider network games in blockchains.\\n\\nIII. PROBLEM FORMULATION\\n\\nLet us consider a complete directed graph $G = (\\\\mathcal{V}, \\\\mathcal{E})$, where $\\\\mathcal{V}$ is the set of nodes and $\\\\mathcal{E}$ is the set of directed edges. Each node in the graph represents a mining server. The hash rate of the mining server $v$ is denoted by $H_v$. We use $H$ to denote the hash rate vector $H := (H_v)_{v \\\\in \\\\mathcal{V}}$. A directed edge $(v_1, v_2) \\\\in \\\\mathcal{E}$ represents a (TCP) link between the nodes $v_1, v_2$. The directed edge represents that node $v_1$ can send messages (e.g., transactions, blocks etc.) to $v_2$ as and when required by the protocol. The time take for a message sent from $v_1$ to reach $v_2$ along the link $(v_1, v_2)$ is denoted by $l(v_1, v_2) \\\\geq 0$.\\n\\n1We assume if $(v_1, v_2) \\\\in \\\\mathcal{E}$ then $(v_2, v_1) \\\\in \\\\mathcal{E}$ for all $v_1, v_2 \\\\in \\\\mathcal{V}$. \"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"shown that the dependence of mining rewards on propagation latency is more intricate than this [35]. Specifically, an honest miner that is well connected with other miners inadvertently creates efficient, low latency paths for other miners by acting as a centrally located bridge between the miners. However, to maximize the marginal gains in reward due to the network, it is important for a miner to have paths to other miners that are, on average, of a lower delay relative to the delays of paths between other miners. For example, if miners are arranged as a star topology with links of unit delay and uniform compute power across nodes, the central node receives a higher reward compared to the leaf nodes by including more blocks on the blockchain. On the other hand, on a complete graph topology with unit delay links and uniform compute power as before, all nodes receive the same reward. A node identically connected to other nodes in the two cases (i.e., the central node in the star topology and any arbitrary node in the complete graph topology: both have direct links to all other nodes) receives different rewards, as rewards depend not only on the node\u2019s own connections but also on how other nodes\u2019 connections. Thus, there is an inherent tension for a miner in increasing her own connectivity to the rest of the network while simultaneously ensuring that the connectivity between other miners do not significantly increase. A systematic research of this tension, and efficient connection policies to maximize marginal mining reward gain due to the network, have not been done to our best knowledge. \\n\\nIn this work, we formalize the p2p topology construction problem as a game between miners and present Cobalt, a decentralized policy for optimizing reward. We consider a simplified setting where only a single node chooses its connections, while the rest of the network\u2019s topology is fixed. We assume that the global topology of the p2p network is unknown to miners. We thus model the problem of optimizing rewards by the connections-deciding miner node as a Markov decision process (MDP) with no state and an action set with a combinatorial number of actions. \\n\\nWe derive the optimal neighbor selection policy using a combinatorial multi-armed bandit (MAB) approach [14]. In the MAB algorithm, the agent (miner) explores various candidate connection configurations, and gradually adapts its connections based on past experience to gain the most mining rewards. A key contribution of our work is a network coordinates based model for efficiently learning the MAB environment [19]. In this model, miners are assigned realvalued vectors from an Euclidean space, which capture the relative location of miners with respect to each other in the network. The coodinates are continuously updated based upon the reward feedback the agent receives from the environment. Thus, despite not having global knowledge of the network initially, we show that it is possible for an agent to learn about the network by just using the observed reward information. \\n\\nTo enable the deployment of MAB algorithm, we have built a simulator. To simplify the reward computation in the simulator, rather than simulating the actual mining process at each step of the MDP, we consider a computationally easier function that only depends on the pairwise shortest path lengths between miners. Importantly, our MDP reward function captures the property that a miner\u2019s mining gains depends on how small the shortest path lengths between the agent and other miners are relative to the shortest path lengths between other miners. Experimentally we show Cobalt outperforms or matches heuristics on diverse network settings. \\n\\nII. RELATED WORK\\n\\nP2P network design for optimizing mining rewards has remained a relatively under-explored topic in the community. The work that is closest to our is Perigee [34] which proposes an adaptive peer-selection algorithm for minimizing block propagation latency in the network. However, Perigee does not model the game-theoretic competition between miners. Subsequent works [11], [43] consider optimizing the network to maximize extractable value (MEV) from transactions. A number of prior works have exposed the impact of the network on mining [12], [26], [28], [37], [40], [47], [48]. While these works generally suggest that better network connectivity translates to higher mining rewards earned, the competitive effects of network connectivity and methods to optimize them have not been discussed. Other related works include KadCast [38] which proposes a Kademila-based structured overlay for efficient block broadcast, and relay networks such as BloXroute [29] for transports blocks quickly across vast geographic distances.\\n\\nThe idea of network coordinates for p2p networks has been prominently explored in the network systems literature since the turn of the millenium, including distributed approaches to learn them [19], [32], [36]. More recently, a number of theoretical works have studied using low-distortion embeddings in finite metrics (i.e., over finite graphs) for various applications, e.g., sparse spanner construction [10], [13], [16], [21].\\n\\nGame theory of blockchains, especially at the consensus layer, has received considerable attention. For example, Lewenberg et al. [33] use game theory to study how mining rewards can be shared across members of a mining pool. On the other hand, prior works have considered various network games outside the context of blockchains [24], [39]. Our work is the first (to our best knowledge) to consider network games in blockchains.\\n\\nIII. PROBLEM FORMULATION\\n\\nLet us consider a complete directed graph $G = (V, E)$, where $V$ is the set of nodes and $E$ is the set of directed edges. Each node in the graph represents a mining server. The hash rate of the mining server $v$ is denoted by $H_v$. We use $H$ to denote the hash rate vector $H := (H_v)_{v \\\\in V}$. A directed edge $(v_1, v_2) \\\\in E$ represents a (TCP) link between the nodes $v_1, v_2$. The directed edge represents that node $v_1$ can send messages (e.g., transactions, blocks etc.) to $v_2$ as and when required by the protocol. The time take for a message sent from $v_1$ to reach $v_2$ along the link $(v_1, v_2)$ is denoted by $l(v_1, v_2) \\\\geq 0$.\\n\\n1We assume if $(v_1, v_2) \\\\in E$ then $(v_2, v_1) \\\\in E$ for all $v_1, v_2 \\\\in V$. \\n\\n\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"COMO-ViT. Given the input feature $F_{l-1} \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times c}$ of COMO-ViT, we conduct two branches of operations. In the first branch, we uniformly split it into $n$ non-overlapping windows $\\\\mathcal{P} = [P^1, P^2, \\\\ldots, P^n] \\\\in \\\\mathbb{R}^{n \\\\times w \\\\times w \\\\times c}$, where $(w, w)$ is the window resolution. SNR [43] and STAR [52] downsample images, losing local structures and some important pixel-level information. Instead, the proposed COMO-ViT completely models the dependencies among all pixels of an image via a local-to-global hierarchical self-attention. Locally, each pixel in a window $P^i$ is regarded as an individual, we thus reshape $P^i$ as follows:\\n\\n$$P^i \\\\rightarrow [p^{i,1}, p^{i,2}, \\\\ldots, p^{i,m}],$$\\n\\nwhere $p^{i,j} \\\\in \\\\mathbb{R}^{1 \\\\times 1 \\\\times c}$, $m = w^2$ is the number of pixels in $P^i$. With a linear projection, we then transform the pixels into a sequence of pixel embeddings $X^i = [x^{i,1}, x^{i,2}, \\\\ldots, x^{i,m}]$, where $x^{i,j} \\\\in \\\\mathbb{R}^c$ is the $j$-th pixel embedding, $c$ is the embedding dimension. For $X^i$, we utilize a local Transformer module to extract deep features as follows:\\n\\n$$Y^{i} = X^i + \\\\text{MSA}(\\\\text{LN}(X^i)), \\\\quad Y^{i} = Y^{i} + \\\\text{MLP}(\\\\text{LN}(Y^{i})), \\\\quad (11)$$\\n\\nwhere $Y^i$ is the feature learned by the local Transformer module, MSA(\u00b7) is the Multi-head Self-Attention [35], LN(\u00b7) is layer normalization [1] for stable training and faster convergence, MLP(\u00b7) is multi-layer perceptron for feature transformation at channel dimension and non-linearity. In such a process, we adopt 1D learnable location embedding to encode the spatial information of pixels.\\n\\nTo complement the non-overlapping window attention, in the second branch which is parallel with local attention, we use a CNN module to model local pixel dependencies in $F_{l-1}$ via an overlapped sliding kernel to recover image details, in which a SE block [11] is used to explore channel relationship to boost representative power:\\n\\n$$F' = \\\\text{Conv}(\\\\text{LN}(F_{l-1})), \\\\quad F_{\\\\text{conv}} = F' \\\\odot \\\\text{SE}(F'). \\\\quad (12)$$\\n\\n$F_{\\\\text{conv}}$ is then split into $n$ non-overlapping windows $Q = [Q^1, Q^2, \\\\ldots, Q^n] \\\\in \\\\mathbb{R}^{n \\\\times w \\\\times w \\\\times c}$, and each $Q^i$ is reshaped:\\n\\n$$Q^i \\\\rightarrow [q^{i,1}, q^{i,2}, \\\\ldots, q^{i,m}], \\\\quad (13)$$\\n\\nWe combine the features from both branches as:\\n\\n$$C = [C^1, C^2, \\\\ldots, C^n], \\\\quad C^i = Q^i + Y^i \\\\quad (14)$$\\n\\nGlobal pixel dependencies are explored by calculating window attention via a global attention module. Firstly, $C$ is transformed into a sequence of window embedding:\\n\\n$$U = [u^1, u^2, \\\\ldots, u^n], \\\\quad u^i = \\\\text{FC}(\\\\text{Vec}(C^i)), \\\\quad (15)$$\\n\\nwhere Vec(\u00b7) is vectorization operation. Then, we utilize a global Transformer module to explore inter-window dependencies, obtaining the feature $F_l \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times c}$, where $l \\\\in \\\\{1, 2, \\\\ldots, L\\\\}$, and $L$ is the COMO-ViT number. When $l = 1$, $F_{l-1}$ is the fused feature $F_f$ in Eq. (9).\\n\\nThe result of the second stage is obtained by decoding $F_L$ with a convolutional layer ($D(\u00b7)$):\\n\\n$$R_{s2} = \\\\text{Conv}(F_L). \\\\quad (16)$$\\n\\nWe visually show $R_{s2}$ in Fig. 4, observing that the illumination is enhanced and image details are also recovered. Especially, the noise is well removed by our COMO-ViT.\\n\\nLGCM. LGCM takes $R_{s2}$ as input, and learns local deep features to perceive illumination gap between $R_{s2}$ and ground truth and elaborately enhances illumination to reduce local color deviation:\\n\\n$$\\\\Gamma_l = \\\\varphi(\\\\text{Conv}_3(R_{s2})), \\\\quad R_{s3} = R_{s2}^{\\\\Gamma_l}. \\\\quad (17)$$\\n\\n| Epochs | Optimizer | Batch size | Learning rate | LR decay | Weight decay | Drop path | Embedding dim | Head | $L$ | Window size | Patch size |\\n|--------|-----------|------------|---------------|----------|--------------|-----------|---------------|------|----|-------------|------------|\\n| 300    | Adam[17]  | 8          | 4e-4          | cosine   | 1e-7         | 0.1       | 15            | 5    | 2  | 16          | 512        |\\n\\nTable 1. Default training and network hyper-parameters used in our method, unless stated otherwise.\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"COMO-ViT, we conduct two branches of operations. In the first branch, we uniformly split it into \\\\( n \\\\) non-overlapping windows \\\\( \\\\mathcal{P} = [\\\\mathbf{P}^1, \\\\mathbf{P}^2, \\\\ldots, \\\\mathbf{P}^n] \\\\in \\\\mathbb{R}^{n \\\\times w \\\\times w \\\\times c} \\\\), where \\\\((w, w)\\\\) is the window resolution. SNR [43] and STAR [ 52 ] downsample images, losing local structures and some important pixel-level information. Instead, the proposed COMO-ViT completely models the dependencies among all pixels of an image via a local-to-global hierarchical selfattention. Locally, each pixel in a window \\\\( \\\\mathbf{P}^i \\\\) is regarded as an individual, we thus reshape \\\\( \\\\mathbf{P}^i \\\\) as follows:\\n\\n\\\\[\\nP^i \\\\rightarrow [p^{i,1}, p^{i,2}, \\\\ldots, p^{i,m}],\\n\\\\]\\n\\nwhere \\\\( p^{i,j} \\\\in \\\\mathbb{R}^{1 \\\\times 1 \\\\times c} \\\\), \\\\( m = w^2 \\\\) is the number of pixels in \\\\( \\\\mathbf{P}^i \\\\). With a linear projection, we then transform the pixels into a sequence of pixel embeddings \\\\( \\\\mathbf{X}^i = [x^{i,1}, x^{i,2}, \\\\ldots, x^{i,m}] \\\\), where \\\\( x^{i,j} \\\\in \\\\mathbb{R}^c \\\\) is the \\\\( j \\\\)-th pixel embedding, \\\\( c \\\\) is the embedding dimension. For \\\\( \\\\mathbf{X}^i \\\\), we utilize a local Transformer module to extract deep features as follows: \\\\( \\\\mathbf{Y}^i = \\\\mathbf{X}^i + \\\\text{MSA}(\\\\text{LN}(\\\\mathbf{X}^i)) \\\\), \\\\( \\\\mathbf{Y}^i = \\\\mathbf{Y}^i + \\\\text{MLP}(\\\\text{LN}(\\\\mathbf{Y}^i)) \\\\),\\n\\nwhere \\\\( \\\\mathbf{Y}^i \\\\) is the feature learned by the local Transformer module, \\\\( \\\\text{MSA}(\\\\cdot) \\\\) is the Multi-head Self-Attention [ 35 ], \\\\( \\\\text{LN}(\\\\cdot) \\\\) is layer normalization [ 1 ] for stable training and faster convergence, \\\\( \\\\text{MLP}(\\\\cdot) \\\\) is multi-layer perceptron for feature transformation at channel dimension and nonlinearity. In such a process, we adopt 1D learnable location embedding to encode the spatial information of pixels. To complement the non-overlapping window attention, in the second branch which is parallel with local attention, we use a CNN module to model local pixel dependencies in \\\\( \\\\mathbf{F}_{l-1} \\\\) via an overlapped sliding kernel to recover image details, in which a SE block [ 11 ] is used to explore channel relationship to boost representative power: \\\\( \\\\mathbf{F}' = \\\\text{Conv}(\\\\text{LN}(\\\\mathbf{F}_{l-1})), \\\\quad \\\\mathbf{F}_{\\\\text{conv}} = \\\\mathbf{F}' \\\\odot \\\\text{SE}(\\\\mathbf{F}') \\\\).\\n\\n\\\\( \\\\mathbf{F}_{\\\\text{conv}} \\\\) is then split into \\\\( n \\\\) non-overlapping windows \\\\( \\\\mathcal{Q} = [\\\\mathbf{Q}^1, \\\\mathbf{Q}^2, \\\\ldots, \\\\mathbf{Q}^n] \\\\in \\\\mathbb{R}^{n \\\\times w \\\\times w \\\\times c} \\\\), and each \\\\( \\\\mathbf{Q}^i \\\\) is reshaped:\\n\\n\\\\[\\n\\\\mathbf{Q}^i \\\\rightarrow [q^{i,1}, q^{i,2}, \\\\ldots, q^{i,m}],\\n\\\\]\\n\\nWe combine the features from both branches as:\\n\\n\\\\[\\n\\\\mathcal{C} = [\\\\mathbf{C}^1, \\\\mathbf{C}^2, \\\\ldots, \\\\mathbf{C}^n], \\\\quad \\\\mathbf{C}^i = \\\\mathbf{Q}^i + \\\\mathbf{Y}^i\\n\\\\]\\n\\nGlobal pixel dependencies are explored by calculating window attention via a global attention module. Firstly, \\\\( \\\\mathcal{C} \\\\) is transformed into a sequence of window embedding:\\n\\n\\\\[\\n\\\\mathbf{U} = [u^1, u^2, \\\\ldots, u^n], \\\\quad u^i = \\\\text{FC}(\\\\text{Vec}(\\\\mathbf{C}^i)),\\n\\\\]\\n\\nwhere \\\\( \\\\text{Vec}(\\\\cdot) \\\\) is vectorization operation. Then, we utilize a global Transformer module to explore inter-window dependencies, obtaining the feature \\\\( \\\\mathbf{F}_l \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times c} \\\\), where \\\\( l \\\\in \\\\{1, 2, \\\\ldots, L\\\\} \\\\), and \\\\( L \\\\) is the COMO-ViT number. When \\\\( l = 1 \\\\), \\\\( \\\\mathbf{F}_{l-1} \\\\) is the fused feature \\\\( \\\\mathbf{F}_f \\\\) in Eq. (9).\\n\\nThe result of the second stage is obtained by decoding \\\\( \\\\mathbf{F}_L \\\\) with a convolutional layer (\\\\( \\\\mathcal{D}(\\\\cdot) \\\\)):\\n\\n\\\\[\\n\\\\mathbf{R}_{s2} = \\\\text{Conv}(\\\\mathbf{F}_L).\\n\\\\]\\n\\nWe visually show \\\\( \\\\mathbf{R}_{s2} \\\\) in Fig. 4, observing that the illumination is enhanced and image details are also recovered. Especially, the noise is well removed by our COMO-ViT.\\n\\n**LGCM.** LGCM takes \\\\( \\\\mathbf{R}_{s2} \\\\) as input, and learns local deep features to perceive illumination gap between \\\\( \\\\mathbf{R}_{s2} \\\\) and ground truth and elaborately enhances illumination to reduce local color deviation:\\n\\n\\\\[\\n\\\\mathbf{\\\\Gamma}_l = \\\\varphi(\\\\text{Conv}_3(\\\\mathbf{R}_{s2})), \\\\quad \\\\mathbf{R}_{s3} = \\\\mathbf{R}_{s2}^{\\\\mathbf{\\\\Gamma}_l}.\\n\\\\]\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of $10^3$ G (Reiners & Christensen 2010), we get a magnetic field strength of $4.5 \\\\times 10^1$ G at the eclipse edge ($\\\\phi_b = 0.31$). A magnetosphere field strength of 10 G is sufficient to trap and dominate plasma (assuming protons and electrons) of number density $< 3 \\\\times 10^{13}$ cm$^{-3}$ and velocity $\\\\sim V_{\\\\text{orb}}$.\\n\\nThompson et al. (1994) suggests that, at the edge of the magnetosphere of the brown dwarf, the magnetic pressure should balance the pulsar wind pressure, while the pulsar wind energy density is $U_E = \\\\frac{E}{4\\\\pi c a^2}$ and the magnetic pressure is $\\\\frac{B_E^2}{8\\\\pi}$. The $\\\\dot{E}$ is the spin-down luminosity, $c$ is the speed of light, $a$ is the orbital separation, and $B_E$ is the magnetic field of the eclipse medium. From this, the magnetic field strength of $B_E$ should be $\\\\approx 8$ G (Wang et al. 2021).\\n\\nInterestingly, the derived theoretical magnetic field strength (45 G) is more than sufficient for the required field strength (8 G) at the eclipsing edge. However, these field strengths are more than three orders of magnitude higher than the value observed in our egress (10 mG).\\n\\n**Pulsar wind**\\n\\nThe third scenario (Fig. 7 c) supplements the second one with pulsar wind and a shock boundary, and fixes the inconsistency mentioned above. Such a picture was proposed by Phinney et al. (1988) as one of the early models. In this picture, a shock boundary exists between the magnetosphere and the pulsar wind. Outside of the shock boundary are high-speed, low-density pulsar wind particles traveling with a low magnetic field, and inside, the slow-moving, high-density plasma trapped by the companion\u2019s magnetic fields. This is similar to the boundary shock observed from the Solar wind and the Earth magnetosphere (Sckopke et al. 1983) where both the electron density and magnetic field rose suddenly as the ISEE-1$^8$ probe traveled downstream of the Solar wind into the Earth magnetosphere.\\n\\nThe majority of energy in the pulsar wind is carried by relativistic particles. The magnetic fields in the pulsar wind could be much smaller than the magnetic field of the companion at the orbital distance. After all, the pulsar\u2019s magnetic field is only $1.6 \\\\times 10^8$ G at its 10 km radius surface (Tab. 1). The pulsar wind is almost transparent to the pulsar emission. This is because of the low density and the high Lorenz factor of the wind particles. The wind particles have motion masses far exceeding their rest masses, causing their Faraday rotation effect to be negligible (Quataert & Gruzinov 2000; Wang et al. 2011). When a moderate amount of slow-moving ionized materials from the companion\u2019s magnetosphere flow out of the boundary and come to the pulsar wind side, the combination of the extra slow electrons and a reasonably low magnetic field (10 mG) environment leads to the incomplete depolarization and the Faraday rotation. As we mentioned in the previous section, such a condition is rarely met (only be observed in MJD 59214). In most of the ingresses and egresses of this pulsar, the out-flowing electrons are either too dense or too variable and often completely depolarize the pulsar signal.\\n\\nThompson et al. (1994) predicted that the pulsar wind could contain an oscillating part around the eclipsing edge with an oscillation length of $cP/2 \\\\approx 500$ km, where $c$ is the speed of light and $P$ is the spin period of the pulsar. It should be noted that such reciprocating magnetic fields in the pulsar wind was already illustrated in the model of Phinney et al. (1988). But such field was never observed until now. We\\n\\n---\\n\\n$^8$ International Sun-Earth Explorer 1\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of $10^3$ G (Reiners & Christensen 2010), we get a magnetic field strength of 4.5\u00d710$^1$ G at the eclipse edge (\\\\(\\\\phi_b = 0.31\\\\)). A magnetosphere field strength of 10 G is sufficient to trap and dominate plasma (assuming protons and electrons) of number density < 3 \u00d7 10$^{13}$ cm$^{-3}$ and velocity \\\\(\\\\sim V_{\\\\text{orb}}\\\\).\\n\\nThompson et al. (1994) suggests that, at the edge of the magnetosphere of the brown dwarf, the magnetic pressure should balance the pulsar wind pressure, while the pulsar wind energy density is \\\\(U_E = \\\\frac{E}{4\\\\pi c a^2}\\\\) and the magnetic pressure is \\\\(\\\\frac{B_E^2}{8\\\\pi}\\\\). The \\\\(\\\\dot{E}\\\\) is the spin-down luminosity, c is the speed of light, a is the orbital separation, and \\\\(B_E\\\\) is the magnetic field of the eclipse medium. From this, the magnetic field strength of \\\\(B_E\\\\) should be \\\\(\\\\approx 8\\\\) G (Wang et al. 2021) .\\n\\nInterestingly, the derived theoretical magnetic field strength (45 G) is more than sufficient for the required field strength (8 G) at the eclipsing edge. However, these field strengths are more than three orders of magnitude higher than the value observed in our egress (10 mG). \\n\\n**Pulsar wind**\\n\\nThe third scenario (Fig. 7 c) supplements the second one with pulsar wind and a shock boundary, and fixes the inconsistency mentioned above. Such a picture was proposed by Phinney et al. (1988) as one of the early models. In this picture, a shock boundary exists between the magnetosphere and the pulsar wind. Outside of the shock boundary are high-speed, low-density pulsar wind particles traveling with a low magnetic field, and inside, the slow-moving, high-density plasma trapped by the companion\u2019s magnetic fields. This is similar to the boundary shock observed from the Solar wind and the Earth magnetosphere (Sckopke et al. 1983) where both the electron density and magnetic field rose suddenly as the ISEE-1 8 probe traveled downstream of the Solar wind into the Earth magnetosphere. \\n\\nThe majority of energy in the pulsar wind is carried by relativistic particles. The magnetic fields in the pulsar wind could be much smaller than the magnetic field of the companion at the orbital distance. After all, the pulsar\u2019s magnetic field is only 1.6\u00d710$^8$ G at its 10 km radius surface (Tab. 1). The pulsar wind is almost transparent to the pulsar emission. This is because of the low density and the high Lorenz factor of the wind particles. The wind particles have motion masses far exceeding their rest masses, causing their Faraday rotation effect to be negligible (Quataert & Gruzinov 2000; Wang et al. 2011). When a moderate amount of slow-moving ionized materials from the companion\u2019s magnetosphere flow out of the boundary and come to the pulsar wind side, the combination of the extra slow electrons and a reasonably low magnetic field (10 mG) environment leads to the incomplete depolarization and the Faraday rotation. As we mentioned in the previous section, such a condition is rarely met (only be observed in MJD 59214). In most of the ingresses and egresses of this pulsar, the out-flowing electrons are either too dense or too variable and often completely depolarize the pulsar signal. \\n\\nThompson et al. (1994) predicted that the pulsar wind could contain an oscillating part around the eclipsing edge with an oscillation length of \\\\(cP/2 \\\\approx 500\\\\) km, where c is the speed of light and P is the spin period of the pulsar. It should be noted that such reciprocating magnetic fields in the pulsar wind was already illustrated in the model of Phinney et al. (1988). But such field was never observed until now. We\\n\\n---\\n\\n8 International Sun-Earth Explorer 1\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"between these works and ours is that they assume the buyer is fully strategic and processes fully how their actions today affect the seller\u2019s decisions tomorrow (whereas we instead model buyers as no-regret learners).\\n\\nThe most related work to ours is in the [BMSW18] model itself. Here we provide a brief summary of the main results in [BMSW18] and their connection to our main results. [BMSW18] studies the one seller one buyer scenario, where the buyer employs a mean-based no-regret algorithm. The authors present three results, each obtained under different assumptions regarding the behavior of the buyers. Firstly (as we have already mentioned earlier in the introduction), [BMSW18] shows that for vanilla mean-based no-regret buyers, [BMSW18] can extract revenue that is an arbitrarily large fraction of the bidder\u2019s expected value. Our Theorem 4 extends this result to the multiple buyer setting, overcoming novel technical and conceptual challenges. Second, [BMSW18] designs a novel (not mean-based) learning algorithm against which the optimal mechanism for the seller is simply Myerson\u2019s auction in each round. Their proof of this result naturally accommodates multiple buyers. Finally, [BMSW18] shows that if the buyer is clever and mean-based no regret (where they do not overbid their value), then the optimal auction has a clean tractable format (pay-your-bid with declining reserve over time). As we have discussed in the \u201cNo Overbidding\u201d section of the introduction, our work shows several formal barriers in extending these results to multiple buyers. In summary, our main result extends their first main result to multiple bidders. Their second result already holds for multiple bidders (so there is nothing for us to extend). Our secondary results establish formal barriers to extending their final main result to multiple bidders.\\n\\nTwo recent follow-ups have extended the setting in [BMSW18] in a different direction. First, [DSS19b] considers the problem of playing a two-player game against a no-regret learner. While technically not an auctions problem, there is thematic overlap with our main result. [DSS19a] extends the single-buyer results in [BMSW18] to be prior-free. Specifically, they show how to design auctions achieving the same guarantees as those in [BMSW18] but where the buyer\u2019s values are chosen adversarially. In comparison to these works, ours is the first to extend the model to consider multiple buyers.\\n\\nFinally, recent work of [CHJ20] considers interaction between a learning buyer and a learning seller. Their seller does not have a prior against which to optimize, and instead itself targets a no-regret guarantee. In comparison, our seller (like the seller in all previously cited works) optimizes expected revenue with respect to a prior.\\n\\n2 Preliminaries\\n\\nWe consider the same setting as [BMSW18], extended to multiple buyers. Specifically, there are \\\\( n \\\\) buyers and \\\\( T \\\\) rounds. In each round, there is a single item for sale. Each buyer \\\\( i \\\\) has value \\\\( v_{i,t} \\\\) for the item during round \\\\( t \\\\), and each \\\\( v_{i,t} \\\\) is drawn from \\\\( \\\\mathcal{D} \\\\) independently (that is, the buyers are i.i.d., and the rounds are i.i.d. as well). For simplicity of exposition (and to match prior work), we assume \\\\( \\\\mathcal{D} \\\\) has finite support \\\\( 0 \\\\leq w_1 < w_2 < \\\\ldots < w_m \\\\leq 1 \\\\) and we define \\\\( q_j \\\\) to be the probability \\\\( w_j \\\\) is drawn from \\\\( \\\\mathcal{D} \\\\).\\n\\nEach round, the seller presents \\\\( K \\\\) arms for the buyers. Each arm is labeled with a bid, and we assume that one of the arms is labeled with 0 (to represent a bid of \u201cdon\u2019t participate\u201d). Note that the same set of arms is presented to all buyers, and the same set of arms is presented in each round.\\n\\nIn each round \\\\( t \\\\), the seller defines an anonymous auction. Specifically, for all \\\\( i, t \\\\), the seller defines\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"between these works and ours is that they assume the buyer is fully strategic and processes fully how their actions today affect the seller\u2019s decisions tomorrow (whereas we instead model buyers as no-regret learners). The most related work to ours is in the [BMSW18] model itself. Here we provide a brief summary of the main results in [BMSW18] and their connection to our main results. [BMSW18] studies the one seller one buyer scenario, where the buyer employs a mean-based no-regret algorithm. The authors present three results, each obtained under different assumptions regarding the behavior of the buyers. Firstly (as we have already mentioned earlier in the introduction), [BMSW18] shows that for vanilla mean-based no-regret buyers, [BMSW18] can extract revenue that is an arbitrarily large fraction of the bidder\u2019s expected value. Our Theorem 4 extends this result to the multiple buyer setting, overcoming novel technical and conceptual challenges. Second, [BMSW18] designs a novel (not mean-based) learning algorithm against which the optimal mechanism for the seller is simply Myerson\u2019s auction in each round. Their proof of this result naturally accommodates multiple buyers. Finally, [BMSW18] shows that if the buyer is clever and mean-based no regret (where they do not overbid their value), then the optimal auction has a clean tractable format (pay-your-bid with declining reserve over time). As we have discussed in the \u201cNo Overbidding\u201d section of the introduction, our work shows several formal barriers in extending these results to multiple buyers. In summary, our main result extends their first main result to multiple bidders. Their second result already holds for multiple bidders (so there is nothing for us to extend). Our secondary results establish formal barriers to extending their final main result to multiple bidders. Two recent follow-ups have extended the setting in [BMSW18] in a different direction. First, [DSS19b] considers the problem of playing a two-player game against a no-regret learner. While technically not an auctions problem, there is thematic overlap with our main result. [DSS19a] extends the single-buyer results in [BMSW18] to be prior-free. Specifically, they show how to design auctions achieving the same guarantees as those in [BMSW18] but where the buyer\u2019s values are chosen adversarially. In comparison to these works, ours is the first to extend the model to consider multiple buyers. Finally, recent work of [CHJ20] considers interaction between a learning buyer and a learning seller. Their seller does not have a prior against which to optimize, and instead itself targets a noregret guarantee. In comparison, our seller (like the seller in all previously cited works) optimizes expected revenue with respect to a prior.  \\n\\n2 Preliminaries\\n\\nWe consider the same setting as [BMSW18], extended to multiple buyers. Specifically, there are \\\\( n \\\\) buyers and \\\\( T \\\\) rounds. In each round, there is a single item for sale. Each buyer \\\\( i \\\\) has value \\\\( v_{i,t} \\\\) for the item during round \\\\( t \\\\), and each \\\\( v_{i,t} \\\\) is drawn from \\\\( \\\\mathcal{D} \\\\) independently (that is, the buyers are i.i.d., and the rounds are i.i.d. as well). For simplicity of exposition (and to match prior work), we assume \\\\( \\\\mathcal{D} \\\\) has finite support \\\\( 0 \\\\leq w_1 < w_2 < \\\\ldots < w_m \\\\leq 1 \\\\) and we define \\\\( q_j \\\\) to be the probability \\\\( w_j \\\\) is drawn from \\\\( \\\\mathcal{D} \\\\).\\n\\nEach round, the seller presents \\\\( K \\\\) arms for the buyers. Each arm is labeled with a bid, and we assume that one of the arms is labeled with 0 (to represent a bid of \u201cdon\u2019t participate\u201d). Note that the same set of arms is presented to all buyers, and the same set of arms is presented in each round. In each round \\\\( t \\\\), the seller defines an anonymous auction. Specifically, for all \\\\( i, t \\\\), the seller defines\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"but consistent with a model where only the gravitational potential of the gas is considered.\\n\\nTo investigate the impact of the SMBH on the kinematics of J0109\u20133047 further, we construct a simple \u201cdispersion\u2013dominated + SMBH\u201d model in QUBEFit. In this model the velocity dispersion is the sum of the SMBH component of Eq. 1 and a constant dispersion value throughout the quasar host ($\\\\sigma_{\\\\text{CII, tot}}^2 = \\\\sigma_{\\\\text{CII, SMBH}}^2 + \\\\sigma_{\\\\text{const}}^2$). The intensity profile is assumed to be exponentially declining as in the constant dispersion model (see Section 3). We note that any additional contribution (besides the central SMBH) to the kinematics as a constant is in agreement with the inferred gas mass profile (see Fig. 4).\\n\\nIn Fig. 5 we show the best-fit dispersion fields for different SMBH masses from $10^8 M_\\\\odot$ to $10^9 M_\\\\odot$. We find that the [C II] kinematics of J0109\u20133047 are clearly incompatible with a $\\\\sim 10^9 M_\\\\odot$ SMBH. A full MCMC fit of the model to the data yields an upper limit of $M_{\\\\text{SMBH}} < 6.5 \\\\times 10^8 M_\\\\odot$ (2$\\\\sigma$) (see Appendix A for the full posterior distribution of the model parameters). However, even for the maximum-likelihood model ($M_{\\\\text{BH}} = 2.4 \\\\times 10^8 M_\\\\odot$), the BIC is slightly higher than for a model without a black hole ($\\\\Delta \\\\text{BIC} = 5.2$), showing that any SMBH contribution to the dispersion velocity field is disfavored by the observations.\\n\\nOne way to alleviate the tension with the rest-frame UV mass measurement could be to change the radial [C II]\u2013emitting gas profile. By definition, the observed [C II] kinematics are a luminosity\u2013weighted, beam\u2013convolved realisation of the intrinsic kinematics. Following Eq. 1, the velocity dispersion increases exponentially close to the black hole, and due to the exponential intensity profile, these inner regions will contribute more to the beam\u2013convolved velocity dispersion measurement in the center. As a result, the observed velocity dispersion could be reduced, if the [C II] intensity profile is not increasing close to the black hole (for example due to feedback).\\n\\nTo address this further, we have used a toy model where the gas density profile follows an exponentially declining profile with a central gap where the [C II] emission is null. As in the fiducial model, the velocity dispersion is composed of the SMBH component and a constant. We use this simple model to calculate the size of the central gap necessary to \u201chide\u201d the SMBH impact on the [C II] kinematics tracer. We find that, for a SMBH with a fixed mass $M_{\\\\text{BH}} = 1.1 \\\\times 10^8 M_\\\\odot$, the best-fit central gap is constrained to be $r < 22$ pc (2$\\\\sigma$) to reproduce the [C II] profile and kinematics. The best-fit model has $r_{\\\\text{pc}} = 0.015^{+0.015}_{-0.010}$ pc (see Appendix A), and is formally ruled out with an increased $\\\\Delta \\\\text{BIC} = 10.42$ compared to the model without a gap. Moreover, a central gap in the gas distribution would be at odds with simulations and observations where the central $\\\\sim 400 - 500$ pc region contains up to $\\\\sim 10$ times the mass of the BH in gas (e.g., Lupi et al. 2022; Walter et al. 2022).\\n\\nIn summary, the flat velocity dispersion profile implies a flat radial mass density profile. The constant dispersion implies that the underlying mass distribution is not centrally peaked, consistent with the expectations of the gas mass distribution derived from the far-infrared continuum emission under standard assumptions. This leaves only few alternatives to explain the absence of a central peak in the velocity dispersion. One possibility is that the gas mass decreases in the central 200 pc in order to compensate the presence of a $0.6 - 1.1 \\\\times 10^9 M_\\\\odot$ black hole and produce a flat mass profile. However, we have previously excluded the presence of a central gap in the [C II]\u2013emitting gas, and the FIR continuum shows no sign of a central gap either. A decrease in the central gas mass would imply fine-tuning of the physical properties of the ISM at the center of J0109\u20133047 (to \u2018offset\u2019 the mass of the SMBH). If such a conspiracy is excluded, the black hole mass is either smaller than expected, as discussed in this section, or the black hole is not located at the center of the galaxy as traced by the dust continuum, as discussed in the next section.\\n\\nIf confirmed, and applicable to the larger population of $z > 6$ luminous quasars, a mass of $\\\\sim 10^8 M_\\\\odot$ for the SMBH in J0109\u20133047 would have several interesting implications for early SMBH growth and formation. First, it would alleviate the need for massive seeds and/or super\u2013Eddington accretion events at $z > 7$ (e.g., Ba\u00f1ados et al. 2018; Wang et al. 2021; Volonteri et al. 2021). Second, a SMBH mass of $10^8 M_\\\\odot$ for a total galaxy (dynamical) mass of $2.34 \\\\times 10^{10} M_\\\\odot$ (see Section 4) would place it on the local relation, meaning that J0109\u20133047 is not part of an overmassive SMBH population at $z > 6$ (Pensabene et al. 2020; Neelamani et al. 2021), and the offset from the local relation seen in the $z > 6$ luminous quasar sample could be due to systematic overestimation of black hole masses. Third, the accreting BH at the heart of J0109\u20133047 would be definitive evidence for super\u2013Eddington accretion at $z > 6$ with an Eddington ratio $\\\\lambda_{\\\\text{Edd}} \\\\gtrsim 5$.\\n\\n6. AN OFFSET OR RECOILING SMBH AT REDSHIFT Z=6.79?\\n\\nThe previous section relied on the assumption that the black hole is located at the center of the host FIR continuum emission. However, if the accreting SMBH is not located at the center of the host galaxy, the [C II] kinematics are not expected to be strongly influenced\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"but consistent with a model where only the gravitational potential of the gas is considered. To investigate the impact of the SMBH on the kinematics of J0109\u20133047 further, we construct a simple \u201cdispersion\u2013dominated + SMBH\u201d model in QUBEFit . In this model the velocity dispersion is the sum of the SMBH component of Eq. 1 and a constant dispersion value throughout the quasar host (\u03c3_{\\\\text{CII, tot}}^2 = \u03c3_{\\\\text{CII, SMBH}}^2 + \u03c3_{\\\\text{const}}^2). The intensity profile is assumed to be exponentially declining as in the constant dispersion model (see Section 3 ). We note that any additional contribution (besides the central SMBH) to the kinematics as a constant is in agreement with the inferred gas mass profile (see Fig. 4 ).\\n\\nIn Fig. 5 we show the best-fit dispersion fields for different SMBH masses from 10^8 M_\u2299 to 10^9 M_\u2299. We find that the [C II ] kinematics of J0109\u20133047 are clearly incompatible with a ~ 10^9 M_\u2299 SMBH. A full MCMC fit of the model to the data yields an upper limit of M_{\\\\text{SMBH}} < 6.5 \\\\times 10^8 M_\u2299 (2\u03c3) (see Appendix A for the full posterior distribution of the model parameters). However, even for the maximum-likelihood model (M_{\\\\text{BH}} = 2.4 \\\\times 10^8 M_\u2299), the BIC is slightly higher than for a model without a black hole (\u0394BIC = 5.2), showing that any SMBH contribution to the dispersion velocity field is disfavored by the observations. One way to alleviate the tension with the rest-frame UV mass measurement could be to change the radial [C II ]\u2013emitting gas profile. By definition, the observed [C II ] kinematics are a luminosity\u2013weighted, beam\u2013 convolved realisation of the intrinsic kinematics. Following Eq. 1 , the velocity dispersion increases exponentially close to the black hole, and due to the exponential intensity profile, these inner regions will contribute more to the beam\u2013convolved velocity dispersion measurement in the center. As a result, the observed velocity dispersion could be reduced, if the [C II ] intensity profile is not increasing close to the black hole (for example due to feedback). To address this further, we have used a toy model where the gas density profile follows an exponentially declining profile with a central gap where the [C II ] emission is null. As in the fiducial model, the velocity dispersion is composed of the SMBH component and a constant. We use this simple model to calculate the size of the central gap necessary to \u201chide\u201d the SMBH impact on the [C II ] kinematics tracer. We find that, for a SMBH with a fixed mass M_{\\\\text{BH}} = 1.1 \\\\times 10^8 M_\u2299, the best-fit central gap is constrained to be r < 22 pc (2\u03c3) to reproduce the [C II ] profile and kinematics. The best-fit model has r_{\\\\text{pc}} = 0.015^{+0.015}_{-0.010} pc (see Appendix A ), and is formally ruled out with an increased \u0394BIC = 10.42 compared to the model without a gap. Moreover, a central gap in the gas distribution would be at odds with simulations and observations where the central region contains up to ~ 10 times the mass of the BH in gas (e.g., Lupi et al. 2022 ; Walter et al. 2022 ).\\n\\nIn summary, the flat velocity dispersion profile implies a flat radial mass density profile. The constant dispersion implies that the underlying mass distribution is not centrally peaked, consistent with the expectations of the gas mass distribution derived from the far-infrared continuum emission under standard assumptions. This leaves only few alternatives to explain the absence of a central peak in the velocity dispersion. One possibility is that the gas mass decreases in the central 200 pc in order to compensate the presence of a 0.6 \u2013 1.1 \\\\times 10^9 M_\u2299 black hole and produce a flat mass profile. However, we have previously excluded the presence of a central gap in the [C II ]\u2013emitting gas, and the FIR continuum shows no sign of a central gap either. A decrease in the central gas mass would imply fine-tuning of the physical properties of the ISM at the center of J0109\u20133047 (to \u2018offset\u2019 the mass of the SMBH). If such a conspiracy is excluded, the black hole mass is either smaller than expected, as discussed in this section, or the black hole is not located at the center of the galaxy as traced by the dust continuum, as discussed in the next section.\\n\\nIf confirmed, and applicable to the larger population of z > 6 luminous quasars, a mass of ~ 10^8 M_\u2299 for the SMBH in J0109\u20133047 would have several interesting implications for early SMBH growth and formation. First, it would alleviate the need for massive seeds and/or super\u2013Eddington accretion events at z > 7 (e.g., Ba\u00f1ados et al. 2018 ; Wang et al. 2021 ; Volonteri et al. 2021 ). Second, a SMBH mass of 10^8 M_\u2299 for a total galaxy (dynamical) mass of 2.34 \\\\times 10^{10} M_\u2299 (see Section 4 ) would place it on the local relation, meaning that J0109\u20133047 is not part of an overmassive SMBH population at z > 6 (Pensabene et al. 2020 ; Neeleman et al. 2021 ), and the offset from the local relation seen in the z > 6 luminous quasar sample could be due to systematic overestimation of black hole masses. Third, the accreting BH at the heart of J0109\u20133047 would be definitive evidence for super\u2013Eddington accretion at z > 6 with an Eddington ratio \u03bb_{\\\\text{Edd}} \\\\gtrsim 5 .\\n\\n6. AN OFFSET OR RECOILING SMBH AT REDSHIFT Z=6.79?\\n\\nThe previous section relied on the assumption that the black hole is located at the center of the host FIR continuum emission. However, if the accreting SMBH is not located at the center of the host galaxy, the [C II ] kinematics are not expected to be strongly influenced\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The particular case of a Lie group. In what follows, we will show the previous reduction process in the particular case when the initial manifold $Q$ is a Lie group $G$. In such a case, one may use the left trivialization of the cotangent bundle $T^*G$ in order to identify $T^*G$ with the product manifold $G \\\\times \\\\mathfrak{g}^*$, where $(\\\\mathfrak{g}, [\\\\cdot, \\\\cdot]_\\\\mathfrak{g})$ is the Lie algebra of $G$, in such a way that the canonical projection $\\\\pi_G : T^*G \\\\to G$ is just the first projection $p_1 : G \\\\times \\\\mathfrak{g}^* \\\\to G$. The left action $\\\\Phi : G \\\\times G \\\\to G$ on $G$ is the one defined by the group operation of $G$. We take the left invariant vector field $Y = \\\\xi$ on $G$ induced by an element $\\\\xi$ of $\\\\mathfrak{g}$. In the first reduction with the cotangent lift of $\\\\Phi$, the reduced space is $(T^*G - 0_G)/G \\\\cong \\\\mathfrak{g}^* - \\\\{0\\\\}$ and the reduced function induced by $Y$ is the restriction to $\\\\mathfrak{g}^* - \\\\{0\\\\}$ of the linear map $\\\\xi^\\\\ell$ associated with $\\\\xi \\\\in \\\\mathfrak{g}$, i.e.\\n\\n$$\\\\xi^\\\\ell : \\\\mathfrak{g}^* - \\\\{0\\\\} \\\\to \\\\mathbb{R}, \\\\quad \\\\xi^\\\\ell(\\\\alpha) = \\\\alpha(\\\\xi).$$\\n\\nOn the other hand, the Lie-Poisson bracket $\\\\{\\\\cdot, \\\\cdot\\\\}_{\\\\mathfrak{g}^*}$ on $(T^*G - 0_G)/G \\\\cong \\\\mathfrak{g}^* - \\\\{0\\\\}$ is characterized by\\n\\n$$\\\\{\\\\xi_1^\\\\ell, \\\\xi_2^\\\\ell\\\\}_{\\\\mathfrak{g}^*} = -[\\\\xi_1, \\\\xi_2]_\\\\mathfrak{g}, \\\\quad \\\\text{for all } \\\\xi_1, \\\\xi_2 \\\\in \\\\mathfrak{g}.$$\\n\\nThe scaling symmetry on $\\\\mathfrak{g}^* - \\\\{0\\\\}$ is just\\n\\n$$\\\\phi^G : (\\\\mathbb{R} - \\\\{0\\\\}) \\\\times (\\\\mathfrak{g}^* - \\\\{0\\\\}) \\\\to (\\\\mathfrak{g}^* - \\\\{0\\\\}), \\\\quad (s, \\\\alpha) \\\\mapsto s\\\\alpha.$$\\n\\nNow, we apply the second reduction step to the (Lie)-Poisson Hamiltonian system $(\\\\mathfrak{g}^* - \\\\{0\\\\}, \\\\{\\\\cdot, \\\\cdot\\\\}_{\\\\mathfrak{g}^*}, \\\\xi^\\\\ell)$, with respect to the scaling symmetry $\\\\phi^G$. In this case, the reduced space is the projective space $\\\\mathbb{P}\\\\mathfrak{g}^*$. The corresponding line bundle $\\\\pi_L : L := (\\\\mathfrak{g}^* - \\\\{0\\\\}) \\\\times (\\\\mathbb{R} - \\\\{0\\\\}) \\\\to \\\\mathbb{P}\\\\mathfrak{g}^*$ is defined by the action\\n\\n$$\\\\tilde{\\\\phi}^G : (\\\\mathbb{R} - \\\\{0\\\\}) \\\\times ((\\\\mathfrak{g}^* - \\\\{0\\\\}) \\\\times \\\\mathbb{R}) \\\\to (\\\\mathfrak{g}^* - \\\\{0\\\\}) \\\\times \\\\mathbb{R}, \\\\quad \\\\tilde{\\\\phi}^G(\\\\alpha, t) = (s\\\\alpha, \\\\frac{t}{s}).$$\\n\\nThe section of the dual line bundle $\\\\pi_{L^*} : L^* \\\\to \\\\mathbb{P}\\\\mathfrak{g}^*$ associated with the linear map $\\\\xi^\\\\ell : \\\\mathfrak{g}^* - \\\\{0\\\\} \\\\to \\\\mathbb{R}$ is\\n\\n$$h_\\\\xi(p(\\\\alpha))([(\\\\alpha, t)]) = t\\\\alpha(\\\\xi),$$\\n\\nwith $[(\\\\alpha, t)] \\\\in L$, where $p : (\\\\mathfrak{g}^* - 0) \\\\to \\\\mathbb{P}\\\\mathfrak{g}^*$ is the quotient projection.\\n\\nThe Kirillov bracket on the projective space $\\\\mathbb{P}\\\\mathfrak{g}^*$ is characterized by\\n\\n$$[h_{\\\\xi_1}, h_{\\\\xi_2}]_{\\\\mathbb{P}\\\\mathfrak{g}^*}(p(\\\\alpha))([(\\\\alpha, t)]) = -h_{[\\\\xi_1, \\\\xi_2]_\\\\mathfrak{g}^*}(p(\\\\alpha))([(\\\\alpha, t)]) = -t\\\\{\\\\xi_1^\\\\ell, \\\\xi_2^\\\\ell\\\\}_{\\\\mathfrak{g}^*}(\\\\alpha) = t\\\\alpha([\\\\xi_1, \\\\xi_2]_\\\\mathfrak{g}^*)(p(\\\\alpha))([(\\\\alpha, t)]).$$\\n\\nThis structure on the line bundle $L \\\\to \\\\mathbb{P}\\\\mathfrak{g}^*$ may be considered as the Kirillov version of the Lie-Poisson structure on $\\\\mathfrak{g}^*$ and for this reason we will use the terminology the Lie-Kirillov structure on $\\\\mathbb{P}\\\\mathfrak{g}^*$.\\n\\nThe reduced dynamics is determined by the $p$-projection of the Lie-Poisson Hamiltonian vector field associated with the linear function $\\\\xi^\\\\ell \\\\in C^\\\\infty(\\\\mathfrak{g}^* - \\\\{0\\\\})$, that is,\\n\\n$$X_{\\\\xi^\\\\ell}^{(\\\\cdot)}_{\\\\mathfrak{g}^*} = \\\\{\\\\cdot, \\\\xi^\\\\ell\\\\}_{\\\\mathfrak{g}^*}.$$\\n\\nNote however, that this $p$-projection of $X_{\\\\xi^\\\\ell}^{(\\\\cdot)}_{\\\\mathfrak{g}^*}$ is just the vector field $X_{h_\\\\xi} \\\\in \\\\mathfrak{X}(\\\\mathbb{P}\\\\mathfrak{g}^*)$, which is locally characterized by (22).\\n\\n5. Reduction of symplectic Hamiltonian systems using first the scaling symmetry and then the standard symmetries\\n\\nAs in the previous section, we have a symplectic Hamiltonian system $(S, \\\\omega, H)$ with a scaling symmetry $\\\\phi^S : \\\\mathbb{R}^\\\\times \\\\times S \\\\to S$ and a symplectic $G$-symmetry $\\\\Phi^G : G \\\\times S \\\\to S$ which are compatible. In what follows we describe the reduction process of the system $(S, \\\\omega, H)$ in two steps, but in the following order: the first reduction is obtained by a scaling symmetry and the second step is done using the standard symmetry.\\n\\nFirst of all, we will show a reduction process for Kirillov structures in the presence of a standard symmetry.\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The particular case of a Lie group. In what follows, we will show the previous reduction process in the particular case when the initial manifold $Q$ is a Lie group $G$. In such a case, one may use the left trivialization of the cotangent bundle $T^*G$ in order to identify $T^*G$ with the product manifold $G \\\\times \\\\mathfrak{g}^*$, where $(\\\\mathfrak{g}, [\\\\cdot, \\\\cdot]_\\\\mathfrak{g})$ is the Lie algebra of $G$, in such a way that the canonical projection $\\\\pi_G^*: T^*G \\\\to G$ is just the first projection $p_1: G \\\\times \\\\mathfrak{g}^* \\\\to G$. The left action $\\\\Phi: G \\\\times G \\\\to G$ on $G$ is the one defined by the group operation of $G$. We take the left invariant vector field $Y = \\\\xi^\\\\ell$ on $G$ induced by an element $\\\\xi$ of $\\\\mathfrak{g}$. In the first reduction with the cotangent lift of $\\\\Phi$, the reduced space is $(T^*G - 0_G)/G \\\\cong \\\\mathfrak{g}^* - \\\\{0\\\\}$ and the reduced function induced by $Y$ is the restriction to $\\\\mathfrak{g}^* - \\\\{0\\\\}$ of the linear map $\\\\xi^\\\\ell$ associated with $\\\\xi \\\\in \\\\mathfrak{g}$, i.e.\\n\\n$$\\\\xi^\\\\ell: \\\\mathfrak{g}^* - \\\\{0\\\\} \\\\to \\\\mathbb{R}, \\\\quad \\\\xi^\\\\ell(\\\\alpha) = \\\\alpha(\\\\xi).$$\\n\\nOn the other hand, the Lie-Poisson bracket $\\\\{\\\\cdot, \\\\cdot\\\\}_{\\\\mathfrak{g}^*}$ on $(T^*G - 0_G)/G \\\\cong \\\\mathfrak{g}^* - \\\\{0\\\\}$ is characterized by\\n\\n$$\\\\{\\\\xi_1^\\\\ell, \\\\xi_2^\\\\ell\\\\}_{\\\\mathfrak{g}^*} = -[\\\\xi_1, \\\\xi_2]_\\\\mathfrak{g}, \\\\quad \\\\text{for all } \\\\xi_1, \\\\xi_2 \\\\in \\\\mathfrak{g}.$$\\n\\nThe scaling symmetry on $\\\\mathfrak{g}^* - \\\\{0\\\\}$ is just\\n\\n$$\\\\phi^G: (\\\\mathbb{R} - \\\\{0\\\\}) \\\\times (\\\\mathfrak{g}^* - \\\\{0\\\\}) \\\\to (\\\\mathfrak{g}^* - \\\\{0\\\\}), \\\\quad (s, \\\\alpha) \\\\mapsto s\\\\alpha.$$\\n\\nNow, we apply the second reduction step to the (Lie)-Poisson Hamiltonian system $(\\\\mathfrak{g}^* - \\\\{0\\\\}, \\\\{\\\\cdot, \\\\cdot\\\\}_{\\\\mathfrak{g}^*}, \\\\xi^\\\\ell)$, with respect to the scaling symmetry $\\\\phi^G$. In this case, the reduced space is the projective space $\\\\mathbb{P}\\\\mathfrak{g}^*$. The corresponding line bundle $\\\\pi_L: L := (\\\\mathfrak{g}^* - \\\\{0\\\\}) \\\\times (\\\\mathbb{R} - \\\\{0\\\\}) \\\\to \\\\mathbb{P}\\\\mathfrak{g}^*$ is defined by the action\\n\\n$$\\\\tilde{\\\\phi}^G: (\\\\mathbb{R} - \\\\{0\\\\}) \\\\times ((\\\\mathfrak{g}^* - \\\\{0\\\\}) \\\\times \\\\mathbb{R}) \\\\to (\\\\mathfrak{g}^* - \\\\{0\\\\}) \\\\times \\\\mathbb{R}, \\\\quad \\\\tilde{\\\\phi}^G(\\\\alpha, t) = (s\\\\alpha, \\\\frac{t}{s}).$$\\n\\nThe section of the dual line bundle $\\\\pi_{L^*}: L^* \\\\to \\\\mathbb{P}\\\\mathfrak{g}^*$ associated with the linear map $\\\\xi^\\\\ell: \\\\mathfrak{g}^* - \\\\{0\\\\} \\\\to \\\\mathbb{R}$ is\\n\\n$$h_\\\\xi(p(\\\\alpha))([(\\\\alpha, t)]) = t\\\\alpha(\\\\xi),$$\\n\\nwith $[(\\\\alpha, t)] \\\\in L$, where $p: (\\\\mathfrak{g}^* - 0) \\\\to \\\\mathbb{P}\\\\mathfrak{g}^*$ is the quotient projection.\\n\\nThe Kirillov bracket on the projective space $\\\\mathbb{P}\\\\mathfrak{g}^*$ is characterized by\\n\\n$$[h_{\\\\xi_1}, h_{\\\\xi_2}]_{\\\\mathbb{P}\\\\mathfrak{g}^*}(p(\\\\alpha))([(\\\\alpha, t)]) = -h_{[\\\\xi_1, \\\\xi_2]_\\\\mathfrak{g}^*}(p(\\\\alpha))([(\\\\alpha, t)]) = -t\\\\{\\\\xi_1^\\\\ell, \\\\xi_2^\\\\ell\\\\}_{\\\\mathfrak{g}^*}(\\\\alpha) = t\\\\alpha([\\\\xi_1, \\\\xi_2]_\\\\mathfrak{g}^*)(p(\\\\alpha))([(\\\\alpha, t)]).$$\\n\\nThis structure on the line bundle $L \\\\to \\\\mathbb{P}\\\\mathfrak{g}^*$ may be considered as the Kirillov version of the LiePoisson structure on $\\\\mathfrak{g}^*$ and for this reason we will use the terminology the Lie-Kirillov structure on $\\\\mathbb{P}\\\\mathfrak{g}^*$.\\n\\nThe reduced dynamics is determined by the $p$-projection of the Lie-Poisson Hamiltonian vector field associated with the linear function $\\\\xi^\\\\ell \\\\in C^\\\\infty(\\\\mathfrak{g}^* - \\\\{0\\\\})$, that is,\\n\\n$$X_{\\\\xi^\\\\ell}^{(\\\\cdot, \\\\cdot)}_{\\\\mathfrak{g}^*} = \\\\{\\\\cdot, \\\\xi^\\\\ell\\\\}_{\\\\mathfrak{g}^*}.$$\\n\\nNote however, that this $p$-projection of $X_{\\\\xi^\\\\ell}^{(\\\\cdot, \\\\cdot)}_{\\\\mathfrak{g}^*}$ is just the vector field $X_{h_\\\\xi} \\\\in \\\\mathfrak{X}(\\\\mathbb{P}\\\\mathfrak{g}^*)$, which is locally characterized by (22).\\n\\n5. Reduction of symplectic Hamiltonian systems using first the scaling symmetry and then the standard symmetries\\n\\nAs in the previous section, we have a symplectic Hamiltonian system $(S, \\\\omega, H)$ with a scaling symmetry $\\\\phi^S: \\\\mathbb{R}^\\\\times \\\\times S \\\\to S$ and a symplectic $G$-symmetry $\\\\Phi^G: G \\\\times S \\\\to S$ which are compatible. In what follows we describe the reduction process of the system $(S, \\\\omega, H)$ in two steps, but in the following order: the first reduction is obtained by a scaling symmetry and the second step is done using the standard symmetry.\\n\\nFirst of all, we will show a reduction process for Kirillov structures in the presence of a standard symmetry.\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"community filtered by Tutorial tag on social media website?), and acts via planning, summarizing by HTML-T5, and then programming by Flan-U-PaLM. See Appendix C for the example workflow. We finetune HTML-T5 with traces that are collected using scripted agents by procedurally generating instructions from human curated templates. This results in 260 episodes on real estate website and 230 episodes on social media website (about 20/10 steps per episode respectively).\\n\\nWe prepare 20 different natural language instructions, and measure the success rate and score for the evaluation. The score represents the percentage of required attributes covered during the episode [81]; for instance (1) apartments for (2) corporate housing with (3) studio bedroom and (4) 1+ bathroom located in (5) oroville, ca. When the agents could search the housing satisfying (1), (2), (5) and not (3), (4), the score would be 60 (= 100 \u00d7 3/5). When the agents could achieve 100 score, that episode would mark as success.\\n\\n**Results** For comparison, we prepare three baselines, consisting of partial plug-in language models and a single LLM prompting different examplers per role: WebAgent replacing closed-loop planning from HTML-T5 with few-shot open-loop planning from Flan-U-PaLM (Plan: \u00d7), replacing HTML summarization from HTML-T5 with regular-expression-based retrieval (Sum: \u00d7), and both of them (Plan: \u00d7, Sum: \u00d7). Table 2 shows that WebAgent with HTML-T5 for planning and summarization (Plan: \u2713, Sum: \u2713) achieves best 65% success and 87.6 score on real-estate and 70% success and 85.8 score on social-media, significantly outperforming single LLM (Plan: \u00d7, Sum: \u00d7), that with open-loop planning (Plan: \u00d7), and that with regular-expression retrieval (Sum: \u00d7) (most of those roughly achieve only 10 - 20% success). This result suggests that closed-loop planning grounded on HTML observations via finetuning of domain language models is much more suitable for open-ended web navigation than open-loop planning with few-shot LLMs, which is remarkable in real-estate (even Sum: \u00d7 achieves 50% success), where the longer planning horizon is needed to fulfill instructions. We guess enhancing the planning ability to decompose the given instructions adaptively and robustly can help further improve WebAgent.\\n\\n| Models                  | Data   | Success | Diff. |\\n|-------------------------|--------|---------|-------|\\n| CC-Net [28]             | 2.4M   | 32.0%   | \u2013     |\\n| WebN-T5-XL [24]         | 12K    | 48.4%   | \u2013     |\\n| LongT5-Base             | 12K    | 53.8%   | 0.0   |\\n| LongT5-Large            | 12K    | 56.3%   | 0.0   |\\n| LongT5-XL               | 12K    | 60.4%   | 0.0   |\\n| Flan-LongT5-Base        | 12K    | 54.1%   | +0.3  |\\n| Flan-LongT5-Large       | 12K    | 56.1%   | -0.2  |\\n| Flan-LongT5-XL          | 12K    | 61.1%   | +0.7  |\\n| HTML-T5-Base (ours)     | 12K    | 57.0%   | +3.2  |\\n| HTML-T5-Large (ours)    | 12K    | 60.8%   | +4.5  |\\n| HTML-T5-XL (ours)       | 12K    | 63.3%   | +2.9  |\\n| Flan-T5-XL [19]         | 347K   | 75.5%   | \u2013     |\\n| Flan-T5-XXL [19]        | 347K   | 79.0%   | \u2013     |\\n| HTML-T5-XL (ours)       | 347K   | 79.4%   | \u2013     |\\n\\nTable 4: Average success rate of MiniWoB++ with 56 tasks. We use 12K demonstrations [42], and compare HTML-T5 among supervised-finetuned baselines [24, 28]. HTML-T5-XL remarkably outperforms WebN-T5-XL, the prior best method, by 14.9%, and HTML-denoising improves the success rate better than instruction tuning. We also finetune HTML-T5 with 347K expert traces [19], which performs better than Flan-T5-XXL (11B parameters) even with 3B parameters. See Appendix H for the detailed results.\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"community filtered by Tutorial tag on social media web- site?), and acts via planning, summarizing by HTML-T5, and then programming by Flan-U-PaLM. See Appendix C for the example workflow. We finetune HTML-T5 with traces that are collected using scripted agents by procedurally generating instructions from human curated templates. This results in 260 episodes on real estate website and 230 episodes on social media website (about 20/10 steps per episode respectively). We prepare 20 different natural language instructions, and measure the success rate and score for the evaluation. The score represents the percentage of required attributes covered during the episode [ 81 ]; for instance (1) apartments for (2) corporate housing with (3) studio bedroom and (4) 1+ bathroom located in (5) oroville, ca . When the agents could search the housing satisfying (1), (2), (5) and not (3), (4), the score would be 60 ( = 100 score, that episode would mark as success. \\n\\nResults For comparison, we prepare three baselines, consisting of partial plug-in language models and a single LLM prompting different examplers per role: WebAgent replacing closed-loop planning from HTML-T5 with fewshot open-loop planning from Flan-U-PaLM ( Plan: X ), replacing HTML summarization from HTML-T5 with regular-expression-based retrieval ( Sum: X ), and both of them ( Plan: X, Sum: X ). Table 2 shows that WebAgent with HTML-T5 for planning and summarization ( Plan: \u2713, Sum: \u2713 ) achieves best 65% success and 87.6 score on real-estate and 70% success and 85.8 score on social-media, significantly outperforming single LLM ( Plan: X, Sum: X ), that with open-loop planning ( Plan: X ), and that with regular-expression retrieval ( Sum: X ) (most of those roughly achieve only 10 20% success). This result suggests that closed-loop planning grounded on HTML observations via finetuning of domain language models is much more suitable for open-ended web navigation than open-loop planning with few-shot LLMs, which is remarkable in real-estate (even Sum: X achieves 50% success), where the longer planning horizon is needed to fulfill instructions. We guess enhancing the planning ability to decompose the given instructions adaptively and robustly can help further improve WebAgent. \\n\\n| Models                  | Data  | Success | Diff. |\\n|-------------------------|-------|---------|-------|\\n| CC-Net [28]             | 2.4M  | 32.0%   | \u2013     |\\n| WebN-T5-XL [24]         | 12K   | 48.4%   | \u2013     |\\n| LongT5-Base             | 12K   | 53.8%   | 0.0   |\\n| LongT5-Large            | 12K   | 56.3%   | 0.0   |\\n| LongT5-XL               | 12K   | 60.4%   | 0.0   |\\n| Flan-LongT5-Base        | 12K   | 54.1%   | +0.3  |\\n| Flan-LongT5-Large       | 12K   | 56.1%   | -0.2  |\\n| Flan-LongT5-XL          | 12K   | 61.1%   | +0.7  |\\n| HTML-T5-Base (ours)     | 12K   | 57.0%   | +3.2  |\\n| HTML-T5-Large (ours)    | 12K   | 60.8%   | +4.5  |\\n| HTML-T5-XL (ours)       | 12K   | 63.3%   | +2.9  |\\n| Flan-T5-XL [19]         | 347K  | 75.5%   | \u2013     |\\n| Flan-T5-XXL [19]        | 347K  | 79.0%   | \u2013     |\\n| HTML-T5-XL (ours)       | 347K  | 79.4%   | \u2013     |\\n\\nTable 4: Average success rate of MiniWoB++ with 56 tasks. We use 12K demonstrations [42], and compare HTML-T5 among supervised-finetuned baselines [24, 28]. HTML-T5-XL remarkably outperforms WebN-T5-XL, the prior best method, by 14.9%, and HTML-denoising improves the success rate better than instruction tuning. We also finetune HTML-T5 with 347K expert traces [19], which performs better than Flan-T5-XXL (11B parameters) even with 3B parameters. See Appendix H for the detailed results.\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[JX23b] Zhuchao Ji and Junyi Xie. Homoclinic orbits, multiplier spectrum and rigidity theorems in complex dynamics. *Forum Math. Pi*, 11:Paper No. e11, 37, 2023.\\n\\n[Lev14] A. Levy. Aim workshop postcritically finite maps in complex and arithmetic dynamics, 2014.\\n\\n[McM87] Curt McMullen. Families of rational maps and iterative root-finding algorithms. *Ann. of Math. (2)*, 125(3):467\u2013493, 1987.\\n\\n[Mil06] John Milnor. On Latt\u00e8s maps. *Dynamics on the Riemann Sphere: A Bodil Branner Festschrift*, page 9, 2006.\\n\\n[MS14] Alice Medvedev and Thomas Scanlon. Invariant varieties for polynomial dynamical systems. *Ann. of Math. (2)*, 179(1):81\u2013177, 2014.\\n\\n[Nar04] W\u0142adys\u0142aw Narkiewicz. *Elementary and analytic theory of algebraic numbers*. Springer Monographs in Mathematics. Springer-Verlag, Berlin, third edition, 2004.\\n\\n[Pak23] Fedor Pakovich. Invariant curves for endomorphisms of $\\\\mathbb{P}^1 \\\\times \\\\mathbb{P}^1$. *Math. Ann.*, 385(1-2):259\u2013307, 2023.\\n\\n[Poo17] Bjorn Poonen. *Rational points on varieties*, volume 186 of *Graduate Studies in Mathematics*. American Mathematical Society, Providence, RI, 2017.\\n\\n[Sil98] Joseph H. Silverman. The space of rational maps on $\\\\mathbb{P}^1$. *Duke Math. J.*, 94(1):41\u201377, 1998.\\n\\n[Sil07] Joseph H. Silverman. *The arithmetic of dynamical systems*, volume 241 of *Graduate Texts in Mathematics*. Springer-Verlag, New York, 2007.\\n\\n[Sil12] Joseph H. Silverman. *Moduli spaces and arithmetic dynamics*, volume 30 of *CRM Monograph Series*. American Mathematical Society, Providence, RI, 2012.\\n\\n[Tuc14] T. Tucker. Problem 6 in the problem list of the aim workshop postcritically finite maps in complex and arithmetic dynamics, 2014.\\n\\n[Xie17] Junyi Xie. The existence of Zariski dense orbits for polynomial endomorphisms of the affine plane. *Compos. Math.*, 153(8):1658\u20131672, 2017.\\n\\n[Xie22] Junyi Xie. The existence of Zariski dense orbits for endomorphisms of projective surfaces (with an appendix in collaboration with T. Tucker). *J. Amer. Math. Soc.*, 2022. published online.\\n\\n[Xie23] Junyi Xie. Remarks on algebraic dynamics in positive characteristic. *J. Reine Angew. Math.*, 797:117\u2013153, 2023.\\n\\n[XY23] Junyi Xie and Xinyi Yuan. Partial heights and the geometric Bombieri-Lang conjecture. arXiv:2305.14789, 2023.\\n\\n[Yua08] Xinyi Yuan. Big line bundles over arithmetic varieties. *Invent. Math.*, 173(3):603\u2013649, 2008.\\n\\n[Zdu14] Anna Zdunik. Characteristic exponents of rational functions. *Bulletin of the Polish Academy of Sciences. Mathematics*, 62(3), 2014.\\n\\n[Zha95] Shou-Wu Zhang. Small points and adelic metrics. *J. Algebraic Geom.*, 4(2):281\u2013300, 1995.\\n\\n[Zha98] Shou-Wu Zhang. Equidistribution of small points on abelian varieties. *Ann. of Math. (2)*, 147(1998):147:159\u2013165, 1998.\\n\\n**Institute for Theoretical Sciences, Westlake University, Hangzhou 310030, China**\\n\\n*Email address: jizhuchao@westlake.edu.cn*\\n\\n**Beijing International Center for Mathematical Research, Peking University, Beijing 100871, China**\\n\\n*Email address: xiejunyi@bicmr.pku.edu.cn*\\n\\n**School of Mathematical Sciences, Peking University, Beijing 100871, China**\\n\\n*Email address: grzhang@stu.pku.edu.cn*\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[JX23b] Zhuchao Ji and Junyi Xie. Homoclinic orbits, multiplier spectrum and rigidity theorems in complex dynamics. *Forum Math. Pi*, 11:Paper No. e11, 37, 2023.\\n\\n[Lev14] A. Levy. Aim workshop postcritically finite maps in complex and arithmetic dynamics, 2014.\\n\\n[McM87] Curt McMullen. Families of rational maps and iterative root-finding algorithms. *Ann. of Math. (2)*, 125(3):467\u2013493, 1987.\\n\\n[Mil06] John Milnor. On Latt\u00e8s maps. *Dynamics on the Riemann Sphere: A Bodil Branner Festschrift*, page 9, 2006.\\n\\n[MS14] Alice Medvedev and Thomas Scanlon. Invariant varieties for polynomial dynamical systems. *Ann. of Math. (2)*, 179(1):81\u2013177, 2014.\\n\\n[Nar04] W\u0142adys\u0142aw Narkiewicz. *Elementary and analytic theory of algebraic numbers*. Springer Monographs in Mathematics. Springer-Verlag, Berlin, third edition, 2004.\\n\\n[Pak23] Fedor Pakovich. Invariant curves for endomorphisms of $\\\\mathbb{P}^1 \\\\times \\\\mathbb{P}^1$. *Math. Ann.*, 385(1-2):259\u2013307, 2023.\\n\\n[Poo17] Bjorn Poonen. *Rational points on varieties*, volume 186 of *Graduate Studies in Mathematics*. American Mathematical Society, Providence, RI, 2017.\\n\\n[Sil98] Joseph H. Silverman. The space of rational maps on $\\\\mathbb{P}^1$. *Duke Math. J.*, 94(1):41\u201377, 1998.\\n\\n[Sil07] Joseph H. Silverman. *The arithmetic of dynamical systems*, volume 241 of *Graduate Texts in Mathematics*. Springer-Verlag, New York, 2007.\\n\\n[Sil12] Joseph H. Silverman. *Moduli spaces and arithmetic dynamics*, volume 30 of *CRM Monograph Series*. American Mathematical Society, Providence, RI, 2012.\\n\\n[Tuc14] T. Tucker. Problem 6 in the problem list of the aim workshop postcritically finite maps in complex and arithmetic dynamics, 2014.\\n\\n[Xie17] Junyi Xie. The existence of Zariski dense orbits for polynomial endomorphisms of the affine plane. *Compos. Math.*, 153(8):1658\u20131672, 2017.\\n\\n[Xie22] Junyi Xie. The existence of Zariski dense orbits for endomorphisms of projective surfaces (with an appendix in collaboration with T. Tucker). *J. Amer. Math. Soc.*, 2022. published online.\\n\\n[Xie23] Junyi Xie. Remarks on algebraic dynamics in positive characteristic. *J. Reine Angew. Math.*, 797:117\u2013153, 2023.\\n\\n[XY23] Junyi Xie and Xinyi Yuan. Partial heights and the geometric Bombieri-Lang conjecture. arXiv:2305.14789, 2023.\\n\\n[Yua08] Xinyi Yuan. Big line bundles over arithmetic varieties. *Invent. Math.*, 173(3):603\u2013649, 2008.\\n\\n[Zdu14] Anna Zdunik. Characteristic exponents of rational functions. *Bulletin of the Polish Academy of Sciences. Mathematics*, 62(3), 2014.\\n\\n[Zha95] Shou-Wu Zhang. Small points and adelic metrics. *J. Algebraic Geom.*, 4(2):281\u2013300, 1995.\\n\\n[Zha98] Shou-Wu Zhang. Equidistribution of small points on abelian varieties. *Ann. of Math. (2)*, 147(1998):147:159\u2013165, 1998.\\n\\n**Institute for Theoretical Sciences, Westlake University, Hangzhou 310030, China**\\n\\n*Email address: jizhuchao@westlake.edu.cn*\\n\\n**Beijing International Center for Mathematical Research ZHUCHAO JI, JUNYI XIE, AND GENG-RUI ZHANG**\\n\\n*Email address: xiejunyi@bicmr.pku.edu.cn*\\n\\n**School of Mathematical Sciences, Peking University, Beijing 100871, China [JX23b] Zhuchao Ji and Junyi Xie. Homoclinic orbits, multiplier spectrum and rigidity theorems in complex dynamics. *Forum Math. Pi*, 11:Paper No. e11, 37, 2023. [Lev14] A. Levy. Aim workshop postcritically finite maps in complex and arithmetic dynamics, 2014. [McM87] Curt McMullen. Families of rational maps and iterative root-finding algorithms. *Ann. of Math. (2)*, 125(3):467\u2013493, 1987. [Mil06] John Milnor. On Latt`es maps. *Dynamics on the Riemann Sphere: A Bodil Branner Festschrift*, page 9, 2006. [MS14] Alice Medvedev and Thomas Scanlon. Invariant varieties for polynomial dynamical systems. *Ann. of Math. (2)*, 179(1):81\u2013177, 2014. [Nar04] W ladys l aw Narkiewicz. *Elementary and analytic theory of algebraic numbers*. Springer Monographs in Mathematics. Springer-Verlag, Berlin, third edition, 2004. [Pak23] Fedor Pakovich. Invariant curves for endomorphisms of $\\\\mathbb{P}^1 \\\\times \\\\mathbb{P}^1$. *Math. Ann.*, 385(12):259\u2013307, 2023. [Poo17] Bjorn Poonen. *Rational points on varieties*, volume 186 of *Graduate Studies in Mathematics*. American Mathematical Society, Providence, RI, 2017. [Sil98] Joseph H. Silverman. The space of rational maps on $\\\\mathbb{P}^1$. *Duke Math. J.*, 94(1):41\u201377, 1998. [Sil07] Joseph H. Silverman. *The arithmetic of dynamical systems*, volume 241 of *Graduate Texts in Mathematics*. Springer-Verlag, New York, 2007. [Sil12] Joseph H. Silverman. *Moduli spaces and arithmetic dynamics*, volume 30 of *CRM Monograph Series*. American Mathematical Society, Providence, RI, 2012. [Tuc14] T. Tucker. Problem 6 in the problem list of the aim workshop postcritically finite maps in complex and arithmetic dynamics, 2014. [Xie17] Junyi Xie. The existence of Zariski dense orbits for polynomial endomorphisms of the affine plane. *Compos. Math.*, 153(8):1658\u20131672, 2017. [Xie22] Junyi Xie. The existence of Zariski dense orbits for endomorphisms of projective surfaces (with an appendix in collaboration with T. Tucker). *J. Amer. Math. Soc.*, 2022. published online. [Xie23] Junyi Xie. Remarks on algebraic dynamics in positive characteristic. *J. Reine Angew. Math.*, 797:117\u2013153, 2023. [XY23] Junyi Xie and Xinyi Yuan. Partial heights and the geometric Bombieri-Lang conjecture. arXiv:2305.14789, 2023. [Yua08] Xinyi Yuan. Big line bundles over arithmetic varieties. *Invent. Math.*, 173(3):603\u2013 649, 2008. [Zdu14] Anna Zdunik. Characteristic exponents of rational functions. *Bulletin of the Polish Academy of Sciences. Mathematics*, 62 "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"some additional components are added to the matter distribution due to which the number of unknowns grow that make more challenging to solve the Einstein field equations analytically. In this regard, such analytical solutions developed via gravitational decoupling (GD) with minimal geometric deformation (MGD) method in both cosmology and astrophysics [21, 22].\\n\\nMultiple methods studied to investigate important properties of self-gravitating objects, including the phenomenon of stability and hydrodynamic equilibrium, the upper limit of the mass-to-radius ratio, the upper limit of superficial redshift, and dynamics of matter content under energy conditions, etc. [23]. One of these techniques is MGD approach, which was initially intended as an optional means of deforming Schwarzschild space-time in framework of the Randall-Sundrum braneworld [24, 25]. Recently, there is a lot of interest in developing novel analytic and an anisotropic solutions for Einstein field equations, which is a difficult task as Einstein field equations are non-linear and difficult to handle. In this way, the method of MGD to gain new models representing relativistic objects with well-determined characteristics have been proposed [26]. For a compact spherical distribution, the analytical solution of an anisotropic fluid as well as the braneworld model of Tolman IV solution have been found [27]. Two essential components are required in which first one is dimensionless coupling constant \u201c\u03b1\u201d to incorporate an extra source into the stress-energy tensor of seed solution. Second one is MGD method on the metric potentials (often on the radial component of metric) in the context of braneworld model. If the seed solution is assumed to be anisotropic, the inclusion of this additional component combined with a static and spherically-symmetric system gives rise to a complex simultaneous equations. The MGD technique separates Einstein field equations into two systems, namely the \u201cEinstein system\u201d and \u201cquasi-Einstein system\u201d, which in comparison to the original system are easy to solve. At this point, a few observations are appropriate, firstly, the decoupled systems satisfy Bianchi Identities and secondly, the extra source may be a scalar, vector, or tensor field [28\u201332]. Moreover, a number of interesting results on the solutions of black hole with 2+1 and 3+1 decomposition obtained in [33\u201336]. Additionally, the solutions of new hairy black hole have just been explored [37], and a mechanism is created as well to turn any non-rotational black hole into a rotational one [38, 39].\\n\\nWhen weak gravitational forces are at work, the hypothesis in GR has effectively aligned with many tests carried out within the solar system, demonstrating its success in cosmology. To get more accurate and dependable results, this theory may need to be modified when dealing with high gravitational fields or while being observed on a big scale. These changes may be very important in explaining the phenomena of accelerated expansion. These modifications are termed as modified theories (see, for instance [40\u201347])\\n\\nMany modified theories of gravity [48\u201352] are taken into account by changing the Einstein-Hilbert action that is frequently used to study both the existence of dark energy and dark matter as well as the mystery of universe rapid expansion. Geometrical representation and scalar tensor representation of $f(G, T)$ gravity has been presented to establish novel junction condition [53].\\n\\nSeveral researchers are interested to explore the gravitational collapse phenomena because it is a prominent case in a strong-field regime [54\u201356]. Jordan [57] developed a full gravitational theory which gave the title of a gravitational scalar field to gravitational constant. Brans and Dicke [58] developed a scalar-tensor field theory named as the Brans-Dicke (BD) theory obtained by substituting a time modifying constant $G(t)$ and with the help of a scalar field ($\\\\Phi$) having interaction along with the geometry. Additionally, the well-known scalar field coupling constant or parameter ($\\\\omega_{BD}$) of the BD theory is a constant that can be adjusted to get the desired outcomes in Jordan frame. It is assumed that the $\\\\Phi$ is reciprocal of the dynamical gravitational constant, i.e., $G(t) = \\\\frac{1}{\\\\Phi(t)}$. The test particles travel along geodesics according to the BD theory, they consequently obey the weak equivalence principle, which states that the gravitational mass and inertial mass are equivalent. Mach principle, agreement with the weak equivalence principle, and Dirac\u2019s large number hypothesis are the main ingredients of the BD theory. This theory includes a metric tensor and a scalar field that describes gravity.\\n\\nThe large value of $\\\\Phi$ describes the fast expansion of the universe, is found by recent study in cosmology, including the redshift and distance-luminosity connection of type Ia Supernovae [59]. The evidence for various cosmic concerns, including the late behavior of the universe, cosmic acceleration, and the inflation issue, etc are also supported by the BD theory [60]. This theory has drawn interest in recent years due to its precision in describing early inflationary era and late time expansion of the cosmos. Several authors studied the Friedmann-Lema\u00eetre-Robertson-Walker model in the context of the BD theory [61\u201363].\\n\\nThe main purpose of this work is to extend [64] in the BD theory. The paper is organized as follows. The appropriate BD theory for the GD formalism is discussed in Sec. II. In Sec. III, the MGD technique to a spherically symmetric geometry filled with two sources in the BD theory is introduced. In Sec. IV, junction conditions are established to match an outside Schwarzschild line element with the inside solution. In Sec. V, we studied mathematical and physical solutions to the modified field equations using the MGD method with constraints apply to matter density and radial pressure for anisotropy in the setting of the BD theory. In Sec. VI, the physical characteristics of an anisotropic stellar structure with the help of polytropic equation of state are provided. The main results are summarized in Sec. VII.\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"some additional components are added to the matter distribution due to which the number of unknowns grow that make more challenging to solve the Einstein field equations analytically. In this regard, such analytical solutions developed via gravitational decoupling (GD) with minimal geometric deformation (MGD) method in both cosmology and astrophysics [21, 22].\\n\\nMultiple methods studied to investigate important properties of self-gravitating objects, including the phenomenon of stability and hydrodynamic equilibrium, the upper limit of the mass-to-radius ratio, the upper limit of superficial redshift, and dynamics of matter content under energy conditions, etc. [23]. One of these techniques is MGD approach, which was initially intended as an optional means of deforming Schwarzschild space-time in framework of the Randall-Sundrum braneworld [24, 25]. Recently, there is a lot of interest in developing novel analytic and an anisotropic solutions for Einstein field equations, which is a difficult task as Einstein field equations are non-linear and difficult to handle. In this way, the method of MGD to gain new models representing relativistic objects with well-determined characteristics have been proposed [26]. For a compact spherical distribution, the analytical solution of an anisotropic fluid as well as the braneworld model of Tolman IV solution have been found [27]. Two essential components are required in which first one is dimensionless coupling constant \u201c\u03b1\u201d to incorporate an extra source into the stress-energy tensor of seed solution. Second one is MGD method on the metric potentials (often on the radial component of metric) in the context of braneworld model. If the seed solution is assumed to be anisotropic, the inclusion of this additional component combined with a static and spherically-symmetric system gives rise to a complex simultaneous equations. The MGD technique separates Einstein field equations into two systems, namely the \u201cEinstein system\u201d and \u201cquasi-Einstein system\u201d, which in comparison to the original system are easy to solve. At this point, a few observations are appropriate, firstly, the decoupled systems satisfy Bianchi Identities and secondly, the extra source may be a scalar, vector, or tensor field [28\u201332]. Moreover, a number of interesting results on the solutions of black hole with 2+1 and 3+1 decomposition obtained in [33\u201336]. Additionally, the solutions of new hairy black hole have just been explored [37], and a mechanism is created as well to turn any non-rotational black hole into a rotational one [38, 39].\\n\\nWhen weak gravitational forces are at work, the hypothesis in GR has effectively aligned with many tests carried out within the solar system, demonstrating its success in cosmology. To get more accurate and dependable results, this theory may need to be modified when dealing with high gravitational fields or while being observed on a big scale. These changes may be very important in explaining the phenomena of accelerated expansion. These modifications are termed as modified theories (see, for instance [40\u201347])\\n\\nMany modified theories of gravity [48\u201352] are taken into account by changing the Einstein-Hilbert action that is frequently used to study both the existence of dark energy and dark matter as well as the mystery of universe rapid expansion. Geometrical representation and scalar tensor representation of $f(G, T)$ gravity has been presented to establish novel junction condition [53].\\n\\nSeveral researchers are interested to explore the gravitational collapse phenomena because it is a prominent case in a strong-field regime [54\u201356]. Jordan [57] developed a full gravitational theory which gave the title of a gravitational scalar field to gravitational constant. Brans and Dicke [58] developed a scalar-tensor field theory named as the BransDicke (BD) theory obtained by substituting a time modifying constant $G(t)$ and with the help of a scalar field ($\\\\Phi$) having interaction along with the geometry. Additionally, the well-known scalar field coupling constant or parameter (\\\\(\\\\omega_{BD}\\\\)) of the BD theory is a constant that can be adjusted to get the desired outcomes in Jordan frame. It is assumed that the $\\\\Phi$ is reciprocal of the dynamical gravitational constant, i.e., $G(t) = \\\\frac{1}{\\\\Phi(t)}$. The test particles travel along geodesics according to the BD theory, they consequently obey the weak equivalence principle, which states that the gravitational mass and inertial mass are equivalent. Mach principle, agreement with the weak equivalence principle, and Dirac\u2019s large number hypothesis are the main ingredients of the BD theory. This theory includes a metric tensor and a scalar field that describes gravity.\\n\\nThe large value of $\\\\Phi$ describes the fast expansion of the universe, is found by recent study in cosmology, including the redshift and distance-luminosity connection of type Ia Supernovae [59]. The evidence for various cosmic concerns, including the late behavior of the universe, cosmic acceleration, and the inflation issue, etc are also supported by the BD theory [60]. This theory has drawn interest in recent years due to its precision in describing early inflationary era and late time expansion of the cosmos. Several authors studied the Friedmann-Lema\u00eetre-Robertson-Walker model in the context of the BD theory [61\u201363].\\n\\nThe main purpose of this work is to extend [64] in the BD theory. The paper is organized as follows. The appropriate BD theory for the GD formalism is discussed in Sec. II. In Sec. III, the MGD technique to a spherically symmetric geometry filled with two sources in the BD theory is introduced. In Sec. IV, junction conditions are established to match an outside Schwarzschild line element with the inside solution. In Sec. V, we studied mathematical and physical solutions to the modified field equations using the MGD method with constraints apply to matter density and radial pressure for anisotropy in the setting of the BD theory. In Sec. VI, the physical characteristics of an anisotropic stellar structure with the help of polytropic equation of state are provided. The main results are summarized in Sec. VII.\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Adaptive Low Rank Adaptation of Segment Anything to Salient Object Detection\\n\\nRuikai Cui, Siyuan He, and Shi Qiu\\nAustralian National University\\n{ruikai.cui, siyuan.he, shi.qiu}@anu.edu.au\\n\\nAbstract.\\nFoundation models, such as OpenAI\u2019s GPT-3 and GPT-4, Meta\u2019s LLaMA, and Google\u2019s PaLM2, have revolutionized the field of artificial intelligence. A notable paradigm shift has been the advent of the Segment Anything Model (SAM), which has exhibited a remarkable capability to segment real-world objects, trained on 1 billion masks and 11 million images. Although SAM excels in general object segmentation, it lacks the intrinsic ability to detect salient objects, resulting in suboptimal performance in this domain. To address this challenge, we present the Segment Salient Object Model (SSOM), an innovative approach that adaptively fine-tunes SAM for salient object detection by harnessing the low-rank structure inherent in deep learning. Comprehensive qualitative and quantitative evaluations across five challenging RGB benchmark datasets demonstrate the superior performance of our approach, surpassing state-of-the-art methods.\\n\\nKeywords: salient object detection \u00b7 large-scale pre-trained models \u00b7 parameter-efficient fine-tuning.\\n\\n1 Introduction\\n\\nFoundation models [3, 14, 23] have received significant interests in recent years, owing to their exceptional performance across a multitude of diverse tasks. These models typically consume billions of parameters, trained on expansive web-scaled datasets for fundamental tasks such as next token prediction [6] or masked region completion [7]. A particularly compelling instance of these models is the Segment-Anything Model (SAM) [14], which has been trained on an unprecedentedly vast dataset comprising 11 million images and 1 billion masks.\\n\\nDespite the Segment-Anything Model\u2019s (SAM) noteworthy proficiency in generating masks to segment real-world objects, it is deficient in the detection of salient objects. This shortcoming leads to suboptimal performance in isolating a single salient object from a given RGB image, a crucial aspect of computer vision that emphasizes the identification of the most visually striking or attention-demanding object within an image.\\n\\nTraditional approaches for harnessing the capabilities of foundation models for downstream tasks generally include fine-tuning the entire model [11] or integrating additional adapter layers [9]. However, most foundation models possess\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Adaptive Low Rank Adaptation of Segment Anything to Salient Object Detection\\n\\nRuikai Cui, Siyuan He, and Shi Qiu\\nAustralian National University\\n{ruikai.cui, siyuan.he, shi.qiu}@anu.edu.au\\n\\nAbstract.\\nFoundation models, such as OpenAI\u2019s GPT-3 and GPT-4, Meta\u2019s LLaMA, and Google\u2019s PaLM2, have revolutionized the field of artificial intelligence. A notable paradigm shift has been the advent of the Segment Anything Model (SAM), which has exhibited a remarkable capability to segment real-world objects, trained on 1 billion masks and 11 million images salient object detection \u00b7 large-scale pre-trained models \u00b7 parameter-efficient fine-tuning.\\n\\n1 Introduction\\nFoundation models [3, 14 , 23 ] have received significant interests in recent years, owing to their exceptional performance across a multitude of diverse tasks These models typically consume billions of parameters, trained on expansive web-scaled datasets for fundamental tasks such as next token prediction [ 6 ] or masked region completion [ 7 ]. A particularly compelling instance of these models is the Segment-Anything Model (SAM) [ 14 ], which has been trained on an unprecedentedly vast dataset comprising 11 million images and 1 billion masks. Despite the Segment-Anything Model\u2019s (SAM) noteworthy proficiency in generating masks to segment real-world objects, it is deficient in the detection of salient objects. This shortcoming leads to suboptimal performance in isolating a single salient object from a given RGB image, a crucial aspect of computer vision that emphasizes the identification of the most visually striking or attention-demanding object within an image. Traditional approaches for harnessing the capabilities of foundation models for downstream tasks generally include fine-tuning the entire model [ 11 ] or integrating additional adapter layers [ 9 ]. However, most foundation models possess\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ten publications were applied to more than one language: six publications considered two programming languages, one publication considered three languages, and three publications considered four languages. This results in an average of 1.33 programming languages considered per publication.\\n\\nIn addition to programming languages considered, we collect training details, such as hardware used and training time for each publication. However, those are not always provided. There are 22 out of 52 publications without hardware details (42%) and 26 out of 52 without training time (50%). 33% shared neither information (17 out of 52 publications). The training time of 26 publications with such details ranges from two hours or less [41, 55, 78, 79, 85] to hundreds of hours [47, 53]. While it is common to perform training on GPUs, there are four publications that did not use any GPU for their training procedure, published from 2015\u20132019 [41, 53, 77, 86]. Commonly, publications used a single GPU for training [39, 40, 43, 44, 49, 55, 63, 68, 75, 78\u201380, 87, 88], sometimes in combination with CPUs. The highest amount of GPUs have been used by Svyatkovskiy et al. [47]. They utilized 5 Lambda V100 boxes, with 16 V100 GPUs each, resulting in 80 GPUs.\\n\\nWhile we focus on the training procedure and the energy associated with creating and sharing an ML model, we note the application of such models can vary highly for different SE tasks. Usually, the reported tested times are lower than the required training time (e.g., more than 100 times quicker than training [40, 75, 76]), but in particular, program repair experiments can require long testing times. For example, Chen et al. [55] applied Sequencer for 130 hours to find patches for 75 bugs. White et al. [56] applied their program repair tool DeepRepair for 2,616 days. Data extraction and preparation steps can also require considerable amounts of time and compute resources, ranging from 5-12 days [73, 78, 81].\\n\\nThe majority of task-specific publications provided access to the full trained models, some of which one needs to request access to [51, 76]. Moreover, there are approaches shared as online tools [44, 49, 86] or IDE extensions [47, 48, 83, 85]. There are also 12 out of 52 publications that did not share the full model, but trained embedding files, which are used by the model. These are marked in Table II with the \u2020 symbol.\\n\\nV. TASK-AGNOSTIC CODE MODELS\\n\\nThis section presents task-agnostic code models which share means of representing source code as embeddings, for a variety of downstream tasks. These models are able to transform code snippets to embeddings, which can be fine-tuned to SE tasks. For example, Lu et al. [108] provided fine-tuning details for the CodeXGLUE benchmark, with information for task-specific training and inference time for each task. The fine-tuning time ranges from 2 GPU hours (defect detection) to 60 hours (text-to-code generation, documentation translation).\\n\\nIn total, we collected 27 task-agnostic models, as shown in Table III. For each publication, we list the model name and the programming languages it was trained on. If available, we list details on hardware configuration and training times. Among the 27 publications, 52% did not provide training time details (14 out of 27) and 26% did not provide their hardware configurations (7 out of 27). For publications without hardware details, training time is not reported as well.\\n\\nAmong the publications that shared training time details, the shortest duration is found for code2vec [101], which was\\n\\n---\\n\\n### Table III\\n\\n| Approach                  | Year | Language                                                                 | Hardware                                                                 | Time in hours | kWh  |\\n|---------------------------|------|--------------------------------------------------------------------------|--------------------------------------------------------------------------|---------------|------|\\n| BLOOM [13]                | 2022 | Java, PHP, C++, Python, JavaScript, C#, Ruby, Lua, TypeScript, GO, C, Scala, Rust | server: 384 NVIDIA A100 GPUs, 80 GB                                     | 1,082,990     | 433,196 |\\n| ProphetNet-x [91]         | 2021 | Go, Java, JS, Php, Python, Ruby                                         | NVIDIA Tesla V100 GPUs                                                  | 30,000        | 15,330 |\\n| CodeBERT [92]             | 2020 | Python, Java, JavaScript, PHP, Ruby, Go                                  | server: 16 NVIDIA Tesla V100 GPUs, 32 GB                               | 1,320         | 10,610 |\\n| Debt [93]                 | 2021 | Java, Python                                                             | 32 NVIDIA V100 GPUs                                                     | 192           | 3,080  |\\n| Code5 [94]                | 2021 | Ruby, JavaScript, Go, Python, Java, Php, C, C#                          | server/cluster: 16 NVIDIA A100 GPUs, 40 GB                             | 288           | 1,930  |\\n| PLBART [95]               | 2021 | Java, Python                                                             | 8 NVIDIA RTX 2080 Ti GPUs                                               | 276           | 925    |\\n| Mastropascolo et al. [96] | 2021 | Java                                                                     | Google Cloud, Colab: 8 TPUs, 35.5 GB memory                             | 343           | 766    |\\n| Graphcodebert [97]        | 2021 | Ruby, JS, Go, Python, Java, PHP                                          | server: 32 NVIDIA Tesla V100 GPUs, 32 GB                               | 83            | 667    |\\n| CodeTrans [98]            | 2021 | Python, Java, JavaScript, PHP, Ruby, Go, C#, SQL, LISP                   | 1 TPU v3-8                                                              | 2,088         | 582    |\\n| GREAT [99]                | 2022 | Python                                                                   | 1 Tesla P100 GPU                                                        | 120           | 51     |\\n| Java2bert [100]           | 2021 | Java                                                                     | 3 NVIDIA Titan X GPUs, 12 GB                                            | 24            | 30     |\\n| code2vec [101]            | 2019 | Java                                                                     | 1 NVIDIA Tesla K80 GPU                                                  | 36            | 18     |\\n| OpenVocabularyNLM [102]   | 2020 | Java, Python                                                             | GPUs                                                                     | 336           | -      |\\n| GraphCode2Vec [103]       | 2022 | Java                                                                     | server: 40 GPUs@2.20GHz, 256GB; 1 NVIDIA Tesla V100 GPU                  | -             | -      |\\n| Spt-code [104]            | 2022 | Java                                                                     | 4 NVIDIA A100s9 GPUs                                                    | -             | -      |\\n| StructCoder [8]           | 2022 | Java, Python, PHP, JavaScript, GO                                       | 4 RTX 8000 GPUs, 48GB                                                   | -             | -      |\\n| Codex [10]                | 2021 | Python                                                                   | Azure                                                                    | -             | -      |\\n| Cotex [105]               | 2021 | Python, Java, JavaScript, PHP, Ruby, Go                                  | 1 TPU v2-8                                                             | -             | -      |\\n| CodeBERT [106]            | 2020 | Python                                                                   | TPU                                                                     | -             | -      |\\n| TSSA [107]                | 2020 | Java                                                                     | 1 NVIDIA P100 GPU, 16 GB; 1 K80 GPU, 16GB memory                        | -             | -      |\\n| CodeGPT [108]             | 2021 | Python, Java                                                             | -                                                                       | -             | -      |\\n| ContraCode [109]          | 2021 | Java "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ten publications were applied to more than one language: six publications considered two programming languages, one publication considered three languages, and three publications considered four languages. This results in an average of 1.33 programming languages considered per publication. In addition to programming languages considered, we collect training details, such as hardware used and training time for each publication. However, those are not always provided. There are 22 out of 52 publications without hardware details (42%) and 26 out of 52 without training time (50%), 33% shared neither information (17 out of 52 publications). The training time of 26 publications with such details ranges from two hours or less [41, 55, 78, 79, 85] to hundreds of hours [47, 53]. While it is common to perform training on GPUs, there are four publications that did not use any GPU for their training procedure, published from 2015\u20132019 [41, 53, 77, 86]. Commonly, publications used a single GPU for training [39, 40, 43, 44, 49, 55, 63, 68, 75, 78\u201380, 87, 88], sometimes in combination with CPUs. The highest amount of GPUs have been used by Svyatkovskiy et al. [47]. They utilized 5 Lambda V100 boxes, with 16 V100 GPUs each, resulting in 80 GPUs. While we focus on the training procedure and the energy associated with creating and sharing an ML model, we note the application of such models can vary highly for different SE tasks. Usually, the reported tested times are lower than the required training time (e.g., more than 100 times quicker than training [40, 75, 76]), but in particular, program repair experiments can require long testing times. For example, Chen et al. [55] applied Sequencer for 130 hours to find patches for 75 bugs. White et al. [56] applied their program repair tool DeepRepair for 2,616 days. Data extraction and preparation steps can also require considerable amounts of time and compute resources, ranging from 5-12 days [73, 78, 81]. The majority of task-specific publications provided access to the full trained models, some of which one needs to request access to [51, 76]. Moreover, there are approaches shared as online tools [44, 49, 86] or IDE extensions [47, 48, 83, 85]. There are also 12 out of 52 publications that did not share the full model, but trained embedding files, which are used by the model. These are marked in Table II with the \u2020 symbol.\\n\\nV. TASK-AGNOSTIC CODE MODELS\\n\\nThis section presents task-agnostic code models which share means of representing source code as embeddings, for a variety of downstream tasks. These models are able to transform code snippets to embeddings, which can be fine-tuned to SE tasks. For example, Lu et al. [108] provided fine-tuning details for the CodeXGLUE benchmark, with information for task-specific training and inference time for each task. The fine-tuning time ranges from 2 GPU hours (defect detection) to 60 hours (text-to-code generation, documentation translation).\\n\\nIn total, we collected 27 task-agnostic models, as shown in Table III. For each publication, we list the model name and the programming languages it was trained on. If available, we list details on hardware configuration and training times. Among the 27 publications, 52% did not provide training time details (14 out of 27) and 26% did not provide their hardware configurations (7 out of 27). For publications without hardware details, training time is not reported as well. Among the publications that shared training time details, the shortest duration is found for code2vec [101], which was\\n\\n---\\n\\n### Table III\\n\\n| Approach                  | Year | Language                                                                 | Hardware                                                                 | Time in hours | kWh  |\\n|---------------------------|------|--------------------------------------------------------------------------|--------------------------------------------------------------------------|---------------|------|\\n| BLOOM [13]                | 2022 | Java, PHP, C++, Python, JavaScript, C#, Ruby, Lua, TypeScript, GO, C, Scala, Rust | server: 384 NVIDIA A100 GPUs, 80 GB                                     | 1,082,990     | 433,196 |\\n| ProphetNet-x [91]         | 2021 | Go, Java, JS, Php, Python, Ruby                                         | NVIDIA Tesla V100 GPUs                                                  | 30,000        | 15,330 |\\n| CodeBERT [92]             | 2020 | Python, Java, JavaScript, PHP, Ruby, Go                                  | server: 16 NVIDIA Tesla V100 GPUs, 32 GB                               | 1,320         | 10,610 |\\n| Debt [93]                 | 2021 | Java, Python                                                             | 32 NVIDIA V100 GPUs                                                     | 192           | 3,080  |\\n| Code5 [94]                | 2021 | Ruby, JavaScript, Go, Python, Java, Php, C, C#                          | server/cluster: 16 NVIDIA A100 GPUs, 40 GB                             | 288           | 1,930  |\\n| PLBART [95]               | 2021 | Java, Python                                                             | 8 NVIDIA RTX 2080 Ti GPUs                                               | 276           | 925    |\\n| Mastropascolo et al. [96] | 2021 | Java                                                                     | Google Cloud, Colab: 8 TPUs, 35.5 GB memory                            | 343           | 766    |\\n| Graphcodebert [97]        | 2021 | Ruby, JS, Go, Python, Java, PHP                                          | server: 32 NVIDIA Tesla V100 GPUs, 32 GB                               | 83            | 667    |\\n| CodeTrans [98]            | 2021 | Python, Java, JavaScript, PHP, Ruby, Go, C#, SQL, LISP                   | 1 TPU v3-8                                                              | 2,088         | 582    |\\n| GREAT [99]                | 2022 | Python                                                                   | 1 Tesla P100 GPU                                                        | 120           | 51     |\\n| Java2bert [100]           | 2021 | Java                                                                     | 3 NVIDIA Titan X GPUs, 12 GB                                           | 24            | 30     |\\n| code2vec [101]            | 2019 | Java                                                                     | 1 NVIDIA Tesla K80 GPU                                                 | 36            | 18     |\\n| OpenVocabularyNLM [102]   | 2020 | Java, Python                                                             | GPUs                                                                    | 336           | -      |\\n| GraphCode2Vec [103]       | 2022 | Java                                                                     | server: 40 CPUs@2.20GHz, 256GB; 1 NVIDIA Tesla V100 GPU                 | -             | -      |\\n| Spt-code [104]            | 2022 | Java                                                                     | 4 NVIDIA A100s9 GPUs                                                   | -             | -      |\\n| StructCoder [8]           | 2021 | Java, Python, PHP, JavaScript, GO                                        | 4 RTX 8000 GPUs, 48GB                                                  | -             | -      |\\n| Codex [10]                | 2021 | Python                                                                   | Azure                                                                   | -             | -      |\\n| Cotex [105]               | 2021 | Python, Java, JavaScript, PHP, Ruby, Go                                  | 1 TPU v2-8                                                             | -             | -      |\\n| CodeBERT [106]            | 2020 | Python                                                                   | TPU                                                                    | -             | -      |\\n| TSSA [107]                | 2020 | Java                                                                     | 1 NVIDIA P100 GPU, 16 GB; 1 K80 GPU, 16GB memory                       | -             | -      |\\n| CodeGPT [108]             | 2021 | Python, Java                                                             | -                                                                       | -             | -      |\\n| ContraCode [109]          | 2021 | Java                                                                     | -                                                                       | -             | - "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For the fourth property we need to find a value for $\\\\beta$ such that $|\\\\text{Opt}(G) - |I_F|| \\\\leq \\\\beta|\\\\text{Opt}(T_G, T'_G) - |F||$. Let $\\\\ell = |I_F|$ be the number of $A_v$ in $F''$ that are covered by 6 components. We know that $|F| \\\\geq |F'| \\\\geq |F''| = 12n - \\\\ell$. Observe:\\n\\n$$|\\\\text{Opt}(T_G, T'_G) - |F|| = |F| - \\\\text{Opt}(T_G, T'_G)$$\\n\\n$$\\\\geq |F''| - \\\\text{Opt}(T_G, T'_G)$$\\n\\n$$= 12n - \\\\ell - \\\\text{Opt}(T_G, T'_G)$$\\n\\n$$= 12n - \\\\ell - (12n - k)$$\\n\\n$$= k - \\\\ell$$\\n\\n$$= |\\\\text{Opt}(G) - |I_F||$$\\n\\nSo we pick $\\\\beta = 1$ and we are done.\\n\\n\\\\[\\\\square\\\\]\\n\\n5 A tight 7k kernel\\n\\nRecall the definitions of common subtrees and common chains from the preliminaries. It is well-known that the following two polynomial-time reduction rules do not alter the size of the uMAF [2]:\\n\\n**Subtree reduction.** If $T$ and $T'$ have a maximal common pendant subtree $S$ with at least two leaves, then reduce $T$ and $T'$ to $T_r$ and $T'_r$, respectively, by replacing $S$ with a single leaf with a new label.\\n\\n**Chain reduction.** If $T$ and $T'$ have a maximal common $n$-chain $C = (\\\\ell_1, \\\\ell_2, \\\\ldots, \\\\ell_n)$ with $n \\\\geq 4$, then reduce $T$ and $T'$ to $T_r = T|X \\\\setminus \\\\{\\\\ell_4, \\\\ell_5, \\\\ldots, \\\\ell_n\\\\}$ and $T'_r = T'|X \\\\setminus \\\\{\\\\ell_4, \\\\ell_5, \\\\ldots, \\\\ell_n\\\\}$, respectively.\\n\\nWhen applied to exhaustion on two unrooted binary trees, at which point we say the trees are fully reduced, these rules yield an instance with (ignoring additive terms) at most $15k$ taxa [10], where $k$ is the size of the uMAF\\\\(^3\\\\), and the analysis is tight.\\n\\nNote that applying the subtree or chain reduction to a caterpillar produces a new caterpillar. In this section we will show that, when applied to exhaustion on two caterpillars, a much smaller kernel is obtained than on general unrooted binary trees.\\n\\n**Theorem 4.** There is a 7k kernel for uMAF on caterpillars using only the common chain and subtree reductions, and this is tight up to a constant additive term.\\n\\n\\\\(^3\\\\)The kernel bound given in [10] is in terms of TBR distance, rather than uMAF, but as noted earlier these quantities only differ by 1, so only additive terms are affected.\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For the fourth property we need to find a value for $\\\\beta$ such that $|\\\\text{Opt}(G) - |I_F|| \\\\leq \\\\beta|\\\\text{Opt}(T_G, T'_G) - |F||$. Let $\\\\ell = |I_F|$ be the number of $A_v$ in $F''$ that are covered by 6 components. We know that $|F| \\\\geq |F'| \\\\geq |F''| = 12n - \\\\ell$. Observe:\\n\\n$$|\\\\text{Opt}(T_G, T'_G) - |F|| = |F| - \\\\text{Opt}(T_G, T'_G)$$\\n\\n$$\\\\geq |F''| - \\\\text{Opt}(T_G, T'_G)$$\\n\\n$$= 12n - \\\\ell - \\\\text{Opt}(T_G, T'_G)$$\\n\\n$$= 12n - \\\\ell - (12n - k)$$\\n\\n$$= k - \\\\ell$$\\n\\n$$= |\\\\text{Opt}(G) - |I_F||$$\\n\\nSo we pick $\\\\beta = 1$ and we are done.\\n\\n\\\\[\\\\square\\\\]\\n\\n5 A tight 7k kernel\\n\\nRecall the definitions of common subtrees and common chains from the preliminaries. It is well-known that the following two polynomial-time reduction rules do not alter the size of the uMAF [2]:\\n\\n**Subtree reduction.** If $T$ and $T'$ have a maximal common pendant subtree $S$ with at least two leaves, then reduce $T$ and $T'$ to $T_r$ and $T'_r$, respectively, by replacing $S$ with a single leaf with a new label. \\n\\n**Chain reduction.** If $T$ and $T'$ have a maximal common $n$-chain $C = (\\\\ell_1, \\\\ell_2, \\\\ldots, \\\\ell_n)$ with $n \\\\geq 4$, then reduce $T$ and $T'$ to $T_r = T|X \\\\setminus \\\\{\\\\ell_4, \\\\ell_5, \\\\ldots, \\\\ell_n\\\\}$ and $T'_r = T'|X \\\\setminus \\\\{\\\\ell_4, \\\\ell_5, \\\\ldots, \\\\ell_n\\\\}$, respectively.\\n\\nWhen applied to exhaustion on two unrooted binary trees, at which point we say the trees are fully reduced, these rules yield an instance with (ignoring additive terms) at most $15k$ taxa [10], where $k$ is the size of the uMAF 3, and the analysis is tight The kernel bound given in [10] is in terms of TBR distance, rather than uMAF, but as noted earlier these quantities only differ by 1, so only additive terms are affected. \\n\\n\\\\[\\\\text{Theorem 4. There is a 7k kernel for uMAF on caterpillars using only the common chain and subtree reductions, and this is tight up to a constant additive term.}\\\\]\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"all $p_i$'s are integers and $B = \\\\frac{4ac}{\\\\varepsilon}$. We can compute $\\\\min(h_{x_1}(x), B)$ exactly using standard dynamic programming in $O(|T_1^0| \\\\cdot B) = \\\\tilde{O}(\\\\frac{1}{\\\\varepsilon^2})$ time.\\n\\nThe complexity of the resulting function is $O(|T_1^0|) = \\\\tilde{O}(\\\\frac{1}{\\\\varepsilon})$.\\n\\nIn $\\\\tilde{O}(n + \\\\frac{1}{\\\\varepsilon^2})$ time, we can compute a function $\\\\tilde{f}_{T_1^0}$ that approximates $\\\\min(f_{T_1^0}, \\\\frac{4c}{\\\\varepsilon^{3/2}})$ with a factor of $1 + \\\\tilde{O}(\\\\varepsilon)$ via Lemma 7. The additive error caused by $\\\\tilde{f}_{T_1^0}$ is at most $\\\\tilde{O}(\\\\varepsilon \\\\cdot \\\\frac{4c}{\\\\varepsilon^{3/2}}) \\\\leq \\\\tilde{O}(\\\\frac{1}{\\\\varepsilon^2})$ as $a \\\\leq \\\\frac{1}{\\\\varepsilon^{1/2}}$, and the complexity of $\\\\tilde{f}_{T_1^0}$ is $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon})$.\\n\\n**Lemma 13.** Let $\\\\tilde{f}_{T_1^0}$ and $\\\\tilde{f}_{T_2^0}$ be two functions that approximate $\\\\max(f_{T_1^0}, p(T_1^0) - \\\\frac{4c}{\\\\varepsilon^{3/2}})$ and $\\\\min(f_{T_2^0}, \\\\frac{4c}{\\\\varepsilon^{3/2}})$ with additive error $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon^2})$ and complexity $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon})$, respectively. We can approximate $\\\\tilde{f}_{T_1^0} \\\\oplus \\\\tilde{f}_{T_2^0}$ with additive error $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon^2})$ and complexity $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon})$ in $\\\\tilde{O}(\\\\frac{n}{\\\\varepsilon^2})$ time.\\n\\n**Proof.** Let $u = \\\\max(0, p(T_1^0) - \\\\frac{4c}{\\\\varepsilon^{3/2}})$. Note that $\\\\tilde{f}_{T_1^0}$ have a range contained in $[u, p(T_1^0)]$, so $\\\\tilde{f}_{T_1^0} - u + 1$ have a range contained in $[1, \\\\frac{4c}{\\\\varepsilon^{3/2}} + 1]$. Since\\n\\n$$\\\\tilde{f}_{T_1^0} \\\\oplus \\\\tilde{f}_{T_2^0} = (\\\\tilde{f}_{T_1^0} - u + 1) \\\\oplus \\\\tilde{f}_{T_2^0} + u - 1.$$ \\n\\nTo approximate $\\\\tilde{f}_{T_1^0} \\\\oplus \\\\tilde{f}_{T_2^0}$, it suffices to approximate $(\\\\tilde{f}_{T_1^0} - u + 1) \\\\oplus \\\\tilde{f}_{T_2^0}$ with an additive error of $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon^2})$. Both $(\\\\tilde{f}_{T_1^0} - u + 1)$ and $\\\\tilde{f}_{T_2^0}$ have ranges contained in $\\\\{0\\\\} \\\\cup [1, \\\\frac{4c}{\\\\varepsilon^{3/2}} + 1]$, therefore, the approximation factor we are allowed to incur is $1 + \\\\tilde{O}(\\\\frac{1}{\\\\varepsilon^2})$. We can approximate the two functions via Lemma 4 using $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon^2} + \\\\frac{a^2}{\\\\varepsilon}) = \\\\tilde{O}(\\\\frac{1}{\\\\varepsilon})$ time. The complexity of the resulting function is $\\\\tilde{O}(\\\\frac{n}{\\\\varepsilon^2}) \\\\leq \\\\tilde{O}(\\\\frac{1}{\\\\varepsilon})$.\\n\\n**Lemma 14.** Let $g$ be a function that approximates $\\\\max(f_{T_1^0}, p(T_1^0) - \\\\frac{4c}{\\\\varepsilon^{3/2}}) \\\\oplus \\\\min(f_{T_2^0}, \\\\frac{4c}{\\\\varepsilon^{3/2}})$ with additive error $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon^2})$ and complexity $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon})$. We can approximate $g \\\\oplus f_{T_2^0}$ with additive error $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon^2})$ and complexity $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon})$ in $\\\\tilde{O}(\\\\frac{n}{\\\\varepsilon^2})$ time.\\n\\n**Proof.** The items in $T_1^0$ have at most $O(\\\\delta) = \\\\tilde{O}(\\\\frac{1}{\\\\varepsilon^2})$ distinct profits. By dividing $T_1^0$ into groups by profits, and merging the functions for the groups via Lemma 5(ii), we can compute a function $\\\\tilde{f}_{T_2^0}$ that approximates $f_{T_2^0}$ with factor $1 + O(\\\\varepsilon)$ and complexity $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon})$ in time $O(|T_2| + \\\\frac{1}{\\\\varepsilon^2}) \\\\leq O(n + \\\\frac{1}{\\\\varepsilon^2})$ time. The additive error of $\\\\tilde{f}_{T_2^0}$ is at most $O(\\\\varepsilon) \\\\cdot f_{T_2^0}(t) \\\\leq O(\\\\frac{1}{\\\\varepsilon^2})$. Merging $g$ and $\\\\tilde{f}_{T_2^0}$ via Lemma 4(i) takes $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon^2})$ time.\\n\\n**Lemma 15.** In $\\\\tilde{O}(n + \\\\frac{1}{\\\\varepsilon^2})$ time, we can compute a function with complexity $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon})$ that approximates $f_{T_2}$ with additive error $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon^2})$ within the domain $[b_1, b_0]$.\\n\\n**Proof.** Computing $\\\\tilde{f}_{T_1^0}$ that approximates $\\\\max(f_{T_1^0}, p(T_1^0) - \\\\frac{4c}{\\\\varepsilon^{3/2}})$ via Lemma 12 takes $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon^2})$ time. Computing $\\\\tilde{f}_{T_2^0}$ that approximates $\\\\min(f_{T_2^0}, \\\\frac{4c}{\\\\varepsilon^{3/2}})$ via Lemma 7 takes $\\\\tilde{O}(n + \\\\frac{1}{\\\\varepsilon^2})$ time. Computing a function $g$ that approximates $\\\\tilde{f}_{T_1} \\\\oplus \\\\tilde{f}_{T_2}$ via Lemma 13 takes $\\\\tilde{O}(\\\\frac{n}{\\\\varepsilon^2})$ time. Finally, approximating $g \\\\oplus f_{T_2}$ via Lemma 14 takes $\\\\tilde{O}(\\\\frac{n}{\\\\varepsilon^2})$ time. The total additive error is bounded by $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon^2})$, and the total running time is\\n\\n$$\\\\tilde{O}(n + \\\\frac{1}{\\\\varepsilon^2} + \\\\frac{a "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"all $p_i$'s are integers and $B = \\\\frac{4ac}{\\\\varepsilon}$. We can compute $\\\\min(h_{x_1}(x), B)$ exactly using standard dynamic programming in $O(|T_1^0| \\\\cdot B) = \\\\tilde{O}(\\\\frac{1}{\\\\varepsilon^2})$ time.\\n\\nThe complexity of the resulting function is $O(|T_1^0|) = \\\\tilde{O}(\\\\frac{1}{\\\\varepsilon})$.\\n\\nIn $\\\\tilde{O}(n + \\\\frac{1}{\\\\varepsilon^2})$ time, we can compute a function $\\\\tilde{f}_{T_1^0}$ that approximates $\\\\min(f_{T_1^0}, \\\\frac{4c}{\\\\varepsilon^{3/2}})$ with a factor of $1 + \\\\tilde{O}(\\\\varepsilon)$ via Lemma 7. The additive error caused by $\\\\tilde{f}_{T_1^0}$ is at most $\\\\tilde{O}(\\\\varepsilon \\\\cdot \\\\frac{4c}{\\\\varepsilon^{3/2}}) \\\\leq \\\\tilde{O}(\\\\frac{1}{\\\\varepsilon^2})$ as $a \\\\leq \\\\frac{1}{\\\\varepsilon^{1/2}}$, and the complexity of $\\\\tilde{f}_{T_1^0}$ is $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon})$.\\n\\n**Lemma 13.** Let $\\\\tilde{f}_{T_1^0}$ and $\\\\tilde{f}_{T_2^0}$ be two functions that approximate $\\\\max(f_{T_1^0}, p(T_1^0) - \\\\frac{4c}{\\\\varepsilon^{3/2}})$ and $\\\\min(f_{T_2^0}, \\\\frac{4c}{\\\\varepsilon^{3/2}})$ with additive error $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon^2})$ and complexity $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon})$, respectively. We can approximate $\\\\tilde{f}_{T_1^0} \\\\oplus \\\\tilde{f}_{T_2^0}$ with additive error $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon^2})$ and complexity $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon})$ in $\\\\tilde{O}(\\\\frac{n}{\\\\varepsilon^2})$ time.\\n\\n**Proof.** Let $u = \\\\max(0, p(T_1^0) - \\\\frac{4c}{\\\\varepsilon^{3/2}})$. Note that $\\\\tilde{f}_{T_1^0}$ have a range contained in $[u, p(T_1^0)]$, so $\\\\tilde{f}_{T_1^0} - u + 1$ have a range contained in $[1, \\\\frac{4c}{\\\\varepsilon^{3/2}} + 1]$. Since\\n\\n$$\\\\tilde{f}_{T_1^0} \\\\oplus \\\\tilde{f}_{T_2^0} = (\\\\tilde{f}_{T_1^0} - u + 1) \\\\oplus \\\\tilde{f}_{T_2^0} + u - 1.$$ \\n\\nTo approximate $\\\\tilde{f}_{T_1^0} \\\\oplus \\\\tilde{f}_{T_2^0}$, it suffices to approximate $(\\\\tilde{f}_{T_1^0} - u + 1) \\\\oplus \\\\tilde{f}_{T_2^0}$ with an additive error of $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon^2})$. Both $(\\\\tilde{f}_{T_1^0} - u + 1)$ and $\\\\tilde{f}_{T_2^0}$ have ranges contained in $\\\\{0\\\\} \\\\cup [1, \\\\frac{4c}{\\\\varepsilon^{3/2}} + 1]$, therefore, the approximation factor we are allowed to incur is $1 + \\\\tilde{O}(\\\\frac{1}{\\\\varepsilon^2})$. We can approximate the two functions via Lemma 4 using $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon^2} + \\\\frac{a^2}{\\\\varepsilon}) = \\\\tilde{O}(\\\\frac{1}{\\\\varepsilon})$ time. The complexity of the resulting function is $\\\\tilde{O}(\\\\frac{n}{\\\\varepsilon^2}) \\\\leq \\\\tilde{O}(\\\\frac{1}{\\\\varepsilon})$.\\n\\n**Lemma 14.** Let $g$ be a function that approximates $\\\\max(f_{T_1^0}, p(T_1^0) - \\\\frac{4c}{\\\\varepsilon^{3/2}}) \\\\oplus \\\\min(f_{T_2^0}, \\\\frac{4c}{\\\\varepsilon^{3/2}})$ with additive error $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon^2})$ and complexity $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon})$. We can approximate $g \\\\oplus f_{T_2^0}$ with additive error $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon^2})$ and complexity $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon})$ in $\\\\tilde{O}(\\\\frac{n}{\\\\varepsilon^2})$ time.\\n\\n**Proof.** The items in $T_1^0$ have at most $O(\\\\delta) = \\\\tilde{O}(\\\\frac{1}{\\\\varepsilon^2})$ distinct profits. By dividing $T_1^0$ into groups by profits, and merging the functions for the groups via Lemma 5(ii), we can compute a function $\\\\tilde{f}_{T_2^0}$ that approximates $f_{T_2^0}$ with factor $1 + O(\\\\varepsilon)$ and complexity $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon})$ in time $O(|T_2| + \\\\frac{1}{\\\\varepsilon^2}) \\\\leq O(n + \\\\frac{1}{\\\\varepsilon^2})$ time. The additive error of $\\\\tilde{f}_{T_2^0}$ is at most $O(\\\\varepsilon) \\\\cdot f_{T_2^0}(t) \\\\leq O(\\\\frac{1}{\\\\varepsilon^2})$. Merging $g$ and $\\\\tilde{f}_{T_2^0}$ via Lemma 4(i) takes $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon^2})$ time.\\n\\n**Lemma 15.** In $\\\\tilde{O}(n + \\\\frac{1}{\\\\varepsilon^2})$ time, we can compute a function with complexity $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon})$ that approximates $f_{T_2}$ with additive error $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon^2})$ within the domain $[b_1, b_0]$.\\n\\n**Proof.** Computing $\\\\tilde{f}_{T_1^0}$ that approximates $\\\\max(f_{T_1^0}, p(T_1^0) - \\\\frac{4c}{\\\\varepsilon^{3/2}})$ via Lemma 12 takes $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon^2})$ time. Computing $\\\\tilde{f}_{T_2^0}$ that approximates $\\\\min(f_{T_2^0}, \\\\frac{4c}{\\\\varepsilon^{3/2}})$ via Lemma 7 takes $\\\\tilde{O}(n + \\\\frac{1}{\\\\varepsilon^2})$ time. Computing a function $g$ that approximates $\\\\tilde{f}_{T_1^0} \\\\oplus \\\\tilde{f}_{T_2^0}$ via Lemma 13 takes $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon^2})$ time. Finally, approximating $g \\\\oplus f_{T_2}$ via Lemma 14 takes $\\\\tilde{O}(\\\\frac{a(\\\\varepsilon^2 - \\\\varepsilon)}{\\\\varepsilon})$ time. The total additive error is bounded by $\\\\tilde{O}(\\\\frac{1}{\\\\varepsilon^2})$, and the total running time is\\n\\n$$\\\\tilde{O}(n + \\\\ "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Many of the earlier mathematical works on rCSPs focused on determining their satisfiability thresholds and verifying the sharpness of SAT-UNSAT transitions. For models that are known not to exhibit RSB, such goals were established. These models include random 2-SAT [CR92,BBC+01], random 1-in-k-SAT [ACIM01], k-XOR-SAT [DM02, DGM+10, PS16], and random linear equations [ACOGM20]. On the other hand, for the models which are predicted to belong to 1RSB class, intensive studies have been conducted to estimate their satisfiability threshold, as shown in [KKKS98, AP04, COP16] (random k-SAT), [AM06, COZ12, COP12] (random k-nae-SAT), and [AN05, CO13, COV13, COEH16] (random graph coloring).\\n\\nMore recently, the satisfiability thresholds for rCSPs that exhibits RSB have been rigorously determined for several models, namely the random regular k-nae-SAT [DSS16b], maximum independent set on d-regular graphs [DSS16a], random regular k-SAT [COP16] and random k-SAT [DSS22] for large k and d. Although determining the location of q-colorability threshold for the sparse Erdos Renyi graph is left open, the condensation threshold \\\\( \\\\alpha_{\\\\text{cond}} \\\\) for random graph coloring, where the free energy becomes non-analytic, was settled in [BCOH+16]. They carried out a technically challenging analysis based on a clever \u201cplanting\u201d technique, where the results were further generalized to other models in [COKPZ18]. Similarly, [BCO16] identified the condensation threshold for random regular k-SAT, where each variable appears \\\\( d/2 \\\\)-times positive and \\\\( d/2 \\\\)-times negative. Further, in the condensation regime \\\\( \\\\alpha \\\\in (\\\\alpha_{\\\\text{cond}}, \\\\alpha_{\\\\text{sat}}) \\\\), many quantities of interest was established for random regular k-nae-SAT with large enough \\\\( k \\\\), matching the statistical physics prediction. Namely, the number of solutions at exponential scale (free energy) [SSZ22], the concentration of the overlap [NSS20, NSS21], and the local weak limit [SS23] were established. Establishing the same quantities for other models in the condensation regime is left open.\\n\\nThe closest result to ours in the literature is by Ayre, Coja-Oghlan, and Greenhill [ACOG22], where they lower bound the chromatic number (or equivalently, upper bound the colorability threshold) of the random regular graph of any degree, which is conjectured to be tight. [ACOG22] also considers the sparse Erdos Renyi graph, which is more complicated since the conjectured chromatic number is defined in terms of a distributional (rather than real-valued) optimization due to the randomness of the local neighborhoods. In this work, we do not consider Erdos Renyi type problems, but we additionally address the question of the uniqueness of the BP fixed point for any \\\\( k \\\\geq 3 \\\\) (unique solution to the equation (1.1)). As in [ACOG22], we use an interpolation bound, which gives an upper bound of the satisfiability threshold also for the (non-regular) random k-nae-SAT model. It would be interesting to address the uniqueness of the BP fixed point for random k-nae-SAT and random k-SAT for small \\\\( k \\\\geq 3 \\\\). We refer to [ST03, MRSY19, YP22, GP23] which addresses the uniqueness of BP fixed point for various models.\\n\\n### 1.2 Proof methods\\n\\nWe aim to rigorously establish the upper bound the satisfiability threshold predicted by the so-called \u20181RSB cavity method\u2019 from statistical physics [DRZ08]. To do so, instead of using moment methods, we use a technique called \u2018interpolation method\u2019 from the theory of spin glasses developed by [FL03, Gue03, PT04]. The interpolation method has been successful in upperbounding the satisfiability threshold for random k-SAT [DSS15] for large \\\\( k \\\\), the free energy for random regular k-nae-SAT [SSZ16], and the colorability threshold for random graphs [ACOG22].\\n\\nWe first introduce the notations and mathematical framework that we use throughout the paper. For both the \\\\( d \\\\)-regular \\\\( k \\\\)-uniform hypergraphs and the \\\\( k \\\\)-nae-SAT formula, we can represent them as (labelled) \\\\((d,k)\\\\)-regular bipartite graph. Let \\\\( V = \\\\{v_1, \\\\ldots, v_n\\\\} \\\\) be the set of variables or nodes and \\\\( F = \\\\{a_1, \\\\ldots, a_m\\\\} \\\\) be the set of clauses or hyperedges. An edge is formed if the variable or node \\\\( v_i \\\\) is included in the clause or hyperedge \\\\( a_j \\\\). For an edge \\\\( e \\\\), we denote \\\\( v(e) \\\\) (resp. \\\\( a(e) \\\\)) by the variable (resp. clause) adjacent to it.\\n\\nDenote \\\\( G = (V, F, E) \\\\) by the resulting bipartite graph. We denote the neighborhood of \\\\( v \\\\in V \\\\) (resp. \\\\( a \\\\in F \\\\)) by \\\\( \\\\delta v := \\\\{a \\\\in F : (av) \\\\in E\\\\} \\\\) (resp. \\\\( \\\\delta a := \\\\{v \\\\in V : (av) \\\\in E\\\\} \\\\)). Throughout, we denote \\\\( \\\\alpha \\\\equiv \\\\frac{m}{n} = \\\\frac{d}{k} \\\\). For the nae-SAT formula, there is an extra label for each edge \\\\( e \\\\in E \\\\), namely the literal \\\\( L_e \\\\in \\\\{0,1\\\\} \\\\), which specifies how the variable \\\\( v(e) \\\\) participates in the clause \\\\( a(e) \\\\). Then, the labelled graph \\\\( \\\\mathcal{G} = (V, F, E, L) \\\\equiv (V, F, E, (L_e)_{e \\\\in E}) \\\\) represents a nae-SAT instance.\\n\\n**Definition 1.3.** Given a nae-SAT instance \\\\( \\\\mathcal{G} = (V, F, E, L) \\\\), \\\\( x \\\\in \\\\{0,1\\\\}^V \\\\) is a (nae-SAT) **solution** if\\n\\n\\\\[\\n\\\\prod_{a \\\\in F} \\\\varphi((x_{v(e)} \\\\oplus L_e)_{e \\\\in \\\\delta a}) = 1,\\n\\\\]\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Many of the earlier mathematical works on rCSPs focused on determining their satisfiability thresholds and verifying the sharpness of sat-unsat transitions. For models that are known not to exhibit RSB, such goals were established. These models include random 2sat [ CR92 , BBC + 01 ], random 1in - k - sat [ ACIM01 ], k - xor - sat [ DM02 , DGM + 10 , PS16 ], and random linear equations [ ACOGM20 ]. On the other hand, for the models which are predicted to belong to 1 rsb class, intensive studies have been conducted to estimate their satisfiability threshold, as shown in [ KKKS98 , AP04 , COP16 ] (random k - sat), [ AM06 , COZ12 , COP12 ] (random graph coloring). More recently, the satisfiability thresholds for r csp s that exhibits RSB have been rigorously determined for several models, namely the random regular k - nae - sat [ DSS16b ], maximum independent set on d - regular graphs [ DSS16a ], random regular k - sat [ COP16 ] and random k - sat [ DSS22 ] for large k and d. Although determining the location of q - colorability threshold for the sparse Erdos Renyi graph is left open, the condensation threshold \\\\( \\\\alpha_{\\\\text{cond}} \\\\) for random graph coloring, where the free energy becomes non-analytic, was settled in [ BCOH + 16 ]. They carried out a technically challenging analysis based on a clever \u201cplanting\u201d technique, where the results were further generalized to other models in [ COKPZ18 ]. Similarly, [ BCO16 ] identified the condensation threshold for random regular k - sat, where each variable appears \\\\( d/2 \\\\) - times positive and \\\\( d/2 \\\\) - times negative. Further, in the condensation regime \\\\( \\\\alpha \\\\in (\\\\alpha_{\\\\text{cond}}, \\\\alpha_{\\\\text{sat}}) \\\\), many quantities of interest was established for random regular k - nae - sat with large enough k, matching the statistical physics prediction. Namely, the number of solutions at exponential scale (free energy) [ SSZ22 ], the concentration of the overlap [ NSS20 , NSS21 ], and the local weak limit [ SS23 ] were established. Establishing the same quantities for other models in the condensation regime is left open. The closest result to ours in the literature is by Ayre, Coja-Oghlan, and Greenhill [ ACOG22 ], where they lower bound the chromatic number (or equivalently, upper bound the colorability threshold) of the random regular graph of any degree, which is conjectured to be tight. [ ACOG22 ] also considers the sparse Erdos Renyi graph, which is more complicated since the conjectured chromatic number is defined in terms of a distributional (rather than real-valued) optimization due to the randomness of the local neighborhoods. In this work, we do not consider Erdos Renyi type problems, but we additionally address the question of the uniqueness of the bp fixed point for any \\\\( k \\\\geq 3 \\\\) (unique solution to the equation ( 1.1 )). As in [ ACOG22 ], we use an interpolation bound, which gives an upper bound of the satisfiability threshold also for the (nonregular) random k - nae - sat model. It would be interesting to address the uniqueness of the bp fixed point for random k - nae - sat and random k - sat for small \\\\( k \\\\geq 3 \\\\). We refer to [ ST03 , MRSY19 , YP22 , GP23 ] which addresses the uniqueness of bp fixed point for various models. 1.2 Proof methods We aim to rigorously establish the upper bound the satisfiability threshold predicted by the so-called \u20181RSB cavity method\u2019 from statistical physics [ DRZ08 ]. To do so, instead of using moment methods, we use a technique called \u2018interpolation method\u2019 from the theory of spin glasses developed by [ FL03 , Gue03 , PT04 ]. The interpolation method has been successful in upperbounding the satisfiability threshold for random k - sat [ DSS15 ] for large k, the free energy for random regular k - nae - sat [ SSZ16 ], and the colorability threshold for random graphs [ ACOG22 ]. We first introduce the notations and mathematical framework that we use throughout the paper. For both the d - regular k - uniform hypergraphs and the k - nae - sat formula, we can represent them as (labelled) \\\\( (d,k) \\\\) - regular bipartite graph. Let \\\\( V = \\\\{ v_1 , \\\\ldots , v_n \\\\} \\\\) be the set of variables or nodes and \\\\( F = \\\\{ a_1 , \\\\ldots , a_m \\\\} \\\\) be the set of clauses or hyperedges. An edge is formed if the variable or node \\\\( v_i \\\\) is included in the clause or hyperedge \\\\( a_j \\\\). For an edge \\\\( e \\\\), we denote \\\\( v(e) \\\\) (resp. \\\\( a(e) \\\\)) by the variable (resp. clause) adjacent to it. Denote \\\\( G = (V,F,E) \\\\) by the resulting bipartite graph. We denote the neighborhood of \\\\( v \\\\in V \\\\) (resp. \\\\( a \\\\in F \\\\)) by \\\\( \\\\delta v := \\\\{ a \\\\in F : (av) \\\\in E \\\\} \\\\) (resp. \\\\( \\\\delta a := \\\\{ v \\\\in V : (av) \\\\in E \\\\} \\\\)). Throughout, we denote \\\\( \\\\alpha \\\\equiv \\\\frac{m}{n} = \\\\frac{d}{k} \\\\). For the nae - sat formula, there is an extra label for each edge \\\\( e \\\\in E \\\\), namely the literal \\\\( L_e \\\\in \\\\{ 0,1 \\\\} \\\\), which specifies how the variable \\\\( v(e) \\\\) participates in the clause \\\\( a(e) \\\\). Then, the labelled graph \\\\( \\\\mathcal{G} = (V,F,E,L) \\\\equiv (V,F,E,(L_e)_{e \\\\in E}) \\\\) represents a nae - sat instance. Definition 1.3. Given a nae - sat instance \\\\( \\\\mathcal{G} = (V,F,E,L) \\\\), \\\\( x \\\\in \\\\{ 0,1 \\\\}^V \\\\) is a (nae - sat) solution if\\n\\n\\\\[\\n\\\\prod_{a \\\\in F} \\\\varphi((x_{v(e)} \\\\oplus L_e)_{e \\\\in \\\\delta a}) = 1 ,\\n\\\\]\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"expanding box like model valid for both radially and temporally varying spherical flows. The generalisation of the expanding box model to radially varying flows was done in Tenerani & Velli (2017). The generalisation to background flows which are also time dependant, motivated by the stellar formation problem, results in a model close to the accelerated expanding box (although our treatment of pressure will be closer to the distorted shearing box models of Ogilvie & Latter (2013); Ogilvie & Barker (2014)). We shall focus, in this paper, on the hydrodynamic case as it posses a number of important features that are worth understanding before generalising to MHD.\\n\\nIn Section 2 we present the derivation of our local model. Sections 2.4 and 2.5 derives symmetries and conservation laws of the local model. Section 3 presents some nonlinear solutions to the local model - and discuss how these relate to the global problem. In Section 4 we derive the linear theory of our local model. We discuss possible extension of our model in Section 5. We present our conclusions in Section 6 and additional mathematical details (including alternative formulations which maybe more convenient for implementation in hydrocodes) are presented in the appendices.\\n\\n2 DERIVATION\\n\\n2.1 Global geometry\\n\\nTo derive a local model for spherical collapse/expansion consider a local neighbourhood of a point, \\\\( o \\\\), located on the equator of a sphere of radius \\\\( R \\\\). The line element of the usual spherical polar coordinate system is\\n\\n\\\\[\\nd s^2 = dR^2 + R^2 (d\\\\theta^2 + \\\\sin^2 \\\\theta \\\\, d\\\\phi^2). \\\\tag{1}\\n\\\\]\\n\\nWe are interested in describing the local dynamics near to \\\\( p \\\\) occurring on a horizontal lengthscale \\\\( L_H \\\\ll R \\\\) (See Figure 1, which show the relationship between the global and local geometries). Without loss of generality, we can locate our local model on the equator of the sphere (\\\\( \\\\theta = \\\\pi/2 \\\\)) meaning we can approximate the line element by\\n\\n\\\\[\\nd s^2 = dR^2 + R^2 (d\\\\theta^2 + d\\\\phi^2) + O((L_H/R)^2 d\\\\phi^2), \\\\tag{2}\\n\\\\]\\n\\nwhich results in metric tensor components,\\n\\n\\\\[\\ng_{RR} = 1, \\\\quad g_{\\\\theta\\\\theta} = g_{\\\\phi\\\\phi} = R^2, \\\\tag{3}\\n\\\\]\\n\\nand inverse metric tensor components\\n\\n\\\\[\\ng^{RR} = 1, \\\\quad g^{\\\\theta\\\\theta} = g^{\\\\phi\\\\phi} = R^{-2}, \\\\tag{4}\\n\\\\]\\n\\nwith all other components zero. The Christoffel Symbols components, for this coordinate system, are\\n\\n\\\\[\\n\\\\Gamma^R_{\\\\theta\\\\theta} = \\\\Gamma^R_{\\\\phi\\\\phi} = -R, \\\\quad \\\\Gamma^\\\\theta_{\\\\theta R} = \\\\Gamma^\\\\phi_{\\\\phi R} = \\\\Gamma^\\\\phi_{R\\\\phi} = R^{-1}, \\\\tag{5}\\n\\\\]\\n\\nwith all others vanishing. The fluid equations in this coordinate system are\\n\\n\\\\[\\nDu^\\\\theta + \\\\frac{2}{R} u^R u^\\\\theta = -R^{-2} \\\\left( \\\\partial_\\\\theta \\\\Phi + \\\\frac{1}{\\\\rho} \\\\partial_\\\\theta p \\\\right), \\\\tag{6}\\n\\\\]\\n\\n\\\\[\\nDu^\\\\phi + \\\\frac{2}{R} u^R u^\\\\phi = -R^{-2} \\\\left( \\\\partial_\\\\phi \\\\Phi + \\\\frac{1}{\\\\rho} \\\\partial_\\\\phi p \\\\right), \\\\tag{7}\\n\\\\]\\n\\n\\\\[\\nDu^R - Ru^\\\\phi u^\\\\phi - Ru^\\\\theta u^\\\\theta = - \\\\left( \\\\partial_R \\\\Phi + \\\\frac{1}{\\\\rho} \\\\partial_R p \\\\right), \\\\tag{8}\\n\\\\]\\n\\n\\\\[\\nD\\\\rho = -p R^{-2} \\\\partial_i (R^2 u^i), \\\\tag{9}\\n\\\\]\\n\\nwhere the Lagrangian derivative is\\n\\n\\\\[\\nD = \\\\partial_t + u^i \\\\partial_i. \\\\tag{10}\\n\\\\]\\n\\nNote, we have listed the \\\\( R \\\\) component of the momentum equation last as it will become the \\\\( z \\\\) momentum equation in the local coordinate system. To close this system of equations we must supplement them with an equation of state determining \\\\( p \\\\), which we assume is barotropic,\\n\\n\\\\[\\np = p(\\\\rho). \\\\tag{11}\\n\\\\]\\n\\n2.2 Spherical Collapse/Expansion\\n\\nFor the background fluid flow we wish to consider a spherically symmetric expanding/contracting fluid in a (potentially time dependant) central potential \\\\( \\\\Phi = \\\\Phi(t, R) \\\\). Consider a spherically symmetric fluid in this potential with density \\\\( \\\\rho_0 = \\\\rho_0(R, t) \\\\) and purely radial velocity field \\\\( U^i = U(R, t) \\\\delta^i_R \\\\). The density of the fluid then evolves according to the continuity equation,\\n\\n\\\\[\\nD_0 \\\\rho_0 = -p_0 R^{-2} \\\\partial_R (R^2 U), \\\\tag{12}\\n\\\\]\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"expanding box like model valid for both radially and temporally varying spherical flows. The generalisation of the expanding box model to radially varying flows was done in Tenerani & Velli ( 2017 ). The generalisation to background flows which are also time dependant, motivated by the stellar formation problem, results in a model close to the accelerated expanding box (although our treatment of pressure will be closer to the distorted shearing box models of Ogilvie & Latter ( 2013 ); Ogilvie & Barker ( 2014 )). We shall focus, in this paper, on the hydrodynamic case as it posses a number of important features that are worth understanding before generalising to MHD. \\n\\nIn Section 2 we present the derivation of our local model. Sections 2.4 and 2.5 derives symmetries and conservation laws of the local model. Section 3 presents some nonlinear solutions to the local model and discuss how these relate to the global problem. In Section 4 we derive the linear theory of our local model. We discuss possible extension of our model in Section 5 . We present our conclusions in Section 6 and additional mathematical details (including alternative formulations which maybe more convenient for implementation in hydrocodes) are presented in the appendices. \\n\\n2 DERIVATION\\n\\n2.1 Global geometry\\n\\nTo derive a local model for spherical collapse/expansion consider a local neighbourhood of a point, \\\\( o \\\\), located on the equator of a sphere of radius \\\\( R \\\\). The line element of the usual spherical polar coordinate system is\\n\\n\\\\[\\nd s^2 = dR^2 + R^2 (d\\\\theta^2 + \\\\sin^2 \\\\theta \\\\, d\\\\phi^2). \\\\tag{1}\\n\\\\]\\n\\nWe are interested in describing the local dynamics near to \\\\( p \\\\) occurring on a horizontal lengthscale \\\\( L_H \\\\ll R \\\\) (See Figure 1 , which show the relationship between the global and local geometries). Without loss of generality, we can locate our local model on the equator of the sphere ( \\\\( \\\\theta = \\\\pi/2 \\\\) ) meaning we can approximate the line element by\\n\\n\\\\[\\nd s^2 = dR^2 + R^2 (d\\\\theta^2 + d\\\\phi^2) + O((L_H/R)^2 d\\\\phi^2), \\\\tag{2}\\n\\\\]\\n\\nwhich results in metric tensor components, \\n\\n\\\\[\\ng_{RR} = 1, \\\\quad g_{\\\\theta\\\\theta} = g_{\\\\phi\\\\phi} = R^2, \\\\tag{3}\\n\\\\]\\n\\nand inverse metric tensor components\\n\\n\\\\[\\ng^{RR} = 1, \\\\quad g^{\\\\theta\\\\theta} = g^{\\\\phi\\\\phi} = R^{-2}, \\\\tag{4}\\n\\\\]\\n\\nwith all other components zero. The Christoffel Symbols components, for this coordinate system, are\\n\\n\\\\[\\n\\\\Gamma^R_{\\\\theta\\\\theta} = \\\\Gamma^R_{\\\\phi\\\\phi} = -R, \\\\quad \\\\Gamma^\\\\theta_{\\\\theta R} = \\\\Gamma^\\\\phi_{\\\\phi R} = \\\\Gamma^\\\\phi_{R\\\\phi} = R^{-1}, \\\\tag{5}\\n\\\\]\\n\\nwith all others vanishing. The fluid equations in this coordinate system are\\n\\n\\\\[\\nDu^\\\\theta + \\\\frac{2}{R} u^R u^\\\\theta = -R^{-2} \\\\left( \\\\partial_\\\\theta \\\\Phi + \\\\frac{1}{\\\\rho} \\\\partial_\\\\theta p \\\\right), \\\\tag{6}\\n\\\\]\\n\\n\\\\[\\nDu^\\\\phi + \\\\frac{2}{R} u^R u^\\\\phi = -R^{-2} \\\\left( \\\\partial_\\\\phi \\\\Phi + \\\\frac{1}{\\\\rho} \\\\partial_\\\\phi p \\\\right), \\\\tag{7}\\n\\\\]\\n\\n\\\\[\\nDu^R - Ru^\\\\phi u^\\\\phi - Ru^\\\\theta u^\\\\theta = - \\\\left( \\\\partial_R \\\\Phi + \\\\frac{1}{\\\\rho} \\\\partial_R p \\\\right), \\\\tag{8}\\n\\\\]\\n\\n\\\\[\\nD\\\\rho = -p R^{-2} \\\\partial_i (R^2 u^i), \\\\tag{9}\\n\\\\]\\n\\nwhere the Lagrangian derivative is\\n\\n\\\\[\\nD = \\\\partial_t + u^i \\\\partial_i. \\\\tag{10}\\n\\\\]\\n\\nNote, we have listed the \\\\( R \\\\) component of the momentum equation last as it will become the \\\\( z \\\\) momentum equation in the local coordinate system. To close this system of equations we must supplement them with an equation of state determining \\\\( p \\\\), which we assume is barotropic, \\n\\n\\\\[\\np = p(\\\\rho). \\\\tag{11}\\n\\\\]\\n\\n2.2 Spherical Collapse/Expansion\\n\\nFor the background fluid flow we wish to consider a spherically symmetric expanding/contracting fluid in a (potentially time dependant) central potential \\\\( \\\\Phi = \\\\Phi(t, R) \\\\). Consider a spherically symmetric fluid in this potential with density \\\\( \\\\rho_0 = \\\\rho_0(R, t) \\\\) and purely radial velocity field \\\\( U^i = U(R, t) \\\\delta^i_R \\\\). The density of the fluid then evolves according to the continuity equation,\\n\\n\\\\[\\nD_0 \\\\rho_0 = -p_0 R^{-2} \\\\partial_R (R^2 U), \\\\tag{12}\\n\\\\]\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We design a unified pipeline implemented by Target-Aware Attention Framework (TAF), which is different from the separate pipeline used by existing methods. In order to evaluate the effectiveness of the unified pipeline, we built a separate pipeline model for comparison. The TAF-separate model adopts the same architecture as TAF. It implements self-attention for both two heads of all Target-Aware Attention modules in the front, while the last Target-Aware Attention module only implements cross-attention for both two heads. We use the Synthetic dataset to conduct ablation study, the results are shown in Table 4. By comparing the TAF-separate model and the TAF model, we can see that the TAF with unified pipeline performs better than the separate pipeline. After introducing the multi-scale projection mechanism to achieve multi-scale attention, the MSTAF-separate and MSTAF models both showed improvements in localization performance. The MSTAF with unified pipeline design and multi-scale attention mechanism presents the best localization performance on all three subsets.\\n\\nIn [16], they use the Scale set to analyze the model\u2019s robustness against scale transformation. To further verify the effectiveness of the multi-scale attention mechanism. We also use Scale set to evaluate our models and the result as shown in Tab 5. In the Scale set, the spliced region is only processed with scale transformation of different degrees for ablation in [16]. It contains 9000 testing pairs and is equally divided into Difficult, Normal, Easy subsets. In the Difficult subset and Normal subset, there are more samples with larger scale degrees. On the contrary, in the Easy subset, the size of the spliced region between the two images tends to be more consistent. We can see that with the help of multi-scale attention mechanism, MSTAF achieves much better localization performance than TAF on the Normal and difficult subset. It demonstrates that MSTAF has an advantage in dealing with various scale transformation samples. After introducing the multi-scale attention mechanism, MSTAF is more robust against scale transformation. The visual comparison can refer to Fig. 6 and Fig. 7.\\n\\n5 CONCLUSION\\n\\nIn this work, we propose a Multi-scale Target-Aware Framework to simplify the pipeline of existing methods. It adopts self-attention for feature extraction and cross-attention for correlation matching simultaneously. This unified design enables feature extraction and correlation matching to mutually promote each other, thereby enhancing the matching performance of the model. We further design a multi-scale attention mechanism to model the matching between image patches of different scales, which further improves the robustness against scale transformation. Experiment results demonstrate that our model is robust against scaling and outperforms state-of-the-art methods.\\n\\nACKNOWLEDGMENTS\\n\\nThis work was supported in part by the Natural Science Foundation of China under Grant 62001304; in part by the Guangdong Basic and Applied Basic Research Foundation under Grant 2022A1515010645; in part by the Foundation for Science and Technology Innovation of Shenzhen under Grant RCBS20210609103708014 and the Key\\n\\n| Model       | Difficult | Normal | Easy  |\\n|-------------|-----------|--------|-------|\\n|             | IoU       | MCC    | NMM   | IoU       | MCC    | NMM   | IoU       | MCC    | NMM   |\\n| TAF-separate| 0.6239    | 0.7167 | 0.2963| 0.8738    | 0.9143 | 0.7744| 0.9490    | 0.9604 | 0.9164|\\n| MSTAF-separate| 0.7712    | 0.8432 | 0.5670| 0.9210    | 0.9486 | 0.8583| 0.9693    | 0.9764 | 0.9488|\\n| TAF         | 0.8001    | 0.8586 | 0.6312| 0.9379    | 0.9605 | 0.8937| 0.9753    | 0.9811 | 0.9617|\\n| MSTAF       | 0.8394    | 0.8918 | 0.7064| 0.9510    | 0.9700 | 0.9151| 0.9788    | 0.9838 | 0.9646|\\n\\nTable 4: Ablation study on the Synthetic set\\n\\n| Model       | Difficult | Normal | Easy  |\\n|-------------|-----------|--------|-------|\\n|             | IoU       | MCC    | NMM   | IoU       | MCC    | NMM   | IoU       | MCC    | NMM   |\\n| TAF         | 0.7009    | 0.7644 | 0.4400| 0.8789    | 0.9160 | 0.7807| 0.9704    | 0.9769 | 0.9550|\\n| MSTAF       | 0.7427    | 0.8022 | 0.5206| 0.9105    | 0.9410 | 0.8406| 0.9752    | 0.9808 | 0.9602|\\n\\nTable 5: Ablation study on the Scale set\\n\\nFigure 8: Visualization of attention maps. The blue point represents the token we selected to present attention maps. Stage i,j represents attention maps from the j Target-Aware Attention modules in stage i.\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We design a unified pipeline implemented by Target-Aware Attention Framework (TAF), which is different from the separate pipeline used by existing methods. In order to evaluate the effectiveness of the unified pipeline, we built a separate pipeline model for comparison. The TAF-separate model adopts the same architecture as TAF. It implements self-attention for both two heads of all Target-Aware Attention modules in the front, while the last Target-Aware Attention module only implements cross-attention for both two heads. We use the Synthetic dataset to conduct ablation study, the results are shown in Table 4. By comparing the TAF-separate model and the TAF model, we can see that the TAF with unified pipeline performs better than the separate pipeline. After introducing the multi-scale projection mechanism to achieve multi-scale attention, the MSTAF-separate and MSTAF models both showed improvements in localization performance. The MSTAF with unified pipeline design and multi-scale attention mechanism presents the best localization performance on all three subsets. In the Difficult subset and Normal subset, there are more samples with larger scale degrees. On the contrary, in the Easy subset, the size of the spliced region between the two images tends to be more consistent. We can see that with the help of multi-scale attention mechanism, MSTAF achieves much better localization performance than TAF on the Normal and difficult subset. It demonstrates that MSTAF has an advantage in dealing with various scale transformation samples. After introducing the multi-scale attention mechanism, MSTAF is more robust against scale transformation. The visual comparison can refer to Fig. 6 and Fig. 7. \\n\\n5 CONCLUSION\\n\\nIn this work, we propose a Multi-scale Target-Aware Framework to simplify the pipeline of existing methods. It adopts self-attention for feature extraction and cross-attention for correlation matching simultaneously. This unified design enables feature extraction and correlation matching to mutually promote each other, thereby enhancing the matching performance of the model. We further design a multi-scale attention mechanism to model the matching between image patches of different scales, which further improves the robustness against scale transformation. Experiment results demonstrate that our model is robust against scaling and outperforms state-of-the-art methods. \\n\\nACKNOWLEDGMENTS\\n\\nThis work was supported in part by the Natural Science Foundation of China under Grant 62001304; in part by the Guangdong Basic and Applied Basic Research Foundation under Grant 2022A1515010645; in part by the Foundation for Science and Technology Innovation of Shenzhen under Grant RCBS20210609103708014 and the Key\\n\\n| Model       | Difficult | Normal | Easy  |\\n|-------------|-----------|--------|-------|\\n|             | IoU       | MCC    | NMM   | IoU       | MCC    | NMM   | IoU       | MCC    | NMM   |\\n| TAF-separate| 0.6239    | 0.7167 | 0.2963| 0.8738    | 0.9143 | 0.7744| 0.9490    | 0.9604 | 0.9164|\\n| MSTAF-separate| 0.7712    | 0.8432 | 0.5670| 0.9210    | 0.9486 | 0.8583| 0.9693    | 0.9764 | 0.9488|\\n| TAF         | 0.8001    | 0.8586 | 0.6312| 0.9379    | 0.9605 | 0.8937| 0.9753    | 0.9811 | 0.9617|\\n| MSTAF       | 0.8394    | 0.8918 | 0.7064| 0.9510    | 0.9700 | 0.9151| 0.9788    | 0.9838 | 0.9646|\\n\\nTable 4: Ablation study on the Synthetic set\\n\\n| Model       | Difficult | Normal | Easy  |\\n|-------------|-----------|--------|-------|\\n|             | IoU       | MCC    | NMM   | IoU       | MCC    | NMM   | IoU       | MCC    | NMM   |\\n| TAF         | 0.7009    | 0.7644 | 0.4400| 0.8789    | 0.9160 | 0.7807| 0.9704    | 0.9769 | 0.9550|\\n| MSTAF       | 0.7427    | 0.8022 | 0.5206| 0.9105    | 0.9410 | 0.8406| 0.9752    | 0.9808 | 0.9602|\\n\\nTable 5: Ablation study on the Scale set\\n\\nFigure 8: Visualization of attention maps. The blue point represents the token we selected to present attention maps. Stage i,j represents attention maps from the j Target-Aware Attention modules in stage i.\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the geometry of these regions is identical to the Einstein-Rosen bridge in a two-sided BTZ geometry with the same mass as the corresponding operator. We will take the interpretation that the details of what appears outside the domain of dependence of $\\\\mathcal{W}$ is part of the definition of the operator associated to that region. So in sum, the action of the Lorentzian cap is\\n\\n$$I_{\\\\text{cap}} = \\\\frac{1}{4\\\\pi G_N} V_{\\\\mathcal{W}} + \\\\sum_{i=\\\\text{BH}} I_i = \\\\frac{\\\\pi}{4G_N} \\\\sum_i \\\\left( \\\\text{Re}(\\\\eta_i) - \\\\frac{1}{3} \\\\right) + \\\\sum_{i=\\\\text{BH}} I_i. \\\\quad (6.8)$$\\n\\nHence we see that it is the sum of three contributions that each depend only on one of the three operators. Thus, we can absorb the corresponding phase generated by the action of the Lorentzian geometry into the definition of the operators, and we will arrive at a real result for the three-point function.\\n\\nHence we arrive at the total action for the single-sided geometry,\\n\\n$$I_{\\\\text{single-sided}} = \\\\frac{1}{2} I_{\\\\text{wormhole}} + i \\\\sum_i f(i). \\\\quad (6.9)$$\\n\\nAgain, it is a nontrivial result that the imaginary term is given by the sum shown above since this allows us to absorb the corresponding phases into the definition of the operators. After eliminating these phases, we recover the three-point function from the single-sided bulk geometry, i.e.,\\n\\n$$e^{-I} \\\\approx G_L(z_1, z_2, z_3), \\\\quad (6.10)$$\\n\\nwhere $G_L(z_1, z_2, z_3)$ is the semiclassical Liouville three-point correlator in [17].\\n\\n7 Discussion\\n\\nIn this paper we discussed three-dimensional asymptotically AdS$_3$ geometries that are sourced by the insertion of boundary operators whose scaling dimensions is heavy as the central charge of the holographic CFT$_2$. The presence of any such operators deforms the AdS geometry by inducing a non vanishing expectation value for the holographic stress tensor, close to the boundary. This is true perturbatively in general dimensions, but in three-dimensions there is an exact solution, due to Ba\u00f1ados [4], that describes such deformation. However, this metric does not describe the full bulk spacetime. When only two black hole operators are inserted, we showed that the full geometry is simply an infinite covering of the Euclidean BTZ black hole [1], but when three or more operators are inserted, we found that the completion of the Ba\u00f1ados metric into the bulk is a wormhole geometry involving multiple asymptotic boundaries. To understand this rather non trivial fact we rephrased the construction of the bulk geometry as a quotient of AdS$_3$ realized by domes and doors. The dome construction is a well know characterization of hyperbolic geometries with an asymptotically AdS$_3$ metric, and more familiar from the study of black hole thermodynamics, see e.g., [38], but the addition of the doors is new as far as we can tell.\\n\\nAs in the description of a Euclidean two-point function geometry in section 2, i.e., as empty AdS$_3$ with identifications, the doors are needed to describe the insertion of boundary\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the geometry of these regions is identical to the Einstein-Rosen bridge in a two-sided BTZ geometry with the same mass as the corresponding operator. We will take the interpretation that the details of what appears outside the domain of dependence of $\\\\mathcal{W}$ is part of the definition of the operator associated to that region. So in sum, the action of the Lorentzian cap is\\n\\n$$I_{\\\\text{cap}} = \\\\frac{1}{4\\\\pi G_N} V_{\\\\mathcal{W}} + \\\\sum_{i=\\\\text{BH}} I_i = \\\\frac{\\\\pi}{4G_N} \\\\sum_i \\\\left( \\\\text{Re}(\\\\eta_i) - \\\\frac{1}{3} \\\\right) + \\\\sum_{i=\\\\text{BH}} I_i. \\\\quad (6.8)$$\\n\\nHence we see that it is the sum of three contributions that each depend only on one of the three operators. Thus, we can absorb the corresponding phase generated by the action of the Lorentzian geometry into the definition of the operators, and we will arrive at a real result for the three-point function. Hence we arrive at the total action for the single-sided geometry, \\\\textit{i.e.},\\n\\n$$I_{\\\\text{single-sided}} = \\\\frac{1}{2} I_{\\\\text{wormhole}} + i \\\\sum_i f(i). \\\\quad (6.9)$$\\n\\nAgain, it is a nontrivial result that the imaginary term is given by the sum shown above since this allows us to absorb the corresponding phases into the definition of the operators. After eliminating these phases, we recover the three-point function from the single-sided bulk geometry, \\\\textit{i.e.},\\n\\n$$e^{-I} \\\\approx G_L(z_1, z_2, z_3), \\\\quad (6.10)$$\\n\\nwhere $G_L(z_1, z_2, z_3)$ is the semiclassical Liouville three-point correlator in [17].\\n\\n7 Discussion\\n\\nIn this paper we discussed three-dimensional asymptotically AdS$_3$ geometries that are sourced by the insertion of boundary operators whose scaling dimensions is heavy as the central charge of the holographic CFT 2 . The presence of any such operators deforms the AdS geometry by inducing a non vanishing expectation value for the holographic stress tensor, close to the boundary. This is true perturbatively in general dimensions, but in three-dimensions there is an exact solution, due to Ba\u02dcnados [ 4 ], that describes such deformation. However, this metric does not describe the full bulk spacetime. When only two black hole operators are inserted, we showed that the full geometry is simply an infinite covering of the Euclidean BTZ black hole [ 1 ], but when three or more operators are inserted, we found that the completion of the Ba\u02dcnados metric into the bulk is a wormhole geometry involving multiple asymptotic boundaries. To understand this rather non trivial fact we rephrased the construction of the bulk geometry as a quotient of AdS$_3$ realized by domes and doors. The dome construction is a well know characterization of hyperbolic geometries with an asymptotically AdS$_3$ metric, and more familiar from the study of black hole thermodynamics, see \\\\textit{e.g.}, [ 38 ], but the addition of the doors is new as far as we can tell. As in the description of a Euclidean two-point function geometry in section 2 , i.e., as empty AdS$_3$ with identifications, the doors are needed to describe the insertion of boundary\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.1.3. The case of a general periodic force. Finally, we remark that in the general case of a \\\\( \\\\theta \\\\)-periodic force of the form\\n\\n\\\\[\\n\\\\mathcal{F}(t/\\\\theta) = \\\\sum_{\\\\ell=1}^{+\\\\infty} F_\\\\ell \\\\cos(\\\\omega(\\\\ell)t), \\\\quad \\\\text{where } \\\\omega(\\\\ell) := \\\\frac{2\\\\pi \\\\ell}{\\\\theta}\\n\\\\]\\n\\n(3.20)\\n\\nwhose real valued Fourier coefficients satisfy \\\\( \\\\sum_{\\\\ell=1}^{+\\\\infty} (\\\\ell F_\\\\ell)^2 < +\\\\infty \\\\), the work performed by the force can be determined from the formula:\\n\\n\\\\[\\nW(n) = \\\\sum_{\\\\ell=1}^{+\\\\infty} \\\\left( \\\\omega(\\\\ell) F_\\\\ell \\\\right)^2 \\\\frac{N(\\\\omega(\\\\ell), n)}{D(\\\\omega(\\\\ell), n)}.\\n\\\\]\\n\\n(3.21)\\n\\nTherefore its behavior, as \\\\( n \\\\) gets large, can be determined from the term by term analysis of the series appearing on the right hand side of (3.21).\\n\\n3.2. Energy. As in Section 3.1 we assume that the periodic force \\\\( \\\\mathcal{F}(t) \\\\) is given by (3.3). The time average of the expectation of the total energy of the chain \\\\( E(\\\\omega, n) \\\\) breaks up into the sum of thermal component \\\\( E_{\\\\text{th}}(\\\\omega, n) = \\\\sum_{x \\\\in \\\\mathbb{Z}_n} \\\\langle \\\\langle e_{x}^{\\\\text{th}} \\\\rangle \\\\rangle \\\\) and the mechanical one \\\\( E_{\\\\text{mech}}(\\\\omega, n) = \\\\sum_{x \\\\in \\\\mathbb{Z}_n} \\\\langle \\\\langle e_{x}^{\\\\text{mech}} \\\\rangle \\\\rangle \\\\), with \\\\( e_{x}^{\\\\text{th}} \\\\) and \\\\( e_{x}^{\\\\text{mech}} \\\\) defined in (2.17) and (2.16), respectively.\\n\\nConsidering the behavior of the thermal energy functional, defined in (2.15), it has been shown in [0], that in the case \\\\( \\\\omega_0 = 0 \\\\) and \\\\( \\\\gamma_- = \\\\gamma_+ \\\\), we have \\\\( \\\\langle \\\\langle e_{x}^{\\\\text{th}} \\\\rangle \\\\rangle = \\\\frac{1}{2}(T_- + T_+) \\\\) for all \\\\( x = 1, \\\\ldots, n - 1 \\\\). If \\\\( \\\\omega_0 > 0 \\\\) and \\\\( \\\\gamma_- = \\\\gamma_+ \\\\), then [0, formulas (38) and (42)] give\\n\\n\\\\[\\n\\\\langle \\\\langle e_{x}^{\\\\text{th}} \\\\rangle \\\\rangle = \\\\frac{1}{2}(T_- + T_+)(1 + o_x), \\\\quad \\\\text{where } |o_x| \\\\leq \\\\frac{C}{g^{x\\\\wedge(n+1-x)}}\\n\\\\]\\n\\nfor some constants \\\\( C > 0, g > 1 \\\\) independent of \\\\( n \\\\). As a result we have \\\\( E_{\\\\text{th}}(\\\\omega, n) \\\\sim n \\\\), as \\\\( n \\\\to +\\\\infty \\\\).\\n\\n3.2.1. Formula for the total mechanical energy functional for a single mode oscillating force. In what follows we consider the behavior of the mechanical component of the energy. Again, assume that the force is given by (3.3). It turns out, see Section C of the Appendix, that the time average over the period of the microscopic mechanical energy density equals\\n\\n\\\\[\\n\\\\langle \\\\langle e_{x}^{\\\\text{mech}} \\\\rangle \\\\rangle = \\\\frac{F^2}{2} \\\\cdot \\\\frac{M_x(\\\\omega, n)}{D(\\\\omega, n)},\\n\\\\]\\n\\n(3.22)\\n\\nwhere \\\\( D(\\\\omega, n) \\\\) is given by (3.5) and\\n\\n\\\\[\\nM_x(\\\\omega, n) = G_x^1(\\\\omega, n)^2(\\\\omega^2 + \\\\omega_0^2) + (\\\\nabla^* G_x^1)(\\\\omega, n)^2 + (2\\\\omega \\\\gamma_-)^2 \\\\left[ G_x(\\\\omega, n)^2 + (\\\\nabla^* G_x)(\\\\omega, n)^2 \\\\right],\\n\\\\]\\n\\n12\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.1.3 The case of a general periodic force. Finally, we remark that in the general case of a \\\\( \\\\theta \\\\)-periodic force of the form\\n\\n\\\\[\\n\\\\mathcal{F}(t/\\\\theta) = \\\\sum_{\\\\ell=1}^{+\\\\infty} F_\\\\ell \\\\cos(\\\\omega(\\\\ell)t), \\\\quad \\\\text{where } \\\\omega(\\\\ell) := \\\\frac{2\\\\pi \\\\ell}{\\\\theta}\\n\\\\]\\n\\nwhose real valued Fourier coefficients satisfy \\\\( \\\\sum_{\\\\ell=1}^{+\\\\infty} (\\\\ell F_\\\\ell)^2 < +\\\\infty \\\\), the work performed by the force can be determined from the formula: \\n\\n\\\\[\\nW(n) = \\\\sum_{\\\\ell=1}^{+\\\\infty} \\\\left( \\\\omega(\\\\ell) F_\\\\ell \\\\right)^2 \\\\frac{N(\\\\omega(\\\\ell), n)}{D(\\\\omega(\\\\ell), n)}.\\n\\\\]\\n\\nTherefore its behavior, as \\\\( n \\\\) gets large, can be determined from the term by term analysis of the series appearing on the right hand side of ( 3.21 ).\\n\\n3.2. Energy. As in Section 3.1 we assume that the periodic force \\\\( \\\\mathcal{F}(t) \\\\) is given by ( 3.3 ). The time average of the expectation of the total energy energy of the chain \\\\( E(\\\\omega, n) \\\\) breaks up into the sum of thermal component \\\\( E_{\\\\text{th}}(\\\\omega, n) = \\\\sum_{x \\\\in \\\\mathbb{Z}} \\\\langle \\\\langle e_{x}^{\\\\text{th}} \\\\rangle \\\\rangle \\\\) and the mechanical one \\\\( E_{\\\\text{mech}}(\\\\omega, n) = \\\\sum_{x \\\\in \\\\mathbb{Z}} \\\\langle \\\\langle e_{x}^{\\\\text{mech}} \\\\rangle \\\\rangle \\\\), with \\\\( e_{x}^{\\\\text{th}} \\\\) and \\\\( e_{x}^{\\\\text{mech}} \\\\) defined in ( 2.17 ) and ( 2.16 ), respectively. \\n\\nConsidering the behavior of the thermal energy functional, defined in ( 2.15 ), it has been shown in [ 0 ], that in the case \\\\( \\\\omega_0 = 0 \\\\) and \\\\( \\\\gamma_- = \\\\gamma_+ \\\\), we have \\\\( \\\\langle \\\\langle e_{x}^{\\\\text{th}} \\\\rangle \\\\rangle = \\\\frac{1}{2} (T_- + T_+) \\\\) for all \\\\( x = 1, \\\\ldots, n - 1 \\\\). If \\\\( \\\\omega_0 > 0 \\\\) and \\\\( \\\\gamma_- = \\\\gamma_+ \\\\), then [ 0 , formulas (38) and (42)] give\\n\\n\\\\[\\n\\\\langle \\\\langle e_{x}^{\\\\text{th}} \\\\rangle \\\\rangle = \\\\frac{1}{2} (T_- + T_+) (1 + o_x), \\\\quad \\\\text{where } |o_x| \\\\leq \\\\frac{C}{g^{x \\\\wedge (n+1-x)}}\\n\\\\]\\n\\nfor some constants \\\\( C > 0, g > 1 \\\\) independent of \\\\( n \\\\). As a result we have \\\\( E_{\\\\text{th}}(\\\\omega, n) \\\\sim n \\\\), as \\\\( n \\\\to +\\\\infty \\\\).\\n\\n3.2.1. e. In what follows we consider the behavior of the mechanical component of the energy. Again, assume that the force is given by ( 3.3 ). It turns out, see Section C of the Appendix, that the time average over the period of the microscopic mechanical energy density equals\\n\\n\\\\[\\n\\\\langle \\\\langle e_{x}^{\\\\text{mech}} \\\\rangle \\\\rangle = \\\\frac{F^2}{2} \\\\cdot \\\\frac{M_x(\\\\omega, n)}{D(\\\\omega, n)},\\n\\\\]\\n\\nwhere \\\\( D(\\\\omega, n) \\\\) is given by ( 3.5 ) and\\n\\n\\\\[\\nM_x(\\\\omega, n) = G_x^1(\\\\omega, n)^2(\\\\omega^2 + \\\\omega_0^2) + (\\\\nabla^* G_x^1)(\\\\omega, n)^2 + (2\\\\omega \\\\gamma_-)^2 \\\\left[ G_x(\\\\omega, n)^2 + (\\\\nabla^* G_x)(\\\\omega, n)^2 \\\\right],\\n\\\\]\\n\\n12\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.2 Study 1\\n\\nBased on the pilot, we decided to proceed by fixing the reported bugs and replacing the usability scale with potential mediators to better assess which users would prefer the different bots. We conducted this study with n=71 participants (see Section 3).\\n\\n4.2.1 Measures\\n\\nIn Study 1, we added 7-level bipolar rating scales for direct comparison (See Section 3.2). We were interested in how much control participants experienced; how natural they felt the chats to be; how well their intent was fulfilled in the chats; and how satisfied they were with them. See Table 2 for items and reliabilities of the constructs.\\n\\nTo account for potential mediation, we assessed interindividual difference variables, namely the most prominent personality measure, big 5, using a 15-item scale (BFI-2-XS) [39]. As insert expansions exist to smooth interaction by shaping expectations, we also assessed our participants need for cognitive closure (NFC-15) [40].\\n\\n| Scale               | Items                                                                 | Reliability                                                                 |\\n|---------------------|----------------------------------------------------------------------|----------------------------------------------------------------------------|\\n| Control             | enabled more personal direction?                                     | Study 1: $\\\\alpha = .77$, $\\\\lambda = .79$ (overall), for scenarios $\\\\lambda = .84$ |\\n|                     | offered you more autonomy?                                            |                                                                            |\\n|                     | let you steer the conversation more?                                  |                                                                            |\\n| Naturalness         | seemed more authentic?                                                | Study 1: $\\\\alpha = .93$, $\\\\lambda = .93$ (overall), for scenarios $\\\\lambda = .95$ |\\n|                     | had a more genuine feel?                                              |                                                                            |\\n|                     | was more natural?                                                     |                                                                            |\\n| Intent-Effectiveness| had more suitable responses?                                          | Study 1: $\\\\alpha = .93$, $\\\\lambda = .93$ (overall), for scenarios $\\\\lambda = .96$ |\\n|                     | lived up to your expectations better?                                 |                                                                            |\\n| Satisfaction        | was more to your liking?                                              | Study 1: $\\\\alpha = .95$, $\\\\lambda = .95$ (overall), for scenarios $\\\\lambda = .97$ |\\n|                     | was more satisfactory?                                                |                                                                            |\\n\\nTable 2: Direct comparison bipolar rating scales with Crohnbach\u2019s $\\\\alpha$ and Guttman\u2019s $\\\\lambda$.\\n\\n4.2.2 Procedure\\n\\nAfter a short demographic questionnaire, n=71 participants were given 3 scenarios (see Figure 2), rating each before turning to the next (see Figure 3.2). Having completed all scenarios and evaluations, feedback was elicited and interindividual variables were assessed.\\n\\n4.2.3 Feedback\\n\\nTo quantify the feedback, we repeated the procedure described for the pilot. Feedback on the study was 69.01% positive (neutral opinions tend to be rated as negative as well, e.g. \\\"I have no opinion about the study.\\\"). On the interface only 52.11% (negatives include no feedback at all, neutral statements such as \\\"It was fast and responsive, just feel like the loading is too big and the lettering also\\\"); but also some on bugs like \\\"It was fine, although sometimes my prompt would trigger a loading animation that the bot would never reply to, so I had to prompt again, which left the loading anim on the screen for one of the bots but not the other, not a big deal.\\\" or \\\"It was frustrating when it could not listen or answer all\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.2 Study 1\\n\\nBased on the pilot, we decided to proceed by fixing the reported bugs and replacing the usability scale with potential mediators to better assess which users would prefer the different bots. We conducted this study with n=71 participants (see Section 3).  \\n\\n4.2.1 Measures\\n\\nIn Study 1, we added 7-level bipolar rating scales for direct comparison (See Section 3.2). We were interested in how much control participants experienced; how natural they felt the chats to be; how well their intent was fulfilled in the chats; and how satisfied they were with them. See Table 2 for items and reliabilities of the constructs.  \\n\\nTo account for potential mediation, we assessed interindividual difference variables, namely the most prominent personality measure, big 5, using a 15-item scale (BFI-2-XS) [ 39 ]. As insert expansions exist to smooth interaction by shaping expectations, we also assessed our participants need for cognitive closure (NFC-15) [40].\\n\\n| Scale               | Items                                                                 | Reliability                                                                 |\\n|---------------------|-----------------------------------------------------------------------|-----------------------------------------------------------------------------|\\n| Control             | enabled more personal direction?                                      | Study 1: $\\\\alpha = .77$, $\\\\lambda = .79$ (overall), for scenarios $\\\\lambda = .84$ |\\n|                     | offered you more autonomy?                                            |                                                                             |\\n|                     | let you steer the conversation more?                                   |                                                                             |\\n| Naturalness         | seemed more authentic?                                                | Study 1: $\\\\alpha = .93$, $\\\\lambda = .93$ (overall), for scenarios $\\\\lambda = .95$ |\\n|                     | had a more genuine feel?                                              |                                                                             |\\n|                     | was more natural?                                                     |                                                                             |\\n| Intent-Effectiveness| had more suitable responses?                                          | Study 1: $\\\\alpha = .93$, $\\\\lambda = .93$ (overall), for scenarios $\\\\lambda = .96$ |\\n|                     | lived up to your expectations better?                                 |                                                                             |\\n| Satisfaction        | was more to your liking?                                              | Study 1: $\\\\alpha = .95$, $\\\\lambda = .95$ (overall), for scenarios $\\\\lambda = .97$ |\\n|                     | was more satisfactory?                                                |                                                                             |\\n\\nTable 2: Direct comparison bipolar rating scales with Cronbach\u2019s $\\\\alpha$ and Guttman\u2019s $\\\\lambda$.\\n\\n4.2.2 Procedure\\n\\nAfter a short demographic questionnaire, n=71 participants were given 3 scenarios (see Figure 2), rating each before turning to the next (see Figure 3.2). Having completed all scenarios and evaluations, feedback was elicited and interindividual variables were assessed.  \\n\\n4.2.3 Feedback\\n\\nTo quantify the feedback, we repeated the procedure described for the pilot. Feedback on the study was 69.01% positive (neutral opinions tend to be rated as negative as well, e.g. \"I have no opinion about the study.\"), on the interface only 52.11% (negatives include no feedback at all, neutral statements such as \"It was fast and responsive, just feel like the loading is too big and the lettering also\", but also some on bugs like \"It was fine, although sometimes my prompt would trigger a loading animation that the bot would never reply to, so I had to prompt again, which left the loading anim on the screen for one of the bots but not the other, not a big deal.\" or \"It was frusting when it could not listen or answer all\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. Introduction\\n\\nThe (logarithmic) Mahler measure of a non-zero rational function \\\\( P \\\\in \\\\mathbb{C}(x_1, \\\\ldots, x_n)^* \\\\) is defined by\\n\\n\\\\[\\n(1) \\\\quad m(P) = m(P(x_1, \\\\ldots, x_n)) := \\\\frac{1}{(2\\\\pi i)^n} \\\\int_{\\\\mathbb{T}^n} \\\\log |P(x_1, \\\\ldots, x_n)| \\\\frac{dx_1}{x_1} \\\\cdots \\\\frac{dx_n}{x_n},\\n\\\\]\\n\\nwhere \\\\( \\\\mathbb{T}^n = \\\\{(x_1, \\\\ldots, x_n) \\\\in \\\\mathbb{C}^* \\\\times \\\\mathbb{C}^* \\\\times \\\\cdots \\\\times \\\\mathbb{C}^* : |x_1| = \\\\cdots = |x_n| = 1\\\\} \\\\).\\n\\nThe first appearance of this quantity (for one variable polynomials) can be traced back to Lehmer\u2019s work [1] on Mersenne numbers, and its several variable form first appeared in the work of Mahler [2] regarding a simpler proof of the Gel\u2019fond-Mahler inequality, and it was later named after him.\\n\\nIn the early 80\u2019s, Smyth [3] discovered the following remarkable identities:\\n\\n\\\\[\\nm(x + y + 1) = \\\\frac{3\\\\sqrt{3}}{4\\\\pi} L(\\\\chi_{-3}, 2),\\n\\\\]\\n\\n\\\\[\\nm(1 + x + y + z) = \\\\frac{7}{2\\\\pi^2} \\\\zeta(3),\\n\\\\]\\n\\nwhere \\\\( L(\\\\chi_{-3}, 2) \\\\) is the Dirichlet \\\\( L \\\\)-function of the quadratic character \\\\( \\\\chi_{-3} \\\\) of conductor 3, and \\\\( \\\\zeta(s) \\\\) is the Riemann zeta function (for more details see [4]). These are two of the initial formulas for several variable cases.\\n\\nLater the work of Boyd [5], Deninger [6], Rodriguez-Villegas [7] and others provided us with interesting connections among Mahler measure, higher regulators, and Beilinson\u2019s conjectures. The conjectural formulas to support their work, such as\\n\\n\\\\[\\nm(P_k(x, y)) \\\\overset{?}{=} r_k L'(E_{N(k)}, 0), \\\\quad r_k \\\\in \\\\mathbb{Q},\\n\\\\]\\n\\nwere eventually proved for certain polynomials, due to Rodriguez-Villegas [7], Rogers and Zudilin [8, 9] et al. Here \\\\( E_{N(k)} \\\\) is an elliptic curve of conductor \\\\( N(k) \\\\) associated to \\\\( P_k \\\\), and the question mark stands for a numerical formula that is true for at least 20 decimal places. (See the book of Brunault and Zudilin [10] for more details.)\\n\\nIn a different direction, Cassaigne and Maillot [11] generalized the formula found by Smyth to \\\\( m(a + bx + cy) \\\\) for arbitrary complex constants \\\\( a, b, \\\\) and \\\\( c \\\\):\\n\\n\\\\[\\n(2) \\\\quad \\\\pi m(ax + by + c) = \\\\begin{cases} \\n\\\\alpha \\\\log |a| + \\\\beta \\\\log |b| + \\\\gamma \\\\log |c| + D \\\\left( \\\\frac{|a|}{|b|} e^{i\\\\gamma} \\\\right) & \\\\text{if } \\\\Delta \\\\text{ holds}, \\\\\\\\\\n\\\\log \\\\max\\\\{|a|, |b|, |c|\\\\} & \\\\text{if } \\\\Delta \\\\text{ does not hold},\\n\\\\end{cases}\\n\\\\]\\n\\nwhere \\\\( \\\\Delta \\\\) stands for the statement that \\\\( |a|, |b|, \\\\) and \\\\( |c| \\\\) are the lengths of the sides of a planar triangle, and in that case, \\\\( \\\\alpha, \\\\beta, \\\\) and \\\\( \\\\gamma \\\\) are the angles opposite to the sides of the lengths \\\\( |a|, |b| \\\\) and \\\\( |c| \\\\) respectively (see Figure 1).\\n\\nWe also remark that the constant coefficient can be replaced by a variable without changing the Mahler measure, in the sense that \\\\( m(ax + by + c) = m(ax + by + cz) \\\\). Additionally, it is\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. Introduction\\n\\nThe (logarithmic) Mahler measure of a non-zero rational function \\\\( P \\\\in \\\\mathbb{C}(x_1, \\\\ldots, x_n)^* \\\\) is defined by\\n\\n\\\\[\\n(1) \\\\quad m(P) = m(P(x_1, \\\\ldots, x_n)) := \\\\frac{1}{(2\\\\pi i)^n} \\\\int_{\\\\mathbb{T}^n} \\\\log |P(x_1, \\\\ldots, x_n)| \\\\frac{dx_1}{x_1} \\\\cdots \\\\frac{dx_n}{x_n},\\n\\\\]\\n\\nwhere \\\\( \\\\mathbb{T}^n = \\\\{(x_1, \\\\ldots, x_n) \\\\in \\\\mathbb{C}^* \\\\times \\\\mathbb{C}^* \\\\times \\\\cdots \\\\times \\\\mathbb{C}^* : |x_1| = \\\\cdots = |x_n| = 1\\\\} \\\\).\\n\\nThe first appearance of this quantity (for one variable polynomials) can be traced back to Lehmer\u2019s work [1] on Mersenne numbers, and its several variable form first appeared in the work of Mahler [2] regarding a simpler proof of the Gel\u2019fond-Mahler inequality, and it was later named after him. In the early 80 \u2019s, Smyth [3] discovered the following remarkable identities: \\\\[\\nm(x + y + 1) = \\\\frac{3\\\\sqrt{3}}{4\\\\pi} L(\\\\chi_{-3}, 2),\\n\\\\]\\n\\\\[\\nm(1 + x + y + z) = \\\\frac{7}{2\\\\pi^2} \\\\zeta(3),\\n\\\\]\\nwhere \\\\( L(\\\\chi_{-3}, 2) \\\\) is the Dirichlet \\\\( L \\\\)-function of the quadratic character \\\\( \\\\chi_{-3} \\\\) of conductor 3, and \\\\( \\\\zeta(s) \\\\) is the Riemann zeta function (for more details see [4]). These are two of the initial formulas for several variable cases. Later the work of Boyd [5], Deninger [6], Rodriguez-Villeags [7] and others provided us with interesting connections among Mahler measure, higher regulators, and Be\u02d8\u0131linson\u2019s conjectures. The conjectural formulas to support their work, such as\\n\\n\\\\[\\nm(P_k(x, y)) \\\\overset{?}{=} r_k L'(E_{N(k)}, 0), \\\\quad r_k \\\\in \\\\mathbb{Q},\\n\\\\]\\nwere eventually proved for certain polynomials, due to Rodriguez-Villegas [7], Rogers and Zudilin [8, 9] et al. Here \\\\( E_{N(k)} \\\\) is an elliptic curve of conductor \\\\( N(k) \\\\) associated to \\\\( P_k \\\\), and the question mark stands for a numerical formula that is true for at least 20 decimal places. (See the book of Brunault and Zudilin [10] for more details.)\\n\\nIn a different direction, Cassaigne and Maillot [11] generalized the formula found by Smyth to \\\\( m(a + bx + cy) \\\\) for arbitrary complex constants \\\\( a, b, \\\\) and \\\\( c \\\\):\\n\\n\\\\[\\n(2) \\\\quad \\\\pi m(ax + by + c) = \\\\begin{cases} \\n\\\\alpha \\\\log |a| + \\\\beta \\\\log |b| + \\\\gamma \\\\log |c| + D \\\\left( \\\\frac{|a|}{|b|} e^{i\\\\gamma} \\\\right) & \\\\text{if } \\\\Delta \\\\text{ holds}, \\\\\\\\\\n\\\\log \\\\max\\\\{|a|, |b|, |c|\\\\} & \\\\text{if } \\\\Delta \\\\text{ does not hold},\\n\\\\end{cases}\\n\\\\]\\n\\nwhere \\\\( \\\\Delta \\\\) stands for the statement that \\\\( |a|, |b|, \\\\) and \\\\( |c| \\\\) are the lengths of the sides of a planar triangle, and in that case, \\\\( \\\\alpha, \\\\beta, \\\\) and \\\\( \\\\gamma \\\\) are the angles opposite to the sides of the lengths \\\\( |a|, |b| \\\\) and \\\\( |c| \\\\) respectively (see Figure 1).\\n\\nWe also remark that the constant coefficient can be replaced by a variable without changing the Mahler measure, in the sense that \\\\( m(ax + by + c) = m(ax + by + cz) \\\\). Additionally, it is\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where the initial $\\\\mathbf{u}_{AD}^{(0)}$ is set as $\\\\mathbf{u}$. By running the iteration (16) $N$ times, we obtain $\\\\mathbf{u}_{AD} = \\\\mathbf{u}_{AD}^{(N)}$ as the $N^{th}$ order van Cittert operator applied to $\\\\mathbf{u}$. However, to make this iteration practical, we need to recall that each application of the filter $G$ requires the inversion of a differential operator. Therefore, denoting by $\\\\tilde{\\\\mathbf{u}}^{(n+1)}$ the quantity $\\\\tilde{\\\\mathbf{u}}^{(n+1)} = G\\\\mathbf{u}_{AD}^{(n)}$, and substituting in our filter from (12), we observe that $\\\\tilde{\\\\mathbf{u}}^{(n+1)}$ is found by solving\\n\\n$$\\n(I - \\\\delta^2 \\\\Delta)^{-1} \\\\mathbf{u}_{AD}^{(n)} = \\\\tilde{\\\\mathbf{u}}^{(n+1)} \\\\iff (I - \\\\delta^2 \\\\Delta)\\\\tilde{\\\\mathbf{u}}^{(n+1)} = \\\\mathbf{u}_{AD}^{(n)}.\\n$$\\n\\nEquation (17) requires to solve a linear system at each iteration $n = 0, \\\\ldots, N-1$. As discussed above, in order to employ the van Cittert AD in a ROM setting, by multiplying (17) by each test function in our ROM space and expanding our prospective solution as a linear combination of ROM basis functions, we obtain the linear system\\n\\n$$\\n(M + \\\\delta^2 S) \\\\mathbf{c}^{(n+1)} = M \\\\mathbf{c}_{AD}^{n}.\\n$$\\n\\nThus, the iterative process (16) amounts to setting $\\\\mathbf{c}_{AD}^{(0)} = \\\\mathbf{c}$, updating the coefficients of the ROM AD velocity as\\n\\n$$\\n\\\\mathbf{c}_{AD}^{(n+1)} = \\\\mathbf{c}_{AD}^{(n)} + \\\\{\\\\mathbf{c} - \\\\mathbf{c}^{(n+1)}\\\\} \\\\quad n = 0, \\\\ldots, N-1,\\n$$\\n\\nand finally defining $\\\\mathbf{c}_{AD} = \\\\mathbf{c}_{AD}^{(N)}$.\\n\\n### 3.3. The Tikhonov AD\\n\\nThe Tikhonov method of approximate deconvolution is defined as\\n\\n$$\\n\\\\mathbf{u}_{AD} = D_{\\\\mu}^T \\\\mathbf{u} = (G^* G + \\\\mu I)^{-1} G^* \\\\mathbf{u},\\n$$\\n\\nwhere $G^*$ denotes the adjoint of the operator $G$, and $\\\\mu \\\\in \\\\mathbb{R}^+$ is a positive constant; we refer, e.g., to [39, section 3.3.1] for further details on the Tikhonov AD. When plugging in the specific filter from (12) and proceeding formally, we can write\\n\\n$$\\n\\\\begin{align*}\\n[G^* G + \\\\mu I] \\\\mathbf{u}_{AD} &= G^* \\\\mathbf{u}, \\\\\\\\\\n[(I - \\\\delta^2 \\\\Delta)^{-1} + \\\\mu (I - \\\\delta^2 \\\\Delta)^{-1}] \\\\mathbf{u}_{AD} &= (I - \\\\delta^2 \\\\Delta)^{-1} \\\\mathbf{u}, \\\\\\\\\\n[I + \\\\mu (I - \\\\delta^2 \\\\Delta^*) (I - \\\\delta^2 \\\\Delta)] \\\\mathbf{u}_{AD} &= (I - \\\\delta^2 \\\\Delta) \\\\mathbf{u}, \\\\\\\\\\n[I + \\\\mu (I - 2 \\\\delta^2 \\\\Delta + \\\\delta^4 \\\\Delta^2)] \\\\mathbf{u}_{AD} &= (I - \\\\delta^2 \\\\Delta) \\\\mathbf{u}.\\n\\\\end{align*}\\n$$\\n\\n(20)\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where the initial $\\\\mathbf{u}_{AD}^{(0)}$ is set as $\\\\mathbf{u}$. By running the iteration (16) $N$ times, we obtain $\\\\mathbf{u}_{AD} = \\\\mathbf{u}_{AD}^{(N)}$ as the $N^{th}$ order van Cittert operator applied to $\\\\mathbf{u}$. However, to make this iteration practical, we need to recall that each application of the filter $G$ requires the inversion of a differential operator. Therefore, denoting by $\\\\tilde{\\\\mathbf{u}}^{(n+1)}$ the quantity $\\\\tilde{\\\\mathbf{u}}^{(n+1)} = G\\\\mathbf{u}_{AD}^{(n)}$, and substituting in our filter from (12), we observe that $\\\\tilde{\\\\mathbf{u}}^{(n+1)}$ is found by solving\\n\\n$$\\n(I - \\\\delta^2 \\\\Delta)^{-1} \\\\mathbf{u}_{AD}^{(n)} = \\\\tilde{\\\\mathbf{u}}^{(n+1)} \\\\iff (I - \\\\delta^2 \\\\Delta)\\\\tilde{\\\\mathbf{u}}^{(n+1)} = \\\\mathbf{u}_{AD}^{(n)}.\\n$$\\n\\nEquation (17) requires to solve a linear system at each iteration $n = 0, \\\\ldots, N-1$. As discussed above, in order to employ the van Cittert AD in a ROM setting, by multiplying (17) by each test function in our ROM space and expanding our prospective solution as a linear combination of ROM basis functions, we obtain the linear system\\n\\n$$\\n(M + \\\\delta^2 S) \\\\mathbf{c}^{(n+1)} = M \\\\mathbf{c}_{AD}^{n}.\\n$$\\n\\nThus, the iterative process (16) amounts to setting $\\\\mathbf{c}_{AD}^{(0)} = \\\\mathbf{c}$, updating the coefficients of the ROM AD velocity as\\n\\n$$\\n\\\\mathbf{c}_{AD}^{(n+1)} = \\\\mathbf{c}_{AD}^{(n)} + \\\\{\\\\mathbf{c} - \\\\mathbf{c}^{(n+1)}\\\\} \\\\quad n = 0, \\\\ldots, N-1,\\n$$\\n\\nand finally defining $\\\\mathbf{c}_{AD} = \\\\mathbf{c}_{AD}^{(N)}$.\\n\\n### 3.3. The Tikhonov AD\\n\\nThe Tikhonov method of approximate deconvolution is defined as\\n\\n$$\\n\\\\mathbf{u}_{AD} = D_{\\\\mu}^T \\\\mathbf{u} = (G^*G + \\\\mu I)^{-1}G^*\\\\mathbf{u},\\n$$\\n\\nwhere $G^*$ denotes the adjoint of the operator $G$, and $\\\\mu \\\\in \\\\mathbb{R}^+$ is a positive constant; we refer, e.g., to [39, section 3.3.1] for further details on the Tikhonov AD. When plugging in the specific filter from (12) and proceeding formally, we can write\\n\\n$$\\n\\\\begin{align*}\\n[G^*G + \\\\mu I] \\\\mathbf{u}_{AD} &= G^*\\\\mathbf{u}, \\\\\\\\\\n[(I - \\\\delta^2 \\\\Delta)^{-1} + \\\\mu(I - \\\\delta^2 \\\\Delta)^{-1}] \\\\mathbf{u}_{AD} &= (I - \\\\delta^2 \\\\Delta)^{-1} \\\\mathbf{u}, \\\\\\\\\\n[I + \\\\mu(I - \\\\delta^2 \\\\Delta)(I - \\\\delta^2 \\\\Delta)^*] \\\\mathbf{u}_{AD} &= (I - \\\\delta^2 \\\\Delta)\\\\mathbf{u}, \\\\\\\\\\n[I + \\\\mu(I - 2\\\\delta^2 \\\\Delta + \\\\delta^4 \\\\Delta^2)] \\\\mathbf{u}_{AD} &= (I - \\\\delta^2 \\\\Delta)\\\\mathbf{u}.\\n\\\\end{align*}\\n$$\\n\\n(20)\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"to accept a trigger of 40 ns and an integration time of 325 ns to collect photons on the sensor tile. The overvoltage of the silicon photomultipliers (SiPMs) is set to 3 V. As this is a digital tile it is possible to disable the SPADs which produce a high number of dark counts. This inhibit fraction is set to 10%. The surface of each sensor tile is covered with a glass plate of 1.1 mm. The sensor tiles are connected to a singles processing unit (SPU) which manages their voltage supply and feeds their data to the data acquisition and processing server (DAPS). During the measurement, the tiles are cooled by a 15 \u00b0C liquid cooling system.\\n\\n2.1.3. Masks\\n\\nWe perform measurements in one and two dimensions, i.e., we reconstruct an image along one axis or on a plane. For these two tasks, we use a one- and a two-dimensional versions of a MURA mask of rank 476, clipped to 31 \u00d7 31 central pixels (see Fig. 2). The mask rank as well as the setup geometry have been optimised via Monte Carlo simulations before the experiment. To construct the physical masks we use tungsten rods of (2.26 \u00d7 2.26 \u00d7 20) mm\u00b3 which are inserted into 3D printed rasters made from Pro Grey Resin. The rod manufacturing reaches a precision of 0.1 mm. The resulting masks have a dimension of (73.6 \u00d7 73.6) mm\u00b2. The rasters have a total thickness of 13 mm and the holes to insert the rods are 10 mm deep. To prevent the rods from falling out, the assembled masks are wrapped in cling film.\\n\\n2.2. Radioactive sources\\n\\nFor image reconstruction, the experimental data were obtained with a radioactive $^{22}$Na source with an activity of 2.89 MBq. The active material in that source covers an area of 1 mm \u00d7 1 mm. As a $\\\\beta^+$-emitter, $^{22}$Na provides two photons of 511 keV emitted back-to-back, which can be used for electronic collimation. For calibration of the detectors we additionally used the 1275 keV gamma line of $^{22}$Na and two more radioactive sources: a $^{137}$Cs source with a gamma line at 662 keV with an activity of 1.73 MBq and a $^{133}$Ba source with\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"to accept a trigger of 40 ns and an integration time of 325 ns to collect photons on the sensor tile. The overvoltage of the silicon photomultipliers (SiPMs) is set to 3 V . As this is a digital tile it is possible to disable the SPADs which produce a high number of dark counts. This inhibit fraction is set to 10 %. The surface of each sensor tile is covered with a glass plate of 1.1 mm. The sensor tiles are connected to a singles processing unit (SPU) which manages their voltage supply and feeds their data to the data acquisition and processing server (DAPS). During the measurement, the tiles are cooled by a 15 \u00b0C liquid cooling system.\\n\\n2.1.3. Masks\\n\\nWe perform measurements in one and two dimensions, i.e., we reconstruct an image along one axis or on a plane. For these two tasks, we use a oneand a two-dimensional versions of a MURA mask of rank 476, clipped to 31 \u00d7 31 central pixels (see Fig. 2). The mask rank as well as the setup geometry have been optimised via Monte Carlo simulations before the experiment. To construct the physical masks we use tungsten rods of (2.26 \u00d7 2.26 \u00d7 20) mm\u00b3 which are inserted into 3D printed rasters made from Pro Grey Resin. The rod manufacturing reaches a precision of 0.1 mm. The resulting masks have a dimension of (73.6 \u00d7 73.6) mm\u00b2. The rasters have a total thickness of 13 mm and the holes to insert the rods are 10 mm deep. To prevent the rods from falling out, the assembled masks are wrapped in cling film. \\n\\n2.2. Radioactive sources\\n\\nFor image reconstruction, the experimental data were obtained with a radioactive 22 Na source with an activity of 2.89 MBq. The active material in that source covers an area of 1 mm \u00d7 1 mm. As a \u03b2\u207a-emitter, Na provides two photons of 511 keV emitted back-to-back, which can be used for electronic collimation. For calibration of the detectors we additionally used the 1275 keV gamma line of Na and two more radioactive sources: a 137 Cs source with a gamma line at 662 keV with an activity of 1.73 MBq and a 133 Ba source with\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"zz0 = cell(1, N);\\nfor i = 1 : N-1\\n    zz0(i) = {[initial_position(:, i); initial_position(:, i + 1)]};\\nend\\nzz0(N) = {[initial_position(:, N); initial_position(:, 1)]};\\nend\\n\\nSuch that the overall problem can be set up with the following function:\\n\\n```\nmatlab\\nfunction [sProb ] = setupSolver(N, sigma)\\n    n = 4;\\n    d = 2;\\n    y = sym('y%d%d', [N n], 'real');\\n    y = y';\\n    [eta, eta_bar] = getEta(N, d, sigma);\\n    F = getObjective(N, y, eta, eta_bar, sigma);\\n    H = getInequalityConstr(N, y, eta_bar);\\n    AA = getCouplingMatrix(N, n);\\n    zz0 = getStartValue(N, sigma);\\n    sProb.llbx = cell(1, N);\\n    sProb.uubx = cell(1, N);\\n    for i = 1 : N\\n        sProb.llbx(i) = mat2cell([-inf; -inf; -inf; -inf], 4, 1);\\n        sProb.uubx(i) = mat2cell([ inf; inf; inf; inf], 4, 1);\\n    end\\n    sProb.locFuns.ffi = cell(1, N);\\n    sProb.locFuns.hhi = cell(1, N);\\n    for i = 1 : N\\n        sProb.locFuns.ffi(i) = {matlabFunction(F(i), 'Vars', {y(:, i)})} ;\\n        sProb.locFuns.hhi(i) = {matlabFunction(H(i), 'Vars', {y(:, i)})} ;\\n    end\\n    sProb.AA = AA;\\n    sProb.zz0 = zz0;\\nend\\n\n```\\n\\n12.2.4 Runtime Analysis\\n\\nFor the runtime analysis, the idea is to run the sensor network localization problem with varying number of sensors both with a decentral and a central optimization step. To do so, firstly a vector with a number of sensors is needed and secondly a vector with variances. Then, the time needed for the decentral and the central optimization is measured and can be plotted.\\n\\n```\nmatlab\\nN = [5, 10, 15 , 20, 25, 30, 35, 40, 50, 60, 70, 80, 90, 100];\\nsigma = [0.5, 1, 1.5, 2, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5];\\ntime = zeros(2, length(N));\\nfor i = 1 : length(N)\\n    sProb = setupSolver(N(i), sigma(i));\\n\n```\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"zz0 = cell(1, N);\\nfor i = 1 : N-1\\n    zz0(i) = {[initial_position(:, i); initial_position(:, i + 1)]};\\nend\\nzz0(N) = {[initial_position(:, N); initial_position(:, 1)]};\\nend\\n\\nSuch that the overall problem can be set up with the following function: \\n\\nfunction [sProb ] = setupSolver(N, sigma)\\n    n = 4;\\n    d = 2;\\n    y = sym('y%d%d', [N n], 'real');\\n    y = y';\\n    [eta, eta_bar] = getEta(N, d, sigma);\\n    F = getObjective(N, y, eta, eta_bar, sigma);\\n    H = getInequalityConstr(N, y, eta_bar);\\n    AA = getCouplingMatrix(N, n);\\n    zz0 = getStartValue(N, sigma);\\n    sProb.llbx = cell(1, N);\\n    sProb.uubx = cell(1, N);\\n    for i = 1 : N\\n        sProb.llbx(i) = mat2cell([-inf; -inf; -inf; -inf], 4, 1);\\n        sProb.uubx(i) = mat2cell([ inf; inf; inf; inf], 4, 1);\\n    end\\n    sProb.locFuns.ffi = cell(1, N);\\n    sProb.locFuns.hhi = cell(1, N);\\n    for i = 1 : N\\n        sProb.locFuns.ffi(i) = {matlabFunction(F(i), 'Vars', {y(:, i)})} ;\\n        sProb.locFuns.hhi(i) = {matlabFunction(H(i), 'Vars', {y(:, i)})} ;\\n    end\\n    sProb.AA = AA;\\n    sProb.zz0 = zz0;\\nend\\n\\n12.2.4 Runtime Analysis\\n\\nFor the runtime analysis, the idea is to \u00b4run the sensor network localization problem with varying number of sensors both with a decentral and a central optimization step. To do so, firstly a vector with a number of sensors is needed and secondly a vector with variances. Then, the time needed for the decentral and the central optimization is measured and can be plotted. \\n\\nN = [5, 10, 15 , 20, 25, 30, 35, 40, 50, 60, 70, 80, 90, 100];\\nsigma = [0.5, 1, 1.5, 2, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5];\\ntime = zeros(2, length(N));\\nfor i = 1 : length(N)\\n    sProb = setupSolver(N(i), sigma(i));\\nend\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"**Definition 4.11.** We say a spin system $\\\\mu$ on $\\\\mathbb{Z}^d$ satisfies the *strong spatial mixing (SSM)* condition if there exist constants $\\\\alpha, \\\\gamma, L > 0$ such that for every $d$-dimensional rectangle $\\\\Lambda \\\\subset \\\\mathbb{Z}^d$ of side length between $L$ and $2L$ and every subset $B \\\\subset \\\\Lambda$, with any pair $(\\\\tau, \\\\tau')$ of boundary configurations on $\\\\partial \\\\Lambda$ that only differ at a vertex $u$, we have\\n\\n$$\\\\|\\\\mu_B^\\\\tau(\\\\cdot) - \\\\mu_B^{\\\\tau'}(\\\\cdot)\\\\|_{TV} \\\\leq \\\\gamma \\\\cdot \\\\exp(-\\\\alpha \\\\cdot \\\\text{dist}(u, B)),$$\\n\\nwhere $\\\\text{dist}(\\\\cdot, \\\\cdot)$ denotes graph distance.\\n\\nThe definition above differs from other variants of SSM in the literature (e.g., [DSVW04, BCSV19, MOS94]) in that $\\\\Lambda$ has been restricted to \u201cregular enough\u201d rectangles. In particular, our variant of SSM is easier to satisfy than those in [DSVW04, MOS94] but more restricting than the one in [BCSV19] (that only considers squares). Nevertheless, it follows from [CP21, MOS94, Ale98, BDC12] that for the ferromagnetic Ising model, this form of SSM holds up to a critical threshold temperature $\\\\beta < \\\\beta_c(2) = \\\\ln(1 + \\\\sqrt{2})$ on $\\\\mathbb{Z}^2$.\\n\\n**Corollary 1.9** from the introduction states that for $b$-marginally bounded monotone spin system on $d$-dimensional cubes $V \\\\subseteq \\\\mathbb{Z}^d$, SSM implies that the mixing time of any systematic scan $P_\\\\phi$ is $O(\\\\log n)$. As mentioned there, this result in turn implies that any systematic scan dynamics for the ferromagnetic Ising model is mixing in $O(\\\\log n)$ steps on boxes of $\\\\mathbb{Z}^2$ when $\\\\beta < \\\\beta_c(2)$. Another interesting consequence of Corollary 1.9 is that we obtain $O(\\\\log n)$ mixing time for any systematic scan dynamics $P_\\\\phi$ for the hardcore model on $\\\\mathbb{Z}^2$ when $\\\\lambda < 2.538$, which is the best known condition for ensuring SSM [SSSY17, RST*13].\\n\\nOur proof of Corollary 1.9 relies on Lemma 4.12 that is restated below. Remarkably, Lemma 4.12 generalizes beyond monotone systems and may be of independent interests.\\n\\n**Lemma 4.12.** For a spin system on a $d$-dimensional cube $V \\\\subseteq \\\\mathbb{Z}^d$, SSM implies $\\\\eta$-spectral independence, where $\\\\eta = O(1)$.\\n\\n*Proof of Corollary 1.9.* Assume a monotone spin system satisfies SSM condition. Then the spin system satisfies $\\\\eta$-spectral independence, where $\\\\eta = O(1)$ by Lemma 4.12. By noting that $\\\\Delta = 2^d$ the corollary follows from Theorem 4.3. \\\\qed\\n\\nLastly, we give a proof of Lemma 4.12. For this, we recall the notion of a $\\\\kappa$-contractive coupling which is known to imply spectral independence. We say a distribution $\\\\mu$ is $\\\\kappa$-contractive with respect to a Markov chain $P$ if for all $X_0, Y_0 \\\\in \\\\Omega$, there exists a coupling of step of $P$ so that\\n\\n$$\\\\mathbb{E}[d(X_1, Y_1) \\\\mid X_0, Y_0] \\\\leq \\\\kappa d(X_0, Y_0),$$\\n\\nwhere $d(\\\\cdot, \\\\cdot)$ denotes the Hamming distance of two configurations. The following lemma from [BCC*22] shows that spectral independence follows from the existence of a contractive coupling with respect to a heat-bath block dynamics.\\n\\n**Lemma 4.13 ([BCC*22]).** If $\\\\mu$ is $\\\\kappa$-contractive with respect to a block dynamics, then $\\\\mu$ is $(\\\\frac{2DM}{1-\\\\kappa})$-spectrally independent, where $M$ is the maximum block size and $D$ is the maximum probability of a vertex being selected as part of a block in any step of the block dynamics.\\n\\nWith this lemma on hand, we can now prove Lemma 4.12.\\n\\n*Proof of Lemma 4.12.* Let $L$ be a sufficiently large constant so that the SSM condition is satisfied; we will choose $L$ later. Let $V$ be a $d$-dimensional cube of $\\\\mathbb{Z}^d$. We define a heat-bath block dynamics $P_B$ with respect to a collection $\\\\mathcal{B}$ of $d$-dimensional rectangles in $V$. Precisely, let $S_0 := \\\\{w \\\\in \\\\mathbb{Z}^d : d_{\\\\infty}(w, v) < L\\\\}$, and let $\\\\mathcal{B}$ be the set of blocks $\\\\{S_0 \\\\cap V\\\\}_{v \\\\in V}$. Given a configuration $X_t$, the heat-bath block dynamics $P_B$ obtains a configuration $X_{t+1}$ in 3 steps as follows:\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"**Definition 4.12.** We say a spin system on a $d$-dimensional cube $V \\\\subseteq \\\\mathbb{Z}^d$, SSM implies $\\\\eta$-spectral independence, where $\\\\eta = O(1)$.\\n\\n*Proof of Corollary 1.9.* Assume a monotone spin system satisfies SSM condition. Then the spin system satisfies $\\\\eta$-spectral independence, where $\\\\eta = O(1)$ by Lemma 4.11. By noting that $\\\\Delta = 2^d$ the corollary follows from Theorem 4.3. \\\\hfill $\\\\square$\\n\\nLastly, we give a proof of Lemma 4.12. For this, we recall the notion of a $\\\\kappa$-contractive coupling which is known to imply spectral independence. We say a distribution $\\\\mu$ is $\\\\kappa$-contractive with respect to a Markov chain $P$ if for all $X_0, Y_0 \\\\in \\\\Omega$, there exists a coupling of step of $P$ so that\\n\\n$$\\\\mathbb{E}[d(X_1, Y_1) \\\\mid X_0, Y_0] \\\\leq \\\\kappa d(X_0, Y_0),$$\\n\\nwhere $d(\\\\cdot, \\\\cdot)$ denotes the Hamming distance of two configurations. The following lemma from [BCC+22] shows that spectral independence follows from the existence of a contractive coupling with respect to a heat-bath block dynamics.\\n\\n**Lemma 4.13 ([BCC+22]).** If $\\\\mu$ is $\\\\kappa$-contractive with respect to a block dynamics, then $\\\\mu$ is $(\\\\frac{2DM}{1-\\\\kappa})$-spectrally independent, where $M$ is the maximum block size and $D$ is the maximum probability of a vertex being selected as part of a block in any step of the block dynamics.\\n\\nWith this lemma on hand, we can now prove Lemma 4.12.\\n\\n*Proof of Lemma 4.12.* Let $L$ be a sufficiently large constant so that the SSM condition is satisfied; we will choose $L$ later. Let $V$ be a $d$-dimensional cube of $\\\\mathbb{Z}^d$. We define a heat-bath block dynamics $P_B$ with respect to a collection $\\\\mathcal{B}$ of $d$-dimensional rectangles in $V$. Precisely, let $S_0 := \\\\{w \\\\in \\\\mathbb{Z}^d : d_{\\\\infty}(w, v) < L\\\\}$, and let $\\\\mathcal{B}$ be the set of blocks $\\\\{S_0 \\\\cap V\\\\}_{v \\\\in V}$. Given a configuration $X_t$, the heat-bath block dynamics $P_B$ obtains a configuration $X_{t+1}$ in 3 steps as follows:\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The initial conditions for the Taylor-Green vortex are\\n\\n\\\\[\\n\\\\begin{align*}\\n\\\\rho(x, y, z) &= 1 \\\\\\\\\\nu(x, y, z) &= \\\\sin(x) \\\\cos(y) \\\\cos(z) \\\\\\\\\\nv(x, y, z) &= -\\\\cos(x) \\\\sin(y) \\\\cos(z) \\\\\\\\\\nw(x, y, z) &= 0 \\\\\\\\\\np(x, y, z) &= 10 + \\\\frac{(\\\\cos(2x) + \\\\cos(2y))(\\\\cos(2x) + 2) - 2}{16}\\n\\\\end{align*}\\n\\\\]\\n\\nwith a pressure value corresponding to a Mach number \\\\( M \\\\approx 0.26 \\\\). The triperiodic domain has side length \\\\( 2\\\\pi \\\\) in all directions and is discretized using \\\\( 32 \\\\times 32 \\\\times 32 \\\\) nodes. The chosen CFL value is sufficiently small that linear invariants are exactly conserved to machine precision for all schemes. The time evolution of the entropy integral for this test is shown in Fig. 2 and it is in agreement with the previous results. In this test, since the pressure is not constant, \\\\( A_p - H e \\\\) is no longer equivalent to \\\\( A_p - A e \\\\); in this case we have better performances from \\\\( A_p - H e \\\\) and \\\\( G_p - G e \\\\) when compared to \\\\( A_p - A p \\\\) and \\\\( A_p - A e \\\\) and this result is found for both fourth-order and six-order accurate fluxes. An improvement can be obtained using an additional term in the expansions and \\\\( \\\\text{KEEP}^{(1)} \\\\) and \\\\( \\\\text{AEP}^{(1)} \\\\) are the schemes which more closely achieve a constant value for the entropy integral. Information about the reliability of the scheme can be obtained thorough the study of the evolution of thermodynamic fluctuations in time. We checked that for all the schemes tested, the density and temperature fluctuations do not have an unbound growth (not shown). This is the desired behaviour, since for inviscid isotropic homogeneous turbulence they are reported to level off to a constant value [18, 2].\\n\\n6 Conclusions\\n\\nWe proposed a new class of asymptotically entropy-preserving fluxes for the discretization of the convective terms in the compressible Euler equations with interesting properties. It provides a consistent asymptotic approximation of an existing entropy-preserving scheme based on the logarithmic mean, and it consists of economical algebraic fluxes based on the harmonic mean. Moreover, at all orders of approximation, the numerical fluxes have the pressure-equilibrium preservation property. The theoretical predictions are confirmed on two test cases, verifying that the new schemes are able to numerically maintain pressure equilibrium and demonstrating good entropy-conservation property. It was also shown that the error on entropy can be reduced by using additional terms in the expansion of the AEC fluxes.\\n\\nThese results suggest that AEC fluxes could be good candidate for the discretization of compressible flow equations in high performance solvers. Due to the their algebraic form, they are less computationally expensive than the fluxes based on the logarithmic mean, while retaining many important properties. In fact, they guarantee the KEP and PEP properties, combined with arbitrarily small error on entropy preservation.\\n\\nA High-order extension\\n\\nThe second-order accurate two-point fluxes presented in this article can be extended to higher-order formulations by using the approach proposed by Ranocha [7] in the context of Discontinuous Galerkin discretization of the Euler equations. The main result of interest for us is that contained in Theorem 3.1 of [7], which can be reformulated in FD terms as follows. We consider a numerical flux \\\\( \\\\mathcal{F}(w_i, w_{i+k}) \\\\) for a generic quantity \\\\( \\\\rho \\\\varphi \\\\), which depends on the values of the variables vector \\\\( w \\\\) in the nodal points \\\\( i \\\\) and \\\\( i + k \\\\). In our context \\\\( \\\\mathcal{F} \\\\) can be any of the numerical fluxes specified in Eqs. (1)-(4),(9)-(10) or (13)-(15) and \\\\( w \\\\) is the set of variables \\\\( (\\\\rho, u, e) \\\\). We will assume that the numerical flux is smooth, symmetrical (i.e. \\\\( \\\\mathcal{F}(w_i, w_{i+k}) = \\\\mathcal{F}(w_{i+k}, w_i) \\\\)) and consistent with the continuous flux \\\\( f \\\\) so that \\\\( \\\\mathcal{F}(w_i, w_i) = f(w_i) \\\\). Under these hypotheses, by following the steps of the proof to Theorem 3.1 in [7], we can show that given a numerical derivative formula of the type \\\\( \\\\partial \\\\varphi_i \\\\approx \\\\sum_k a_k \\\\varphi_{i+k} \\\\) then an approximation of the derivative \\\\( \\\\partial f \\\\) is given by\\n\\n\\\\[\\n\\\\sum_{k=1}^{L} 2a_k \\\\mathcal{F}(w_i, w_{i+k})\\n\\\\]\\n\\nand it has the same order of accuracy as the original derivative formula with weights \\\\( a_k \\\\). If one considers central derivative formulas, for which \\\\( a_k = -a_{-k} \\\\), by using the symmetry of the flux \\\\( \\\\mathcal{F} \\\\), Eq. (16) can be rewritten as\\n\\n\\\\[\\n\\\\sum_{k=1}^{L} 2a_k (\\\\mathcal{F}(w_i, w_{i+k}) - \\\\mathcal{F}(w_{i-k}, w_i))\\n\\\\]\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The initial conditions for the Taylor-Green vortex are\\n\\n\\\\[\\n\\\\begin{align*}\\n\\\\rho(x, y, z) &= 1 \\\\\\\\\\nu(x, y, z) &= \\\\sin(x) \\\\cos(y) \\\\cos(z) \\\\\\\\\\nv(x, y, z) &= -\\\\cos(x) \\\\sin(y) \\\\cos(z) \\\\\\\\\\nw(x, y, z) &= 0 \\\\\\\\\\np(x, y, z) &= 10 + \\\\frac{(\\\\cos(2x) + \\\\cos(2y))(\\\\cos(2x) + 2) - 2}{16}\\n\\\\end{align*}\\n\\\\]\\n\\nwith a pressure value corresponding to a Mach number \\\\( M \\\\approx 0.26 \\\\). The triperiodic domain has side length \\\\( 2\\\\pi \\\\) in all directions and is discretized using \\\\( 32 \\\\times 32 \\\\times 32 \\\\) nodes. The chosen CFL value is sufficiently small that linear invariants are exactly conserved to machine precision for all schemes. The time evolution of the entropy integral for this test is shown in Fig. 2 and it is in agreement with the previous results. In this test, since the pressure is not constant, \\\\( A_p - H e \\\\) is no longer equivalent to \\\\( A_p - A e \\\\); in this case we have better performances from \\\\( A_p - H e \\\\) and \\\\( G_p - G e \\\\) when compared to \\\\( A_p - A p \\\\) and \\\\( A_p - A e \\\\) and this result is found for both fourth-order and six-order accurate fluxes. An improvement can be obtained using an additional term in the expansions and KEEP\\\\(^{(1)}\\\\) and AEP are the schemes which more closely achieve a constant value for the entropy integral. Information about the reliability of the scheme can be obtained thorough the study of the evolution of thermodynamic fluctuations in time. We checked that for all the schemes tested, the density and temperature fluctuations do not have an unbound growth (not shown). This is the desired behaviour, since for inviscid isotropic homogeneous turbulence they are reported to level off to a constant value [18, 2].\\n\\n6 Conclusions\\n\\nWe proposed a new class of asymptotically entropy-preserving fluxes for the discretization of the convective terms in the compressible Euler equations with interesting properties. It provides a consistent asymptotic approximation of an existing entropy-preserving scheme based on the logarithmic mean, and it consists of economical algebraic fluxes based on the harmonic mean. Moreover, at all orders of approximation, the numerical fluxes have the pressure-equilibrium preservation property. The theoretical predictions are confirmed on two test cases, verifying that the new schemes are able to numerically maintain pressure equilibrium and demonstrating good entropy-conservation property. It was also shown that the error on entropy can be reduced by using additional terms in the expansion of the AEC fluxes. These results suggest that AEC fluxes could be good candidate for the discretization of compressible flow equations in high performance solvers. Due to the their algebraic form, they are less computationally expensive than the fluxes based on the logarithmic mean, while retaining many important properties. In fact, they guarantee the KEP and PEP properties, combined with arbitrarily small error on entropy preservation. \\n\\nA High-order extension\\n\\nThe second-order accurate two-point fluxes presented in this article can be extended to higher-order formulations by using the approach proposed by Ranocha [ 7 ] in the context of Discontinuous Galerkin discretization of the Euler equations. The main result of interest for us is that contained in Theorem 3.1 of [ 7 ], which can be reformulated in FD terms as follows. We consider a numerical flux \\\\( F(w_i, w_{i+k}) \\\\) for a generic quantity \\\\( \\\\rho \\\\varphi \\\\), which depends on the values of the variables vector \\\\( w \\\\) in the nodal points \\\\( i \\\\) and \\\\( i + k \\\\). In our context \\\\( F \\\\) can be any of the numerical fluxes specified in Eqs. (1)-(4),(9)-(10) or (13)-(15) and \\\\( w \\\\) is the set of variables \\\\( (\\\\rho, u, e) \\\\). We will assume that the numerical flux is smooth, symmetrical (i.e. \\\\( F(w_i, w_{i+k}) = F(w_{i+k}, w_i) \\\\)) and consistent with the continuous flux \\\\( f \\\\) so that \\\\( F(w_i, w_i) = f(w_i) \\\\). Under these hypotheses, by following the steps of the proof to Theorem 3.1 in [ 7 ], we can show that given a numerical derivative formula of the type \\\\( \\\\partial \\\\varphi_i \\\\approx \\\\sum_k a_k \\\\varphi_{i+k} \\\\) then an approximation of the derivative \\\\( \\\\partial f \\\\) is given by\\n\\n\\\\[\\n\\\\sum_{k=1}^{L} 2a_k F(w_i, w_{i+k})\\n\\\\]\\n\\nand it has the same order of accuracy as the original derivative formula with weights \\\\( a_k \\\\). If one considers central derivative formulas, for which \\\\( a_k = -a_{-k} \\\\), by using the symmetry of the flux \\\\( F \\\\), Eq. (16) can be rewritten as\\n\\n\\\\[\\n\\\\sum_{k=1}^{L} 2a_k (F(w_i, w_{i+k}) - F(w_{i-k}, w_i))\\n\\\\]\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C Empirical data\\n\\nWe observe the changes of activity over time of the empirical networks for the four data sets (Figure 11). The US school presents periodic patterns, varying from low contact periods when the students are in class to high contact periods when there is a recreational time. The US flight and Conference networks have circadian patterns as there are respectively less flights and less contacts at night. Finally, the Resistance game does not present any periodic change in its activity as every player of the game is looking at someone else at each time step.\\n\\n![Figure 11](image)\\n\\n**Figure 11:** Number of events as a function of time for the four data sets: the US school (panel a), the US flight (panel b), the Conference (panel c) and the Resistance game (panel d). The US school network contains high activity periods during recreational moments of the students\u2019 day, while the US flight and the Conference networks present circadian patterns. The Resistance game network does not have particular periodic activity changes.\\n\\nReferences\\n\\n[1] Petter Holme and Jari Saram\u00e4ki. Temporal networks. *Physics reports*, 519(3):97\u2013125, 2012.\\n\\n[2] Petter Holme. Modern temporal network theory: a colloquium. *The European Physical Journal B*, 88:1\u201330, 2015.\\n\\n[3] Naoki Masuda and Renaud Lambiotte. *A guide to temporal networks*. World Scientific, 2016.\\n\\n[4] Alain Barrat and Ciro Cattuto. Temporal networks of face-to-face human interactions. *Temporal networks*, pages 191\u2013216, 2013.\\n\\n[5] Sune Lehmann. Fundamental structures in temporal communication networks. *Temporal Network Theory*, pages 25\u201348, 2019.\\n\\n[6] Mohammed Saqr and Sonsoles L\u00f3pez-Pernas. The why, the what and the how to model a dynamic relational learning process with temporal networks. In *Proceedings of the NetSciLA22 workshop*, 2022.\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C Empirical data\\n\\nWe observe the changes of activity over time of the empirical networks for the four data sets (Figure 11). The US school presents periodic patterns, varying from low contact periods when the students are in class to high contact periods when there is a recreational time. The US flight and Conference networks have circadian patterns as there are respectively less flights and less contacts at night. Finally, the Resistance game does not present any periodic change in its activity as every player of the game is looking at someone else at each time step. \\n\\n![Figure 11](image)\\n\\n**Figure 11:** Number of events as a function of time for the four data sets: the US school (panel a), the US flight (panel b), the Conference (panel c) and the Resistance game (panel d). The US school network contains high activity periods during recreational moments of the students\u2019 day, while the US flight and the Conference networks present circadian patterns. The Resistance game network does not have particular periodic activity changes.\\n\\nReferences\\n\\n[1] Petter Holme and Jari Saram\u00a8aki. Temporal networks. *Physics reports*, 519(3):97\u2013125, 2012. [2] Petter Holme. Modern temporal network theory: a colloquium. *The European Physical Journal B*, 88:1\u201330, 2015. [3] Naoki Masuda and Renaud Lambiotte. *A guide to temporal networks*. World Scientific, 2016. [4] Alain Barrat and Ciro Cattuto. Temporal networks of face-to-face human interactions. *Temporal networks*, pages 191\u2013216, 2013. [5] Sune Lehmann. Fundamental structures in temporal communication networks. *Temporal Network Theory*, pages 25\u201348, 2019. [6] Mohammed Saqr and Sonsoles L\u00b4opez-Pernas. The why, the what and the how to model a dynamic relational learning process with temporal networks. In *Proceedings of the NetSciLA22 workshop*, 2022. \\n\\n16\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hence, surely has an infinite first moment. On the other hand, for every \\\\( \\\\mu \\\\) definition of the group operation and the \\\\( - \\\\) convergent and thus \\\\( |3| \\\\), we obtain the following.\\n\\n\\\\[\\n\\\\sum_{n \\\\geq 1} \\\\frac{1}{n(n+1)} = 1,\\n\\\\]\\n\\nso that \\\\( \\\\sum_{n \\\\geq 1} \\\\frac{1}{n} \\\\) diverges implies that \\\\( \\\\mathbb{E}(|\\\\text{supp}(\\\\sigma_1)|) \\\\) is infinite. Moreover, since \\\\( \\\\| (r_n, 0) \\\\|_{\\\\text{std}} \\\\geq |\\\\text{supp}(r_n)| \\\\), we also have that \\\\( \\\\mu \\\\) has an infinite first moment. On the other hand, for every \\\\( \\\\varepsilon > 0 \\\\) the series \\\\( \\\\sum_{n \\\\geq 1} \\\\frac{n^{1-\\\\varepsilon}}{n(n+1)} \\\\) is convergent and thus \\\\( \\\\mathbb{E}(|\\\\text{supp}(\\\\sigma_1)|^{1-\\\\varepsilon}) \\\\) is finite. The element \\\\( r_n \\\\) has word length at most \\\\( 3n \\\\) (since it can be expressed as the product of at most \\\\( n \\\\) transpositions together with \\\\( 2n \\\\) movements in the \\\\( Z \\\\) coordinate), and hence\\n\\n\\\\[\\n\\\\sum_{n \\\\geq 1} \\\\frac{\\\\| (r_n, 0) \\\\|_{\\\\text{std}}^{1-\\\\varepsilon}}{n(n+1)} \\\\leq 3^{1-\\\\varepsilon} \\\\sum_{n \\\\geq 1} \\\\frac{n^{1-\\\\varepsilon}}{n(n+1)},\\n\\\\]\\n\\nwhich is finite. Hence, \\\\( \\\\mu \\\\) has a finite \\\\( (1-\\\\varepsilon) \\\\)-moment.\\n\\nLet us show that the value \\\\( F_n(0), n \\\\geq 1 \\\\), almost surely changes infinitely often. By definition of the group operation and the \\\\( \\\\mu \\\\)-random walk, we can write \\\\( F_n = F_{n-1} \\\\circ (S_n, \\\\sigma_n) \\\\). Hence, \\\\( F_n(0) \\\\neq F_{n-1}(0) \\\\) if and only if \\\\( S_n \\\\cdot \\\\sigma_n(0) \\\\neq 0 \\\\), which can be rewritten as \\\\( \\\\sigma_n(-S_n) \\\\neq -S_n \\\\), by using the definition of the action of \\\\( Z \\\\) on \\\\( \\\\text{FSym}(Z) \\\\) (here we use an additive notation for the group operation on \\\\( Z \\\\)).\\n\\nThe induced random walk on \\\\( Z \\\\) is drifted to the negative numbers, and hence almost surely \\\\( S_n \\\\xrightarrow{n \\\\to \\\\infty} -\\\\infty \\\\). Also, at time \\\\( n \\\\) the projection to \\\\( Z \\\\) satisfies \\\\( S_n \\\\geq -n \\\\), since the\\n\\n\\\\[\\n\\\\sum_{|x| \\\\leq (n-1)N_{k_j-1}} \\\\beta_{k_j}^q(x) \\\\theta(-x) \\\\\\\\\\n\\\\leq \\\\sum_{|x| \\\\leq (n-1)N_{k_j-1}} \\\\beta_{k_j}^q(x) \\\\\\\\\\n= \\\\beta_{k_j}^q \\\\left( \\\\left[ -(n-1)N_{k_j-1}, (n-1)N_{k_j-1} \\\\right] \\\\right) \\\\\\\\\\n\\\\leq \\\\varepsilon_n,\\n\\\\]\\n\\nwhere we used the fact that \\\\( \\\\beta_{k_j} \\\\) satisfies Equation (2), with \\\\( k_j \\\\geq p_n \\\\).\\n\\nWe conclude that \\\\( \\\\nu^{\\\\ast n}(0) = \\\\gamma_1(0) + \\\\gamma_2(0) \\\\leq 2\\\\varepsilon_n \\\\), which finishes the proof. \\\\( \\\\square \\\\)\\n\\nIf we weaken the hypothesis \\\\( \\\\mathbb{E}(|\\\\text{supp}(\\\\sigma_1)|) < \\\\infty \\\\) from Lemma 4.10 it is possible that the permutation coordinate never stabilizes. Indeed, with ideas similar to an example of [Kai83], we obtain the following.\\n\\n**Proposition 4.14.** The group \\\\( \\\\text{Shuffler}(\\\\mathbb{Z}) \\\\) admits probability measures \\\\( \\\\mu \\\\) with an infinite first moment and a finite \\\\( (1-\\\\varepsilon) \\\\)-moment, for every \\\\( 0 < \\\\varepsilon < 1 \\\\), that induce a transient random walk on \\\\( Z \\\\) and for which the permutation coordinate of the \\\\( \\\\mu \\\\)-random walk does not stabilize. Such measures can be chosen to satisfy \\\\( \\\\mathbb{E}(|\\\\text{supp}(\\\\sigma_1)|) = \\\\infty \\\\) and \\\\( \\\\mathbb{E}(|\\\\text{supp}(\\\\sigma_1)|^{1-\\\\varepsilon}) < \\\\infty \\\\) for every \\\\( 0 < \\\\varepsilon < 1 \\\\).\\n\\n**Proof.** For each \\\\( n \\\\geq 1 \\\\), denote by \\\\( r_n : \\\\mathbb{Z} \\\\to \\\\mathbb{Z} \\\\) the permutation\\n\\n\\\\[\\nr_n(x) = \\\\begin{cases} \\n  x + 1, & \\\\text{if } 0 \\\\leq x < n - 1, \\\\\\\\\\n  0, & \\\\text{if } x = n - 1, \\\\\\\\\\n  x, & \\\\text{otherwise}.\\n\\\\end{cases}\\n\\\\]\\n\\nWe define the measure \\\\( \\\\mu \\\\) on \\\\( \\\\text{Shuffler}(\\\\mathbb{Z}) \\\\) as follows. Let\\n\\n\\\\[\\n\\\\mu((\\\\text{id}, 1)) = 1/8, \\\\quad \\\\mu((\\\\text{id}, -1)) = 3/8,\\n\\\\]\\n\\nand\\n\\n\\\\[\\n\\\\mu((r_n, 0)) = \\\\frac{1}{2n(n+1)}, \\\\quad \\\\text{for } n \\\\geq 1.\\n\\\\]\\n\\nNote that \\\\( \\\\sum_{n \\\\geq 1} \\\\frac{1}{n(n+1)} = 1 \\\\), so that \\\\( \\\\mu \\\\) is indeed a probability measure. Also note that \\\\( |\\\\text{supp}(r_n)| = n \\\\). From this, the fact that the harmonic series \\\\( \\\\sum_{n \\\\geq 1} \\\\frac{1}{n} \\\\) diverges implies that \\\\( \\\\mathbb{E}(|\\\\text{supp}(\\\\sigma_1)|) \\\\) is infinite. Moreover, since \\\\( \\\\| (r_n, 0) \\\\|_{\\\\text{std}} \\\\geq |\\\\text{supp}(r_n)| \\\\), we also have that \\\\( \\\\mu \\\\) has an infinite first moment. On the other hand, for every \\\\( \\\\varepsilon > 0 \\\\) the series \\\\( \\\\sum_{n \\\\geq 1} \\\\frac{n^{1-\\\\varepsilon}}{n(n+1)} \\\\) is convergent and thus \\\\( \\\\mathbb{E}(|\\\\text{supp}(\\\\sigma_1)|^{1-\\\\varepsilon}) \\\\) is finite. The element \\\\( r_n \\\\) has word length at most \\\\( 3n \\\\) (since it can be expressed as the product of at most \\\\( n \\\\) transpositions together with \\\\( 2n \\\\) movements in the \\\\( Z \\\\) coordinate), and hence\\n\\n\\\\[\\n\\\\sum_{n \\\\geq 1} \\\\frac{\\\\| (r_n, 0) \\\\|_{\\\\text{std}}^{1-\\\\varepsilon}}{n(n+1)} \\\\leq 3^{1-\\\\varepsilon} \\\\sum_{n \\\\geq 1} \\\\frac{n^{1-\\\\varepsilon}}{n(n+1)},\\n\\\\]\\n\\nwhich is finite. Hence, \\\\( \\\\mu \\\\) has a finite \\\\( (1-\\\\varepsilon) \\\\)-moment.\\n\\nLet us show that the value \\\\( F_n(0), n \\\\geq 1 \\\\), almost surely changes infinitely often. By definition of the group operation and the \\\\( \\\\mu \\\\)-random walk, we can write \\\\( F_n = F_{n-1} \\\\circ (S_n, \\\\sigma_n) \\\\). Hence, \\\\( F_n(0) \\\\neq F_{n-1}(0) \\\\) if and only if \\\\( S "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hence, surely has an infinite first moment. On the other hand, for every \\\\( \\\\mu \\\\) definition of the group operation and the \\\\( - \\\\) convergent and thus \\\\( |3| \\\\), we obtain the following.\\n\\n\\\\[\\n\\\\sum_{n \\\\geq 1} \\\\frac{1}{n(n+1)} = 1,\\n\\\\]\\n\\nso that \\\\( \\\\sum_{n \\\\geq 1} \\\\frac{1}{n} \\\\) diverges implies that \\\\( \\\\mathbb{E}(|\\\\text{supp}(\\\\sigma_1)|) \\\\) is infinite. Moreover, since \\\\( \\\\| (r_n, 0) \\\\|_{\\\\text{std}} \\\\geq |\\\\text{supp}(r_n)| \\\\), we also have that \\\\( \\\\mu \\\\) has an infinite first moment. On the other hand, for every \\\\( \\\\varepsilon > 0 \\\\) the series \\\\( \\\\sum_{n \\\\geq 1} \\\\frac{n^{1-\\\\varepsilon}}{n(n+1)} \\\\) is convergent and thus \\\\( \\\\mathbb{E}(|\\\\text{supp}(\\\\sigma_1)|^{1-\\\\varepsilon}) \\\\) is finite. The element \\\\( r_n \\\\) has word length at most \\\\( 3n \\\\) (since it can be expressed as the product of at most \\\\( n \\\\) transpositions together with \\\\( 2n \\\\) movements in the \\\\( Z \\\\) coordinate), and hence\\n\\n\\\\[\\n\\\\sum_{n \\\\geq 1} \\\\frac{\\\\| (r_n, 0) \\\\|_{\\\\text{std}}^{1-\\\\varepsilon}}{n(n+1)} \\\\leq 3^{1-\\\\varepsilon} \\\\sum_{n \\\\geq 1} \\\\frac{n^{1-\\\\varepsilon}}{n(n+1)},\\n\\\\]\\n\\nwhich is finite. Hence, \\\\( \\\\mu \\\\) has a finite \\\\( (1-\\\\varepsilon) \\\\)-moment.\\n\\nLet us show that the value \\\\( F_n(0), n \\\\geq 1 \\\\), almost surely changes infinitely often. By definition of the group operation and the \\\\( \\\\mu \\\\)-random walk, we can write \\\\( F_n = F_{n-1} \\\\circ (S_n, \\\\sigma_n) \\\\). Hence, \\\\( F_n(0) \\\\neq F_{n-1}(0) \\\\) if and only if \\\\( S_n \\\\cdot \\\\sigma_n(0) \\\\neq 0 \\\\), which can be rewritten as \\\\( \\\\sigma_n(-S_n) \\\\neq -S_n \\\\), by using the definition of the action of \\\\( Z \\\\) on \\\\( \\\\text{FSym}(Z) \\\\) (here we use an additive notation for the group operation on \\\\( Z \\\\)).\\n\\nThe induced random walk on \\\\( Z \\\\) is drifted to the negative numbers, and hence almost surely \\\\( S_n \\\\xrightarrow{n \\\\to \\\\infty} -\\\\infty \\\\). Also, at time \\\\( n \\\\) the projection to \\\\( Z \\\\) satisfies \\\\( S_n \\\\geq -n \\\\), since the\\n\\n\\\\[\\n\\\\sum_{|x| \\\\leq (n-1)N_{k_j-1}} \\\\beta_{k_j}^q(x) \\\\theta(-x) \\\\\\\\\\n\\\\leq \\\\sum_{|x| \\\\leq (n-1)N_{k_j-1}} \\\\beta_{k_j}^q(x) \\\\\\\\\\n= \\\\beta_{k_j}^q \\\\left( \\\\left[ -(n-1)N_{k_j-1}, (n-1)N_{k_j-1} \\\\right] \\\\right) \\\\\\\\\\n\\\\leq \\\\varepsilon_n,\\n\\\\]\\n\\nwhere we used the fact that THE POISSON BOUNDARY OF LAMPSHUFFLER GROUPS\\n\\n\\\\[\\n\\\\nu^\\\\ast n(0) = \\\\gamma_1(0) + \\\\gamma_2(0) \\\\leq 2\\\\varepsilon_n,\\n\\\\]\\n\\nwhich finishes the proof.\\n\\nIf we weaken the hypothesis \\\\( \\\\mathbb{E}(|\\\\text{supp}(\\\\sigma_1)|) < \\\\infty \\\\) from Lemma 4.10 it is possible that the permutation coordinate never stabilizes. Indeed, with ideas similar to an example of [Kai83], we obtain the following.\\n\\n**Proposition 4.14.** The group Shuffler(\\\\( Z \\\\)) admits probability measures \\\\( \\\\mu \\\\) with an infinite first moment and a finite \\\\( (1-\\\\varepsilon) \\\\)-moment, for every \\\\( 0 < \\\\varepsilon < 1 \\\\), that induce a transient random walk on \\\\( Z \\\\) and for which the permutation coordinate of the \\\\( \\\\mu \\\\)-random walk does not stabilize. Such measures can be chosen to satisfy \\\\( \\\\mathbb{E}(|\\\\text{supp}(\\\\sigma_1)|) = \\\\infty \\\\) and \\\\( \\\\mathbb{E}(|\\\\text{supp}(\\\\sigma_1)|^{1-\\\\varepsilon}) < \\\\infty \\\\) for every \\\\( 0 < \\\\varepsilon < 1 \\\\).\\n\\n**Proof.** For each \\\\( n \\\\geq 1 \\\\), denote by \\\\( r_n : Z \\\\to Z \\\\) the permutation\\n\\n\\\\[\\nr_n(x) = \\\\begin{cases} \\n  x + 1, & \\\\text{if } 0 \\\\leq x < n - 1, \\\\\\\\\\n  0, & \\\\text{if } x = n - 1, \\\\\\\\\\n  x, & \\\\text{otherwise}.\\n\\\\end{cases}\\n\\\\]\\n\\nWe define the measure \\\\( \\\\mu \\\\) on Shuffler(\\\\( Z \\\\)) as follows. Let\\n\\n\\\\[\\n\\\\mu((\\\\text{id}, 1)) = 1/8, \\\\quad \\\\mu((\\\\text{id}, -1)) = 3/8,\\n\\\\]\\n\\nand\\n\\n\\\\[\\n\\\\mu((r_n, 0)) = \\\\frac{1}{2n(n+1)}, \\\\quad \\\\text{for } n \\\\geq 1.\\n\\\\]\\n\\nNote that \\\\( \\\\sum_{n \\\\geq 1} \\\\frac{1}{n(n+1)} = 1 \\\\), so that \\\\( \\\\mu \\\\) is indeed a probability measure. Also note that \\\\( |\\\\text{supp}(r_n)| = n \\\\). From this, the fact that the harmonic series \\\\( \\\\sum_{n \\\\geq 1} \\\\frac{1}{n} \\\\) diverges implies that \\\\( \\\\mathbb{E}(|\\\\text{supp}(\\\\sigma_1)|) \\\\) is infinite. Moreover, since \\\\( \\\\| (r_n, 0) \\\\|_{\\\\text{std}} \\\\geq |\\\\text{supp}(r_n)| \\\\), we also have that \\\\( \\\\mu \\\\) has an infinite first moment. On the other hand, for every \\\\( \\\\varepsilon > 0 \\\\) the series \\\\( \\\\sum_{n \\\\geq 1} \\\\frac{n^{1-\\\\varepsilon}}{n(n+1)} \\\\) is convergent and thus \\\\( \\\\mathbb{E}(|\\\\text{supp}(\\\\sigma_1)|^{1-\\\\varepsilon}) \\\\) is finite. The element \\\\( r_n \\\\) has word length at most \\\\( 3n \\\\) (since it can be expressed as the product of at most \\\\( n \\\\) transpositions together with \\\\( 2n \\\\) movements in the \\\\( Z \\\\) coordinate), and hence\\n\\n\\\\[\\n\\\\sum_{n \\\\geq 1} \\\\frac{\\\\| (r_n, 0) \\\\|_{\\\\text{std}}^{1-\\\\varepsilon}}{n(n+1)} \\\\leq 3^{1-\\\\varepsilon} \\\\sum_{n \\\\geq 1} \\\\frac{n^{1-\\\\varepsilon}}{n(n+1)},\\n\\\\]\\n\\nwhich is finite. Hence, \\\\( \\\\mu \\\\) has a finite \\\\( (1-\\\\varepsilon) \\\\)-moment.\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.3 Metrics\\n\\nThree metrics were widely used across the papers: accuracy, specificity, and sensitivity. The equations for these four metrics can be seen below.\\n\\n\\\\[\\n\\\\text{Accuracy} = \\\\frac{\\\\text{Correctly classified speech}}{\\\\text{Total speech samples}} \\\\tag{1}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Specificity} = \\\\frac{\\\\text{Correctly classified healthy speech}}{\\\\text{Total healthy speech}} \\\\tag{2}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Sensitivity} = \\\\frac{\\\\text{Correctly classified pathological speech}}{\\\\text{Total pathological speech}} \\\\tag{3}\\n\\\\]\\n\\nAnother metric was often used in the papers using multi-class classification - unweighted average recall (UAR). This metric is calculated by averaging the recall value for each of the specific pathologies included in the dataset. Equation 4 shows how it is calculated, where \\\\( N \\\\) is the number of pathologies in the dataset and \\\\( R_i \\\\) is the recall of the \\\\( i \\\\)th pathology in the dataset.\\n\\n\\\\[\\n\\\\text{UAR} = \\\\frac{\\\\sum_{i=1}^{N} R_i}{N} \\\\tag{4}\\n\\\\]\\n\\n2.4 Binary Classification Literature\\n\\n? (2020) investigate the classification of cancer patients from healthy controls using six machine learning algorithms. The dataset used includes recordings of the prolonged vowel /ah/ from 50 male laryngeal\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.3 Metrics\\n\\nThree metrics were widely used across the papers: accuracy, specificity, and sensitivity. The equations for these four metrics can be seen below. \\n\\n\\\\[\\n\\\\text{Accuracy} = \\\\frac{\\\\text{Correctly classified speech}}{\\\\text{Total speech samples}} \\\\tag{1}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Specificity} = \\\\frac{\\\\text{Correctly classified healthy speech}}{\\\\text{Total healthy speech}} \\\\tag{2}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Sensitivity} = \\\\frac{\\\\text{Correctly classified pathological speech}}{\\\\text{Total pathological speech}} \\\\tag{3}\\n\\\\]\\n\\nAnother metric was often used in the papers using multi-class classification unweighted average recall (UAR). This metric is calculated by averaging the recall value for each of the specific pathologies included in the dataset. Equation 4 shows how it is calculated, where N is the number of pathologies in the dataset and \\\\( R_i \\\\) is the recall of the ith pathology in the dataset. \\n\\n\\\\[\\n\\\\text{UAR} = \\\\frac{\\\\sum_{i=1}^{N} R_i}{N} \\\\tag{4}\\n\\\\]\\n\\n2.4 Binary Classification Literature\\n\\n? (2020) investigate the classification of cancer patients from healthy controls using six machine learning algorithms. The dataset used includes recordings of the prolonged vowel /ah/ from 50 male laryngeal\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"for this paper, since a majority of features in keystroke sounds are within the lower frequencies [15, 3, 4] and would therefore be less distinguishable on a linear scale. Meanwhile, MFCC involves performing the discrete cosine transform on a mel-spectrogram, producing a compressed representation that prioritises the frequencies used in human speech. Since, for this paper, human speech is not the target, and the removal of frequencies could risk the loss of relevant data, MFCC was decided to be less suitable than mel-spectrograms.\\n\\n**Data augmentation:** Prior to feature extraction, signals were time-shifted randomly by up to 40% in either direction. This time shifting is an instance of data augmentation, in which the amount of data input to a DL model is artificially increased by slightly adjusting existing inputs [28]. The mel-spectrograms were then generated using 64 mel bands, a window length of 1024 samples and hop length of 500 (255 for the MacBook keystrokes, given their shorter length), resulting in 64x64 images. Using the spectrograms, a second method of data augmentation was implemented called masking. This method involves taking a random 10% of both the time and frequency axis and setting all values within those ranges to the mean of the spectrogram, essentially \u2018blocking out\u2019 a portion of the image. Using time warping and spectrogram masking combined is called SpecAugment and was found to encourage the model to generalise and avoid overfitting the training data [25, 10].\\n\\nHaving converted keystrokes from each data set into a more visual medium, more direct comparisons could be made. MacBook keystrokes (similar to the keystrokes examined in the literature [4, 39, 6]) have only 2 visible peaks: the \u2018push\u2019 and \u2018release\u2019 peaks respectively. The 2 peak structures shown in Fig. 2 are similar to each other, implying that such a structure is native to the MacBook keyboard regardless of recording method, a noticeable difference however is the large range of frequencies present in the zoom recording. The Zoom peaks extend much higher than that of the phone-based recordings, indicating significant data in multiple frequencies that were not present when recorded via phone.\\n\\nThe overall data preparation procedure for our data was inspired by the structure presented in [10] and is shown in Fig. 3.\\n\\n### 3.2 Model Selection and Implementation\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"for this paper, since a majority of features in keystroke sounds are within the lower frequencies [15, 3, 4] and would therefore be less distinguishable on a linear scale. Meanwhile, MFCC involves performing the discrete cosine transform on a mel-spectrogram, producing a compressed representation that prioritises the frequencies used in human speech. Since, for this paper, human speech is not the target, and the removal of frequencies could risk the loss of relevant data, MFCC was decided to be less suitable than mel-spectrograms. \\n\\n**Data augmentation:** Prior to feature extraction, signals were time-shifted randomly by up to 40% in either direction. This time shifting is an instance of data augmentation, in which the amount of data input to a DL model is artificially increased by slightly adjusting existing inputs [28]. The mel-spectrograms were then generated using 64 mel bands, a window length of 1024 samples and hop length of 500 (255 for the MacBook keystrokes, given their shorter length), resulting in 64x64 images. Using the spectrograms, a second method of data augmentation was implemented called masking. This method involves taking a random 10].% of both the time and frequency axis and setting all values within those ranges to the mean of the spectrogram, essentially \u2018blocking out\u2019 a portion of the image. Using time warping and spectrogram masking combined is called SpecAugment and was found to encourage the model to generalise and avoid overfitting the training data [25, 10].\\n\\nHaving converted keystrokes from each Figure 2: Waveform and corresponding mel-spectrogram of Left: Phone recording, and Right: Zoom recording. data set into a more visual medium, more direct comparisons could be made. MacBook keystrokes (similar to the keystrokes examined in the literature [4, 39, 6]) have only 2 visible peaks: the \u2018push\u2019 and \u2018release\u2019 peaks respectively. The 2 peak structures shown in Fig. 2 are similar to each other, implying that such a structure is native to the MacBook keyboard regardless of recording method, a noticeable difference however is the large range of frequencies present in the zoom recording. The Zoom peaks extend much higher than that of the phone-based recordings, indicating significant data in multiple frequencies that were not present when recorded via phone. \\n\\nThe overall data preparation procedure for our data was inspired by the structure presented in [10] and is shown in Fig. 3. \\n\\n### 3.2 Model Selection and Implementation\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abreu, E., \u00d3, J. & Medeiros, E. Properties of positive harmonic functions on the half-space with a nonlinear boundary condition. *J. Differential Equations*. **248**, 617-637 (2010), [https://doi.org/10.1016/j.jde.2009.07.006](https://doi.org/10.1016/j.jde.2009.07.006)\\n\\nAdams, R. & Fournier, J. Sobolev spaces. (Elsevier/Academic Press, Amsterdam, 2003)\\n\\nAleksandrov, A. Uniqueness theorems for surfaces in the large. I. *Amer. Math. Soc. Transl. (2)*. **21** pp. 341-354 (1962), [https://doi.org/10.1090/trans2/021/09](https://doi.org/10.1090/trans2/021/09)\\n\\nAlexandrov, A. A characteristic property of spheres. *Ann. Mat. Pura Appl. (4)*. **58** pp. 303-315 (1962), [https://doi.org/10.1007/BF02413056](https://doi.org/10.1007/BF02413056)\\n\\nAllegretto, W. & Huang, Y. A Picone\u2019s identity for the p-Laplacian and applications. *Nonlinear Anal.* **32**, 819-830 (1998), [https://doi.org/10.1016/S0362-546X](https://doi.org/10.1016/S0362-546X)(97)00530-0\\n\\nBonder, J. & Rossi, J. Existence results for the p-Laplacian with nonlinear boundary conditions. *J. Math. Anal. Appl.* **263**, 195-223 (2001), [https://doi.org/10.1006/jmaa.2001.7609](https://doi.org/10.1006/jmaa.2001.7609)\\n\\nChipot, M., Chleb\u00edk, M., Fila, M. & Shafrir, I. Existence of positive solutions of a semilinear elliptic equation in $\\\\mathbb{R}^n_+$ with a nonlinear boundary condition. *J. Math. Anal. Appl.* **223**, 429-471 (1998), [https://doi.org/10.1006/jmaa.1998.5958](https://doi.org/10.1006/jmaa.1998.5958)\\n\\nCuesta, M. & Tak\u00e1\u010d, P. A strong comparison principle for positive solutions of degenerate elliptic equations. *Differential Integral Equations*. **13**, 721-746 (2000)\\n\\nDamascelli, L. & Pacella, F. Monotonicity and symmetry of solutions of p-Laplace equations, $1 < p < 2$, via the moving plane method. *Ann. Scuola Norm. Sup. Pisa Cl. Sci. (4)*. **26**, 689-707 (1998), [http://www.numdam.org/item?id=ASNSP_1998](http://www.numdam.org/item?id=ASNSP_1998)\\(4\\)26\\(4\\)689_0\\n\\nDamascelli, L. & Sciunzi, B. Regularity, monotonicity and symmetry of positive solutions of m-Laplace equations. *J. Differential Equations*. **206**, 483-515 (2004), [https://doi.org/10.1016/j.jde.2004.05.012](https://doi.org/10.1016/j.jde.2004.05.012)\\n\\nDegiovanni, M., Musesti, A. & Squassina, M. On the regularity of solutions in the Pucci-Serrin identity. *Calc. Var. Partial Differential Equations*. **18**, 317-334 (2003), [https://doi.org/10.1007/s00526-003-0208-y](https://doi.org/10.1007/s00526-003-0208-y)\\n\\n\u00d3, J. & Medeiros, E. Remarks on least energy solutions for quasilinear elliptic problems in $\\\\mathbb{R}^N$. *Electron. J. Differential Equations*. pp. No. 83, 14 (2003)\\n\\nEscobar, J. Sharp constant in a Sobolev trace inequality. *Indiana Univ. Math. J.*. **37**, 687-698 (1988), [https://doi.org/10.1512/iumj.1988.37.37033](https://doi.org/10.1512/iumj.1988.37.37033)\\n\\nFarina, A., Montoro, L. & Sciunzi, B. Monotonicity and one-dimensional symmetry for solutions of $-\\\\Delta_p u = f(u)$ in half-spaces. *Calc. Var. Partial Differential Equations*. **43**, 123-145 (2012), [https://doi.org/10.1007/s00526-011-0405-z](https://doi.org/10.1007/s00526-011-0405-z)\\n\\nFarina, A., Montoro, L. & Sciunzi, B. Monotonicity of solutions of quasilinear degenerate elliptic equation in half-spaces. *Math. Ann.*. **357**, 855-893 (2013), [https://doi.org/10.1007/s00208-013-0919-0](https://doi.org/10.1007/s00208-013-0919-0)\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abreu, E., \u00d3, J. & Med CNPq through grant 308900/2019-7. R. Clemente acknowledges partial support from CNPq through grant 304454/2022-2. \\n\\nAbreu, E., \u00b4 O, J. & Medeiros, E. Properties of positive harmonic functions on the half-space with a nonlinear boundary condition. *J. Differential Equations*. **248**, 617-637 (2010), Adams, R. & Fournier, J. Sobolev spaces. (Elsevier/Academic Press, Amsterdam,2003) Aleksandrov, A. Uniqueness theorems for surfaces in the large. I. *Amer. Math. Soc. Transl. (2)*. **21** pp. 341-354 (1962), [https://doi.org/10.1016/j.jde.2009.07.006](https://doi.org/10.1016/j.jde.2009.07.006) [https://doi.org/10.1090/trans2/021/09](https://doi.org/10.1090/trans2/021/09) Alexandrov, A. A characteristic property of spheres. *Ann. Mat. Pura Appl. (4)*. **58** pp. 303-315 (1962), [https://doi.org/10.1007/BF02413056](https://doi.org/10.1007/BF02413056) Allegretto, W. & Huang, Y. A Picone\u2019s identity for the p-Laplacian and applications. *Nonlinear Anal.* **32**, 819-830 (1998), [https://doi.org/10.1016/S0362-546X](https://doi.org/10.1016/S0362-546X)(97)00530-0 Bonder, J. & Rossi, J. Existence results for the p-Laplacian with nonlinear boundary conditions. *J. Math. Anal. Appl.**. **263**, 195-223 (2001), Chipot, M., Chleb\u0131\u00b4\u0131k, M., Fila, M. & Shafrir, I. Existence of positive solutions of a semilinear elliptic equation in $\\\\mathbb{R}^n_+$ with a nonlinear boundary condition. *J. Math. Anal. Appl.* **223**, 429-471 (1998), [https://doi.org/10.1006/jmaa.2001.7609](https://doi.org/10.1006/jmaa.2001.7609) [https://doi.org/10.1006/jmaa.1998.5958](https://doi.org/10.1006/jmaa.1998.5958) Cuesta, M. & Tak\u00b4a\u02c7c, P. A strong comparison principle for positive solutions of degenerate elliptic equations. *Differential Integral Equations*. **13**, 721-746 (2000) Damascelli, L. & Pacella, F. Monotonicity and symmetry of solutions of p-Laplace equations, 1 < p < 2, via the moving plane method. *Ann. Scuola Norm. Sup. Pisa Cl. Sci. (4)*. **26**, 689-707 (1998), [http://www.numdam.org/item?id=ASNSP](http://www.numdam.org/item?id=ASNSP) 1998 4 26 4 689 0 Damascelli, L. & Sciunzi, B. Regularity, monotonicity and symmetry of positive solutions of m-Laplace equations. *J. Differential Equations*. **206**, 483-515 (2004), Degiovanni, M., Musesti, A. & Squassina, M. On the regularity of solutions in the Pucci-Serrin identity. *Calc. Var. Partial Differential Equations*. **18**, 317-334 (2003), \u00b4O, J. & Medeiros, E. Remarks on least energy solutions for quasilinear elliptic problems in $\\\\mathbb{R}^N$. *Electron. J. Differential Equations*. pp. No. 83, 14 (2003) Escobar, J. Sharp constant in a Sobolev trace inequality. *Indiana Univ. Math. J.*. **37**, 687-698 (1988), [https://doi.org/10.1016/j.jde.2004.05.012](https://doi.org/10.1016/j.jde.2004.05.012) [https://doi.org/10.1007/s00526-003-0208-y](https://doi.org/10.1007/s00526-003-0208-y) [https://doi.org/10.1512/iumj.1988.37.37033](https://doi.org/10.1512/iumj.1988.37.37033) Farina, A., Montoro, L. & Sciunzi, B. Monotonicity and one-dimensional symmetry for solutions of $-\\\\Delta_p u = f(u)$ in half-spaces. *Calc. Var. Partial Differential Equations*. **43**, 123-145 (2012), [https://doi.org/10.1007/s00526-011-0405-z](https://doi.org/10.1007/s00526-011-0405-z) Farina, A., Montoro, L. & Sciunzi, B. Monotonicity of solutions of quasilinear degenerate elliptic equation in half-spaces. *Math. Ann.* **357**, 855-893 (2013), [https://doi.org/10.1007/s00208-013-0919-0](https://doi.org/10.1007/s00208-013-0919-0)\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"to $Ax = b$ has full support. By Proposition 4, columns of $A$ are integrally independent. It follows from Theorem 6 that inequality (5) holds.\\n\\n**Remark 1.** We demonstrate how to modify the proof of Theorem 6 to obtain the bound (7) given in [AADLO22]. We use the same notation as in the proof of Theorem 6. For any $m \\\\times m$ submatrix of $A$ whose columns are indexed by $J$ where $|J| = m$, we have\\n\\n$$\\\\left| \\\\det(A_{[m] \\\\times J}) \\\\right| = |\\\\det(D)| \\\\cdot |\\\\det((U^{-1})_{[m] \\\\times J})|$$\\n\\n$$= \\\\gcd(A) \\\\cdot |\\\\det((U^{-1})_{[m] \\\\times J})|$$\\n\\n$$= \\\\gcd(A) \\\\cdot |\\\\det(U_{[n] \\\\setminus J \\\\times [m+1:n]}|).$$\\n\\nWe also know that\\n\\n$$\\\\prod_{i \\\\in [n] \\\\setminus J} q_i \\\\left| \\\\det(U_{[n] \\\\setminus J \\\\times [m+1:n]}) \\\\right|,$$\\n\\nand thus\\n\\n$$\\\\prod_{i \\\\in [n] \\\\setminus J} q_i \\\\left| \\\\frac{\\\\det(A_{[m] \\\\times J})}{\\\\gcd(A)} \\\\right|,$$\\n\\nwhere $q_i, i \\\\in [n] \\\\setminus J$ are primes numbers and with the same prime repeating at most $m$ times in $\\\\{q_i \\\\mid i \\\\in [n] \\\\setminus J\\\\}$. Recall notation $\\\\Omega_m(z) = \\\\sum_{i=1}^{k} \\\\min\\\\{s_i, m\\\\}$ for the prime factorization of $z = r_1^{s_1} \\\\cdots r_k^{s_k}$ with multiplicities $s_1, ..., s_k \\\\in \\\\mathbb{Z}_{\\\\geq 0}$. Clearly, when $x \\\\mid y$, $\\\\Omega_m(x) \\\\leq \\\\Omega_m(y)$. Thus, $\\\\Omega_m(\\\\prod_{i \\\\in [n] \\\\setminus J} q_i) \\\\leq \\\\Omega_m\\\\left(\\\\frac{|\\\\det(A_{[m] \\\\times J})|}{\\\\gcd(A)}\\\\right)$. Moreover, since the multiplicity of each $q_i$ in $\\\\prod_{i \\\\in [n] \\\\setminus J} q_i$ is at most $m$, we have $\\\\Omega_m(\\\\prod_{i \\\\in [n] \\\\setminus J} q_i) = \\\\left|\\\\{n\\\\} \\\\setminus J\\\\right| = n - m$. Therefore, $n - m \\\\leq \\\\Omega_m\\\\left(\\\\frac{|\\\\det(A_{[m] \\\\times J})|}{\\\\gcd(A)}\\\\right)$. Since $J$ is an arbitrary subset of $[n]$ with cardinality $m$, we obtain $n \\\\leq m + \\\\min_{r \\\\in \\\\mathbb{Z}_{\\\\geq 0}} \\\\Omega_m\\\\left(\\\\frac{|\\\\det(A_{[m] \\\\times J})|}{\\\\gcd(A)}\\\\right)$. Applying the same argument as in the proof of Theorem 2 above, we obtain $f(A) \\\\leq m + \\\\min_{r \\\\in \\\\mathbb{Z}_{\\\\geq 0}} \\\\Omega_m\\\\left(\\\\frac{|\\\\det(A_{[m] \\\\times J})|}{\\\\gcd(A)}\\\\right)$.\\n\\n### 3.3 Upper bound on $h(m, t)$\\n\\nRecall $h(m, t)$ is the maximum, taken over all integer matrices $A$ with $m$ rows and largest entry $t$, of the smallest support size of an integer solution to $Ax = b$ for some $b \\\\in \\\\mathcal{L}(A)$ (formally defined in (3)). Using a relation between the size of largest entry of a matrix and the size of its subdeterminant, we want to use results in Section 3.2 to obtain an upper bound for $h(m, t)$. We will use the well-known Hadamard inequality [Had93], which gives us an upper bound on the determinant of a matrix, i.e. for any matrix $B \\\\in \\\\mathbb{R}^{m \\\\times m}$,\\n\\n$$|\\\\det(B)| \\\\leq (\\\\sqrt{m}|B|_{\\\\infty})^m.$$\\n\\nApplying this to inequality (5), we obtain the following corollary.\\n\\n**Corollary 9.** $h(m, t)$ satisfies\\n\\n$$\\\\left(\\\\sqrt{m} t\\\\right)^m \\\\geq p_2^m p_3^m \\\\cdots p_{\\\\left\\\\lfloor \\\\frac{m}{2} \\\\right\\\\rfloor}^m \\\\left|\\\\frac{h(m, t)}{m}\\\\right|^{\\\\frac{h(m, t)}{m}}.$$\\n\\n(9)\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"to $Ax = b$ has full support. By Proposition 4 , columns of $A$ are integrally independent. It follows from Theorem 6 to obtain the bound ( 7 ) given in [ AADLO22 ]. We use the same notation as in the proof of Theorem 6 . For any $m \\\\times m$ submatrix of $A$ whose columns are indexed by $J$ where $|J| = m$, we have\\n\\n$$\\\\left| \\\\det(A_{[m] \\\\times J}) \\\\right| = |\\\\det(D)| \\\\cdot |\\\\det((U^{-1})_{[m] \\\\times J})|$$\\n\\n$$= \\\\gcd(A) \\\\cdot |\\\\det((U^{-1})_{[m] \\\\times J})|$$\\n\\n$$= \\\\gcd(A) \\\\cdot |\\\\det(U_{[n] \\\\setminus J \\\\times [m+1:n]})|.$$ \\n\\nWe also know that\\n\\n$$\\\\prod_{i \\\\in [n] \\\\setminus J} q_i \\\\left| \\\\det(U_{[n] \\\\setminus J \\\\times [m+1:n]}) \\\\right|,$$\\n\\nand thus\\n\\n$$\\\\prod_{i \\\\in [n] \\\\setminus J} q_i \\\\left| \\\\frac{\\\\det(A_{[m] \\\\times J})}{\\\\gcd(A)} \\\\right|,$$\\n\\nwhere $q_i, i \\\\in [n] \\\\setminus J$ are primes numbers and with the same prime repeating at most $m$ times in $\\\\{q_i \\\\mid i \\\\in [n] \\\\setminus J\\\\}$. Recall notation $\\\\Omega_m(z) = \\\\sum_{i=1}^{k} \\\\min\\\\{s_i, m\\\\}$ for the prime factorization of $z = r_1^{s_1} \\\\cdots r_k^{s_k}$ with multiplicities $s_1, \\\\ldots, s_k \\\\in \\\\mathbb{Z}_{\\\\geq 0}$. Clearly, when $x \\\\mid y$, $\\\\Omega_m(x) \\\\leq \\\\Omega_m(y)$. Thus, $\\\\Omega_m(\\\\prod_{i \\\\in [n] \\\\setminus J} q_i) \\\\leq \\\\Omega_m\\\\left(\\\\frac{|\\\\det(A_{[m] \\\\times J})|}{\\\\gcd(A)}\\\\right)$. Moreover, since the multiplicity of each $q_i$ in $\\\\prod_{i \\\\in [n] \\\\setminus J} q_i$ is at most $m$, we have $\\\\Omega_m(\\\\prod_{i \\\\in [n] \\\\setminus J} q_i) = |[n] \\\\setminus J| = n - m$. Therefore, $n - m \\\\leq \\\\Omega_m\\\\left(\\\\frac{|\\\\det(A_{[m] \\\\times J})|}{\\\\gcd(A)}\\\\right)$. Since $J$ is an arbitrary subset of $[n]$ with cardinality $m$, we obtain $n \\\\leq m + \\\\min_{r \\\\in \\\\mathbb{R}^m} \\\\Omega_m\\\\left(\\\\frac{|\\\\det(A_{[m] \\\\times J})|}{\\\\gcd(A)}\\\\right)$. Applying the same argument as in the proof of Theorem 2 above, we obtain $f(A) \\\\leq m + \\\\min_{r \\\\in \\\\mathbb{R}^m} \\\\Omega_m\\\\left(\\\\frac{|\\\\det(A_{[m] \\\\times J})|}{\\\\gcd(A)}\\\\right)$.\\n\\n### 3.3 Upper bound on $h(m, t)$\\n\\nRecall $h(m, t)$ is the maximum, taken over all integer matrices $A$ with $m$ rows and largest entry $t$, of the smallest support size of an integer solution to $Ax = b$ for some $b \\\\in \\\\mathcal{L}(A)$ (formally subdeterminant, we want to use results in Section 3.2 to obtain an upper bound for $h(m, t)$). We will use the well-known Hadamard inequality [Had93], which gives us an upper bound on the determinant of a matrix, i.e. for any matrix $B \\\\in \\\\mathbb{R}^{m \\\\times m}$,\\n\\n$$|\\\\det(B)| \\\\leq (\\\\sqrt{m}|B|_{\\\\infty})^m.$$ \\n\\nApplying this to inequality (5), we obtain the following corollary.\\n\\n**Corollary 9.** $h(m, t)$ satisfies\\n\\n$$\\\\left(\\\\sqrt{m} t\\\\right)^m \\\\geq p_2^m p_3^m \\\\cdots p_{\\\\lfloor \\\\frac{m}{2} \\\\rfloor}^m \\\\left|\\\\frac{h(m, t)}{m}\\\\right|^{\\\\frac{h(m, t)}{m}}.$$ \\n\\n(9)\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.9 Comparative Analysis of Smaller Dataset (65 Images) and Larger Dataset (2700 Images) using Principal Component Analysis\\n\\nWe conducted an experiment using Principal Component Analysis (PCA) for image reconstruction with both smaller (65 images) and larger (2700 images) datasets, utilizing 50 principal components for representation. The explained variance for the smaller dataset was approximately 98.19%, while for the larger one it was roughly 98.35% (Figure 18).\\n\\nHowever, visual inspections revealed a marked degradation in quality between the two datasets (Figure 18). In the 65-image dataset, the reconstructions showed anomalies like purplish lines in the background and bluish color distortions. For the 2700-image dataset, the performance was even worse, with greenish horizontal lines appearing in the foreground and failure in capturing essential details. Several factors may contribute to this unexpected result. The increased complexity and variability within the larger dataset might have overwhelmed PCA\u2019s ability to represent finer details. Since PCA relies on linear assumptions, it might have failed to handle the nonlinear structures and dependencies that become more pronounced with the increase in data complexity. The 1.5% \u2013 2% unexplained variance might contain critical information affecting the visual quality, especially in a larger, more intricate dataset. Moreover, the choice of 50 components might have been insufficient for capturing nuanced variations in the larger dataset, despite sufficing for the smaller one. These observations highlight PCA\u2019s limitations in handling highly complex image data and emphasize that capturing a high percentage of variance does not guarantee accurate or visually pleasing reconstruction.\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.9 Comparative Analysis of Smaller Dataset (65 Images) and Larger Dataset (2700 Images) using Principal Component Analysis (PCA) for image reconstruction with both smaller (65 images) and larger (2700 images) datasets, utilizing 50 principal components for representation. The explained variance for the smaller dataset was approximately 98.19%, while for the larger one it was roughly 98.35% (Figure 18). However, visual inspections revealed a marked degradation in quality between the two datasets(Figure 18). In the 65image dataset, the reconstructions showed anomalies like purplish lines in the background and bluish color distortions. For the 2700-image dataset, the performance was even worse, with greenish horizontal lines appearing in the foreground and failure in capturing essential details. Several factors may contribute to this unexpected result. The increased complexity and variability within the larger dataset might have overwhelmed PCA\u2019s ability to represent finer details. Since PCA relies on linear assumptions, it might have failed to handle the nonlinear structures and dependencies that become more pronounced with the increase in data complexity. The 1.5% \u2013 2% unexplained variance might contain critical information affecting the visual quality, especially in a larger, more intricate dataset. Moreover, the choice of 50 components might have been insufficient for capturing nuanced variations in the larger dataset, despite sufficing for the smaller one. These observations highlight PCA\u2019s limitations in handling highly complex image data and emphasize that capturing a high percentage of variance does not guarantee accurate or visually pleasing reconstruction. \"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"beam alignment case, an excessive number of antennas is adverse to the network coverage probability. As for the perfect beam alignment case, Fig. 6(b) shows that, as the number of antennas becomes large, the outage probability decreases, and the trend of decreasing gradually slows down. This figure implies that from the perspective such as hardware cost, it is not necessary to equip the UAV with too many antennas since the performance gain is very small.\\n\\nC. Effect of UAV Deployment Height\\n\\nWith regards to the effect of UAV deployment height, Fig. 6(a) shows that the higher deployment of UAVs will lead to a larger outage probability under the imperfect alignment case. The reason is as follows. The higher height implies the larger coverage area of the main-lobe beam on certain tiers, which introduces more interference from more UAVs to the typical UE with the main-lobe pointed. When the beam is mis-pointed, this kind of effect on SINR becomes worse. Besides that, for the same projection point, the higher height indicates that the transmission link is more likely to be LoS, which means the signal strength from the interfering UAV becomes stronger. Hence, when the misalignment for the beamforming is non-negligible, the lower deployment of UAVs is preferred for both schemes. Under the case of perfect beam alignment, from Fig. 6(b), it can be seen that the outage probability of MAPAS is smaller for the lower height of UAVs. The explanation is the same as before. However, in terms of the performance of CDAS, it seems that the outage probability for the higher altitude of UAVs can be better. This is mainly because of the fact the UE is associated with the closest UAV and their link can be either LoS or NLoS. As mentioned before, for the same projection point, the link from UAV at the lower height is much more likely to be NLoS, which reduces the desired signal strength at the UE whereby degrading the outage performance.\\n\\nD. Effect of UAV Density\\n\\nFig. 7 plots the outage probability versus the number of antennas for UAV under different UAV densities for both imperfect and perfect alignment cases. Under the imperfect beam alignment scenario, as expected, the outage probability drops at first and then rises with the increase in the number of antennas. Moreover, since more interfering UAVs are involved in the system, the higher UAV density leads to worse outage probability performance. Besides that, Fig. 7(a) shows that the optimal number of UAV antennas increases as the UAV density rises, e.g., for MAPAS, the optimal number antennas is 4 for $\\\\lambda_k = 0.5 \\\\times 10^{-5}$ m$^{-2}$ while it goes to 16 for $\\\\lambda_k = 5 \\\\times 10^{-5}$ m$^{-2}$. The reason is as follows. When the UAV density is very sparse, the number of interfering UAVs falling into the region covered by the main-lobe beam of the UE is very small. In other words, the interference is not that severe. Then the serving UAV needs to ensure that the typical UE is covered by its main-lobe beam; hence, a larger main-lobe beamwidth (equivalently, a smaller number of antennas) is preferred. However, when the interfering UAVs are very dense, the interference becomes very severe. One way to reduce the interference is to reduce the density of interfering UAVs with main-lobe beam pointed to the typical UE. From our analysis, this can be achieved by narrowing the main-lobe beamwidth. Note that it cannot be too narrow, because this can degrade the signal strength from the serving UAV due to the beam misalignment. Hence, a relatively larger number of antennas is preferred for the case of denser UAVs. Under the perfect beam scenario, for the MAPAS, sparse UAVs lead to a lower outage probability as expected. However, this is not the case for the CDAS. Fig. 7(b) shows that the denser UAVs can even result in a better outage probability, especially when the number of antennas is large. The reason is as follows. When the UAV density is very sparse, the closest UAV (i.e., the serving UAV) can be very far away, which consequently leads to a very weak signal strength from the serving UAV. Increasing the density of UAVs somehow improves the signal strength from the serving\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"beam alignment case, an excessive number of antennas is adverse to the network coverage probability. As for the perfect beam alignment case, Fig. 6(b) shows that, as the number of antennas becomes large, the outage probability decreases, and the trend of decreasing gradually slows down. This figure implies that from the perspective such as hardware cost, it is not necessary to equip the UAV with too many antennas since the performance gain is very small. \\n\\nC. Effect of UAV Deployment Height\\n\\nWith regards to the effect of UAV deployment height, Fig. 6(a) shows that the higher deployment of UAVs will lead to a larger outage probability under the imperfect alignment case. The reason is as follows. The higher height implies the larger coverage area of the main-lobe beam on certain tiers, which introduces more interference from more UAVs to the typical UE with the main-lobe pointed. When the beam is mispointed, this kind of effect on SINR becomes worse. Besides that, for the same projection point, the higher height indicates that the transmission link is more likely to be LoS, which means the signal strength from the interfering UAV becomes stronger. Hence, when the misalignment for the beamforming is non-negligible, the lower deployment of UAVs is preferred for both schemes. Under the case of perfect beam alignment, from Fig. 6(b), it can be seen that the outage probability of MAPAS is smaller for the lower height of UAVs. The explanation is the same as before. However, in terms of the performance of CDAS, it seems that the outage probability for the higher altitude of UAVs can be better. This is mainly because of the fact the UE is associated with the closest UAV and their link can be either LoS or NLoS. As mentioned before, for the same projection point, the link from UAV at the lower height is much more likely to be NLoS, which reduces the desired signal strength at the UE whereby degrading the outage performance. \\n\\nD. Effect of UAV Density\\n\\nFig. 7 plots the outage probability versus the number of antennas for UAV under different UAV densities for both imperfect and perfect alignment cases. Under the imperfect beam alignment scenario, as expected, the outage probability drops at first and then rises with the increase in the number of antennas. Moreover, since more interfering UAVs are involved in the system, the higher UAV density leads to worse outage probability performance. Besides that, Fig. 7(a) shows that the optimal number of UAV antennas increases as the UAV density rises, e.g., for MAPAS, the optimal number antennas is 4 for $\\\\lambda_k = 0.5 \\\\times 10^{-5}$ m$^{-2}$ while it goes to 16 for $\\\\lambda_k = 5 \\\\times 10^{-5}$ m$^{-2}$. The reason is as follows. When the UAV density is very sparse, the number of interfering UAVs falling into the region covered by the main-lobe beam of the UE is very small. In other words, the interference is not that severe. Then the serving UAV needs to ensure that the typical UE is covered by its main-lobe beam; hence, a larger main-lobe beamwidth (equivalently, a smaller number of antennas) is preferred. However, when the interfering UAVs are very dense, the interference becomes very severe. One way to reduce the interference is to reduce the density of interfering UAVs with main-lobe beam pointed to the typical UE. From our analysis, this can be achieved by narrowing the main-lobe beamwidth. Note that it cannot be too narrow, because this can degrade the signal strength from the serving UAV due to the beam misalignment. Hence, a relatively larger number of antennas is preferred for the case of denser UAVs. Under the perfect beam scenario, for the MAPAS, sparse UAVs lead to a lower outage probability as expected. However, this is not the case for the CDAS. Fig. 7(b) shows that the denser UAVs can even result in a better outage probability, especially when the number of antennas is large. The reason is as follows. When the UAV density is very sparse, the closest UAV (i.e., the serving UAV) can be very far away, which consequently leads to a very weak signal strength from the serving UAV. Increasing the density of UAVs somehow improves the signal strength from the serving\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This iteration was introduced by Nadel in [31] where he also proved that periodic points of order two or three must be K\u00e4hler-Einstein metrics. This was generalized [20, 33] to periodic points \\\\( \\\\omega \\\\) of any order, that is, those satisfying \\\\( \\\\rho^k_\\\\omega = \\\\lambda \\\\omega \\\\) for any \\\\( k \\\\in \\\\mathbb{N} \\\\). The iteration (3) can be regarded as a discretization of the K\u00e4hler\u2013Ricci flow. Observe that, if the manifold \\\\( M \\\\) is compact, by Calabi\u2019s conjecture one does not need positivity assumptions to reverse the construction above and define the inverse Ricci\u2013K\u00e4hler iterations \\\\( \\\\rho^{-k}_\\\\omega \\\\). Both of this discrete dynamical systems have been studied in the literature, see for instance [5, 13].\\n\\nHere we focus on the following generalized Monge-Amp\u00e8re equation on a complex manifold \\\\( M \\\\)\\n\\n\\\\[\\n\\\\rho^k_\\\\omega = \\\\lambda \\\\Omega\\n\\\\]\\n\\nwhere \\\\( \\\\Omega \\\\) is again a K\u00e4hler form. In particular we study equation (4) for a special class of well-behaved K\u00e4hler metrics. Our second result is the following theorem dealing with K\u00e4hler metrics induced by the flat metric (which are well-behaved by Example 4 below).\\n\\n**Theorem 2.** Let \\\\( M \\\\) be a complex manifold with two metrics \\\\( g \\\\) and \\\\( G \\\\) induced by the flat metric. If the corresponding K\u00e4hler forms \\\\( \\\\omega, \\\\Omega \\\\) satisfy \\\\( \\\\rho^k_\\\\omega = \\\\lambda \\\\Omega \\\\) for some \\\\( k \\\\geq 1 \\\\) and \\\\( \\\\lambda \\\\in \\\\mathbb{R} \\\\), then \\\\( (M, g) \\\\) is a totally geodesic submanifold of the flat ambient space.\\n\\nObserve that this result can be seen as a generalization of [40, Theorem 2.1]. Namely, when \\\\( k = 1 \\\\) and \\\\( g = G \\\\), Theorem 2 shows that a K\u00e4hler\u2013Einstein submanifolds of \\\\( \\\\mathbb{C}^n \\\\) with the flat metric is necessarily Ricci-flat, hence a totally geodesic submanifold. It is worth pointing out that the metric \\\\( g \\\\) is assumed to be induced by a flat one for simplicity, but the hypothesis on \\\\( g \\\\) can be sensibly relaxed, cf. Remark 11 below.\\n\\nOur third and last result deals with K\u00e4hler\u2013Ricci solitons (KRS). Recall that a K\u00e4hler metric \\\\( G \\\\) is a KRS if there exists a holomorphic vector field \\\\( X \\\\) (the solitonic vector field) such that \\\\( \\\\text{Ric}(G) = \\\\mu G + L_X G \\\\) where \\\\( \\\\text{Ric}(G) \\\\) denotes the Ricci tensor of \\\\( G \\\\) and \\\\( L_X \\\\) the Lie derivative in the direction of \\\\( X \\\\). K\u00e4hler\u2013Ricci solitons are important generalizations of K\u00e4hler\u2013Einstein metrics which arise in the study of the K\u00e4hler\u2013Ricci flow. Our next result show that KRS cannot arise as K\u00e4hler\u2013Ricci iterations of metrics induced by complex space forms.\\n\\n**Theorem 3.** Let \\\\( M \\\\) be a complex manifold with two real analytic K\u00e4hler metrics \\\\( g \\\\) and \\\\( G \\\\). Assume that \\\\( g \\\\) is induced by a complex space form and \\\\( G \\\\) is a KRS. If the corresponding K\u00e4hler forms \\\\( \\\\omega, \\\\Omega \\\\) satisfy \\\\( \\\\rho^k_\\\\omega = \\\\lambda \\\\Omega \\\\) for some \\\\( \\\\lambda \\\\neq 0 \\\\) and \\\\( k \\\\geq 0 \\\\), then \\\\( G \\\\) is trivial, i.e. K\u00e4hler\u2013Einstein.\\n\\nObserve that, when \\\\( \\\\lambda = 0 \\\\), one cannot draw any conclusion. Take for instance \\\\( g \\\\) to be the flat metric on \\\\( \\\\mathbb{C} \\\\) and \\\\( G \\\\) to be Hamilton\u2019s cigar KRS [17]. It is worth mentioning that we do not know of any K\u00e4hler\u2013Einstein metric \\\\( G \\\\) and K\u00e4hler metric \\\\( g \\\\) induced by a complex space form which satisfy \\\\( \\\\rho^k_\\\\omega = \\\\lambda \\\\Omega \\\\) for \\\\( \\\\lambda < 0 \\\\) unless \\\\( g = G \\\\) and \\\\( k = 1 \\\\). In that case \\\\( M \\\\) is forced to be a totally geodesic submanifold in \\\\( \\\\mathbb{C} \\\\mathbb{H}^n \\\\) [40]. On the other hand, such examples for \\\\( \\\\lambda > 0 \\\\) are discussed in Section 3, see in particular Proposition 2.\\n\\nAlso in this case we generalize some recent results on KRS induced by complex space forms which indeed served as partial motivation for this paper. In particular, if we assume \\\\( k = 0 \\\\) (and \\\\( \\\\lambda > 0 \\\\)) in Theorem 3, we recover [25, Theorem 1.1]. If we restrict to compact complex manifolds, then this result can be rephrased in terms of the inverse K\u00e4hler\u2013Ricci iterations. Namely, Theorem 3 shows that none of the inverse K\u00e4hler\u2013Ricci iterations \\\\( \\\\rho^{-k}_\\\\omega \\\\) of a non-trivial KRS \\\\( (g, X) \\\\) on a compact complex manifold \\\\( M \\\\) can be induced by a complex space form.\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This iteration was introduced by Nadel in [31] where he also proved that periodic points of order two or three must be K\u00a8ahler-Einstein metrics. This was generalized [20, 33] to periodic points $\\\\omega$ of any order, that is, those satisfying $\\\\rho^k_\\\\omega = \\\\lambda \\\\omega$ for any $k \\\\in \\\\mathbb{N}$. The iteration (3) can be regarded as a discretization of the K\u00a8ahler\u2013Ricci flow. Observe that, if the manifold $M$ is compact, by Calabi\u2019s conjecture one does not need positivity assumptions to reverse the construction above and define the inverse Ricci\u2013K\u00a8ahler iterations $\\\\rho^{-k}_\\\\omega$. Both of this discrete dynamical systems have been studied in the literature, see for instance [5, 13].\\n\\nHere we focus on the following generalized Monge-Amp`ere equation on a complex manifold $M$\\n\\n$$\\\\rho^k_\\\\omega = \\\\lambda \\\\Omega$$\\n\\nwhere $\\\\Omega$ is again a K\u00a8ahler form. In particular we study equation (4) for a special class of wellbehaved K\u00a8ahler metrics. Our second result is the following theorem dealing with K\u00a8ahler metrics induced by the flat metric (which are well-behaved by Example 4 below).\\n\\n**Theorem 2.** Let $M$ be a complex manifold with two metrics $g$ and $G$ induced by the flat metric. If the corresponding K\u00a8ahler forms $\\\\omega, \\\\Omega$ satisfy $\\\\rho^k_\\\\omega = \\\\lambda \\\\Omega$ for some $k \\\\geq 1$ and $\\\\lambda \\\\in \\\\mathbb{R}$, then $(M, g)$ is a totally geodesic submanifold of the flat ambient space. \\n\\nObserve that this result can be seen as a generalization of [40, Theorem 2.1]. Namely, when $k = 1$ and $g = G$, Theorem 2 shows that a K\u00a8ahler\u2013Einstein submanifolds of $\\\\mathbb{C}^n$ with the flat metric is necessarily Ricci-flat, hence a totally geodesic submanifold. It is worth pointing out that the metric $g$ is assumed to be induced by a flat one for simplicity, but the hypothesis on $g$ can be sensibly relaxed, cf. Remark 11 below. \\n\\nOur third and last result deals with K\u00a8ahler\u2013Ricci solitons (KRS). Recall that a K\u00a8ahler metric $G$ is a KRS if there exists a holomorphic vector field $X$ (the solitonic vector field) such that $\\\\text{Ric}(G) = \\\\mu G + L_X G$ where $\\\\text{Ric}(G)$ denotes the Ricci tensor of $G$ and $L_X$ the Lie derivative in the direction of $X$. K\u00a8ahler\u2013Ricci solitons are important generalizations of K\u00a8ahler\u2013Einstein metrics which arise in the study of the K\u00a8ahler\u2013Ricci flow. Our next result show that KRS cannot arise as K\u00a8ahler\u2013Ricci iterations of metrics induced by complex space forms. \\n\\n**Theorem 3.** Let $M$ be a complex manifold with two real analytic K\u00a8ahler metrics $g$ and $G$. Assume that $g$ is induced by a complex space form and $G$ is a KRS. If the corresponding K\u00a8ahler forms $\\\\omega, \\\\Omega$ satisfy $\\\\rho^k_\\\\omega = \\\\lambda \\\\Omega$ for some $\\\\lambda \\\\neq 0$ and $k \\\\geq 0$, then $G$ is trivial, i.e. K\u00a8ahler\u2013Einstein. \\n\\nObserve that, when $\\\\lambda = 0$, one cannot draw any conclusion. Take for instance $g$ to be the flat metric on $\\\\mathbb{C}$ and $G$ to be Hamilton\u2019s cigar KRS [17]. It is worth mentioning that we do not know of any K\u00a8ahler\u2013Einstein metric $G$ and K\u00a8ahler metric $g$ induced by a complex space form which satisfy $\\\\rho^k_\\\\omega = \\\\lambda \\\\Omega$ for $\\\\lambda < 0$ unless $g = G$ and $k = 1$. In that case $M$ is forced to be a totally geodesic submanifold in $\\\\mathbb{C}H^n$ [40]. On the other hand, such examples for $\\\\lambda > 0$ are discussed in Section 3, see in particular Proposition 2. \\n\\nAlso in this case we generalize some recent results on KRS induced by complex space forms which indeed served as partial motivation for this paper. In particular, if we assume $k = 0$ (and $\\\\lambda > 0$) in Theorem 3, we recover [25, Theorem 1.1]. If we restrict to compact complex manifolds, then this result can be rephrased in terms of the inverse K\u00a8ahler\u2013Ricci iterations. Namely, Theorem 3 shows that none of the inverse K\u00a8ahler\u2013Ricci iterations $\\\\rho^{-k}_\\\\omega$ of a non-trivial KRS $(g, X)$ on a compact complex manifold $M$ can be induced by a complex space form.\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Finally, let us simplify Term 1:\\n\\n\\\\[\\n\\\\text{Term 1} = \\\\exp \\\\left\\\\{ \\\\{ - i \\\\alpha_1(s) E \\\\} \\\\exp \\\\left\\\\{ \\\\{ - i \\\\alpha_2(s) P \\\\} \\\\exp \\\\left\\\\{ \\\\{ - i \\\\alpha_3(s) Q \\\\} \\\\left( \\\\exp \\\\left\\\\{ \\\\{ - i \\\\alpha_4(s) H \\\\} \\\\alpha'_4(s) H \\\\right) \\\\right. \\\\right. \\\\\\\\\\n= \\\\alpha'_4(s) \\\\exp \\\\left\\\\{ \\\\{ - i \\\\alpha_1(s) E \\\\} \\\\exp \\\\left\\\\{ \\\\{ - i \\\\alpha_2(s) P \\\\} \\\\exp \\\\left\\\\{ \\\\{ - i \\\\alpha_3(s) Q \\\\} H \\\\right) \\\\exp \\\\left\\\\{ \\\\{ - i \\\\alpha_4(s) H \\\\} \\\\right. \\\\right. \\\\\\\\\\n= \\\\alpha'_4(s) e^{-i \\\\alpha_1(s) E} e^{-i \\\\alpha_2(s) P} \\\\left( \\\\exp \\\\left\\\\{ \\\\{ - i \\\\alpha_3(s) Q \\\\} H \\\\right) \\\\exp \\\\left\\\\{ \\\\{ - i \\\\alpha_4(s) H \\\\} \\\\right.\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Finally, let us simplify Term 1:\\n\\n\\\\[\\n\\\\text{Term 1} = \\\\exp \\\\left\\\\{ \\\\{ - i \\\\alpha_1(s) E \\\\} \\\\exp \\\\left\\\\{ \\\\{ - i \\\\alpha_2(s) P \\\\} \\\\exp \\\\left\\\\{ \\\\{ - i \\\\alpha_3(s) Q \\\\} \\\\left( \\\\exp \\\\left\\\\{ \\\\{ - i \\\\alpha_4(s) H \\\\} \\\\alpha'_4(s) H \\\\right) \\\\right. \\\\right. \\\\\\\\\\n= \\\\alpha'_4(s) \\\\exp \\\\left\\\\{ \\\\{ - i \\\\alpha_1(s) E \\\\} \\\\exp \\\\left\\\\{ \\\\{ - i \\\\alpha_2(s) P \\\\} \\\\exp \\\\left\\\\{ \\\\{ - i \\\\alpha_3(s) Q \\\\} H \\\\right) \\\\exp \\\\left\\\\{ \\\\{ - i \\\\alpha_4(s) H \\\\} \\\\right. \\\\right. \\\\\\\\\\n= \\\\alpha'_4(s) e^{-i \\\\alpha_1(s) E} e^{-i \\\\alpha_2(s) P} \\\\left( \\\\exp \\\\left\\\\{ \\\\{ - i \\\\alpha_3(s) Q \\\\} H \\\\right) \\\\exp \\\\left\\\\{ \\\\{ - i \\\\alpha_4(s) H \\\\} \\\\right.\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The first integral of (5.21) can be estimated as follows: for any $\\\\epsilon > 0$,\\n\\\\[\\n\\\\int_{\\\\frac{1}{2}}^{1} \\\\int_{|y'| \\\\leq 1} \\\\frac{1}{(t-s)^{\\\\frac{1}{2}}(|x-y'|^2 + (t-s))^{\\\\frac{n-1}{2}}} |\\\\partial_s g_k^T(s)| \\\\, dy' \\\\, ds \\\\\\\\\\n\\\\lesssim \\\\int_{\\\\frac{1}{2}}^{1} \\\\frac{1}{(t-s)^{\\\\frac{1}{2}}(|x| + 1 + \\\\sqrt{t-s})^{n-1}} \\\\, ds \\\\\\\\\\n\\\\lesssim \\\\frac{1}{\\\\langle x \\\\rangle^{n-1}} \\\\int_{\\\\sqrt{t-\\\\frac{1}{2}}}^{\\\\sqrt{t-\\\\frac{1}{2}}} \\\\log \\\\left( 2 + \\\\frac{1}{u} \\\\right) \\\\, du \\\\\\\\\\n\\\\lesssim \\\\frac{1}{\\\\langle x \\\\rangle^{n-1}} \\\\int_{\\\\sqrt{t-\\\\frac{1}{2}}}^{\\\\sqrt{t-\\\\frac{1}{2}}} \\\\frac{1}{v^\\\\epsilon} \\\\, dv \\\\\\\\\\n\\\\lesssim \\\\frac{(t-\\\\frac{1}{2})^{\\\\frac{1}{2}}}{\\\\langle x \\\\rangle^{n-1}}.\\n\\\\]\\n\\nThe second integral of (5.21) can be estimated as follows:\\n\\\\[\\n\\\\int_{\\\\frac{1}{2}}^{1} \\\\int_{|y'| \\\\leq 1} \\\\frac{1}{(t-s)^{\\\\frac{1}{2}}(|x-y'|^2 + (t-s))^{\\\\frac{n-1}{2}}} |\\\\partial_s g_k^T(s)| \\\\, dy' \\\\, ds \\\\\\\\\\n\\\\lesssim \\\\int_{\\\\frac{1}{2}}^{1} \\\\frac{1}{(t-s)^{\\\\frac{1}{2}}(|x| + \\\\sqrt{t-s} + 1)^{n-1}} \\\\frac{1}{(1-s)^{1-a}} \\\\, ds \\\\mathbb{1}_{|x| < 2} + \\\\int_{\\\\frac{1}{2}}^{1} \\\\frac{1}{(t-s)^{\\\\frac{1}{2}}(1-s)^{1-a}} \\\\, ds \\\\mathbb{1}_{|x| > 2} \\\\\\\\\\n\\\\lesssim \\\\int_{\\\\sqrt{t-\\\\frac{1}{2}}}^{\\\\sqrt{t-\\\\frac{1}{2}}} \\\\frac{1}{(1-t+u^2)^{1-a}} \\\\, du \\\\mathbb{1}_{|x| < 2} + \\\\frac{\\\\text{LN}^*}{|x|^{n-1}} \\\\mathbb{1}_{|x| > 2} \\\\\\\\\\n\\\\lesssim \\\\int_{\\\\sqrt{t-\\\\frac{1}{2}}}^{\\\\sqrt{t-\\\\frac{1}{2}}} \\\\frac{1}{u^\\\\epsilon(1-t+u^2)^{1-a}} \\\\, du \\\\mathbb{1}_{|x| < 2} + \\\\frac{\\\\text{LN}^*}{|x|^{n-1}} \\\\mathbb{1}_{|x| > 2} \\\\\\\\\\n\\\\lesssim \\\\frac{1}{(t-1)^{\\\\frac{1}{2}}} \\\\int_{0}^{\\\\frac{1}{2}} \\\\frac{dv}{v^{1-a}(t-1+v)^{\\\\frac{1}{2}}} \\\\mathbb{1}_{|x| < 2} + \\\\frac{\\\\text{LN}^*}{|x|^{n-1}} \\\\mathbb{1}_{|x| > 2} \\\\\\\\\\n\\\\lesssim \\\\frac{\\\\text{LN}^*}{(t-1)^{\\\\frac{1}{2}}} \\\\mathbb{1}_{|x| < 2} + \\\\frac{\\\\text{LN}^*}{|x|^{n-1}} \\\\mathbb{1}_{|x| > 2}.\\n\\\\]\\n\\nThus we conclude that\\n\\\\[\\n(5.22) \\\\quad |I_2| \\\\lesssim \\\\frac{(t-\\\\frac{1}{2})^{\\\\frac{1}{2}}}{\\\\langle x \\\\rangle^{n-1}} + \\\\mathbb{1}_{|x| < 2} \\\\frac{\\\\text{LN}^*}{(t-1)^{\\\\frac{1}{2}}} + \\\\mathbb{1}_{|x| > 2} \\\\frac{\\\\text{LN}^*}{|x|^{n-1}}.\\n\\\\]\\n\\nHence (5.17), (5.19) and (5.22) give\\n\\\\[\\n|I_2| \\\\lesssim \\\\mathbb{1}_{|x| < 2} \\\\left( \\\\mathbb{1}_{t \\\\leq \\\\frac{1}{2}} \\\\left( t - \\\\frac{1}{4} \\\\right)^{\\\\frac{1}{2}} + \\\\mathbb{1}_{\\\\frac{1}{2} \\\\leq t < 1} \\\\left( 1 + \\\\frac{1}{(1-t)^{\\\\frac{1}{2}+\\\\epsilon-a}} \\\\right) + \\\\mathbb{1}_{t > 1} \\\\frac{\\\\text{LN}^*}{(t-1)^{\\\\frac{1}{2}}} \\\\right) \\\\\\\\\\n+ \\\\mathbb{1}_{|x| > 2} \\\\left( \\\\mathbb{1}_{t \\\\leq \\\\frac{1}{2}} \\\\frac{1}{|x|^{n-1}} + \\\\mathbb{1}_{\\\\frac{1}{2} \\\\leq t < 1} \\\\left( \\\\frac{1}{|x|^{n-1}} + \\\\frac{\\\\text{LN}^*}{|x|^{n-1}} \\\\right) \\\\right).\\n\\\\]\\n\\nFor the estimates of the higher spatial derivatives, the similar argument as above gives the result, and this proves the pressure estimate. $\\\\square$\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The first integral of (5.21) can be estimated as follows KYUNGKEUN KANG AND CHANHONG MIN\\n\\n\\\\[\\n\\\\int_{\\\\frac{1}{2}}^{1} \\\\int_{|y'| \\\\leq 1} \\\\frac{1}{(t-s)^{\\\\frac{1}{2}}(|x-y'|^2 + (t-s))^{\\\\frac{n-1}{2}}} |\\\\partial_s g_k^T(s)| \\\\, dy' \\\\, ds \\\\\\\\\\n\\\\lesssim \\\\int_{\\\\frac{1}{2}}^{1} \\\\frac{1}{(t-s)^{\\\\frac{1}{2}}(|x| + 1 + \\\\sqrt{t-s})^{n-1}} \\\\, ds \\\\\\\\\\n\\\\lesssim \\\\frac{1}{\\\\langle x \\\\rangle^{n-1}} \\\\int_{\\\\sqrt{t-\\\\frac{1}{2}}}^{\\\\sqrt{t-\\\\frac{1}{2}}} \\\\log \\\\left( 2 + \\\\frac{1}{u} \\\\right) \\\\, du \\\\\\\\\\n\\\\lesssim \\\\frac{1}{\\\\langle x \\\\rangle^{n-1}} \\\\int_{\\\\sqrt{t-\\\\frac{1}{2}}}^{\\\\sqrt{t-\\\\frac{1}{2}}} \\\\frac{1}{v^2} \\\\, dv \\\\\\\\\\n\\\\lesssim \\\\frac{(t-\\\\frac{1}{2})^{\\\\frac{1}{2}}}{\\\\langle x \\\\rangle^{n-1}}.\\n\\\\]\\n\\nThe second integral of ( 5.21 ) can be estimated as follows: for any \\\\( \\\\epsilon > 0 \\\\),\\n\\n\\\\[\\n\\\\int_{\\\\frac{1}{2}}^{1} \\\\int_{|y'| \\\\leq 1} \\\\frac{1}{(t-s)^{\\\\frac{1}{2}}(|x-y'|^2 + (t-s))^{\\\\frac{n-1}{2}}} |\\\\partial_s g_k^T(s)| \\\\, dy' \\\\, ds \\\\\\\\\\n\\\\lesssim \\\\int_{\\\\frac{1}{2}}^{1} \\\\frac{1}{(t-s)^{\\\\frac{1}{2}}(|x| + \\\\sqrt{t-s} + 1)^{n-1}} \\\\frac{1}{(1-s)^{1-a}} \\\\, ds \\\\mathbb{1}_{|x| < 2} + \\\\int_{\\\\frac{1}{2}}^{1} \\\\frac{1}{(t-s)^{\\\\frac{1}{2}}(1-s)^{1-a}} \\\\, ds \\\\mathbb{1}_{|x| > 2} \\\\\\\\\\n\\\\lesssim \\\\int_{\\\\sqrt{t-\\\\frac{1}{2}}}^{\\\\sqrt{t-\\\\frac{1}{2}}} \\\\log \\\\left( 2 + \\\\frac{1}{u} \\\\right) \\\\frac{1}{(1-t+u^2)^{1-a}} \\\\, du \\\\mathbb{1}_{|x| < 2} + \\\\frac{\\\\text{LN}^*}{|x|^{n-1}} \\\\mathbb{1}_{|x| > 2} \\\\\\\\\\n\\\\lesssim \\\\int_{\\\\sqrt{t-\\\\frac{1}{2}}}^{\\\\sqrt{t-\\\\frac{1}{2}}} \\\\frac{1}{u^a(1-t+u^2)^{1-a}} \\\\, du \\\\mathbb{1}_{|x| < 2} + \\\\frac{\\\\text{LN}^*}{|x|^{n-1}} \\\\mathbb{1}_{|x| > 2} \\\\\\\\\\n\\\\lesssim \\\\frac{1}{(t-1)^{\\\\frac{1}{2}}} \\\\int_{0}^{\\\\frac{1}{2}} \\\\frac{dv}{v^{1-a}(t-1+v)^{\\\\frac{1}{2}}} \\\\mathbb{1}_{|x| < 2} + \\\\frac{\\\\text{LN}^*}{|x|^{n-1}} \\\\mathbb{1}_{|x| > 2} \\\\\\\\\\n\\\\lesssim \\\\frac{\\\\text{LN}^*}{(t-1)^{\\\\frac{1}{2}}} \\\\mathbb{1}_{|x| < 2} + \\\\frac{\\\\text{LN}^*}{|x|^{n-1}} \\\\mathbb{1}_{|x| > 2}.\\n\\\\]\\n\\nThus we conclude that\\n\\n\\\\[\\n(5.21 ) can be estimated as follows: for any \\\\( \\\\epsilon > 0 \\\\),\\n\\\\]\\n\\n\\\\[\\n|I_2| \\\\lesssim \\\\frac{(t-\\\\frac{1}{2})^{\\\\frac{1}{2}}}{\\\\langle x \\\\rangle^{n-1}} + \\\\mathbb{1}_{|x| < 2} \\\\frac{\\\\text{LN}^*}{(t-1)^{\\\\frac{1}{2}}} + \\\\mathbb{1}_{|x| > 2} \\\\frac{\\\\text{LN}^*}{|x|^{n-1}}.\\n\\\\]\\n\\nHence (5.17 ), ( 5.19 ) and ( 5.22 ) give\\n\\n\\\\[\\n|I_2| \\\\lesssim \\\\mathbb{1}_{|x| < 2} \\\\left( \\\\mathbb{1}_{t \\\\leq \\\\frac{1}{2}} \\\\left( t - \\\\frac{1}{4} \\\\right)^{\\\\frac{1}{2}} + \\\\mathbb{1}_{\\\\frac{1}{2} \\\\leq t < 1} \\\\left( 1 + \\\\frac{1}{(1-t)^{\\\\frac{1}{2}+a}} \\\\right) + \\\\mathbb{1}_{t > 1} \\\\frac{\\\\text{LN}^*}{(t-1)^{\\\\frac{1}{2}}} \\\\right) \\\\\\\\\\n+ \\\\mathbb{1}_{|x| > 2} \\\\left( \\\\mathbb{1}_{t \\\\leq \\\\frac{1}{2}} \\\\frac{1}{|x|^{n-1}} + \\\\mathbb{1}_{\\\\frac{1}{2} \\\\leq t < 1} + \\\\mathbb{1}_{t > 1} \\\\left( \\\\frac{1}{|x|^{n-1}} + \\\\frac{\\\\text{LN}^*}{|x|^{n-1}} \\\\right) \\\\right).\\n\\\\]\\n\\nFor the estimates of the higher spatial derivatives, the similar argument as above gives the result, and this proves the pressure estimate. \\\\( \\\\square \\\\)\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"corresponding statistical estimators. In this case, we also use the LP/B03/ExD model from the spec-z as reference to check the impact of the GaZNet redshift in terms of accuracy and scatter. Basically, the results show that, for the same correlations seen in Fig. 3, the relative bias of the different configurations is not worsened, meaning that the accuracy of the mass estimates is not affected by the use of the morpho-z. This is eventually a consequence of the good accuracy of these latter as seen in Fig. 2. On the other hand, we register an evident increase of the NMAD as a consequence of the morpho-z intrinsic statistical errors and outlier fractions, which is also mirrored by the scatter of the residual, at the bottom of the 1-to-1 relations, which is now of the order of 0.23 dex, for log $M_*/M_\\\\odot > 9$, and 0.49 dex for log $M_*/M_\\\\odot < 9$, on average. These large scatter at low stellar masses are mainly caused by the trend we see that below log $M_*/M_\\\\odot = 8.5$, where stellar masses are systematically overestimated compared to those obtained with the spec-z. This is not an effect that comes from the particular set-up of the fitting procedure, as shown by the comparison of the LP/B03/ExD/morpho-z against the same set-up with spec-z (bottom/left plot in Fig. 4). Even in this latter case, we see that below log $M_*/M_\\\\odot = 8.5$ the positive bias is similar to the ones of all other configurations. We track the motivation of this systematics to some bias of the GaZNet redshifts for a group of objects at very low redshifts ($z < 0.05$ see Fig. 2), which turn-out to have also low masses. This can be due to some residual contamination from stars, not picked in the spectra classification, or just a failure of the GaZNet predictions at very low-z, which clearly impact the mass predictions. We will come back to this on Sect. 4. However, still looking at the LP/B03/ExD/morpho-z vs. spec-z, above log $M_*/M_\\\\odot = 8.5$, the bias is almost absent and the only relevant effect is the GaZNet redshift scatter that, from the NMAD, is quantified in 0.09. This is confirmed by noticing that the general increase of the NMAD from the spectroscopic sample to the morpho-metric sample, in Table 2, is compatible with the sum in quadrature of the NMAD of the former with 0.09 coming from the latter, consistently with some pseudo-Gaussian distributions. This is consistent with a log-normal distribution of the uncertainties of the stellar masses, which are confirmed by the outlier fractions that are all of the order of 5-6% above 2$\\\\sigma$ of the\\n\\nFigure 4  Stellar mass estimates as for Fig. 3 but using the GaZNet morpho-metric redshifts.\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"corresponding statistical estimators. In this case, we also use the LP / B03 / ExD model from the spec-\\\\(z\\\\) as reference to check the impact of the GaZNet redshift in terms of accuracy and scatter. Basically, the results show that, for the same correlations seen in Fig. 3 , the relative bias of the di ff erent configurations is not worsened, meaning that the accuracy of the mass estimates is not a ff ected by the use of the morphoto-\\\\(z\\\\). This is eventually a consequence of the good accuracy of these latter as seen in Fig. 2 . On the other hand, we register an evident increase of the NMAD as a consequence of the morphoto-\\\\(z\\\\) intrinsic statistical errors and outlier fractions, which is also mirrored by the scatter of the residual, at the bottom of the 1-to-1 relations, which is now of the order of 0.23 dex, for \\\\(\\\\log M_*/M_\\\\odot > 9\\\\), and 0.49 dex for \\\\(\\\\log M_*/M_\\\\odot < 9\\\\), on average. These large scatter at low stellar masses are mainly caused by the trend we see that below \\\\(\\\\log M_*/M_\\\\odot = 8.5\\\\), where stellar masses are systematically overestimated compared to those obtained with the spec-\\\\(z\\\\). This is not an e ff ect that comes from the particular set-up of the fitting procedure, as shown by the comparison of the LP / B03 / ExD / morphoto-\\\\(z\\\\) against the same set-up with spec-\\\\(z\\\\) (bottom / left plot in Fig. 4 ). Even in this latter case, we see that below \\\\(\\\\log M_*/M_\\\\odot = 8.5\\\\) the positive bias is similar to the ones of all other configurations. We track the motivation of this systematics to some bias of the GaZNet redshifts for a group of objects at very low redshifts (\\\\(z < 0.05\\\\) see Fig. 2 ), which turn-out to have also low masses. This can be due to some residual contamination from stars, not picked in the spectra classification, or just a failure of the GaZNet predictions at very low-\\\\(z\\\\), which clearly impact the mass predictions. We will come back to this on Sect. 4 . However, still looking at the LP / B03 / ExD morphoto-\\\\(z\\\\) vs. spec-\\\\(z\\\\), above \\\\(\\\\log M_*/M_\\\\odot = 8.5\\\\), the bias is almost absent and the only relevant ef ff ect is the GaZNet redshift scatter that, from the NMAD, is quantified in 0.09. This is confirmed by noticing that the general increase of the NMAD from the spectroscopic sample to the morphoto-metric sample, in Table 2 , is compatible with the sum in quadrature of the NMAD of the former with 0.09 coming from the latter, consistently with some pseudo-Gaussian distributions. This is consistent with a log-normal distribution of the uncertainties of the stellar masses, which are confirmed by the outlier fractions that are all of the order of 5-6% above 2\\\\(\\\\sigma\\\\) of the\\n\\nFigure 4  Stellar mass estimates as for Fig. 3 but using the GaZNet morphoto-metric redshifts.\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Improving Generalization in Visual Reinforcement Learning via Conflict-aware Gradient Agreement Augmentation\\n\\nSiao Liu Zhaoyu Chen Yang Liu Yuzheng Wang Dingkang Yang Zhile Zhao\\nZiqing Zhou Xie Yi Wei Li Wenqiang Zhang Zhongxue Gan\\n\\nAcademy for Engineering & Technology, Fudan University\\n{saliu20, zhaoyuchen20, yang.liu20, yzwang20, dkyang20, fd.liwei, wqzhang, ganzhongxue}@fudan.edu.cn\\n{zhilezhao21, ziqingzhou21, yixie22}@m.fudan.edu.cn\\n\\nAbstract\\n\\nLearning a policy with great generalization to unseen environments remains challenging but critical in visual reinforcement learning. Despite the success of augmentation combination in the supervised learning generalization, naively applying it to visual RL algorithms may damage the training efficiency, suffering from serve performance degradation. In this paper, we first conduct qualitative analysis and illuminate the main causes: (i) high-variance gradient magnitudes and (ii) gradient conflicts existed in various augmentation methods. To alleviate these issues, we propose a general policy gradient optimization framework, named Conflict-aware Gradient Agreement Augmentation (CG2A), and better integrate augmentation combination into visual RL algorithms to address the generalization bias. In particular, CG2A develops a Gradient Agreement Solver to adaptively balance the varying gradient magnitudes, and introduces a Soft Gradient Surgery strategy to alleviate the gradient conflicts. Extensive experiments demonstrate that CG2A significantly improves the generalization performance and sample efficiency of visual RL algorithms.\\n\\n1. Introduction\\n\\nWith the development of deep learning in various tasks [28, 26, 25, 27, 7, 6, 38, 40, 39, 24], visual Reinforcement Learning (RL) has achieved impressive success in various fields such as robotic control [11], autonomous driving [17], and game-playing [35]. Previous works usually formulate it as a Partially Observable Markov Decision Process (POMDP) [33], and the agent receives high-dimensional image observations as inputs. As depicted in [15, 14], visual RL generalization refers to the ability of a pretrained RL agent to perform well in unseen environments. Due to the dynamic nature of the real world, even minor perturbations in the environment can result in significant semantic shifts in the visual observations, which makes visual RL generalization challenging.\\n\\nTo improve generalization performance, data augmentation [29] is a widely adopted technique in reinforcement learning. Numerous studies [22, 13] utilize data augmentation methods to generate synthetic data and diversify the training environments, yielding considerable performance improvements. However, recent methods [14, 3, 44] mostly select a single augmentation technique to improve the generalization capability, resulting in a poor performance in the environments with observations varying far from the augmented images. For instance, ColorJitter [23] is the preferred choice for addressing color variations, but agents trained with such augmentation still hard to cope with intricate texture patterns. In other words, the generalization ability heavily relies on the selection of specific data augmentation technique, which is so-called generalization bias.\\n\\nCompared to single data augmentation, Augmentation Combination (AC) [16] integrates multiple data augmentation methods to enhance the diversity of augmentations and alleviate the generalization bias, which is a more promising pre-processing solution. Unfortunately, there is a dilemma in incorporating AC into visual RL. Although data augmentation combination can effectively improve generalization capability in the supervised visual tasks, RL algorithms are quite sensitive to excessive variations, resulting in performance degradation and training sample inefficiency. Therefore, it is necessary to rethink why visual RL algorithms cannot benefit from AC as much as supervised learning.\\n\\nFrom the perspective of gradient optimization, we conduct numerous qualitative analysis to illustrate the causes of performance degradation and training collapse that occur when employing augmentation combinations during train-\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Improving Generalization in Visual Reinforcement Learning via Conflict-aware Gradient Agreement Augmentation\\n\\nSiao Liu Zhaoyu Chen Yang Liu Yuzheng Wang Dingkang Yang Zhile Zhao\\nZiqing Zhou Xie Yi Wei Li Wenqiang Zhang Zhongxue Gan\\n\\nAcademy for Engineering & Technology, Fudan University\\n{saliu20, yzwang20, dkyang20, fd liwei, wqzhang, ganzhongxue}@fudan.edu.cn\\n{zhilezhao21, ziqingzhou21, yixie22}@m.fudan.edu.cn\\n\\nAbstract\\n\\nLearning a policy with great generalization to unseen environments remains challenging but critical in visual re-}\\n\\nforcement learning. Despite the success of augmenta-}\\n\\ntion combination in the supervised learning generalization, naively applying it to visual RL algorithms may damage the training efficiency, suffering from serve performance degradation. In this paper, we first conduct qualitative analysis and illuminate the main causes: (i) high-variance gradient magnitudes and (ii) gradient conflicts existed in various augmentation methods. To alleviate these issues, we propose a general policy gradient optimization frame-}\\n\\nwork, named Conflict-aware Gradient Agreement Augmen-}\\n\\ntation (CG2A), and better integrate augmentation combina-}\\n\\ntion into visual RL algorithms to address the generalization bias. In particular, CG2A develops a Gradient Agreement Solver to adaptively balance the varying gradient magni-}\\n\\ntudes, and introduces a Soft Gradient Surgery strategy to al-}\\n\\nleviate the gradient conflicts. Extensive experiments demon-}\\n\\nstrate that CG2A significantly improves the generalization performance and sample efficiency of visual RL algorithms. 1. Introduction\\n\\nWith the development of deep learning in various tasks [28, 26, 25, 27, 7, 6, 38, 40, 39, 24], visual Reinforcement Learning (RL) has achieved impressive success in various fields such as robotic control [11], autonomous driving [17], and game-playing [35]. Previous works usually formulate it as a Partially Observable Markov Decision Process (POMDP) [33], and the agent receives highdimensional image observations as inputs. As depicted in [15, 14], visual RL generalization refers to the ability of a pretrained RL agent to perform well in unseen environments. Due to the dynamic nature of the real world, even minor perturbations in the environment can result in significant semantic shifts in the visual observations, which makes visual RL generalization challenging. To improve generalization performance, data augmentation [29] is a widely adopted technique in reinforcement learning. Numerous studies [22, 13] utilize data augmentation methods to generate synthetic data and diversify the training environments, yielding considerable performance improvements. However, recent methods [14, 3, 44] mostly select a single augmentation technique to improve the generalization capability, resulting in a poor performance in the environments with observations varying far from the augmented images. For instance, ColorJitter [23] is the preferred choice for addressing color variations, but agents trained with such augmentation still hard to cope with intricate texture patterns. In other words, the generalization ability heavily relies on the selection of specific data augmentation technique, which is so-called generalization bias. Compared to single data augmentation, Augmentation Combination (AC) [16] integrates multiple data augmentation methods to enhance the diversity of augmentations and alleviate the generalization bias, which is a more promising pre-processing solution. Unfortunately, there is a dilemma in incorporating AC into visual RL. Although data augmentation combination can effectively improve generalization capability in the supervised visual tasks, RL algorithms are quite sensitive to excessive variations, resulting in performance degradation and training sample inefficiency. Therefore, it is necessary to rethink why visual RL algorithms cannot benefit from AC as much as supervised learning. From the perspective of gradient optimization, we conduct numerous qualitative analysis to illustrate the causes of performance degradation and training collapse that occur when employing augmentation combinations during train-\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lemma 2.9. Let \\\\( x = \\\\Delta^k \\\\) for some nonzero integer \\\\( k \\\\). We have \\\\( \\\\text{SSS}(\\\\Delta^k) = \\\\{ \\\\Delta^k \\\\} \\\\). The centralizer of \\\\( \\\\Delta^k \\\\) in \\\\( G(m, \\\\ell) \\\\) is either \\\\( G(m, \\\\ell) \\\\) if \\\\( k\\\\ell \\\\) is a multiple of \\\\( m \\\\), or cyclic and generated by \\\\( \\\\Delta \\\\) otherwise.\\n\\nProof. We have that \\\\( y \\\\in G(m, \\\\ell) \\\\) lies in \\\\( \\\\text{SSS}(\\\\Delta^k) \\\\) only if \\\\( \\\\inf(y) = k = \\\\sup(y) \\\\). The only element satisfying this is \\\\( \\\\Delta^k \\\\), which is conjugate to itself. Thus we have \\\\( \\\\text{SSS}(\\\\Delta^k) = \\\\{ \\\\Delta^k \\\\} \\\\).\\n\\nNow, let \\\\( s(j, q) \\\\) be a simple element in \\\\( M(m, \\\\ell) \\\\). Since both 1 and \\\\( \\\\Delta \\\\) conjugate \\\\( \\\\Delta^k \\\\) to itself, we can assume that \\\\( q \\\\in [1, m - 1] \\\\). We have\\n\\n\\\\[\\n\\\\begin{align*}\\n    s(j, q)^{-1} \\\\Delta^k s(j, q) &= s(j, q) \\\\Delta^k s(j, q) \\\\\\\\\\n    &= s(j + q, \\\\ell - q) \\\\Delta^k s(j, q) \\\\\\\\\\n    &= \\\\Delta^k s(j + q + (k - 1)\\\\ell, \\\\ell - q) s(j, q).\\n\\\\end{align*}\\n\\\\]\\n\\nIn order for this element to lie in \\\\( \\\\text{SSS}(\\\\Delta^k) \\\\), the word \\\\( s(j + q + (k - 1)\\\\ell, \\\\ell - q) s(j, q) \\\\) must not be greedy. This is equivalent to \\\\( j + k\\\\ell \\\\equiv j[m] \\\\). If \\\\( k\\\\ell \\\\) is a multiple of \\\\( m \\\\), this is true for all \\\\( j \\\\in [0, m - 1] \\\\), and we obtain \\\\( s(j, q)^{-1} \\\\Delta^k s(j, q) = \\\\Delta^k \\\\): the arrows from \\\\( \\\\Delta^k \\\\) to itself in \\\\( \\\\text{CG}(\\\\Delta^k) \\\\) are given by all the simple elements. Otherwise, \\\\( j + k\\\\ell \\\\equiv j[m] \\\\) is never true for \\\\( j \\\\in [1, m - 1] \\\\) and the only arrows from \\\\( \\\\Delta^k \\\\) to itself in \\\\( \\\\text{CG}(\\\\Delta^k) \\\\) are given by 1 and \\\\( \\\\Delta \\\\). \\\\( \\\\square \\\\)\\n\\nLemma 2.10. Let \\\\( x = \\\\Delta^k s(i, p) \\\\) be a periodic element in \\\\( M(m, \\\\ell) \\\\) with \\\\( p \\\\in [1, m - 1] \\\\). We have \\\\( \\\\text{SSS}(x) = \\\\{ \\\\Delta^k s(n, p) \\\\mid n \\\\in [0, m - 1] \\\\} \\\\). The centralizer of \\\\( \\\\Delta^k s(0, p) \\\\) in \\\\( G(m, \\\\ell) \\\\) is cyclic and generated by \\\\( s(p, m) \\\\).\\n\\nProof. The assumption that \\\\( x \\\\) is periodic is equivalent to \\\\( k\\\\ell + p \\\\equiv 0[m] \\\\) by Lemma 2.8. Let \\\\( s(j, q) \\\\) be a simple element in \\\\( M(m, \\\\ell) \\\\). We have\\n\\n\\\\[\\nx^{s(j, q)} = s(j, q)^{-1} \\\\Delta^k s(i, p) s(j, q) = \\\\Delta^k s(j + q + (k - 1)\\\\ell, \\\\ell - q) s(i, p) s(j, q).\\n\\\\]\\n\\nAgain, in order for this to lie in \\\\( \\\\text{SSS}(x) \\\\), we must have either \\\\( j + k\\\\ell \\\\equiv i[m] \\\\) or \\\\( i + p \\\\equiv j[m] \\\\). Since \\\\( k\\\\ell + p \\\\equiv 0[m] \\\\), those two assertions are equivalent. If they are satisfied, then we have\\n\\n\\\\[\\nx^{s(j, q)} = \\\\Delta^k s(j + q - p, p) = \\\\Delta^k s(i + q, p).\\n\\\\]\\n\\nIn particular, \\\\( s(p, n) \\\\) gives a conjugating element from \\\\( \\\\Delta^k s(0, p) \\\\) to \\\\( \\\\Delta^k s(n, p) \\\\) for \\\\( n \\\\in [0, m - 1] \\\\). Moreover, for \\\\( \\\\Delta^k s(n, p) \\\\in \\\\text{SSS}(x) \\\\), the simples \\\\( s \\\\) such that \\\\( (\\\\Delta^k s(n, p))^s \\\\in \\\\text{SSS}(x) \\\\) are all divisible by \\\\( s(n + p, 1) \\\\). The conjugacy graph of \\\\( x \\\\) is then given by\\n\\n\\\\[\\n\\\\begin{align*}\\n    \\\\Delta^k s(0, p) &\\\\xrightarrow{s(p, 1)} \\\\Delta^k s(1, p) &\\\\xrightarrow{s(p + 1, 1)} \\\\cdots &\\\\xrightarrow{s(p + m - 2, 1)} \\\\Delta^k s(m - 1, p), \\\\\\\\\\n    s(p + m - 1, 1)\\n\\\\end{align*}\\n\\\\]\\n\\nand the centralizer of \\\\( \\\\Delta^k s(0, p) \\\\) is cyclic and generated by\\n\\n\\\\[\\ns(p, 1) s(p + 1, 1) \\\\cdots s(p + m - 1, 1) = s(p, m).\\n\\\\]\\n\\n\\\\( \\\\square \\\\)\\n\\nWe can use these two lemmas to determine the center of circular groups. Recall that the Garside automorphism \\\\( \\\\phi \\\\), corresponding to conjugacy by \\\\( \\\\Delta \\\\) on the right, sends a simple element \\\\( s(i, p) \\\\) to \\\\( s(i + \\\\ell, p) \\\\). If \\\\( m \\\\neq 1 \\\\neq \\\\ell \\\\), then the smallest trivial power of \\\\( \\\\phi \\\\) is \\\\( \\\\phi_{m, \\\\ell}^m \\\\), and \\\\( \\\\Delta_{m, \\\\ell}^m \\\\) is the smallest central power of \\\\( \\\\Delta \\\\) in \\\\( G(m, \\\\ell) \\\\).\\n\\nCorollary 2.11. (Center of circular groups)\\n\\nLet \\\\( m, \\\\ell \\\\) be two positive integers. If \\\\( m = 1 \\\\) or \\\\( \\\\ell = 1 \\\\), then \\\\( G(m, \\\\ell) \\\\simeq \\\\mathbb{Z} \\\\) is abelian. If \\\\( m = \\\\ell = 2 \\\\), then \\\\( G(m, \\\\ell) = \\\\mathbb{Z}^2 \\\\) is abelian. Otherwise \\\\( Z(G(m, \\\\ell)) \\\\) is infinite cyclic and generated by \\\\( \\\\Delta_{m, \\\\ell}^m \\\\).\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lemma 2.9. Let \\\\( x = \\\\Delta^k \\\\) for some nonzero integer \\\\( k \\\\). We have \\\\( \\\\text{SSS}(\\\\Delta^k) = \\\\{ \\\\Delta^k \\\\} \\\\). The centralizer of \\\\( \\\\Delta^k \\\\) in \\\\( G(m, \\\\ell) \\\\) is either \\\\( G(m, \\\\ell) \\\\) if \\\\( k\\\\ell \\\\) is a multiple of \\\\( m \\\\), or cyclic and generated by \\\\( \\\\Delta \\\\) otherwise.\\n\\nProof. We have that \\\\( y \\\\in G(m, \\\\ell) \\\\) lies in \\\\( \\\\text{SSS}(\\\\Delta^k) \\\\) only if \\\\( \\\\inf(y) = k = \\\\sup(y) \\\\). The only element satisfying this is \\\\( \\\\Delta^k \\\\), which is conjugate to itself. Thus we have \\\\( \\\\text{SSS}(\\\\Delta^k) = \\\\{ \\\\Delta^k \\\\} \\\\).\\n\\nNow, let \\\\( s(j, q) \\\\) be a simple element in \\\\( M(m, \\\\ell) \\\\). Since both 1 and \\\\( \\\\Delta \\\\) conjugate \\\\( \\\\Delta^k \\\\) to itself, we can assume that \\\\( q \\\\in [1, m - 1] \\\\). We have\\n\\n\\\\[\\n\\\\begin{align*}\\n    s(j, q)^{-1} \\\\Delta^k s(j, q) &= s(j, q) \\\\Delta^k s(j, q) \\\\\\\\\\n    &= s(j + q, \\\\ell - q) \\\\Delta^k s(j, q) \\\\\\\\\\n    &= \\\\Delta^k s(j + q + (k - 1)\\\\ell, \\\\ell - q) s(j, q).\\n\\\\end{align*}\\n\\\\]\\n\\nIn order for this element to lie in \\\\( \\\\text{SSS}(\\\\Delta^k) \\\\), the word \\\\( s(j + q + (k - 1)\\\\ell, \\\\ell - q) s(j, q) \\\\) must not be greedy. This is equivalent to \\\\( j + k\\\\ell \\\\equiv j[m] \\\\). If \\\\( k\\\\ell \\\\) is a multiple of \\\\( m \\\\), this is true for all \\\\( j \\\\in [0, m - 1] \\\\), and we obtain \\\\( s(j, q)^{-1} \\\\Delta^k s(j, q) = \\\\Delta^k \\\\): the arrows from \\\\( \\\\Delta^k \\\\) to itself in \\\\( \\\\text{CG}(\\\\Delta^k) \\\\) are given by all the simple elements. Otherwize, \\\\( j + k\\\\ell \\\\equiv j[m] \\\\) is never true for \\\\( j \\\\in [1, m - 1] \\\\) and the only arrows from \\\\( \\\\Delta^k \\\\) to itself in \\\\( \\\\text{CG}(\\\\Delta^k) \\\\) are given by 1 and \\\\( \\\\Delta \\\\). \\\\( \\\\square \\\\)\\n\\nLemma 2.10. Let \\\\( x = \\\\Delta^k s(i, p) \\\\) be a periodic element in \\\\( M(m, \\\\ell) \\\\) with \\\\( p \\\\in [1, m - 1] \\\\). We have \\\\( \\\\text{SSS}(x) = \\\\{ \\\\Delta^k s(n, p) \\\\mid n \\\\in [0, m - 1] \\\\} \\\\). The centralizer of \\\\( \\\\Delta^k s(0, p) \\\\) in \\\\( G(m, \\\\ell) \\\\) is cyclic and generated by \\\\( s(p, m) \\\\).\\n\\nProof. The assumption that \\\\( x \\\\) is periodic is equivalent to \\\\( k\\\\ell + p \\\\equiv 0[m] \\\\) by Lemma 2.8. Let \\\\( s(j, q) \\\\) be a simple element in \\\\( M(m, \\\\ell) \\\\). We have\\n\\n\\\\[\\nx^{s(j, q)} = s(j, q)^{-1} \\\\Delta^k s(i, p) s(j, q) = \\\\Delta^k s(j + q + (k - 1)\\\\ell, \\\\ell - q) s(i, p) s(j, q).\\n\\\\]\\n\\nAgain, in order for this to lie in SSS(\\\\( x \\\\)), we must have either \\\\( j + k\\\\ell \\\\equiv i[m] \\\\) or \\\\( i + p \\\\equiv j[m] \\\\). Since \\\\( k\\\\ell + p \\\\equiv 0[m] \\\\), those two assertions are equivalent. If thay are satisfied, then we have\\n\\n\\\\[\\nx^{s(j, q)} = \\\\Delta^k s(j + q - p, p) = \\\\Delta^k s(i + q, p).\\n\\\\]\\n\\nIn particular, \\\\( s(p, n) \\\\) gives a conjugating element from \u2206\\\\( k \\\\)s(0, p) to \u2206\\\\( k \\\\)s(n, p) for \\\\( n \\\\in [0, m - 1] \\\\). Moreover, for \u2206\\\\( k \\\\)s(n, p) \u2208 SSS(\\\\( x \\\\)), the simples \\\\( s \\\\) such that (\u2206\\\\( k \\\\)s(n, p))\\\\( s \\\\) \u2208 SSS(\\\\( x \\\\)) are all divisible by \\\\( s(n + p, 1) \\\\). The conjugacy graph of \\\\( x \\\\) is then given by\\n\\n\\\\[\\n\\\\begin{align*}\\n    \\\\Delta^k s(0, p) &\\\\xrightarrow{s(p, 1)} \\\\Delta^k s(1, p) &\\\\xrightarrow{s(p + 1, 1)} \\\\cdots &\\\\xrightarrow{s(p + m - 2, 1)} \\\\Delta^k s(m - 1, p), \\\\\\\\\\n    s(p + m - 1, 1)\\n\\\\end{align*}\\n\\\\]\\n\\nand the centralizer of \u2206\\\\( k \\\\)s(0, p) is cyclic and generated by\\n\\n\\\\[\\ns(p, 1) s(p + 1, 1) \\\\cdots s(p + m - 1, 1) = s(p, m).\\n\\\\]\\n\\n\\\\( \\\\square \\\\)\\n\\nWe can use these two lemmas to determine the center of circular groups. Recall that the Garside automorphism \\\\( \\\\phi \\\\), corresponding to conjugacy by \u2206on the right, sends a simple element \\\\( s(i, p) \\\\) to \\\\( s(i + \\\\ell, p) \\\\). If \\\\( m \\\\neq 1 \\\\neq \\\\ell \\\\), then the smallest trivial power of \\\\( \\\\phi \\\\) is \\\\( \\\\phi_{m, \\\\ell}^m \\\\), and \u2206\\\\( m, \\\\ell \\\\) is the smallest central power of \u2206 in \\\\( G(m, \\\\ell) \\\\).\\n\\nCorollary 2.11. (Center of circular groups)\\n\\nLet \\\\( m, \\\\ell \\\\) be two positive integers. If \\\\( m = 1 \\\\) or \\\\( \\\\ell = 1 \\\\), then \\\\( G(m, \\\\ell) \\\\simeq \\\\mathbb{Z} \\\\) is abelian. If \\\\( m = \\\\ell = 2 \\\\), then \\\\( G(m, \\\\ell) = \\\\mathbb{Z}^2 \\\\) is abelian. Otherwise \\\\( Z(G(m, \\\\ell)) \\\\) is infinite cyclic and generated by \u2206\\\\( m, \\\\ell \\\\).\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ture during a significant decrease in disk contribution, as well as the study of timing characteristics in that state.\\n\\n6 ACKNOWLEDGEMENTS\\n\\nWe would like to thank the anonymous referee for their valuable suggestions, which improved the quality of this work. This work has utilized data from AstroSat mission which is archived at Indian Space Science Data Centre (ISSDC). We are grateful to the SXT and LAXPC POC teams for providing the data and requisite softwares to perform data analysis. NH acknowledges the financial support provided by Department of Science and Technology (DST) under the INSPIRE fellowship scheme. AG, RM and SS acknowledges the financial support provided by Department of Space, Govt of India (No.DS_2B-13012(2)/2/2022-Sec.2).\\n\\n7 DATA AVAILABILITY\\n\\nThe data used in the publication is publicly available for download at [https://astrobrowse.issdc.gov.in/astro_archive/archive/Home.jsp](https://astrobrowse.issdc.gov.in/astro_archive/archive/Home.jsp) using the observation IDs mentioned in Tab 1.\\n\\nREFERENCES\\n\\nAgrawal P., et al., 2017, Journal of Astrophysics and Astronomy, 38, 30\\nAntia H., et al., 2017, The Astrophysical Journal Supplement Series, 231, 10\\nAsplund M., et al., 2009, Annual review of astronomy and astrophysics, 47, 481\\nBelloni T. M., et al., 2011, arXiv preprint arXiv:1109.3388\\nBhargava Y., et al., 2022, Monthly Notices of the Royal Astronomical Society, 512, 6067\\nCapitano F., et al., 2009, Monthly Notices of the Royal Astronomical Society, 398, 1194\\nChevalier C., Ilovaisky S., 1992, International Astronomical Union Circular, 5520, 1\\nConnors R., et al., 2021, The Astronomer\u2019s Telegram, 14725, 1\\nDauser T., et al., 2014, Monthly Notices of the Royal Astronomical Society: Letters, 444, L100\\nDavis S. W., et al., 2005, The Astrophysical Journal, 621, 372\\nDone C., et al., 2007, The Astronomy and Astrophysics Review, 15, 1\\nDraghis P. A., et al., 2022, arXiv preprint arXiv:2210.02479, DR22\\nEbisawa K., et al., 1993, Astrophysical Journal, Part 1 (ISSN 0004-637X), vol. 403, no. 2, p. 684-689, 403, 684\\nEbisawa K., et al., 2003, The Astrophysical Journal, 597, 780\\nGarcia J., Kallman T. R., 2010, The Astrophysical Journal, 718, 695\\nGarg A., et al., 2022, Monthly Notices of the Royal Astronomical Society\\nGierli\u0144ski M., Done C., 2004, Monthly Notices of the Royal Astronomical Society, 347, 885\\nGierli\u0144ski M., et al., 1999, Monthly Notices of the Royal Astronomical Society, 309, 496\\nHarmon B., et al., 1992, International Astronomical Union Circular, 5510, 2\\nHusain N., et al., 2022, Monthly Notices of the Royal Astronomical Society, 510, 4040\\nKallman T., Bautista M., 2001, The Astrophysical Journal Supplement Series, 133, 221\\nKitamoto S., et al., 1984, Publications of the Astronomical Society of Japan, 36, 799\\nKubota A., et al., 2007, Publications of the Astronomical Society of Japan, 59, S185\\nLi L.-X., et al., 2005, The Astrophysical Journal Supplement Series, 157, 335\\nMatlaky T., et al., 1972, The Astrophysical Journal, 174, L53\\nMcClintock J. E., et al., 2006, The Astrophysical Journal, 652, 518\\nMerloni A., et al., 2000, Monthly Notices of the Royal Astronomical Society, 313, 193\\nMiller J., et al., 2008, The Astrophysical Journal, 680, 1359\\nMineshige S., et al., 1994, The Astrophysical Journal, 426, 308\\nMitsuda K., et al., 1984, Publications of the Astronomical Society of Japan, 36, 741\\nMorningstar W. R., Miller J. M., 2014, The Astrophysical Journal Letters, 793, L33\\nMudambi S. P., et al., 2022, Monthly Notices of the Royal Astronomical Society, 517, 4489\\nNegoro H., et al., 2021a, The Astronomer\u2019s Telegram, 14701, 1\\nNegoro H., et al., 2021b, The Astronomer\u2019s Telegram, 14708, 1\\nNegoro H., et al., 2022, The Astronomer\u2019s Telegram, 15715, 1\\nOrosz J., et al., 2002, in American Astronomical Society Meeting Abstracts. pp 15\u201311\\nOrosz J., et al., 2003, in Proceedings of IAU Symposium.\\nPark S. Q., et al., 2004, The Astrophysical Journal, 610, 378\\nPonti G., et al., 2012, Monthly Notices of the Royal Astronomical Society: Letters, 422, L11\\nPrabakar G., et al., 2023, Monthly Notices of the Royal Astronomical Society\\nRawat D., et al., 2022, Monthly Notices of the Royal Astronomical Society, 511, 1841\\nReeves J., et al., 2008, Monthly Notices of the Royal Astronomical Society: Letters, 385, L108\\nRemillard R. A., McClintock J. E., 2006, Annu. Rev. Astron. Astrophys., 44, 49\\nShafee R., et al., 2005, The Astrophysical Journal, 636, L113\\nShakura N. I., Sunyaev R. A., 1973, Astronomy and Astrophysics, 24, 337\\nShimura T., Takahara F., 1995, The Astrophysical Journal, 445, 780\\nSingh K. P., et al., 2016, in Space Telescopes and Instrumentation 2016: Ultraviolet to Gamma Ray. pp 389\u2013398\\nSingh K., et al., 2017, Journal of Astrophysics and Astronomy, 38, 1\\nSteiner J. F., et al., 2010, The Astrophysical Journal Letters, 718, L117\\nTakahashi H., Makishima K., 2006, Proceedings of the The X-ray Universe 2005 (ESA SP-604). 26-30 September 2005, El Escorial, Madrid, Spain. Editor: A. Wilson, p. 309, 604, 309\\nTrigo M. D., et al., 2007, Astronomy & Astrophysics, 462, 657\\nVerner D., Ferland G. J., Korista K., Yakovlev D., 1996, arXiv preprint astro-ph/9601009\\nWang J., et al., 2022, The Astronomer\u2019s Telegram, 15253, 1\\nWatarai K.-y., et al., 2000 "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ture during a significant decrease in disk contribution, as well as the study of timing characteristics in that state.  \\n\\n6 ACKNOWLEDGEMENTS\\n\\nWe would like to thank the anonymous referee for their valuable suggestions, which improved the quality of this work. This work has utilized data from AstroSat mission which is archived at Indian Space Science Data Centre (ISSDC). We are grateful to the SXT and LAXPC POC teams for providing the data and requisite softwares to perform data analysis. NH acknowledges the financial support provided by Department of Science and Technology (DST) under the INSPIRE fellowship scheme. AG, RM and SS acknowledges the financial support provided by Department of Space, Govt of India (No.DS_2B-13012(2)/ / 2 / 2022-Sec.2).  \\n\\n7 DATA AVAILABILITY\\n\\nThe data used in the publication is publicly available for download at [https://astrobrowse.issdc.gov.in/astro_](https://astrobrowse.issdc.gov.in/astro_) archive/archive/Home.jsp using the observation IDs mentioned in Tab 1 .\\n\\nREFERENCES\\n\\nAgrawal P., et al., 2017, Journal of Astrophysics and Astronomy, 38, 30 Antia H., et al., 2017, The Astrophysical Journal Supplement Series, 231, 10 Asplund M., et al., 2009, Annual review of astronomy and astrophysics, 47, 481 Belloni T. M., et al., 2011, arXiv preprint arXiv:1109.3388 Bhargava Y., et al., 2022, Monthly Notices of the Royal Astronomical Society, 512, 6067 Capitanio F., et al., 2009, Monthly Notices of the Royal Astronomical Society, 398, 1194 Chevalier C., Ilovaisky S., 1992, International Astronomical Union Circular, 5520, 1 Connors R., et al., 2021, The Astronomer\u2019s Telegram, 14725, 1 Dauser T., et al., 2014, Monthly Notices of the Royal Astronomical Society: Letters, 444, L100 Davis S. W., et al., 2005, The Astrophysical Journal, 621, 372 Done C., et al., 2007, The Astronomy and Astrophysics Review, 15, 1 Draghis P. A., et al., 2022, arXiv preprint arXiv:2210.02479, DR22 Ebisawa K., et al., 1993, Astrophysical Journal, Part 1 (ISSN 0004-637X), vol. 403, no. 2, p. 684-689., 403, 684 Ebisawa K., et al., 2003, The Astrophysical Journal, 597, 780 Garcia J., Kallman T. R., 2010, The Astrophysical Journal, 718, 695 Garg A., et al., 2022, Monthly Notices of the Royal Astronomical Society Gierli\u00b4nski M., Done C., 2004, Monthly Notices of the Royal Astronomical Society, 347, 885 Gierli\u00b4nski M., et al., 1999, Monthly Notices of the Royal Astronomical Society, 309, 496 Harmon B., et al., 1992, International Astronomical Union Circular, 5510, 2 Husain N., et al., 2022, Monthly Notices of the Royal Astronomical Society, 510, 4040 Kallman T., Bautista M., 2001, The Astrophysical Journal Supplement Series, 133, 221 Kitamoto S., et al., 1984, Publications of the Astronomical Society of Japan, 36, 799 Kubota A., et al., 2007, Publications of the Astronomical Society of Japan, 59, S185 Li L.-X., et al., 2005, The Astrophysical Journal Supplement Series, 157, 335 Matilsky T., et al., 1972, The Astrophysical Journal, 174, L53 McClintock J. E., et al., 2006, The Astrophysical Journal, 652, 518 Merloni A., et al., 2000, Monthly Notices of the Royal Astronomical Society, 313, 193 Miller J., et al., 2008, The Astrophysical Journal, 680, 1359 Mineshige S., et al., 1994, The Astrophysical Journal, 426, 308 Mitsuda K., et al., 1984, Publications of the Astronomical Society of Japan, 36, 741 Morningstar W. R., Miller J. M., 2014, The Astrophysical Journal Letters, 793, L33 Mudambi S. P., et al., 2022, Monthly Notices of the Royal Astronomical Society, 517, 4489 Negoro H., et al., 2021a, The Astronomer\u2019s Telegram, 14701, 1 Negoro H., et al., 2021b, The Astronomer\u2019s Telegram, 14708, 1 Negoro H., et al., 2022, The Astronomer\u2019s Telegram, 15715, 1 Orosz J., et al., 2002, in American Astronomical Society Meeting Abstracts. pp 15\u201311 Orosz J., et al., 2003, in Proceedings of IAU Symposium. Park S. Q., et al., 2004, The Astrophysical Journal, 610, 378 Ponti G., et al., 2012, Monthly Notices of the Royal Astronomical Society: Letters, 422, L11 Prabakar G., et al., 2023, Monthly Notices of the Royal Astronomical Society Rawat D., et al., 2022, Monthly Notices of the Royal Astronomical Society, 511, 1841 Reeves J., et al., 2008, Monthly Notices of the Royal Astronomical Society: Letters, 385, L108 Remillard R. A., McClintock J. E., 2006, Annu. Rev. Astron. Astrophys., 44, 49 Shafee R., et al., 2005, The Astrophysical Journal, 636, L113 Shakura N. I., Sunyaev R. A., 1973, Astronomy and Astrophysics, 24, 337 Shimura T., Takahara F., 1995, The Astrophysical Journal, 445, 780 Singh K. P., et al., 2016, in Space Telescopes and Instrumentation 2016: Ultraviolet to Gamma Ray. pp 389\u2013398 Singh K., et al., 2017, Journal of Astrophysics and Astronomy, 38, 1 Steiner J. F., et al., 2010, The Astrophysical Journal Letters, 718, L117 Takahashi H., Makishima K., 2006, Proceedings of the The X-ray Universe 2005 (ESA SP-604). 26-30 September 2005, El Escorial, Madrid, Spain. Editor: A. Wilson, p. 309, 604, 309 Trigo M. D., et al., 2007, Astronomy & Astrophysics, 462, 657 Verner D., Ferland G. J., Korista K., Yakovlev D., 1996, arXiv preprint astro-ph/9601009 Wang J., et al., 2022, The Astronomer\u2019s Telegram, 15253, 1 Watarai K.-y., et al., 2000, Publications of the Astronomical Society of Japan, 52, 133 Wilms J., et al., 2000, The Astrophysical Journal, 542, 914 Zhang X., et al., 2022, The "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the presynaptic and postsynaptic neuron activities have low correlation their connection are likely to be removed. The latter process is called synaptic pruning and it is considered essential for optimizing activity propagation and memory capacity (Chklovskii, Mel, and Svoboda, 2004; Knoblauch et al., 2014; Knoblauch and Sommer, 2016). Furthermore, it is commonly believed that synaptic pruning and rewiring dysfunction are one of the neural correlate of developmental disorders such as autism or schizophrenia (Bourgeron, 2009; Moyer, Shelton, and Sweet, 2015), leading to, respectively, an higher or lower synaptic density with respect to neurotypical subjects (Hutsler and Zhang, 2010; Pagani et al., 2021; Glantz and Lewis, 2000).\\n\\nIn the last decades computational neuroscience has investigated brain dynamics at different scales, from cellular (Markram et al., 2015) to mesoscopic and macroscopic through mean-field approaches (Wilson and Cowan, 1972; Amit and Brunel, 1997; Hopfield, 1984; Renart, Brunel, and Wang, 2004; Leon et al., 2013; di Santo et al., 2018; Capone et al., 2019; Carlu et al., 2020). Regarding synaptic plasticity, computational models were mostly focused on plasticity mechanisms that involve strengthening or weakening of existing synapses, like short-term plasticity (STP) (Tsodyks, Pawelzik, and Markram, 1998) or spike timing-dependent plasticity (STDP) (G\u00fctig et al., 2003) and on their role in short-term, long-term, working memory and learning (Mongillo, Barak, and Tsodyks, 2008; Tiddia et al., 2022b; Song, Miller, and Abbott, 2000; qiang Bi and ming Poo, 2001; Golosio et al., 2021; Capone et al., 2022). Only in recent times computational models of structural plasticity and connectivity rearrangements during learning were developed, showing intriguing results. Knoblauch et al. (2014) and Knoblauch and Sommer (2016) describe a model of structural plasticity based on \u201ceffectual connectivity\u201d, defined in these works as the fraction of synapses able to represent a memory stored in a network. By structural plasticity, effectual connectivity is improved, since synapses that do not code for the memory are moved in order to optimize network\u2019s connectivity. Their model defines synapses using a Markov model of three states: potential (i.e. not instantiated), instantiated but silent or instantiated and consolidated. Structural plasticity is thus related to the passage of the synapses from a potential state to an instantiated state (and vice versa), whereas changes only related to the synaptic weight are described by the consolidation of the instantiated synapses. With such a model, it is possible to show that networks with structural plasticity have higher or comparable memory capacity to networks with dense connectivity and it is possible to explain some cognitive mechanism such as the spacing effect (Knoblauch et al., 2014).\\n\\nSpiess et al. (2016) simulated a spiking neural network with structural plasticity and STDP, showing that structural plasticity reduces the amount of noise of the network after a learning process, thus making the network able to have a clearer output. Furthermore, such a network with structural plasticity shows higher learning speed than the same network with only STDP implemented.\\n\\nSome new insights about the importance of synaptic pruning are also shown in Navlakha, Barth, and Bar-Joseph (2015), in which different pruning rates were studied suggesting that a slowly decreasing rate of pruning over time leads to more efficient network architectures.\\n\\nAs discussed above, the biochemical and biophysical mechanisms underlying structural plasticity are extremely complex and only partially understood to date. For this reason, rather than attempting to build a biologically detailed model, this work exploits a relatively simple phenomenological model, including both the activity-driven and the homeostatic contributions; despite the lower complexity, this model accounts for the effects of structural plasticity in terms of the consolidation of synaptic connections between neurons with a high activity correlation as well as those of pruning and rewiring the connections for which this correlation is lower. This approach is also justified by the requirement for a simple and effective computational model suitable for simulating networks with a relatively large number of neurons and connections and for representing learning processes with sizable numbers of training and validation patterns. This model will then serve as the foundation for the creation of a mean-field-based theoretical framework for learning through synaptic plasticity capable of accounting for a variety of biological network properties. This framework will be used in a training and validation procedure to characterize learning and memory capacity of plastic neuronal networks as the number of training patterns and other model parameters vary. The results will then be compared with those obtained through simulations based on firing-rate-based neuronal networks. The model consid-\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the presynaptic and postsynaptic neuron activities have low correlation their connection are likely to be removed. The latter process is called synaptic pruning and it is considered essential for optimizing activity propagation and memory capacity(Chklovskii, Mel, and Svoboda, 2004; Knoblauch et al., 2014; Knoblauch and Sommer, 2016). Furthermore, it is commonly believed that synaptic pruning and rewiring dysfunction are one of the neural correlate of developmental disorders such as autism or schizophrenia (Bourgeron, 2009; Moyer, Shelton, and Sweet, 2015), leading to, respectively, an higher or lower synaptic density with respect to neurotypical subjects(Hutsler and Zhang, 2010; Pagani et al., 2021; Glantz and Lewis, 2000). In the last decades computational neuroscience has investigated brain dynamics at different scales, from cellular (Markram et al., 2015) to mesoscopic and macroscopic through mean-field approaches (Wilson and Cowan, 1972; Amit and Brunel, 1997; Hopfield, 1984; Renart, Brunel, and Wang, 2004; Leon et al., 2013; di Santo et al., 2018; Capone et al., 2019; Carlu et al., 2020). Regarding synaptic plasticity, computational models were mostly focused on plasticity mechanisms that involve strengthening or weakening of existing synapses, like short-term plasticity (STP) (Tsodyks, Pawelzik, and Markram, 1998) or spike timing-dependent plasticity (STDP) (G\u00a8utig et al., 2003) and on their role in short-term, long-term, working memory and learning (Mongillo, Barak, and Tsodyks, 2008; Tiddia et al., 2022b; Song, Miller, and Abbott, 2000; qiang Bi and ming Poo, 2001; Golosio et al., 2021; Capone et al., 2022). Only in recent times computational models of structural plasticity and connectivity rearrangements during learning were developed, showing intriguing results. Knoblauch et al. (2014) and Knoblauch and Sommer (2016) describe a model of structural plasticity based on \u201deffectual connectivity\u201d, defined in these works as the fraction of synapses able to represent a memory stored in a network. By structural plasticity, effectual connectivity is improved, since synapses that do not code for the memory are moved in order to optimize network\u2019s connectivity. Their model defines synapses using a Markov model of three states: potential (i.e. not instantiated), instantiated but silent or instantiated and consolidated. Structural plasticity is thus related to the passage of the synapses from a potential state to an instantiated state (and vice versa), whereas changes only related to the synaptic weight are described by the consolidation of the instantiated synapses. With such a model, it is possible to show that networks with structural plasticity have higher or comparable memory capacity to networks with dense connectivity and it is possible to explain some cognitive mechanism such as the spacing effect (Knoblauch et al., 2014). Spiess et al. (2016) simulated a spiking neural network with structural plasticity and STDP, showing that structural plasticity reduces the amount of noise of the network after a learning process, thus making the network able to have a clearer output. Furthermore, such a network with structural plasticity shows higher learning speed than the same network with only STDP implemented. Some new insights about the importance of synaptic pruning are also shown in Navlakha, Barth, and Bar-Joseph (2015), in which different pruning rates were studied suggesting that a slowly decreasing rate of pruning over time leads to more efficient network architectures. As discussed above, the biochemical and biophysical mechanisms underlying structural plasticity are extremely complex and only partially understood to date. For this reason, rather than attempting to build a biologically detailed model, this work exploits a relatively simple phenomenological model, including both the activity-driven and the homeostatic contributions; despite the lower complexity, this model accounts for the effects of structural plasticity in terms of the consolidation of synaptic connections between neurons with a high activity correlation as well as those of pruning and rewiring the connections for which this correlation is lower. This approach is also justified by the requirement for a simple and effective computational model suitable for simulating networks with a relatively large number of neurons and connections and for representing learning processes with sizable numbers of training and validation patterns. This model will then serve as the foundation for the creation of a mean-field-based theoretical framework for learning through synaptic plasticity capable of accounting for a variety of biological network properties. This framework will be used in a training and validation procedure to characterize learning and memory capacity of plastic neuronal networks as the number of training patterns and other model parameters vary. The results will then be compared with those obtained through simulations based on firing-rate-based neuronal networks. The model consid-\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"length $\\\\xi_L$ which is given by\\n\\n$$\\\\xi_L^2 \\\\equiv \\\\frac{1}{4 \\\\sin^2(k/2)} \\\\left( \\\\frac{\\\\langle q^2 \\\\rangle}{\\\\langle |q(k)|^2 \\\\rangle} - 1 \\\\right),$$\\n\\n(11)\\n\\nwhere $q(\\\\vec{k})$ is\\n\\n$$q(\\\\vec{k}) \\\\equiv \\\\frac{1}{N} \\\\sum_j s_j^{(1)} s_j^{(2)} e^{i \\\\vec{k} \\\\cdot \\\\vec{r}_j},$$\\n\\n(12)\\n\\nwith $\\\\vec{r}_j$ the position of the $j$-th NP, $\\\\vec{k} = (2\\\\pi/L, 0, 0)$ and $k = ||\\\\vec{k}|| = 2\\\\pi/L$.\\\\textsuperscript{39}\\n\\nErrors in the measurements of these quantities have been calculated as the mean squared deviations of the sample-to-sample fluctuations.\\n\\n### III. RESULTS\\n\\n#### A. Phase diagram for isotropic HS-like configurations\\n\\nIn this section we investigate the magnetic order as a function of the volume fraction $\\\\Phi$ for frozen configurations obtained from equilibrium states of hard sphere fluids in the range $0 < \\\\Phi \\\\leq 0.49$. $\\\\Phi$ measures the degree of spatial disorder on such configurations. We will show that for decreasing $\\\\Phi$ (which means increasing disorder) SG order replaces the FM order.\\n\\nA first overview can be grasped from Figs. 1-2. Fig. 1(a) displays plots of the specific heat $c$ vs $T$ for $\\\\Phi = 0.4$. The curves exhibit a marked lambda-shaped peak. Their evident dependence on the number of NP indicates the presence of a singular point in the curve that corresponds to $N \\\\to \\\\infty$ at $T_c \\\\approx 1.9$. That singular behavior is expected in PM-FM second order transitions. Data are consistent with a logarithmic divergence of $c$ with $N$. Fig. 1(b) shows the plots obtained for $\\\\Phi = 0.1$. In contrast to the previous ones, these plots are smooth and depend little on the sample size. So, there is no sign of any singular behavior. This is expected in PM-SG transitions with strong structural disorder.\\n\\nFM order entails the presence of non-vanishing magnetization $m$. Fig. 2(a) displays $m_1$ vs $T$ for $\\\\Phi = 0.4$ at several $N$. They show that $m_1$ tends to non-zero values for $N \\\\to \\\\infty$ and low $T$, revealing the existence of strong FM order. The curves plotted in Fig. 2(b) for the magnetic susceptibility $\\\\chi_m$ vs $T$ confirm this conclusion as they show peaks that become sharper for large $N$. An extrapolation of the positions of the maxima of those peaks vs $1/N$ provides a value for the transition temperature, $T_c(\\\\Phi = 0.4) \\\\approx 1.9(1)$, in agreement with the estimated $T_c$ obtained from the analysis of Fig. 1(a).\\n\\nFor $T < T_c$ we find that $\\\\chi_m$ does not diverge with $N$, a fact that validates the above conclusions on FM order. All that is in contrast to the results obtained for $\\\\Phi = 0.1$, shown in Fig. 2(c) where we see how the values of $\\\\chi_m$ increase with $N$ for low $T$. Data are consistent with a trend $\\\\chi_m \\\\sim N^p$ for $p = 0.45$ and $T \\\\leq 0.2$. This behavior suggests the existence of a SG phase.\\n\\nLet us discuss now the threshold value of $\\\\Phi$ at which the FM order disappears. Mean-field calculations predict that FM order persists for $\\\\Phi \\\\geq \\\\Phi_c = \\\\pi/20 \\\\sim 0.157$.\\n\\nThe plots in Fig. 3 show that the FM order persists at $\\\\Phi = 0.18$. The curves of $m_1$ vs $T$ in panel (a) indicate an increase in magnetization with $N$ at low $T$, although they also exhibit relevant finite size effects. The Binder parameter of panel (b) allows to determine the transition temperature within good precision. In general this parameter tends to 1 for $N \\\\to \\\\infty$ in FM phases, while from the law of large numbers it follows that in PM phases $B_m \\\\to 0$ as $N$ increases. On the other hand, since $B_m$ is dimensionless, it must be independent of $N$ at the critical point. As a consequence, curves of $B_m$ vs $T$ for different values of $N$ cross at $T_c$ for second order transitions. Instead, in presence of an intermediate marginal phase of quasi-long-range FM order, the curves do not cross but join. Plots of $B_m$ vs $T$ for several $N$ are shown in Fig. 3(b) for $\\\\Phi = 0.18$. Those curves cross at a well defined critical temperature for $N \\\\geq 512$.\\\\textsuperscript{38} With similar results obtained for $\\\\Phi \\\\geq 0.17$, we can draw a line of transition between PM and FM phases.\\n\\nThe corresponding plots for $\\\\Phi = 0.14$ are shown in Fig. 4. The qualitatively different results illustrate the absence of FM order at this value of $\\\\Phi$. The plots of the magnetization in panel (a) show that $m_1$ gradually decreases as $N$ increases for all $T$. The data of $m_1$ for low temperature agree with an algebraic decay $m_1 \\\\sim N^p$ for $p < 1/2$, hence a marginal order is a priori not excluded. However, the plots of $B_m$ vs $T$ from panel (b)\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"length $\\\\xi_L$ which is given by\\n\\n$$\\\\xi_L^2 \\\\equiv \\\\frac{1}{4 \\\\sin^2(k/2)} \\\\left( \\\\frac{\\\\langle q^2 \\\\rangle}{\\\\langle |q(k)|^2 \\\\rangle} - 1 \\\\right),$$\\n\\n(11)\\n\\nwhere $q(\\\\vec{k})$ is\\n\\n$$q(\\\\vec{k}) = \\\\frac{1}{N} \\\\sum_j s_j^{(1)} s_j^{(2)} e^{i \\\\vec{k} \\\\cdot \\\\vec{r}_j},$$\\n\\n(12)\\n\\nwith $\\\\vec{r}_j$ the position of the $j$-th NP, $\\\\vec{k} = (2\\\\pi/L, 0, 0)$ and $k = ||\\\\vec{k}|| = 2\\\\pi/L$.\\\\textsuperscript{39}\\n\\nErrors in the measurements of these quantities have been calculated as the mean squared deviations of the sample-to-sample fluctuations.\\\\textsuperscript{39}\\n\\nIII. RESULTS\\n\\nA. Phase diagram for isotropic HS-like configurations\\n\\nIn this section we investigate the magnetic order as a function of the volume fraction $\\\\Phi$ for frozen configurations obtained from equilibrium states of hard sphere fluids in the range $0 < \\\\Phi \\\\leq 0.49$. $\\\\Phi$ measures the degree of spatial disorder on such configurations. We will show that for decreasing $\\\\Phi$ (which means increasing disorder) SG order replaces the FM order.\\\\textsuperscript{39}\\n\\nA first overview can be gras [width=85mm]figure1.pdf\\n\\nFig. 1-2. Fig. 1(a) displays plots of the specific heat $c$ vs $T$ for $\\\\Phi = 0.4$. The curves exhibit a marked lambda-shaped peak. Their evident dependence on the number of NP indicates the presence of a singular point in the curve that corresponds to $N \\\\to \\\\infty$ at $T_c \\\\approx 1.9$. That singular behavior is expected in PM-FM second order transitions. Data are consistent with a logarithmic divergence of with $N$. Fig. 1(b) shows the plots obtained for $\\\\Phi = 0.1$. In contrast to the previous ones, these plots are smooth and depend little on the sample size. So, there is no sign of any singular behavior. This is expected in PM-SG transitions with strong structural disorder.\\\\textsuperscript{39}\\n\\nFM order entails the presence of non-vanishing magnetization $m$. Fig. 2(a) displays $m_1$ vs $T$ for $\\\\Phi = 0.4$ at several $N$. They show that $m_1$ tends to non-zero values for $N \\\\to \\\\infty$ and low $T$, revealing the existence of strong FM order. The curves plotted in Fig. 2(b) for the magnetic susceptibility $\\\\chi_m$ vs $T$ confirm this conclusion as they show peaks that become sharper for large $N$. An extrapolation of the positions of the maxima of those peaks vs $1/N$ provides a value for the transition temperature, $T_c(\\\\Phi = 0.4) \\\\approx 1.9(1)$, in agreement with the estimated $T_c$ obtained from the analysis of Fig. 1(a). For $T < T_c$ we find that $\\\\chi_m$ does not diverge with $N$, a fact that validates the above conclusions on FM order. All that is in contrast to the results obtained for $\\\\Phi = 0.1$, shown in Fig. 2(c) where we see how the values of $\\\\chi_m$ increase with $N$ for low $T$. Data are consistent with a trend $\\\\chi_m \\\\sim N^p$ for $p = 0.45$ and $T \\\\leq 0.2$. This behavior suggests the existence of a SG phase.\\\\textsuperscript{39}\\n\\nLet us discuss now the threshold value of $\\\\Phi$ at which the FM order disappears. Mean-field calculations predict that FM order persists for $\\\\Phi \\\\geq \\\\Phi_c = \\\\pi/20 \\\\sim 0.157$.\\\\textsuperscript{39}\\n\\nThe plots in Fig. 3 show that the FM order persists at $\\\\Phi = 0.18$. The curves of $m_1$ vs $T$ in panel (a) Plots of the magnetization $m_1$ versus $T$ for volume fraction $\\\\Phi = 0.4$. Symbols $\\\\vartriangle$, $\\\\triangle$, $\\\\circ$, $\\\\ast$, and $\\\\bullet$ stand for $N = 125, 216, 512, 1000$ and $1728$ respectively. (b) Plots of the magnetic susceptibility $\\\\chi_m$ versus $T$ for volume fraction $\\\\Phi = 0.4$. Same symbols as in (a). (c) Same as in (b) but for volume fraction $\\\\Phi = 0.1$.\\\\textsuperscript{39}\\n\\nFig. 2. (a) Plots of the magnetization $m_1$ versus $T$ for volume fraction $\\\\Phi = 0.4$. Symbols $\\\\vartriangle$, $\\\\triangle$, $\\\\circ$, $\\\\ast$, and $\\\\bullet$ stand for $N = 125, 216, 512, 1000$ and $1728$ respectively. (b) Plots of the Binder cumulant of the magnetization $B_m$ vs $T$ for $\\\\Phi = 0.18$. Same symbols as in (a).\\\\textsuperscript{39}\\n\\nFig. 3. (a) Plots of the magnetization $m_1$ vs $T$ for $\\\\Phi = 0.18$. Symbols $\\\\vartriangle$, $\\\\triangle$, $\\\\circ$, and $\\\\ast$ stand for $N = 216, 512, 1000$ and $1728$ respectively. (b) Plots of the Binder cumulant of the magnetization $B_m$ vs $T$ for $\\\\Phi = 0.18$. Same symbols as in (a).\\\\textsuperscript{39}\\n\\nThe corresponding plots for $\\\\Phi = 0.14$ are shown in Fig. 4. The qualitatively different results illustrate the absence of FM order at this value of $\\\\Phi$. The plots of the magnetization in panel (a) show that $m_1$ gradually decreases as $N$ increases for all $T$. The data of $m_1$ for low temperature agree with an algebraic decay $m_1 \\\\sim N^p$ for $p < 1/2$, hence a marginal order is a priori not excluded. However, the plots of $B_m$ vs $T$ from panel (b)\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Let us now informally discuss the case when $\\\\alpha > 0$. For simplicity, we consider (13). The limit on the right-hand side is non-zero, which suggests that there is a residual dependence between the $k(N)$ spins under the Gibbs measure. The reason for the non-zero limit is the fact that the distribution of $\\\\mathcal{P}_{k(N)}$ and the corresponding binomial distribution satisfy central limit theorems with different variances, the variance of $\\\\mathcal{P}_{k(N)}$ being strictly larger, which comes from the fact that the spins are positively correlated under the Gibbs measure. The distance between these normal distributions appears on the right-hand side of (13). In Theorem 3.5, we shall determine a mixed binomial distribution which approximates the distribution of $\\\\mathcal{P}_{k(N)}$ under $\\\\mu_N$. In some sense, this describes the residual dependence between the spins under the Gibbs measure.\\n\\nRemark 1.2. The exchangeability of the measure $\\\\mu_N$ has been used to investigate the Curie-Weiss model for example, in [17, Section 5.2] and [2]. In particular, an explicit representation of $\\\\mu_N$ as a mixture of Bernoulli measures (valid for each fixed $N$) can be found in [17, Theorem 5.6]. A general propagation of chaos principle stating that the distribution of $k$ entries in a finite exchangeable vector of length $n$ can be approximated by a mixture of i.i.d. distributions can found in [7].\\n\\nThe paper is organized as follows. Our proof relies on local limit theorems for the magnetization $m_N$ and also for the total number of positive spins $\\\\mathcal{P}_N$ under $\\\\mu_N$. In some regimes those are known. We collect the corresponding results in Section 2 below. The proofs of these local limit theorems, which we have not been able to locate in the literature, are given in Section 4. The proof of Theorem 1.1 is given in Section 3, including the statement of residual dependence. Two auxiliary technical results related to calculations of the total variation distance are presented in Section 5.\\n\\n2. LOCAL LIMIT THEOREM FOR THE MAGNETIZATION\\n\\nDenote by $\\\\mathcal{N}(m, v^2)$ a Gaussian distribution with mean $m$ and variance $v^2$, so\\n\\n$$\\\\mathcal{N}(m, v^2)(A) = \\\\int_A \\\\varphi(t; m, v^2)dt, \\\\quad A \\\\in \\\\mathcal{B}(\\\\mathbb{R}).$$\\n\\nPut $\\\\delta_N := (1 - (-1)^N)/2$. This correction term appears below in the local limit theorems for $m_N$, since $Nm_N$ always has the same parity as $N$.\\n\\n**Proposition 2.1.** Assume that $h \\\\neq 0$ or $0 < \\\\beta < 1$. Then\\n\\n$$\\\\mu_N \\\\left( \\\\sqrt{N}(m_N - m(\\\\beta, h)) \\\\in \\\\cdot \\\\right) \\\\Rightarrow \\\\mathcal{N} \\\\left( 0, \\\\frac{\\\\delta_N}{\\\\beta^2} \\\\right), \\\\quad N \\\\to \\\\infty,$$\\n\\nand the following local limit theorem holds true:\\n\\n$$\\\\lim_{N \\\\to \\\\infty} \\\\sqrt{N} \\\\sup_{\\\\ell \\\\in \\\\mathbb{Z}} \\\\left| \\\\mu_N \\\\left( \\\\frac{Nm_N + \\\\delta_N}{2} = \\\\ell \\\\right) - \\\\varphi \\\\left( \\\\ell; \\\\frac{Nm(\\\\beta, h)}{2}, \\\\frac{N\\\\delta_N^2}{4} \\\\right) \\\\right| = 0. \\\\quad (16)$$\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Let us now informally discuss WHEN DOES THE CHAOS IN THE CURIE-WEISS MODEL STOP TO PROPAGATE? the case when $\\\\alpha > 0$. For simplicity, we consider (13). The limit on the right-hand side is non-zero, which suggests that there is a residual dependence between the $k(N)$ spins under the Gibbs measure. The reason for the non-zero limit is the fact that the distribution of $\\\\mathcal{P}_{k(N)}$ and the corresponding binomial distribution satisfy central limit theorems with different variances, the variance of $\\\\mathcal{P}_{k(N)}$ being strictly larger, which comes from the fact that the spins are positively correlated under the Gibbs measure. The distance between these normal distributions appears on the right-hand side of (13). In Theorem 3.5, we shall determine a mixed binomial distribution which approximates the distribution of $\\\\mathcal{P}_{k(N)}$ under $\\\\mu_N$. In some sense, this describes the residual dependence between the spins under the Gibbs measure. \\n\\nRemark 1.2. The exchangeability of the measure $\\\\mu_N$ has been used to investigate the Curie-Weiss model for example, in [17, Section 5.2] and [2]. In particular, an explicit representation of $\\\\mu_N$ as a mixture of Bernoulli measures (valid for each fixed $N$) can be found in [17, Theorem 5.6]. A general propagation of chaos principle stating that the distribution of $k$ entries in a finite exchangeable vector of length $n$ can be approximated by a mixture of i.i.d. distributions can found in [7].\\n\\nThe paper is organized as follows. Our proof relies on local limit theorems for the magnetization $m_N$ and also for the total number of positive spins $\\\\mathcal{P}_N$ under $\\\\mu_N$. In some regimes those are known. We collect the corresponding results in Section 2 below. The proofs of these local limit theorems, which we have not been able to locate in the literature, are given in Section 4. The proof of Theorem 1.1 is given in Section 3, including the statement of residual dependence. Two auxiliary technical results related to calculations of the total variation distance are presented in Section 5. OCAL LIMIT THEOREM FOR THE MAGNETIZATION\\n\\nDenote by $\\\\mathcal{N}(m, v^2)$ a Gaussian distribution with mean $m$ and variance $v^2$, so\\n\\n$$\\\\mathcal{N}(m, v^2)(A) = \\\\int_A \\\\varphi(t; m, v^2)dt, \\\\quad A \\\\in \\\\mathcal{B}(\\\\mathbb{R}).$$\\n\\nPut $\\\\delta_N := (1 - (-1)^N)/2$. This correction term appears below in the local limit theorems for $m_N$, since $Nm_N$ always has the same parity as $N$.\\n\\n**Proposition 2.1.** Assume that $h \\\\neq 0$ or $0 < \\\\beta < 1$. Then\\n\\n$$\\\\mu_N \\\\left( \\\\sqrt{N} (m_N - m(\\\\beta, h)) \\\\in \\\\cdot \\\\right) \\\\Rightarrow \\\\mathcal{N} \\\\left( 0, \\\\frac{\\\\delta_N}{\\\\beta, h} \\\\right), \\\\quad N \\\\to \\\\infty,$$\\n\\nand the following local limit theorem holds true: \\n\\n$$\\\\lim_{N \\\\to \\\\infty} \\\\sup_{t \\\\in \\\\mathbb{Z}} \\\\left| \\\\mu_N \\\\left( \\\\frac{Nm_N + \\\\delta_N}{2} = t \\\\right) - \\\\varphi \\\\left( t; \\\\frac{Nm(\\\\beta, h)}{2}, \\\\frac{N\\\\delta_N^2}{4} \\\\right) \\\\right| = 0. \\\\quad (16)$$\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing with Non-Learnable Primitives\\n\\nChuntao Ding\\\\textsuperscript{1*} Zhichao Lu\\\\textsuperscript{2\u2020} Shangguang Wang\\\\textsuperscript{3} Ran Cheng\\\\textsuperscript{4} Vishnu N. Boddeti\\\\textsuperscript{5}\\n\\n\\\\textsuperscript{1}Beijing Jiaotong University \\\\textsuperscript{2}Sun Yat-sen University \\\\textsuperscript{3}Beijing University of Posts and Telecommunications \\\\textsuperscript{4}Southern University of Science and Technology \\\\textsuperscript{5}Michigan State University\\n\\nchuntaoding@163.com \\\\{luzhichaocn, ranchengcn\\\\}@gmail.com sgwang@bupt.edu.cn vishnu@msu.edu\\n\\nAbstract\\n\\nMulti-task learning (MTL) seeks to learn a single model to accomplish multiple tasks by leveraging shared information among the tasks. Existing MTL models, however, have been known to suffer from negative interference among tasks. Efforts to mitigate task interference have focused on either loss/gradient balancing or implicit parameter partitioning with partial overlaps among the tasks. In this paper, we propose ETR-NLP to mitigate task interference through a synergistic combination of non-learnable primitives (NLPs) and explicit task routing (\\\\textsuperscript{\u2020}). Our key idea is to employ non-learnable primitives to extract a diverse set of task-agnostic features and recombine them into a shared branch common to all tasks and explicit task-specific branches reserved for each task. The non-learnable primitives and the explicit decoupling of learnable parameters into shared and task-specific ones afford the flexibility needed for minimizing task interference. We evaluate the efficacy of ETR-NLP networks for both image-level classification and pixel-level dense prediction MTL problems. Experimental results indicate that ETR-NLP significantly outperforms state-of-the-art baselines with fewer learnable parameters and similar FLOPs across all datasets. Code is available at this URL.\\n\\n1. Introduction\\n\\nMulti-task learning (MTL) is commonly employed to improve learning efficiency and performance of multiple tasks by using supervised signals from other related tasks [6, 33, 49]. These models have led to impressive results across numerous tasks. However, there is well-documented evidence [18,28,41,53] that these models are suffering from task interference [53], thereby limiting multi-task networks (MTNs) from realizing their full potential.\\n\\n\\\\textsuperscript{*}Work done as a visiting scholar at Michigan State University.\\n\\\\textsuperscript{\u2020}Corresponding author\\n\\nFor instance, consider the learning progression of an MTN with a standard learnable convolutional layer in Figure 1a (blue curve). Observe that the model learns rapidly, we posit, by exploiting all the shared information between the tasks, i.e., gradients pointing in similar directions. However, the performance starts degrading on further training since the model needs to exploit dissimilar information between the tasks for further improvement, i.e., gradients point in different directions. The latter can be verified by observing the similarity (centered kernel alignment [19]), or the lack thereof, between the gradients for each pair of tasks in Figure 1b.\\n\\nSeveral approaches were proposed for mitigating task interference in MTNs, including loss/gradient balancing [17, 21, 22, 34, 52], parameter partitioning [2, 28, 30, 37] and architectural design [8, 18, 29]. Despite the diversity of these approaches, they share two common characteristics, (i) all parameters are learned, either for a pre-trained task or for the multiple tasks at hand, (ii) the learned parameters are either fully shared across all tasks or are shared across a partial set of tasks through implicit partitioning, i.e., with no direct control over which parameters are shared across which tasks. Both of these features limit the flexibility of existing\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing with Non-Learnable Primitives\\n\\nChuntao Ding1*, Zhichao Lu2\u2020, Shangguang Wang3, Ran Cheng4, Vishnu N. Boddeti5\\n1 Beijing Jiaotong University 2 Sun Yat-sen University 3 Beijing University of Posts and Work done as a visiting scholar at Michigan State University. 4 Southern University of Science and Beijing University of Posts and Telecommunications 5 Michigan State University\\n\\nchuntaoding@16 Southern University of Science and Technology 5 Michigan State University\\n\\nchuntaoding@163.com {luzhichaocn, ranchengcn}@gmail.com sgwang@bupt.edu.cn vishnu@msu.edu\\n\\nAbstract\\n\\nMulti-task learning (MTL) seeks to learn a single model to accomplish multiple tasks by leveraging shared infor- mation among the tasks. Existing MTL models, however, have been known to suffer from negative interference among tasks. Efforts to mitigate task interference have focused on either loss/gradient balancing or implicit parameter par- titioning with partial overlaps among the tasks. In this paper, we propose ETR-NLP to mitigate task interference through a synergistic combination of non-learnable prim- itives (NLPs) and explicit task routing (). Our key idea is to employ non-learnable primitives to extract a diverse set of task-agnostic features and recombine them into a shared branch common to all tasks and explicit task-specific branches reserved for each task. The non-learnable prim- itives and the explicit decoupling of learnable parame- ters into shared and task-specific ones afford the flexibility needed for minimizing task interference. We evaluate the efficacy of ETR-NLP networks for both image-level clas- sification and pixel-level dense prediction MTL problems. Experimental results indicate that ETR-NLP significantly outperforms state-of-the-art baselines with fewer learnable parameters and similar FLOPs across all datasets. Code is available at this URL.\\n\\n1. Introduction\\n\\nMulti-task learning (MTL) is commonly employed to improve learning efficiency and performance of multiple tasks by using supervised signals from other related tasks [ 6 , 33 , 49 ]. These models have led to impressive results across numerous tasks. However, there is well-documented evidence [ 18 , 28 , 41 , 53 ] that these models are suffering from task interference [ 53 ], thereby limiting multi-task networks (MTNs) from realizing their full potential. able parameters (gray) learn rapidly and then suffer from performance degradation due to conflicting gradients from task interference. Networks with non-learnable primitives (NLPs; blue) do not suffer from task interference by design, while explicit task routing (ETR; green), and ETR with NLPs (red) do not eliminate but sured via CKA [ 19 ] across all pairs of tasks for different layers of a standard MTN at the end of training. Observe the acute lack of correlation between tasks (low off-diagonal magnitude). For instance, consider the learning progression of an MTN with a standard learnable convolutional layer in Figure 1a (blue curve). Observe that the model learns rapidly, we posit, by exploiting all the shared information between the tasks, i.e., gradients pointing in similar directions. However, the performance starts degrading on further training since the model needs to exploit dissimilar information between the tasks for further improvement, i.e., gradients point in different directions. The latter can be verified by observing the similarity (centered kernel alignment [ 19 ]), or the lack thereof, between the gradients for each pair of tasks in Figure 1b.\\n\\nSeveral approaches were proposed for mitigating task interference in MTNs, including loss/gradient balancing [ 17 , 21 , 22 , 34 , 52 ], parameter partitioning [ 2 , 28 , 30 , 37 ] and archi- tectural design [ 8 , 18 , 29 ]. Despite the diversity of these approaches, they share two common characteristics, (i) all parameters are learned, either for a pre-trained task or for the multiple tasks at hand, (ii) the learned parameters are either fully shared across all tasks or are shared across a partial set of tasks through implicit partitioning, i.e., with no direct control over which parameters are shared across which tasks. Both of these features limit the flexibility of existing\\n\\nFigure 1. (a) Learning progression of multi-task networks (MTNs) on CelebA for eight tasks. Hard-sharing models with fully learnable parameters (gray) learn rapidly and then suffer from performance degradation due to conflicting gradients from task interference. Networks with non-learnable primitives (NLPs; blue) do not suffer from task interference by design, while explicit task routing (ETR; green), and ETR with NLPs (red) do not eliminate but suffer less from task interference. (b) Gradient correlations measured via CKA [ 19 ] across all pairs of tasks for different layers of a standard MTN at the end of training. Observe the acute lack of correlation between tasks (low off-diagonal magnitude).\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1.5. The control group. We refer to [8] for a discussion about this section. Consider \\\\( \\\\Gamma \\\\subset \\\\text{PSL}(3, \\\\mathbb{C}) \\\\) a (discrete or not) subgroup which acts on \\\\( \\\\mathbb{P}^2_\\\\mathbb{C} \\\\) with a point \\\\( p \\\\) which is fixed by all of \\\\( \\\\Gamma \\\\). Choose an arbitrary line \\\\( \\\\ell \\\\) in \\\\( \\\\mathbb{P}^2_\\\\mathbb{C} \\\\setminus \\\\{p\\\\} \\\\), and notice we have a canonical projection:\\n\\n\\\\[\\n\\\\pi = \\\\pi_{p,\\\\ell} : \\\\mathbb{P}^2_\\\\mathbb{C} \\\\setminus \\\\{p\\\\} \\\\rightarrow \\\\ell,\\n\\\\]\\ngiven by \\\\( \\\\pi(x) = \\\\frac{x - p}{x - \\\\ell} \\\\). It is clear that this map is holomorphic and it allows us to define a group homomorphism:\\n\\n\\\\[\\n\\\\Pi = \\\\Pi_{p,\\\\ell} : \\\\Gamma \\\\rightarrow \\\\text{Bihol}(\\\\ell) \\\\cong \\\\text{PSL}(2, \\\\mathbb{C}),\\n\\\\]\\n\\nby \\\\( \\\\Pi(g)(x) = \\\\pi(g(x)) \\\\). If we choose another line, say \\\\( \\\\ell' \\\\), one gets similarly a projection \\\\( \\\\pi' = \\\\pi_{p,\\\\ell'} : \\\\mathbb{P}^2_\\\\mathbb{C} \\\\setminus \\\\{p\\\\} \\\\rightarrow \\\\ell' \\\\), and a group homomorphism \\\\( \\\\Pi' = \\\\Pi_{p,\\\\ell'} : \\\\Gamma \\\\rightarrow \\\\text{PSL}(2, \\\\mathbb{C}) \\\\). It is an exercise to see that \\\\( \\\\Pi \\\\) and \\\\( \\\\Pi' \\\\) are equivalent in the sense that there is a biholomorphism \\\\( h : \\\\ell \\\\rightarrow \\\\ell' \\\\) inducing an automorphism \\\\( H \\\\) of \\\\( \\\\text{PSL}(2, \\\\mathbb{C}) \\\\) such that \\\\( H \\\\circ \\\\Pi = \\\\Pi' \\\\). As before, the line \\\\( \\\\ell \\\\) is called the horizon.\\n\\nThis leads to the following definition:\\n\\n**Definition 1.7.** Let \\\\( \\\\Gamma \\\\subset \\\\text{PSL}(3, \\\\mathbb{C}) \\\\) be a discrete group as above. We call \\\\( \\\\Pi = \\\\Pi_{p,\\\\ell} \\\\) the control morphism (or map) and its image \\\\( \\\\Pi(\\\\Gamma) \\\\subset \\\\text{PSL}(2, \\\\mathbb{C}) \\\\), is the control group. These are well-defined and independent of \\\\( \\\\ell \\\\) up to an automorphism of \\\\( \\\\text{PSL}(2, \\\\mathbb{C}) \\\\).\\n\\nThe control map and the control group allow us to get information about the dynamics of \\\\( \\\\Gamma \\\\) by looking at a subgroup of \\\\( \\\\text{PSL}(2, \\\\mathbb{C}) \\\\), which is far easier to handle. The prize we pay is that the control group in \\\\( \\\\text{PSL}(2, \\\\mathbb{C}) \\\\) may not be discrete.\\n\\n2. Purely parabolic groups\\n\\nWe now follow [5] and look at the discrete subgroups in \\\\( \\\\text{PSL}(3, \\\\mathbb{C}) \\\\) that, besides the identity, have only parabolic elements. These are called purely parabolic and there are five families of such groups; three of them split into various subfamilies according to their limit set (and their control group, see [8]). All of these are elementary.\\n\\nThe simplest purely parabolic groups are cyclic, generated by a parabolic element. As described above, there are three types of such elements in \\\\( \\\\text{PSL}(3, \\\\mathbb{C}) \\\\), described by the Jordan normal form of their lifts to \\\\( \\\\text{SL}(3, \\\\mathbb{C}) \\\\). Each of these belongs to a different type of the families we describe below. The first type generates torus groups (see definitions below), the second generates Abelian Kodaira groups and the ellipto-parabolic elements generate elliptic groups.\\n\\n(i) Elliptic groups. These are the only purely parabolic groups that are not conjugate to subgroups of the Heisenberg group \\\\( \\\\text{Heis}(3, \\\\mathbb{C}) \\\\) and they are subgroups of fundamental groups of elliptic surfaces. These have limit set a single line. Up to conjugation these groups are of the form:\\n\\n\\\\[\\n\\\\text{Ell}(W, \\\\mu) = \\\\left\\\\{ \\\\begin{bmatrix} \\\\mu(w) & \\\\mu(w)w & 0 \\\\\\\\ 0 & \\\\mu(w) & 0 \\\\\\\\ 0 & 0 & \\\\mu(w)^{-2} \\\\end{bmatrix} : w \\\\in W \\\\right\\\\},\\n\\\\]\\n\\nwhere \\\\( W \\\\subset \\\\mathbb{C} \\\\) is an additive discrete subgroup and \\\\( \\\\mu : W \\\\rightarrow S^1 \\\\) is a group morphism.\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1.5. The control group. We refer to [8] for a discussion about this section. Consider \\\\( \\\\Gamma \\\\subset \\\\text{PSL}(3, \\\\mathbb{C}) \\\\) a (discrete or not) subgroup which acts on \\\\( \\\\mathbb{P}^2_\\\\mathbb{C} \\\\) with a point \\\\( p \\\\) which is fixed by all of \\\\( \\\\Gamma \\\\). Choose an arbitrary line \\\\( \\\\ell \\\\) in \\\\( \\\\mathbb{P}^2_\\\\mathbb{C} \\\\setminus \\\\{p\\\\} \\\\), and notice we have a canonical projection: \\n\\n\\\\[\\n\\\\pi = \\\\pi_{p,\\\\ell} : \\\\mathbb{P}^2_\\\\mathbb{C} \\\\setminus \\\\{p\\\\} \\\\rightarrow \\\\ell,\\n\\\\]\\n\\ngiven by \\\\( \\\\pi(x) = \\\\frac{x - p}{x - \\\\ell} \\\\). It is clear that this map is holomorphic and it allows us to define a group homomorphism: \\n\\n\\\\[\\n\\\\Pi = \\\\Pi_{p,\\\\ell} : \\\\Gamma \\\\rightarrow \\\\text{Bihol}(\\\\ell) \\\\cong \\\\text{PSL}(2, \\\\mathbb{C}),\\n\\\\]\\n\\nby \\\\( \\\\Pi(g)(x) = \\\\pi(g(x)) \\\\). If we choose another line, say \\\\( \\\\ell' \\\\), one gets similarly a projection \\\\( \\\\pi' = \\\\pi_{p,\\\\ell'} : \\\\mathbb{P}^2_\\\\mathbb{C} \\\\setminus \\\\{p\\\\} \\\\rightarrow \\\\ell' \\\\), and a group homomorphism \\\\( \\\\Pi' = \\\\Pi_{p,\\\\ell'} : \\\\Gamma \\\\rightarrow \\\\text{PSL}(2, \\\\mathbb{C}) \\\\). It is an exercise to see that \\\\( \\\\Pi \\\\) and \\\\( \\\\Pi' \\\\) are equivalent in the sense that there is a biholomorphism \\\\( h : \\\\ell \\\\rightarrow \\\\ell' \\\\) inducing an automorphism \\\\( H \\\\) of \\\\( \\\\text{PSL}(2, \\\\mathbb{C}) \\\\) such that \\\\( H \\\\circ \\\\Pi = \\\\Pi' \\\\). As before, the line \\\\( \\\\ell \\\\) is called the horizon.\\n\\nThis leads to the following definition: \\n\\n**Definition 1.7.** Let \\\\( \\\\Gamma \\\\subset \\\\text{PSL}(3, \\\\mathbb{C}) \\\\) be a discrete group as above. We call \\\\( \\\\Pi = \\\\Pi_{p,\\\\ell} \\\\) the control morphism (or map) and its image \\\\( \\\\Pi(\\\\Gamma) \\\\subset \\\\text{PSL}(2, \\\\mathbb{C}) \\\\), is the control group. These are well-defined and independent of \\\\( \\\\ell \\\\) up to an automorphism of \\\\( \\\\text{PSL}(2, \\\\mathbb{C}) \\\\).\\n\\nThe control map and the control group allow us to get information about the dynamics of \\\\( \\\\Gamma \\\\) by looking at a subgroup of \\\\( \\\\text{PSL}(2, \\\\mathbb{C}) \\\\), which is far easier to handle. The prize we pay is that the control group in \\\\( \\\\text{PSL}(2, \\\\mathbb{C}) \\\\) may not be discrete.\\n\\n2. Purely parabolic groups\\n\\nWe now follow [5] and look at the discrete subgroups in \\\\( \\\\text{PSL}(3, \\\\mathbb{C}) \\\\) that, besides the identity, have only parabolic elements. These are called purely parabolic and there are five families of such groups; three of them split into various subfamilies according to their limit set (and their control group, see [8]). All of these are elementary. The simplest purely parabolic groups are cyclic, generated by a parabolic element. As described above, there are three types of such elements in \\\\( \\\\text{PSL}(3, \\\\mathbb{C}) \\\\), described by the Jordan normal form of their lifts to \\\\( \\\\text{SL}(3, \\\\mathbb{C}) \\\\). Each of these belongs to a different type of the families we describe below. The first type generates torus groups (see definitions below), the second generates Abelian Kodaira groups and the ellipto-parabolic elements generate elliptic groups. \\n\\n(i) Elliptic groups. These are the only purely parabolic groups that are not conjugate to subgroups of the Heisenberg group \\\\( \\\\text{Heis}(3, \\\\mathbb{C}) \\\\) and they are subgroups of fundamental groups of elliptic surfaces. These have limit set a single line. Up to conjugation these groups are of the form: \\n\\n\\\\[\\n\\\\text{Ell}(W, \\\\mu) = \\\\left\\\\{ \\\\begin{bmatrix} \\\\mu(w) & \\\\mu(w)w & 0 \\\\\\\\ 0 & \\\\mu(w) & 0 \\\\\\\\ 0 & 0 & \\\\mu(w)^{-2} \\\\end{bmatrix} : w \\\\in W \\\\right\\\\},\\n\\\\]\\n\\nwhere \\\\( W \\\\subset \\\\mathbb{C} \\\\) is an additive discrete subgroup and \\\\( \\\\mu : W \\\\rightarrow S^1 \\\\) is a group morphism. \"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The discovery of the top quark, the most massive elementary particle yet known, has given us a distinct window into investigating the physics of the Standard Model and Beyond. With a plethora of top quarks to be produced in the High Luminosity era of the LHC, the exploration of its rare decays holds great promise in revealing potential new physics phenomena. We consider higher-dimensional operators contributing to top decays in the SMEFT and its extension by a light singlet species of spin 0, 1/2, or 1, and exhibit that the HL-LHC may observe many exotic top decays in a variety of channels. Light singlets which primarily talk to the SM through such a top interaction may also lead to distinctive long-lived particle signals. Searching for such long-lived particles in top-quark decays has the additional advantage that the SM decay of the other top quark in the same event provides a natural trigger.\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The discovery of the top quark, the most massive elementary particle yet known, has given us a distinct window into investigating the physics of the Standard Model and Beyond. With a plethora of top quarks to be produced in the High Luminosity era of the LHC: Rare Top Decays with Light Singlets\\n\\nHenning Bahl*, Seth Koren\u2020, and Lian-Tao Wang\u2021\\n\\nDepartment of Physics and Enrico Fermi Institute, University of Chicago, 5720 South Ellis Avenue, Chicago, IL 60637 USA\\n\\nThe discovery of the top quark, the most massive elementary particle yet known, has given us a distinct window into investigating the physics of the Standard Model and Beyond. With a plethora of top quarks to be produced in the High Luminosity era of the LHC, the exploration of its rare decays holds great promise in revealing potential new physics phenomena. We consider higher-dimensional operators contributing to top decays in the SMEFT and its extension by a light singlet species of spin 0, 1/2, or 1, and exhibit that the HL-LHC may observe many exotic top decays in a variety of channels. Light singlets which primarily talk to the SM through such a top interaction may also lead to distinctive long-lived particle signals. Searching for such long-lived particles in top-quark decays has the additional advantage that the SM decay of the other top quark in the same event provides a natural trigger.\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7.3 Computational Performance of the Optimization Proxies\\n\\nThis section presents numerical experiments used to assess the performance of the proposed optimization proxies (Proxies) against the optimization models (GDO) and the greedy heuristic (GH).\\n\\nOptimality Gap:\\n\\nTable 3 presents the optimality gaps of various approaches, including the results of Model (1) under various time constraints. In the table, the columns under \u201cGap of Model (1)\u201d denote the optimality gaps of the model under various time limits. Similarly, columns Gap for GH and Proxies denote optimality gaps for GH and the optimization proxies. In addition, columns Time(s) denote the solving times for GH and Proxies.\\n\\n| Instance | Model (1) | GH | Proxies |\\n|----------|-----------|----|---------|\\n|          | 1s | 5s | 10s | 30s | 60s | 1800s | Gap | Time (s) | Gap | Time (s) |\\n| M        | 2.59 | 0.55 | 0.48 | 0.48 | 0.48 | 0.48 | 3.84 | 3.12 | 1.14 | 0.33 |\\n| L        | 51.15 | 5.22 | 2.18 | 1.71 | 1.41 | 1.39 | 12.85 | 13.28 | 3.80 | 1.10 |\\n| XL       | 77.35 | 14.02 | 10.41 | 2.93 | 2.07 | 0.93 | 17.01 | 121.55 | 5.21 | 2.49 |\\n\\nRecall that Model (1) produces solutions that exhibit considerable variability when the total commodity volume is perturbed as detailed in Table 4 and 5. As such, it is unlikely to be practical in scenarios with planners in the loop. Hence, the table compares the optimization proxies and the heuristics GH with an \u201cidealized\u201d benchmark. With this caveat in place, observe the performance of the optimization proxies under tight time constraints. Proxies generate solutions with low optimality gaps and may be up to 10 to 50 times faster than GH, and around 10 times faster than Model (1) solved with Gurobi. Second, although Model (1) efficiently produces solutions with low optimality gaps, closing the optimizality gap proves to be a significant challenge due to the poor LP relaxation. The performance of GH is also impeded by the inefficiencies of the LP relaxation, as it solves the LP relaxations over many iterations; it takes the GH around 30 iterations for terminal M, 200 iterations for terminal L, and more than 1000 iterations for terminal XL to generate a feasible solution.\\n\\nConsistency:\\n\\nTables 4 and 5 report the consistency of solutions obtained from different models in terms of the normalized distance to the reference load plan and the total variation of the generated solutions. As GDO requires running Model (1) and Model (2) sequentially, these experiments set the same time limits for the two stages. For example, if a time limit of 30 seconds is set, GDO runs Model (1) for 30 seconds and subsequently runs Model (2) using the best upper bound obtained from Model (1) for another 30 seconds.\\n\\nThe high-level result is that proxies are ideally suited to produce consistent plans. Table 4 shows that the proxies accurately predict, in a few seconds, the results produced by GDO after an hour. Furthermore, Table 5\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7.3 Computational Performance of the Optimization Proxies\\n\\nThis section presents numerical experiments used to assess the performance of the proposed optimization proxies (Proxies) against the optimization models (GDO) and the greedy heuristic (GH). \\n\\nOptimality Gap:\\n\\nTable 3 presents the optimality gaps of various approaches, including the results of Model (1) under various time constraints. In the table, the columns under \u201cGap of Model (1) \u201d denote the optimality gaps of the model under various time limits. Similarly, columns Gap for GH and Proxies denote optimality gaps for GH and the optimization proxies. In addition, columns Time(s) denote the solving times for GH and Proxies. \\n\\n| Instance | Model (1) produces solutions that exhibit considerable variability when the total commodity volume is perturbed as detailed in Table 4 and 5. As such, it is unlikely to be practical in scenarios with planners in the loop. Hence, the table compares the optimization proxies and the heuristics GH with an \u201cidealized\u201d benchmark. With this caveat in place, observe the performance of the optimization proxies under tight time constraints. Proxies generate solutions with low optimality gaps and may be up to 10 to 50 times faster than GH, and around 10 times faster than Model (1) solved with Gurobi. Second, although Model (1) efficiently produces solutions with low optimality gaps, closing the optimizality gap proves to be a significant challenge due to the poor LP relaxation. The performance of GH is also impeded by the inefficiencies of the LP relaxation, as it solves the LP relaxations over many iterations; it takes the GH around 30 iterations for terminal L, and more than 1000 iterations for terminal XL to generate a feasible solution. \\n\\nConsistency:\\n\\nTables 4 and 5 report the consistency of solutions obtained from different models in terms of the normalized distance to the reference load plan and the total variation of the generated solutions. As GDO requires running Model (1) and Model (2) using the best upper bound obtained from Model (1) for another 30 seconds. \\n\\nThe high-level result is that proxies are ideally suited to produce consistent plans. Table 4 shows that the proxies accurately predict, in a few seconds, the results produced by GDO after an hour. Furthermore, Table 5\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Panel A: Regions differ in location parameter, initial min. wage is small | Outcome | Emp. | p10 | p25 | p50 | p90 |\\n|---|---|---|---|---|---|---|\\n| Mean causal effect | -0.011 | 0.026 | 0.013 | 0.007 | 0.003 |\\n| Fraction affected | -0.013 | 0.028 | 0.014 | 0.008 | 0.004 |\\n| Gap measure | -0.010 | 0.021 | 0.011 | 0.006 | 0.003 |\\n\\n| Panel B: Regions differ in location parameter, initial min. wage is large | Outcome | Emp. | p10 | p25 | p50 | p90 |\\n|---|---|---|---|---|---|---|\\n| Mean causal effect | -0.076 | 0.155 | 0.079 | 0.049 | 0.025 |\\n| Fraction affected | -0.079 | 0.112 | 0.081 | 0.054 | 0.031 |\\n| Gap measure | -0.055 | 0.074 | 0.058 | 0.038 | 0.022 |\\n\\n| Panel C: Identical regions receive different location shocks, initial min. wage is small | Outcome | Emp. | p10 | p25 | p50 | p90 |\\n|---|---|---|---|---|---|---|\\n| Mean causal effect | -0.006 | 0.015 | 0.007 | 0.004 | 0.002 |\\n| Effective min. wage | -0.006 | 0.011 | 0.003 | 0.000 | -0.002 |\\n| Effective min. wage, p90 | -0.006 | 0.013 | 0.005 | 0.002 | 0.000 |\\n\\n| Panel D: Identical regions receive different location shocks, initial min. wage is large | Outcome | Emp. | p10 | p25 | p50 | p90 |\\n|---|---|---|---|---|---|---|\\n| Mean causal effect | -0.058 | 0.129 | 0.062 | 0.037 | 0.018 |\\n| Effective min. wage | -0.059 | 0.092 | 0.024 | 0.000 | -0.019 |\\n| Effective min. wage, p90 | -0.059 | 0.112 | 0.043 | 0.019 | 0.000 |\\n\\n**Notes:** In all panels, the national minimum wage increases by 20 log points from the first period to the second. In Panels A and B, regions differ only in the location parameter $\\\\mu_{ij}$, assumed to be constant over time. In Panels C and D, regions are identical in period one, but differ in $\\\\mu_{ij}$ in period two. Each panel displays average results for 5,000 simulations, each with 50 regions. For each outcome, the numbers correspond to the mean true ATE across simulations, the mean estimates of causal effects based on the regressions listed on the left, and the average standard error associated with the estimates (in parentheses, clustered at the region level).\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Normal-markdown model: ideal cases\\n\\n| Panel A: Regions differ in location parameter, initial min. wage is small | Outcome | Emp. | p10 | p25 | p50 | p90 |\\n|---|---|---|---|---|---|---|\\n| Mean causal effect | -0.011 | 0.026 | 0.013 | 0.007 | 0.003 |\\n| Fraction affected | -0.013 | 0.028 | 0.014 | 0.008 | 0.004 |\\n| Gap measure | -0.010 | 0.021 | 0.011 | 0.006 | 0.003 |\\n\\n| Panel B: Regions differ in location parameter, initial min. wage is large | Outcome | Emp. | p10 | p25 | p50 | p90 |\\n|---|---|---|---|---|---|---|\\n| Mean causal effect | -0.076 | 0.155 | 0.079 | 0.049 | 0.025 |\\n| Fraction affected | -0.079 | 0.112 | 0.081 | 0.054 | 0.031 |\\n| Gap measure | -0.055 | 0.074 | 0.058 | 0.038 | 0.022 |\\n\\n| Panel C: Identical regions receive different location shocks, initial min. wage is small | Outcome | Emp. | p10 | p25 | p50 | p90 |\\n|---|---|---|---|---|---|---|\\n| Mean causal effect | -0.006 | 0.015 | 0.007 | 0.004 | 0.002 |\\n| Effective min. wage | -0.006 | 0.011 | 0.003 | 0.000 | -0.002 |\\n| Effective min. wage, p90 | -0.006 | 0.013 | 0.005 | 0.002 | 0.000 |\\n\\n| Panel D: Identical regions receive different location shocks, initial min. wage is large | Outcome | Emp. | p10 | p25 | p50 | p90 |\\n|---|---|---|---|---|---|---|\\n| Mean causal effect | -0.058 | 0.129 | 0.062 | 0.037 | 0.018 |\\n| Effective min. wage | -0.059 | 0.092 | 0.024 | 0.000 | -0.019 |\\n| Effective min. wage, p90 | -0.059 | 0.112 | 0.043 | 0.019 | 0.000 |\\n\\nNotes: In all panels, the national minimum wage increases by 20 log points from the first period to the second. In Panels A and B, regions differ only in the location parameter $\\\\mu_{r,t}$, assumed to be constant over time. In Panels C and D, regions are identical in period one, but differ in $\\\\mu_{r,t}$ in period two. Each panel displays average results for 5,000 simulations, each with 50 regions. For each outcome, the numbers correspond to the mean true ATE across simulations, the mean estimates of causal effects based on the regressions listed on the left, and the average standard error associated with the estimates (in parentheses, clustered at the region level).\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lemma 3.4. Under the hypothesis of Theorem 3.1, for every $p_0 > 2$ there exists a constant $C_1$ such that for all $p \\\\geq p_0$ and $\\\\zeta \\\\in C_0^\\\\infty(\\\\mathbb{R}^n; \\\\mathbb{R}_{>0})$\\n\\n$$\\\\|\\\\nabla(\\\\zeta|\\\\psi|^{p/2})\\\\|_2 \\\\leq C_1 p^\\\\alpha \\\\|(|\\\\nabla \\\\zeta| + \\\\zeta)|\\\\psi|^{p/2}\\\\|_2,$$  \\n\\n(29)\\n\\nwhere $\\\\alpha = (\\\\mu + 1)/2$.\\n\\nProof. The proof of this lemma is based on (22) and it follows the proof of (Theorem 5.1 in [0]). Note that (22) becomes useless for $p = 2$. Agmon works with the stronger inequality (19), which doesn\u2019t degenerate at $p = 2$. By fixing a number $p_0 > 2$ and looking at numbers $p \\\\geq p_0$ we can reuse Agmon\u2019s proof. From (22) and H\u00f6lder\u2019s inequality it follows that\\n\\n$$(p - 2) \\\\int dx \\\\zeta^2 |\\\\psi|^{p-2} |\\\\nabla|\\\\psi||^2 \\\\leq 2 \\\\left( \\\\int dx \\\\zeta^2 |\\\\psi|^{p-2} |\\\\nabla|\\\\psi||^2 \\\\right)^{1/2} \\\\left( \\\\int dx |\\\\nabla \\\\zeta|^2 |\\\\psi|^p \\\\right)^{1/2} + \\\\int dx \\\\zeta^2 q_- |\\\\psi|^p$$\\n\\nUsing (26) this can be rewritten as\\n\\n$$(p - 2) \\\\frac{4}{p^2} \\\\int_{=A^2} dx \\\\zeta^2 |\\\\nabla|\\\\psi||^{p/2}|^2 \\\\leq 2 \\\\frac{2}{p} \\\\left( \\\\int_{=A} dx \\\\zeta^2 |\\\\nabla|\\\\psi||^{p/2}|^2 \\\\right)^{1/2} \\\\left( \\\\int_{=B} dx |\\\\nabla \\\\zeta|^2 (|\\\\psi|^{p/2})^2 \\\\right)^{1/2} + \\\\int_{=C} dx \\\\zeta^2 q_- (|\\\\psi|^{p/2})^2.$$  \\n\\nNote that by Lemma 3.3 and assumption (13) all integrals are finite. The above inequality is equivalent to\\n\\n$$A^2 \\\\leq \\\\frac{p}{p - 2} AB + \\\\frac{p^2}{4(p - 2)} C.$$  \\n\\nNext, we use that $p/(p - 2) \\\\leq p_0/(p_0 - 2) =: D_0$ for $p \\\\geq p_0$ and $AB \\\\leq \\\\delta A^2 + B^2/\\\\delta$ for all $\\\\delta > 0$:\\n\\n$$A^2 \\\\leq D_0 \\\\left( \\\\delta A^2 + \\\\frac{B^2}{\\\\delta} \\\\right) + \\\\frac{p}{4} D_0 C \\\\iff (1 - D_0 \\\\delta) A^2 \\\\leq D_0 \\\\frac{B^2}{\\\\delta} + \\\\frac{p}{4} D_0 C.$$  \\n\\nWe choose $\\\\delta = \\\\frac{1}{2D_0}$, so that\\n\\n$$\\\\frac{1}{2} A^2 \\\\leq 2D_0^2 B^2 + \\\\frac{p}{4} D_0 C.$$  \\n\\nUsing $\\\\sqrt{a + b} \\\\leq \\\\sqrt{a} + \\\\sqrt{b}$ it follows that\\n\\n$$A \\\\leq 2D_0 B + \\\\sqrt{\\\\frac{D_0}{2}} \\\\sqrt{p} \\\\sqrt{C}.$$  \\n\\n10\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lemma 3.4. Under the hypothesis of Theorem 3.1, for every $p_0 > 2$ there exists a constant $C_1$ such that for all $p \\\\geq p_0$ and $\\\\zeta \\\\in C_0^\\\\infty(\\\\mathbb{R}^n; \\\\mathbb{R}_{>0})$\\n\\n$$\\\\|\\\\nabla(\\\\zeta|\\\\psi|^{p/2})\\\\|_2 \\\\leq C_1 p^\\\\alpha \\\\|(|\\\\nabla \\\\zeta| + \\\\zeta)|\\\\psi|^{p/2}\\\\|_2,$$  \\n\\n(29)\\n\\nwhere $\\\\alpha = (\\\\mu + 1)/2$.\\n\\nProof. The proof of this lemma is based on (22) and it follows the proof of (Theorem 5.1 in [0]). Note that (22) becomes useless for $p = 2$. Agmon works with the stronger inequality (19), which doesn\u2019t degenerate at $p = 2$. By fixing a number $p_0 > 2$ and looking at numbers $p \\\\geq p_0$ we can reuse Agmon\u2019s proof. From (22) and H\u00a8older\u2019s inequality it follows that\\n\\n$$(p - 2) \\\\int dx \\\\zeta^2 |\\\\psi|^{p-2} |\\\\nabla|\\\\psi||^2 \\\\leq 2 \\\\left( \\\\int dx \\\\zeta^2 |\\\\psi|^{p-2} |\\\\nabla|\\\\psi||^2 \\\\right)^{1/2} \\\\left( \\\\int dx |\\\\nabla \\\\zeta|^2 |\\\\psi|^p \\\\right)^{1/2} + \\\\int dx \\\\zeta^2 q_- |\\\\psi|^p$$\\n\\nUsing (26) this can be rewritten as\\n\\n$$(p - 2) \\\\frac{4}{p^2} \\\\int_{=A^2} dx \\\\zeta^2 |\\\\nabla|\\\\psi|^{p/2}|^2 \\\\leq 2 \\\\frac{2}{p} \\\\left( \\\\int dx \\\\zeta^2 |\\\\nabla|\\\\psi|^{p/2}|^2 \\\\right)^{1/2} \\\\left( \\\\int dx |\\\\nabla \\\\zeta|^2 (|\\\\psi|^{p/2})^2 \\\\right)^{1/2} + \\\\int dx \\\\zeta^2 q_- (|\\\\psi|^{p/2})^2.$$  \\n\\nNote that by Lemma 3.3 and assumption (13) all integrals are finite. The above inequality is equivalent to\\n\\n$$A^2 \\\\leq \\\\frac{p}{p - 2} AB + \\\\frac{p^2}{4(p - 2)} C.$$  \\n\\nNext, we use that $p/(p - 2) \\\\leq p_0/(p_0 - 2) =: D_0$ for $p \\\\geq p_0$ and $AB \\\\leq \\\\delta A^2 + B^2/\\\\delta$ for all $\\\\delta > 0$:\\n\\n$$A^2 \\\\leq D_0 \\\\left( \\\\delta A^2 + \\\\frac{B^2}{\\\\delta} \\\\right) + \\\\frac{p}{4} D_0 C$$\\n\\n$$\\\\iff (1 - D_0 \\\\delta) A^2 \\\\leq D_0 \\\\frac{B^2}{\\\\delta} + \\\\frac{p}{4} D_0 C.$$  \\n\\nWe choose $\\\\delta = \\\\frac{1}{2D_0}$, so that\\n\\n$$\\\\frac{1}{2} A^2 \\\\leq 2D_0^2 B^2 + \\\\frac{p}{4} D_0 C.$$  \\n\\nUsing $\\\\sqrt{a + b} \\\\leq \\\\sqrt{a} + \\\\sqrt{b}$ it follows that\\n\\n$$A \\\\leq 2D_0 B + \\\\sqrt{\\\\frac{D_0}{2}} \\\\sqrt{p} \\\\sqrt{C}.$$  \\n\\n10\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: BLEU score for 4 language pair model 1. The rows are the source language and the columns are the target language. Cells in bold represent the translation directions used in training\\n\\n|     | kn  | ml  | te  | ta  |\\n|-----|-----|-----|-----|-----|\\n| kn  | -   | 7.7 | 1.0 | 0.8 |\\n| ml  | 8.9 | -   | 5.7 | 0.6 |\\n| te  | 0.5 | 3.2 | -   | 7.4 |\\n| ta  | 5.8 | 4.9 | 7.4 | -   |\\n\\n4.3 4 language pairs\\n\\nA single encoder-decoder model is trained on 4 language pairs. We run experiments with 3 types of language pair combinations:\\n\\n- **Model 1**: 2 unique language pairs in forward and reverse direction: kn\u2194ml, te\u2194ta. Results documented in Table 3\\n- **Model 2**: 4 unique language pairs: kn\u2192ml, ml\u2192te, te\u2192ta, ta\u2192kn. Results documented in Table 4\\n- **Model 3**: 4 unique language pairs with VOLT: kn\u2192ml, ml\u2192te, te\u2192ta, ta\u2192kn. Results documented in Table 5\\n\\nBoth techniques expose the model to 1/3 of the total translation directions during training but the first technique is built to test model performance in very low resource conditions; when there are only 2 sources of parallel corpora available. In comparison, the second model is exposed to 1/3 of the total translation directions with each source-target language combination being unique. We ensure that in every model, both the encoder and decoder see each language at least once during training.\\n\\nWe observe that BLEU score for zero-shot translation lags by 5.03 on average compared to the performance of trained language pairs when we train on both directions of 2 language pairs only. In comparison, the zero-shot translation BLEU score lags by 5.98 BLEU on average for the model trained on 4 unique language pairs. The BLEU score for trained translation directions is always in the 6-8 BLEU range. The 4 language pair model trained with VOLT outperforms both the 32000 vocabulary models in zero-shot translation performance with zero shot scores lagging by 3.53 on average from the trained directions.\\n\\nTable 4: BLEU score for 4 language pair model 2. The rows are the source language and the columns are the target language. Cells in bold represent the translation directions used in training\\n\\n|     | kn  | ml  | te  | ta  |\\n|-----|-----|-----|-----|-----|\\n| kn  | -   | 7.4 | 0.4 | 0.5 |\\n| ml  | 1.0 | -   | 7.0 | 0.4 |\\n| te  | 0.8 | 4.7 | -   | 7.1 |\\n| ta  | 8.9 | 4.5 | 0.6 | -   |\\n\\nTable 5: BLEU score for 4 language pair model 3. The rows are the source language and the columns are the target language. Cells in bold represent the translation directions used in training\\n\\n|     | kn  | ml  | te  | ta  |\\n|-----|-----|-----|-----|-----|\\n| kn  | -   | 6.5 | 4.5 | 0.8 |\\n| ml  | 6.8 | -   | 6.4 | 5.5 |\\n| te  | 0.7 | 2.4 | -   | 6.6 |\\n| ta  | 8.1 | 1.7 | 4.5 | -   |\\n\\n4.4 6 language pairs\\n\\n2 additional language pairs are added to the 4 unique language pairs of model 2 and a transformer model is trained on all 6 language pairs. The model now sees 1/2 of all possible translation directions during training. Table 6 shows the results obtained from the 6 language pair model.\\n\\nWe observe that zero-shot translation performance increases drastically. Zero-shot directions now lag by 1.76 BLEU to the trained translation directions.\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: BLEU score for 4 language pairs. We run experiments with 3 types of language pair combinations: \\n\\n- **Model 1**: 2 unique language pairs in forward and reverse direction: kn\u2194ml, te\u2194ta. Results documented in Table : 4 unique language pairs: kn\u2192ml, ml\u2192te, te\u2192ta, ta\u2192kn. Results documented in Table 4\\n- **Model 3**: 4 unique language pairs with VOLT: kn\u2192ml, ml\u2192te, te\u2192ta, ta\u2192kn. Results documented in Table 5 Both techniques expose the model to 1/3 of the total translation directions during training but the first technique is built to test model performance in very low resource conditions; when there are only 2 sources of parallel corpora available. In comparison, the second model is exposed to 1/3 of the total translation directions with each source-target language combination being unique. We ensure that in every model, both the encoder and decoder see each language atleast once during training. We observe that BLEU score for zero-shot translation lags by 5.03 on average compared to the performance of trained language pairs when we train on both directions of 2 language pairs only. In comparison, the zero-shot translation BLEU score lags by 5.98 BLEU on average for the model trained on 4 unique language pairs. The BLEU score for trained translation directions is always in the 6-8 BLEU range. The 4 language pair model trained with VOLT outperforms both the 32000 vocabulary models in zero-shot translation performance with zero shot scores lagging by 3.53 on average from the trained directions. \\n\\nTable 4: BLEU score for 4 language pairs. The model now sees 1/2 of all possible translation directions during training. Table 6 shows the results obtained from the 6 language pair model. We observe that zero-shot translation performance increases drastically. Zero-shot directions now lag by 1.76 BLEU to the trained translation directions. \\n\\n|     | kn  | ml  | te  | ta  |\\n|-----|-----|-----|-----|-----|\\n| kn  | -   | 7.7 | 1.0 | 0.8 |\\n| ml  | 8.9 | -   | 5.7 | 0.6 |\\n| te  | 0.5 | 3.2 | -   | 7.4 |\\n| ta  | 5.8 | 4.9 | 7.4 | -   |\\n\\n|     | kn  | ml  | te  | ta  |\\n|-----|-----|-----|-----|-----|\\n| kn  | -   | 7.4 | 0.4 | 0.5 |\\n| ml  | 1.0 | -   | 7.0 | 0.4 |\\n| te  | 0.8 | 4.7 | -   | 7.1 |\\n| ta  | 8.9 | 4.5 | 0.6 | -   |\\n\\n|     | kn  | ml  | te  | ta  |\\n|-----|-----|-----|-----|-----|\\n| kn  | -   | 6.5 | 4.5 | 0.8 |\\n| ml  | 6.8 | -   | 6.4 | 5.5 |\\n| te  | 0.7 | 2.4 | -   | 6.6 |\\n| ta  | 8.1 | 1.7 | 4.5 | -   |\\n\\n4.3 4 language pairs\\n\\n4.4 6 language pairs\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"states. As shown in Fig. 8 (c) and (d), the qubit gradually relaxes to $|0\\\\rangle$ after the SFQ operation. Therefore, when the $\\\\pi$ pulse is applied to the qubit immediately after the SFQ operation, the qubit cannot be completely excited to the $|1\\\\rangle$ state. As the recovery time prolongs, the population of $|1\\\\rangle$ after the $\\\\pi$ pulse also gradually increases to 1.\\n\\nAppendix C: Extraction of Quasiparticle Density Near Qubit\\n\\nThe $x_{QP,qubit}$ extracted using $x_{QP,qubit} = (\\\\Gamma(t) - \\\\Gamma_0)/C$ is a kind of average QP density during the $T_1$ measurement, which can be regarded as the average density of QP over a period of time $t_{avg}$ ($\\\\sim T_1$) after $t_R$. To intuitively illustrate the validity of the $x_{QP,qubit}$ evolution extracted in this way to analyze the QP propagation mechanism, we compare (i) the $x_{QP,qubit}$ extracted with a very short measurement time ($t_{avg} = 0$) in an ideal measurement, and (ii) the $x_{QP,qubit}$ extracted with several microsecond measurement times ($t_{avg} = 3, 6, 12, 18 \\\\mu s$) in a real experiment. We assume that the gray curve ($t_{avg} = 0$) in the Fig. 9 below, which is similar to the trend of the extracted $x_{QP,qubit}$ evolution in the main-text, is the real QP density evolution. The trends and timescales ($\\\\sim$several microseconds) of $x_{QP,qubit}$ evolution vary little over all values of $t_{avg}$ listed.\\n\\nAppendix D: Diffusion of Quasiparticles in Superconducting Quantum-Classical Hybrid Circuits\\n\\nMost QPs propagate diffusively when the local QP density is relatively low. The local QP density is $x_{QP} = n_{QP}/n_{CP}$, where $n_{QP}$ is the QP density and $n_{CP}$ the Cooper pair density. As the local QP density increases, QPs have a greater probability of recombination, and phonon-mediated propagation becomes the leading mechanism. We calculated the diffusion equation by finite element simulation, thereby estimating the contribution of QP diffusion to the QP propagation in the device described in the paper. We set a boundary of\\n\\n![FIG. 8. (a) Pulse sequence applied to measure qubits state after an SFQ pulse train. The transmission of the readout resonator is measured at $f_r$. (b) Qubit readout resonator spectroscopy vs. DC/SFQ converter drive pulse duration $t_{SFQ}$. (c) Pulse sequence applied to measure qubits state with variable delay $t_{delay}$ after a 25$\\\\mu$s SFQ pulse train. The transmission of the readout resonator is measured at $f_r$. (d) Qubit readout resonator spectroscopy vs. delay between SFQ circuit operation and readout pulse $t_{delay}$. The frequencies corresponding to the readout resonator when the state of qubit is $|0\\\\rangle$, $|1\\\\rangle$, and punch out have been marked.](image)\\n\\n![FIG. 9. Effect of measurement time on the extracted $x_{QP,qubit}$ evolution](image)\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"states. As shown in Fig. 8 (c) and (d), the qubit gradually relaxes to $|0\\\\rangle$ after the SFQ operation. Therefore, when the $\\\\pi$ pulse is applied to the qubit immediately after the SFQ operation, the qubit cannot be completely excited to the $|1\\\\rangle$ state. As the recovery time prolongs, the population of $|1\\\\rangle$ after the $\\\\pi$ pulse also gradually increases to 1. \\n\\nAppendix C: Extraction of Quasiparticle Density Near Qubit\\n\\nThe $x_{QP,qubit}$ extracted using $x_{QP,qubit} = (\\\\Gamma(t) - \\\\Gamma_0)/C$ is a kind of average QP density during the $T_1$ measurement, which can be regarded as the average density of QP over a period of time $t_{avg}$ ($\\\\sim T_1$) after $t_R$. To intuitively illustrate the validity of the $x_{QP,qubit}$ evolution extracted in this way to analyze the QP propagation mechanism, we compare (i) the $x_{QP,qubit}$ extracted with a very short measurement time (t$_{avg} = 0$) in an ideal measurement, and (ii) the $x_{QP,qubit}$ extracted with several microsecond measurement times (t$_{avg} = 3, 6, 12, 18 \\\\mu s$) in a real experiment. We assume that the gray curve (t$_{avg} = 0$) in the Fig. 9 below, which is similar to the trend of the extracted $x_{QP,qubit}$ evolution in the maintext, is the real QP density evolution. The trends and timescales (several microseconds) of $x_{QP,qubit}$ evolution vary little over all values of t$_{avg}$ listed.\\n\\nAppendix D: Diffusion of Quasiparticles in Superconducting Quantum-Classical Hybrid Circuits\\n\\nMost QPs propagate diffusively when the local QP density is relatively low. The local QP density is $x_{QP} = n_{QP}/n_{CP}$, where $n_{QP}$ is the QP density and $n_{CP}$ the Cooper pair density. As the local QP density increases, QPs have a greater probability of recombination, and phonon-mediated propagation becomes the leading mechanism. We calculated the diffusion equation by finite element simulation, thereby estimating the contribution of QP diffusion to the QP propagation in the device described in the paper. We set a boundary of\\n\\n![FIG. 8. (a) Pulse sequence applied to measure qubits state after an SFQ pulse train. The transmission of the readout resonator is measured at $f_r$. (b) Qubit readout resonator spectroscopy vs. DC/SFQ converter drive pulse duration $t_{SFQ}$. (c) Pulse sequence applied to measure qubits state with variable delay $t_{delay}$ after a 25$\\\\mu$s SFQ pulse train. The transmission of the readout resonator is measured at $f_r$. (d) Qubit readout resonator spectroscopy vs. delay between SFQ circuit operation and readout pulse $t_{delay}$. The frequencies corresponding to the readout resonator when the state of qubit is $|0\\\\rangle$, $|1\\\\rangle$, and punch out have been marked.](image)\\n\\n![FIG. 9. Effect of measurement time on the extracted $x_{QP,qubit}$ evolution](image)\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the other cliques, ensuring a fully connected graph. All nodes and edges are associated with information through embeddings, described below.\\n\\n**GNN stage** The second stage employs a generalized message-passing GNN following [5] to perform several prediction tasks on this graph simultaneously:\\n\\n1. **Keypoint association prediction:** we model association between body keypoints and their corresponding pedicle keypoints as binary edge classification on the over-connected k-NN graph.\\n2. **Body keypoint level prediction:** for body keypoints, we model the spine level prediction as multi-class node classification.\\n3. **Keypoint legitimacy prediction:** to filter out false-positive keypoints, we additionally compute an binary legitimacy prediction for each node.\\n\\nTo perform these task, our message-passing GNN maintains edge and node embeddings which are updated in each layer. A message-passing layer performs a node update and edge update operation. Denoting the feature vector of a node \\\\( v \\\\) by \\\\( x_v \\\\), and the feature vector of a directed edge \\\\((u, v)\\\\) by \\\\( x_{uv} \\\\), the node and edge features are updated as follows:\\n\\n\\\\[\\n\\\\begin{align*}\\n    x'_v &= \\\\bigoplus_{v \\\\in \\\\mathcal{N}_u \\\\cup \\\\{u\\\\}} \\\\psi_{\\\\text{node}}(x_u, x_v, x_{uv}), \\\\\\\\\\n    x'_{uv} &= \\\\psi_{\\\\text{edge}}(x_u, x_v, x_{uv})\\n\\\\end{align*}\\n\\\\]\\n\\nHere \\\\( \\\\bigoplus \\\\) denotes a symmetric pooling operation (in our case max pooling) over the neighborhood \\\\( \\\\mathcal{N}_u \\\\). \\\\( \\\\psi_{\\\\text{node}} \\\\) and \\\\( \\\\psi_{\\\\text{edge}} \\\\) are trainable parametric functions: in our case, two distinct two-layer MLPs with ReLU nonlinearities. After \\\\( N \\\\) such message-passing layers we obtain an embedding vector for each node and edge. Each node/edge embedding is passed through a linear layer (distinct for nodes and edges) to obtain a vector of node class logits or a single edge prediction logit, respectively. The last entry in the node prediction vector is interpreted as a node legitimacy prediction score: nodes predicted as illegitimate are discarded for the output.\\n\\nThe node input features \\\\( x_u \\\\in \\\\mathbb{R}^7 \\\\) consist of the one-hot encoded keypoint type (body, left or right pedicle) and the segment input information (a pseudo-probability in \\\\([0, 1]\\\\) for each of the four spine segments of belonging to that segment, computed by applying a sigmoid to the heatmap network\u2019s output channels which represent the different spine segments). The edge input features \\\\( x_{uv} \\\\in \\\\mathbb{R}^4 \\\\) consist of the normalized direction vector of the edge and the distance between the two endpoints.\\n\\nThe output of the GNN contains finer spine-level classification (i.e. C1-C7, T1-T13, L1-L6, S1-S2), keypoint-level legitimacy (legitimate vs. false-positive detection) and body-pedicle association via edge prediction, implicitly defining the orientation of each vertebra. Prediction scores of corresponding directed edges \\\\((u, v)\\\\) and \\\\((v, u)\\\\) are symmetrized by taking the mean.\\n\\nIn our experiments we consider variations to our architecture: weight sharing between consecutive GNN layers, multiple heads with a shared backbone (jointly trained) and dedicated networks (separately trained) for edge/node prediction.\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the other cliques, ensuring a fully connected graph. All nodes and edges are associated with information through embeddings, described below. \\n\\n**GNN stage** The second stage employs a generalized message-passing GNN following [5] to perform several prediction tasks on this graph simultaneously: \\n\\n1. **Keypoint association prediction:** we model association between body keypoints and their corresponding pedicle keypoints as binary edge classification on the over-connected k-NN graph. \\n2. **Body keypoint level prediction:*** for body keypoints, we model the spine level prediction as multi-class node classification. \\n3. **Keypoint legitimacy prediction:** to filter out false-positive keypoints, we additionally compute an binary legitimacy prediction for each node. \\n\\nTo perform these task, our message-passing GNN maintains edge and node embeddings which are updated in each layer. A message-passing layer performs a node update and edge update operation. Denoting the feature vector of a node \\\\( v \\\\) by \\\\( x_v \\\\), and the feature vector of a directed edge \\\\((u, v)\\\\) by \\\\( x_{uv} \\\\), the node and edge features are updated as follows: \\n\\n\\\\[\\n\\\\begin{align*}\\n    x'_u &= \\\\bigoplus_{v \\\\in \\\\mathcal{N}_u \\\\cup \\\\{u\\\\}} \\\\psi_{\\\\text{node}}(x_u, x_v, x_{uv}), \\\\\\\\\\n    x'_{uv} &= \\\\psi_{\\\\text{edge}}(x_u, x_v, x_{uv})\\n\\\\end{align*}\\n\\\\]  \\n\\n(1)\\n\\nHere \\\\( \\\\bigoplus \\\\) denotes a symmetric pooling operation (in our case max pooling) over the neighborhood \\\\( \\\\mathcal{N}_u \\\\). \\\\( \\\\psi_{\\\\text{node}} \\\\) and \\\\( \\\\psi_{\\\\text{edge}} \\\\) are trainable parametric functions: in our case, two distinct two-layer MLPs with ReLU nonlinearities. After \\\\( N \\\\) such message-passing layers we obtain an embedding vector for each node and edge. Each node/edge embedding is passed through a linear layer (distinct for nodes and edges) to obtain a vector of node class logits or a single edge prediction logit, respectively. The last entry in the node prediction vector is interpreted as a node legitimacy prediction score: nodes predicted as illegitimate are discarded for the output. \\n\\nThe node input features \\\\( x_u \\\\in \\\\mathbb{R}^7 \\\\) consist of the one-hot encoded keypoint type (body, left or right pedicle) and the segment input information (a pseudoprobability in \\\\([0, 1]\\\\) for each of the four spine segments of belonging to that segment, computed by applying a sigmoid to the heatmap network\u2019s output channels which represent the different spine segments). The edge input features \\\\( x_{uv} \\\\in \\\\mathbb{R}^4 \\\\) consist of the normalized direction vector of the edge and the distance between the two endpoints. \\n\\nThe output of the GNN contains finer spine-level classification (i.e. C1-C7, T1-T13, L1-L6, S1-S2), keypoint-level legitimacy (legitimate vs. false-positive detection) and body-pedicle association via edge prediction, implicitly defining the orientation of each vertebra. Prediction scores of corresponding directed edges \\\\((u, v)\\\\) and \\\\((v, u)\\\\) are symmetrized by taking the mean. \\n\\nIn our experiments we consider variations to our architecture: weight sharing between consecutive GNN layers, multiple heads with a shared backbone (jointly trained) and dedicated networks (separately trained) for edge/node prediction. \\n\\n---\\n\\n4 V. B\u00fcrgin et al.\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"taking $\\\\Omega \\\\to \\\\Omega + i0^+$, where we add infinitesimally small imaginary part as it is required in the retarded Green\u2019s function. Then $a = (\\\\Omega + i0^+)^2 + \\\\ldots$ gets small imaginary part $2i\\\\Omega 0^+$ defining the value of $\\\\text{csgn}$. Taking the limit of infinitesimal $0^+$ we arrive at the following expression for the trace of the Green\u2019s function\\n\\n$$\\n\\\\text{Tr}G(\\\\Omega) = \\\\frac{2\\\\Omega}{\\\\pi} \\\\int_0^\\\\pi dk_y \\\\left\\\\{ \\\\frac{\\\\text{sgn} \\\\Omega}{\\\\sqrt{a^2 - b^2}} \\\\right\\\\} \\\\text{sgn} \\\\Omega \\\\frac{1}{\\\\sqrt{a^2 - b^2}} \\\\text{ for } a^2 > b^2, \\\\quad (13)\\n$$\\n\\nwhere we used the fact that the integrand is an even function of $k_y$. Here the values $a$ and $b$ are the functions of the variables $k_y$ and $\\\\Omega$ and the parameter $m$ as determined by Eqs. (10).\\n\\nThe DOS is proportional to the imaginary part of $\\\\text{Tr}G(\\\\Omega)$, therefore it is nonzero only in the region of $\\\\Omega$ where\\n\\n$$\\na^2 - b^2 \\\\leq 0. \\\\quad (14)\\n$$\\n\\nOutside the region (14) $\\\\text{Tr}G$ is real and the DOS is equal to zero, i.e., $\\\\rho(\\\\Omega) = 0$.\\n\\nThe next step in evaluation of Eq. (13) is to perform integration over $k_y$. It is convenient to replace $y = \\\\cos k_y$ and $dk_y = -dy(1 - y^2)^{-1/2}$. The boundaries of the integration are $-1 \\\\leq y \\\\leq 1$, where changing the order of the integration boundaries results an additional minus sign. The integration over $y$ is performed differently depending on the value of the parameter $m$. Therefore, in what follows we are considering three cases with $|m| = 1$, $|m| > 1$, and $|m| < 1$, separately.\\n\\n1. Case $|m| = 1$\\n\\nCalculations are simpler in the special case of $|m| = 1$. The function in the denominator of the integral (13) can be written explicitly as $a^2 - b^2 = (\\\\Omega^2 - 1)(\\\\Omega^2 - 5 - 4my)$. It is linear in $y$ and changes the sign only once at the point $y = y_0 \\\\text{sgn} m$, where $y_0 = (\\\\Omega^2 - 5)/4$. Solving the condition (14) with respect to $\\\\Omega$ and using the fact that $|y| \\\\leq 1$ we obtain that DOS is nonzero only for $1 \\\\leq |\\\\Omega| \\\\leq 3$.\\n\\nThe condition (14) considered with respect to $y$ determines the boundaries of integration in Eq. (13). For $m = 1$ it is satisfied for $y_0 \\\\leq y \\\\leq 1$. Then the imaginary part of Eq. (13) in terms of variable $y$ gives the DOS in the implicit form\\n\\n$$\\n\\\\rho(\\\\Omega) = \\\\frac{|\\\\Omega|}{\\\\pi^2 \\\\sqrt{\\\\Omega^2 - 1}} \\\\int_{y_0}^1 \\\\frac{dy}{\\\\sqrt{(1 - y^2)(y - y_0)}}, \\\\quad (15)\\n$$\\n\\nwhere the definition (7) is used. The similar expression will appears for $m = -1$: the boundaries of integration are $-1 \\\\leq y \\\\leq -y_0$ and denominator in the integrand is $\\\\sqrt{(1 - y^2)(y - y_0)}$. The corresponding DOS can be transformed into the result Eq. (15) by replacing the integration variable $y \\\\to -y$. So Eq. (15) is valid for both cases $m = \\\\pm 1$.\\n\\nThe integral over $y$ can be calculated in terms of the complete elliptical integral of the first kind $K(x)$ using the identity (3.131.5) from [20]\\n\\n$$\\n\\\\int_{u_2}^{u_3} \\\\frac{dy}{\\\\sqrt{(u_3 - y)(y - u_2)(y - u_1)}} = \\\\frac{2}{\\\\sqrt{u_3 - u_1}} K \\\\left[ \\\\frac{(u_3 - u_2)}{(u_3 - u_1)} \\\\right], \\\\quad (16)\\n$$\\n\\nwhere $u_3 > u_2 > u_1$. In Eq. (15) these parameters are $u_1 = -1$, $u_2 = y_0$, $u_3 = 1$ and the result of integration is equal to $\\\\sqrt{2}K \\\\left[ \\\\sqrt{(1 - y_0)/2} \\\\right]$. Then the DOS for $|m| = 1$ is\\n\\n$$\\n\\\\rho_{|m|=1}(\\\\Omega) = \\\\begin{cases} \\\\frac{\\\\sqrt{2}}{\\\\pi^2 \\\\sqrt{\\\\Omega^2 - 1}} K \\\\left[ \\\\frac{\\\\sqrt{9 - \\\\Omega^2}}{8} \\\\right], & \\\\text{if } 1 \\\\leq \\\\Omega^2 \\\\leq 9, \\\\\\\\ 0, & \\\\text{otherwise}. \\\\end{cases} \\\\quad (17)\\n$$\\n\\nFor convenience, in the Appendix A we provide the definitions of the elliptic integrals.\\n\\n2. Case $|m| > 1$\\n\\nTo calculate the DOS for all other values of the parameter $m$ we need first to analyze the function in denominator of Eq. (13). For $|m| \\\\neq 1$ we can write $a^2 - b^2 = 4(m^2 - 1)(y - y_1)(y - y_2)$. Here the left and right zeros of the function are denoted as $y_1 = \\\\min(\\\\tilde{y}_1, \\\\tilde{y}_2)$ and $y_2 = \\\\max(\\\\tilde{y}_1, \\\\tilde{y}_2)$, respectively, where\\n\\n$$\\n\\\\tilde{y}_1 = \\\\frac{\\\\Omega^2 - 1 - (m + 1)^2}{2(m + 1)}, \\\\quad \\\\tilde{y}_2 = \\\\frac{\\\\Omega^2 - 1 - (m - 1)^2}{2(m - 1)}. \\\\quad (18)\\n$$\\n\\nSo we have to set $y_1 = \\\\tilde{y}_1$, $y_2 = \\\\tilde{y}_2$ if the the following condition is met\\n\\n$$\\n\\\\frac{\\\\Omega^2 + m^2 - 2}{m^2 - 1} > 0, \\\\quad (19)\\n$$\\n\\nand to set $y_1 = \\\\tilde{y}_2$, $y_2 = \\\\tilde{y}_1$, otherwise.\\n\\nLets consider the case $|m| > 1$, then factor $m^2 - 1$ is positive. Rewriting Eq. (13) in terms of variable $y$ and substituting it in Eq. (7), we obtain the nonzero DOS in the implicit form\\n\\n$$\\n\\\\rho(\\\\Omega) = \\\\frac{|\\\\Omega|}{\\\\pi^2 \\\\sqrt{m^2 - 1}} \\\\int \\\\frac{dy}{\\\\sqrt{(1 - y^2)(y_1 - y)(y - y_2)}}, \\\\quad (20)\\n$$\\n\\nHere the boundaries of integration are determined by Eq. (14), which reads $y_1 \\\\geq y \\\\geq y_2$, implying $|y| \\\\leq 1$. There are two cases, in which these conditions can be satisfied by the integration variable $y$:\\n\\n$$\\n|y_1| \\\\leq 1, |y_2| > 1, \\\\quad |y_1| > 1, |y_2| \\\\leq 1. \\\\quad (21)\\n$$\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"taking $\\\\Omega \\\\to \\\\Omega + i0^+$, where we add infinitesimally small imaginary part as it is required in the retarded Green\u2019s function. Then $a = (\\\\Omega + i0^+)^2 + \\\\ldots$ gets small imaginary part $2i\\\\Omega 0^+$ defining the value of $\\\\text{csgn}$. Taking the limit of infinitisimal $0^+$ we arrive at the following expression for the trace of the Green\u2019s function\\n\\n$$\\n\\\\text{Tr}G(\\\\Omega) = \\\\frac{2\\\\Omega}{\\\\pi} \\\\int_0^\\\\pi dk_y \\\\left\\\\{ \\\\frac{\\\\text{sgn} \\\\Omega}{\\\\sqrt{a^2 - b^2}} \\\\right\\\\} \\\\text{sgn} \\\\Omega \\\\text{ for } a^2 > b^2, \\\\quad \\\\text{for } a^2 \\\\leq b^2,\\n$$\\n\\n(13)\\n\\nwhere we used the fact that the integrand is an even function of $k_y$. Here the values $a$ and $b$ are the functions of the variables $k_y$ and $\\\\Omega$ and the parameter $m$ as determined by Eqs. (10). The DOS is proportional to the imaginary part of $\\\\text{Tr}G(\\\\Omega)$, therefore it is nonzero only in the region of $\\\\Omega$ where\\n\\n$$\\na^2 - b^2 \\\\leq 0.\\n$$\\n\\n(14)\\n\\nOutside the region (14) $\\\\text{Tr}G$ is real and the DOS is equal to zero, i.e., $\\\\rho(\\\\Omega) = 0$.\\n\\nThe next step in evaluation of Eq. (13) is to perform integration over $k_y$. It is convenient to replace $y = \\\\cos k_y$ and $dk_y = -dy(1 - y^2)^{-1/2}$. The boundaries of the integration are $-1 \\\\leq y \\\\leq 1$, where changing the order of the integration boundaries results an additional minus sign. The integration over $y$ is performed differently depending on the value of the parameter $m$. Therefore, in what follows we are considering three cases with $|m| = 1$, $|m| > 1$, and $|m| < 1$, separately.\\n\\n1. Case $|m| = 1$\\n\\nCalculations are simpler in the special case of $|m| = 1$. The function in the denominator of the integral (13) can be written explicitly as $a^2 - b^2 = (\\\\Omega^2 - 1)(\\\\Omega^2 - 5 - 4my)$. It is linear in $y$ and changes the sign only once at the point $y = y_0 \\\\text{sgn} m$, where $y_0 = (\\\\Omega^2 - 5)/4$. Solving the condition (14) with respect to $\\\\Omega$ and using the fact that $|y| \\\\leq 1$ we obtain that DOS is nonzero only for $1 \\\\leq |\\\\Omega| \\\\leq 3$.\\n\\nThe condition (14) considered with respect to $y$ determines the boundaries of integration in Eq. (13). For $m = 1$ it is satisfied for $y_0 \\\\leq y \\\\leq 1$. Then the imaginary part of Eq. (13) in terms of variable $y$ gives the DOS in the implicit form\\n\\n$$\\n\\\\rho(\\\\Omega) = \\\\frac{|\\\\Omega|}{\\\\pi^2 \\\\sqrt{\\\\Omega^2 - 1}} \\\\int_{y_0}^1 \\\\frac{dy}{\\\\sqrt{(1 - y^2)(y - y_0)}},\\n$$\\n\\n(15)\\n\\nwhere the definition (7) is used. The similar expression will appears for $m = -1$: the boundaries of integration are $-1 \\\\leq y \\\\leq -y_0$ and denominator in the integrand is $\\\\sqrt{(1 - y^2)(y - y_0)}$. The corresponding DOS can be transformed into the result Eq. (15) by replacing the integration variable $y \\\\to -y$. So Eq. (15) is valid for both cases $m = \\\\pm 1$.\\n\\nThe integral over $y$ can be calculated in terms of the complete elliptical integral of the first kind $K(x)$ using the identity (3.131.5) from [20]}\\n\\n$$\\n\\\\int_{u_2}^{u_3} \\\\frac{dy}{\\\\sqrt{(u_3 - y)(y - u_2)(y - u_1)}} = \\\\frac{2}{\\\\sqrt{u_3 - u_1}} K \\\\left[ \\\\frac{(u_3 - u_2)}{(u_3 - u_1)} \\\\right],\\n$$\\n\\n(16)\\n\\nwhere $u_3 > u_2 > u_1$. In Eq. (15) these parameters are $u_1 = -1$, $u_2 = y_0$, $u_3 = 1$ and the result of integration is equal to $\\\\sqrt{2}K \\\\left[ \\\\sqrt{(1 - y_0)/2} \\\\right]$. Then the DOS for $|m| = 1$ is\\n\\n$$\\n\\\\rho_{|m|=1}(\\\\Omega) = \\\\begin{cases} \\\\frac{\\\\sqrt{2}}{\\\\pi^2 \\\\sqrt{\\\\Omega^2 - 1}} K \\\\left[ \\\\frac{\\\\sqrt{9 - \\\\Omega^2}}{8} \\\\right], & \\\\text{if } 1 \\\\leq \\\\Omega^2 \\\\leq 9, \\\\\\\\ 0, & \\\\text{otherwise}. \\\\end{cases}\\n$$\\n\\n(17)\\n\\nFor convenience, in the Appendix A we provide the def-initions of the elliptic integrals.\\n\\n2. Case $|m| > 1$\\n\\nTo calculate the DOS for all other values of the parameter $m$ we need first to analyze the function in denominator of Eq. (13). For $|m| \\\\neq 1$ we can write $a^2 - b^2 = 4(m^2 - 1)(y - y_1)(y - y_2)$. Here the left and right zeros of the function are denoted as $y_1 = \\\\min(\\\\tilde{y}_1, \\\\tilde{y}_2)$ and $y_2 = \\\\max(\\\\tilde{y}_1, \\\\tilde{y}_2)$, respectively, where\\n\\n$$\\n\\\\tilde{y}_1 = \\\\frac{\\\\Omega^2 - 1 - (m + 1)^2}{2(m + 1)},\\n$$\\n\\n$$\\n\\\\tilde{y}_2 = \\\\frac{\\\\Omega^2 - 1 - (m - 1)^2}{2(m - 1)}.\\n$$\\n\\n(18)\\n\\nSo we have to set $y_1 = \\\\tilde{y}_1$, $y_2 = \\\\tilde{y}_2$ if the the following condition is met\\n\\n$$\\n\\\\frac{\\\\Omega^2 + m^2 - 2}{m^2 - 1} > 0,\\n$$\\n\\n(19)\\n\\nand to set $y_1 = \\\\tilde{y}_2$, $y_2 = \\\\tilde{y}_1$, otherwise.\\n\\nLets consider the case $|m| > 1$, then factor $m^2 - 1$ is positive. Rewriting Eq. (13) in terms of variable $y$ and substituting it in Eq. (7), we obtain the nonzero DOS in the implicit form\\n\\n$$\\n\\\\rho(\\\\Omega) = \\\\frac{|\\\\Omega|}{\\\\pi^2 \\\\sqrt{m^2 - 1}} \\\\int \\\\frac{dy}{\\\\sqrt{(1 - y^2)(y_1 - y)(y - y_2)}},\\n$$\\n\\n(20)\\n\\nHere the boundaries of integration are determined by Eq. (14), which reads $y_1 \\\\geq y \\\\geq y_2$, implying $|y| \\\\leq 1$. There are two cases, in which these conditions can be satisfied by the integration variable $y$:\\n\\n$$\\n|y_1| \\\\leq 1, |y_2| > 1,\\n$$\\n\\n$$\\n|y_1| > 1, |y_2| \\\\leq 1.\\n$$\\n\\n(21)\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the context of high-order schemes, mathematical frameworks have been developed in order to construct schemes of arbitrary spatial discretization order [4, 5]. Among the well-documented ones are the Discontinuous Galerkin [6, 7], the Spectral Differences [8] and the Spectral Volumes [9] class of schemes. More recently, another approach for constructing high-order schemes has been introduced in the literature: the Flux Reconstruction [10], also referred to as FR. This framework is capable of creating compact high-order schemes for solving partial differential equations. Special focus was given to the treatment of advection terms [10] and diffusion terms [11], which are integral components of high-fidelity mathematical models used in fluid dynamics, such as the Navier-Stokes equations.\\n\\nIn the Flux Reconstruction approach, the solution is known at multiple discrete points, also known as nodes, within a discrete domain element: a cell. A continuous solution is reconstructed within the cell by performing an interpolation using a basis of Lagrange polynomials. In general, the overall resulting solution will be continuous only within a cell, usually displaying discontinuities across the interfaces between two adjacent cells. Therefore, if nothing else is done, there is no interaction between nearby cells. The main idea behind the FR framework is to introduce means for information to propagate through the domain. This is done by defining common values for the property fluxes (and its derivatives, if needed) across a cell interface. For the advection terms, the interface fluxes are usually reconstructed in an upwind manner, with the Roe flux [12] being a popular procedure to be used. For the diffusion terms, the common fluxes are taken to be, in its most general case, a weighted average between the fluxes of the immediate adjacent cells. A corrected, continuous, flux function can, then, be reconstructed within a cell, in such a way that the previously defined common interface values are respected. Finally, the time-derivatives of the solution properties can be evaluated by using the corrected fluxes and, then, integrated by using an appropriate time-march scheme selected by the user.\\n\\nAn interesting characteristic of the FR framework is that the continuous reconstruction of the flux terms is performed by using a special set of functions, called \u201ccorrection functions\u201d, and, depending on the function(s) used, different high-order schemes are achieved. For instance, schemes such as the nodal Discontinuous Galerkin [6] and the Staggered Grid [13] can be recovered by using the Flux Reconstruction in combination with an appropriate correction function. One important step that must be made towards the comprehension of the properties of a scheme is the assessment of its stability bounds and effective order of accuracy. In the case of the FR framework, part of this process has already been done by Huynh [10] and documented for a limited number of scheme orders. The present paper aims to expand this analysis by using the FR framework with 5 different correction functions and using cells with up to 10 internal nodes. The resulting schemes are, then, used to solve the 1-D advection model equation. Observations are done regarding the effects of the artificial dissipation over the transient solution, obtained by using a forth-order Runge-Kutta time-marching procedure (RK4). Further insights regarding the stability and accuracy of the resulting schemes are obtained by performing a Fourier analysis on each one of them.\\n\\n2 NUMERICAL FORMULATION\\n\\nIn this section, a brief explanation is given regarding the Flux Reconstruction approach for the construction of high-order schemes. The intention is to give the reader sufficient background knowledge regarding the inner workings of the FR approach before performing the Fourier stability analysis in the next section.\\n\\nThe equation of interest is the initial value problem (IVP) given by the 1-D advection model equation:\\n\\n\\\\[\\n\\\\begin{align*}\\n\\\\frac{\\\\partial u}{\\\\partial t} + \\\\frac{\\\\partial f}{\\\\partial x} &= 0, \\\\quad \\\\text{with} \\\\quad f = au \\\\\\\\\\nu(x,t)|_{t=0} &= u_0(x)\\n\\\\end{align*}\\n\\\\]\\n\\nwhere \\\\( t \\\\) is the time coordinate, \\\\( x \\\\) is the space coordinate, \\\\( u(x,t) \\\\) is the property being transported by a wave, whose dynamics is given by Eq. (1), with constant speed \\\\( a \\\\). Finally, \\\\( f \\\\) is the flux term and the main subject of interest to the FR approach. The discrete domain can be constructed by dividing its continuous counterpart into multiple cells \\\\( E \\\\), where the \\\\( j \\\\)-th cell is denoted by \\\\( E_j \\\\) and has a length \\\\( h_j \\\\). Each cell is composed of \\\\( K \\\\) nodes, which is where the discrete solution, \\\\( u(x,t) \\\\), is evaluated at. The centroid of cell \\\\( E_j \\\\) is located at \\\\( x_j \\\\), and the coordinate of the \\\\( k \\\\)-th node of the \\\\( j \\\\)-th cell is denoted by \\\\( x_{j,k} \\\\). Following the same naming convention, the solution evaluated at \\\\( x_{j,k} \\\\) is defined as \\\\( u_{j,k} \\\\), which is a function of time only. The coordinate of the interface located between elements \\\\( E_j \\\\) and \\\\( E_{j+1} \\\\) is\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the context of high-order schemes, mathematical frameworks have been developed in order to construct schemes of arbitrary spatial discretization order [4, 5]. Among the well-documented ones are the Discontinuous Galerkin [6, 7], the Spectral Differences [8] and the Spectral Volumes [9] class of schemes. More recently, another approach for constructing high-order schemes has been introduced in the literature: the Flux Reconstruction [10], also referred to as FR. This framework is capable of creating compact high-order schemes for solving partial differential equations. Special focus was given to the treatment of advection terms [10] and diffusion terms [11], which are integral components of high-fidelity mathematical models used in fluid dynamics, such as the Navier-Stokes equations. In the Flux Reconstruction approach, the solution is known at multiple discrete points, also known as nodes, within a discrete domain element: a cell. A continuous solution is reconstructed within the cell by performing an interpolation using a basis of Lagrange polynomials. In general, the overall resulting solution will be continuous only within a cell, usually displaying discontinuities across the interfaces between two adjacent cells. Therefore, if nothing else is done, there is no interaction between nearby cells. The main idea behind the FR framework is to introduce means for information to propagate through the domain. This is done by defining common values for the property fluxes (and its derivatives, if needed) across a cell interface. For the advection terms, the interface fluxes are usually reconstructed in an upwind manner, with the Roe flux [12] being a popular procedure to be used. For the diffusion terms, the common fluxes are taken to be, in its most general case, a weighted average between the fluxes of the immediate adjacent cells. A corrected, continuous, flux function can, then, be reconstructed within a cell, in such a way that the previously defined common interface values are respected. Finally, the time-derivatives of the solution properties can be evaluated by using the corrected fluxes and, then, integrated by using an appropriate time-march scheme selected by the user. An interesting characteristic of the FR framework is that the continuous reconstruction of the flux terms is performed by using a special set of functions, called \u201ccorrection functions\u201d, and, depending on the function(s) used, different high-order schemes are achieved. For instance, schemes such as the nodal Discontinuous Galerkin [6] and the Staggered Grid [13] can be recovered by using the Flux Reconstruction in combination with an appropriate correction function. One important step that must be made towards the comprehension of the properties of a scheme is the assessment of its stability bounds and effective order of accuracy. In the case of the FR framework, part of this process has already been done by Huynh [10] and documented for a limited number of scheme orders. The present paper aims to expand this analysis by using the FR framework with 5 different correction functions and using cells with up to 10 internal nodes. The resulting schemes are, then, used to solve the 1-D advection model equation. Observations are done regarding the effects of the artificial dissipation over the transient solution, obtained by using a forth-order Runge-Kutta time-marching procedure (RK4). Further insights regarding the stability and accuracy of the resulting schemes are obtained by performing a Fourier analysis on each one of them.  \\n\\n2 NUMERICAL FORMULATION\\n\\nIn this section, a brief explanation is given regarding the Flux Reconstruction approach for the construction of high-order schemes. The intention is to give the reader sufficient background knowledge regarding the inner workings of the FR approach before performing the Fourier stability analysis in the next section. The equation of interest is the initial value problem (IVP) given by the 1-D advection model equation: \\n\\n\\\\[\\n\\\\begin{align*}\\n\\\\frac{\\\\partial u}{\\\\partial t} + \\\\frac{\\\\partial f}{\\\\partial x} &= 0, \\\\quad \\\\text{with} \\\\quad f = au \\\\\\\\\\nu(x,t)|_{t=0} &= u_0(x)\\n\\\\end{align*}\\n\\\\]  \\n\\n(1)\\n\\nwhere \\\\( t \\\\) is the time coordinate, \\\\( x \\\\) is the space coordinate, \\\\( u(x,t) \\\\) is the property being transported by a wave, whose dynamics is given by Eq. (1), with constant speed \\\\( a \\\\). Finally, \\\\( f \\\\) is the flux term and the main subject of interest to the FR approach. The discrete domain can be constructed by dividing its continuous counterpart into multiple cells \\\\( E \\\\), where the \\\\( j \\\\)-th cell is denoted by \\\\( E_j \\\\) and has a length \\\\( h_j \\\\). Each cell is composed of \\\\( K \\\\) nodes, which is where the discrete solution, \\\\( u(x,t) \\\\), is evaluated at. The centroid of cell \\\\( E_j \\\\) is located at \\\\( x_j \\\\), and the coordinate of the \\\\( k \\\\)-th node of the \\\\( j \\\\)-th cell is denoted by \\\\( x_{j,k} \\\\). Following the same naming convention, the solution evaluated at \\\\( x_{j,k} \\\\) is defined as \\\\( u_{j,k} \\\\), which is a function of time only. The coordinate of the interface located between elements \\\\( E_j \\\\) and \\\\( E_{j+1} \\\\) is\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"further, the time scales becomes sequenced as\\n\\\\[ t_c - \\\\hat{t} < t_i < t_f < t_c + \\\\hat{t}, \\\\]  \\n(48)\\nwhere the system enters into the S regime. The scenario of the adiabatic-impulse approximation is illustrated in Fig. 7.\\n\\nACKNOWLEDGMENTS\\n\\nWe thank Yan He for useful discussion. This work is supported by NSFC under Grants No. 11074177.\\n\\nAppendix A: Solution of TDBdG equations\\n\\nWe can solve the TDBdG equations given by Eq. (6) exactly by mapping them to the Landau-Zener problem. Then, the time-dependent Bogoliubov coefficients can be given by\\n\\n\\\\[ v_q(z) = C_1 D_{-s_q-1}(iz) + C_2 D_{-s_q-1}(-iz), \\\\]  \\n(A1)\\n\\\\[ u_q(z) = \\\\frac{e^{i\\\\pi/4}}{\\\\sqrt{\\\\tau_Q}} \\\\sin q \\\\left( i \\\\frac{d}{dz} + \\\\frac{i z}{2} \\\\right) v_q(z), \\\\]  \\n(A2)\\n\\nwith free complex parameters \\\\( C_1 \\\\) and \\\\( C_2 \\\\). Here, \\\\( D_m(z) \\\\) is the complex parabolic cylinder function, \\\\( z = 2\\\\sqrt{\\\\tau_Q} \\\\left( \\\\frac{1}{\\\\tau_Q} + \\\\cos q \\\\right) e^{i\\\\pi/4} \\\\), and \\\\( s_q = -i\\\\tau_Q \\\\sin^2 q \\\\). To reduce the above rigorous solution, we need to apply the asymptotes of \\\\( D_m(z) \\\\) that are given by [89]\\n\\n\\\\[ D_m(z) = e^{-z^2/4} z^m, \\\\forall |\\\\arg(z)| < 3\\\\pi/4, \\\\]  \\n(A3)\\n\\\\[ D_m(z) = e^{-z^2/4} z^m - \\\\frac{\\\\sqrt{2\\\\pi}}{\\\\Gamma(-m)} e^{-im\\\\pi} e^{z^2/4} z^{-m-1}, \\\\]  \\n\\\\[ \\\\forall -5\\\\pi/4 < |\\\\arg(z)| < -\\\\pi/4, \\\\]  \\n(A4)\\n\\nfor \\\\( |z| \\\\gg 1 \\\\) and\\n\\n\\\\[ D_m(z) = \\\\frac{2^{m/2} \\\\sqrt{\\\\pi}}{\\\\Gamma(\\\\frac{1}{2} - \\\\frac{m}{2})} - \\\\frac{2^{1/2} + \\\\frac{m}{2}}{\\\\Gamma(-\\\\frac{m}{2})} + O(z^2), \\\\]  \\n(A5)\\n\\nfor \\\\( |z| \\\\to 0 \\\\).\\n\\nFurthermore, in numerical simulations, the time-dependent parameter should start at a finite value. We choose a sufficiently large but finite initial transverse field, so the initial conditions of Eqs. (A1) and (A2) can be expanded into a powers of \\\\( 1/g_i \\\\),\\n\\n\\\\[ u_q(t_i)^2 = 1 - \\\\frac{\\\\sin^2 q}{4g_i^2} + O(\\\\frac{1}{g_i}), \\\\]  \\n(A6)\\n\\\\[ v_q(t_i)^2 = 1 - u_q(t_i)^2, \\\\]  \\n(A7)\\n\\nBased on this approximation, the two constants, \\\\( C_1 \\\\) and \\\\( C_2 \\\\), can be expressed as\\n\\n\\\\[ |C_1|^2 = u_q(t_i)^2 e^{-\\\\frac{\\\\pi}{2} \\\\tau_Q \\\\sin^2 q \\\\tau_Q \\\\sin^2 q}, \\\\]  \\n(A8)\\n\\\\[ |C_2|^2 = 0, \\\\]  \\n(A9)\\n\\nfor \\\\( |z_i| \\\\gg 1 \\\\), and\\n\\n\\\\[ C_1 = \\\\frac{v_q(t_i)}{\\\\sqrt{2\\\\pi}} - \\\\frac{(-1)^{3/4} u_q(t_i)}{\\\\sqrt{\\\\tau_Q}} + O(\\\\tau_Q, \\\\tau_Q^2), \\\\]  \\n(A10)\\n\\\\[ C_2 = \\\\frac{v_q(t_i)}{\\\\sqrt{2\\\\pi}} + \\\\frac{(-1)^{3/4} u_q(t_i)}{\\\\sqrt{\\\\tau_Q}} + O(\\\\tau_Q, \\\\tau_Q^2), \\\\]  \\n(A11)\\n\\nfor \\\\( |z_i| \\\\ll 1 \\\\), where \\\\( z_i \\\\) is defined by Eq. (9).\\n\\n[1] T. W. B. Kibble, Journal of Physics A: Mathematical and General 9, 1387 (1976).\\n[2] T. W. B. Kibble, Physics Reports 67, 183 (1980).\\n[3] W. H. Zurek, Nature 217, 505 (1985).\\n[4] W. H. Zurek, Acta physica polonica. B 24, 1301 (1993).\\n[5] W. Zurek, Physics Reports 276, 177 (1996).\\n[6] I. Chuang, R. Durrer, N. Turok, and B. Yurke, Science 251, 1336 (1991).\\n[7] M. J. Bowick, L. Chandar, E. A. Schiff, and A. M. Sridhastava, Science 263, 943 (1994).\\n[8] V. M. H. Ruutu, V. B. Eltssov, A. J. Gill, T. W. B. Kibble, M. Krusius, Y. G. Maklin, B. Pla\u00e7ais, G. E. Volovik, and W. Xu, Nature 382, 334 (1996).\\n[9] C. B\u00e4uerle, Y. M. Bunkov, S. N. Fisher, H. Godfrin, and G. R. Pickett, Nature 382, 332 (1996).\\n[10] R. Monaco, J. Mygind, and R. J. Rivers, Phys. Rev. Lett. 89, 080603 (2002).\\n[11] A. Maniv, E. Polturak, and G. Koren, Phys. Rev. Lett. 91, 197001 (2003).\\n[12] L. E. Sadler, J. M. Higbie, S. R. Leslie, M. Vengalattore, and D. M. Stamper-Kurn, Nature 443, 312 (2006).\\n[13] C. N. Weiler, T. W. Neely, D. R. Scherer, A. S. Bradley, M. J. Davis, and B. P. Anderson, Nature 455, 948 (2008).\\n[14] D. Golubchik, E. Polturak, and G. Koren, Phys. Rev. Lett. 104, 247002 (2010).\\n[15] G. D. Chiara, A. del Campo, G. Morigi, M. B. Plenio, and A. Retzker, New Journal of Physics 12, 115003 (2010).\\n[16] S. M. Griffin, M. Lilienblum, K. T. Delaney, Y. Kumagai, M. Fiebig, and N. A. Spaldin, Phys. Rev. X 2, 041022 (2012).\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"further, the time scales becomes sequenced as\\n\\\\[ t_c - \\\\hat{t} < t_i < t_f < t_c + \\\\hat{t}, \\\\]  \\n(48)\\nwhere the system enters into the S regime. The scenario of the adiabatic-impulse approximation is illustrated in Fig. 7 .\\n\\nACKNOWLEDGMENTS\\n\\nWe thank Yan He for useful discussion. This work is supported by NSFC under Grants No. 11074177. \\n\\nAppendix A: Solution of TDBdG equations\\n\\nWe can solve the TDBdG equations given by Eq. ( 6 ) exactly by mapping them to the Landau-Zener problem. Then, the time-dependent Bogoliubov coefficients can be given by\\n\\n\\\\[ v_q(z) = C_1 D_{-s_q-1}(iz) + C_2 D_{-s_q-1}(-iz), \\\\]  \\n(A1)\\n\\\\[ u_q(z) = \\\\frac{e^{i\\\\pi/4}}{\\\\sqrt{\\\\tau_Q}} \\\\sin q \\\\left( i \\\\frac{d}{dz} + \\\\frac{i z}{2} \\\\right) v_q(z), \\\\]  \\n(A2)\\n\\nwith free complex parameters \\\\( C_1 \\\\) and \\\\( C_2 \\\\). Here, \\\\( D_m(z) \\\\) is the complex parabolic cylinder function, \\\\( z = 2\\\\sqrt{\\\\tau_Q} \\\\left( \\\\frac{1}{\\\\tau_Q} + \\\\cos q \\\\right) e^{i\\\\pi/4} \\\\), and \\\\( s_q = -i\\\\tau_Q \\\\sin^2 q \\\\). To reduce the above rigorous solution, we need to apply the asymptotes of \\\\( D_m(z) \\\\) that are given by [ 89 ]\\n\\n\\\\[ D_m(z) = e^{-z^2/4} z^m, \\\\forall |\\\\arg(z)| < 3\\\\pi/4, \\\\]  \\n(A3)\\n\\\\[ D_m(z) = e^{-z^2/4} z^m - \\\\frac{\\\\sqrt{2\\\\pi}}{\\\\Gamma(-m)} e^{-im\\\\pi} e^{z^2/4} z^{-m-1}, \\\\]  \\n\\\\[ \\\\forall -5\\\\pi/4 < \\\\arg(z) < -\\\\pi/4, \\\\]  \\n(A4)\\n\\nfor \\\\( |z| \\\\gg 1 \\\\) and\\n\\n\\\\[ D_m(z) = \\\\frac{2^{m/2} \\\\sqrt{\\\\pi}}{\\\\Gamma(\\\\frac{1}{2} - \\\\frac{m}{2})} - \\\\frac{2^{1/2} + \\\\frac{m}{2}}{\\\\Gamma(-\\\\frac{m}{2})} + O(z^2), \\\\]  \\n(A5)\\n\\nfor \\\\( |z| \\\\to 0 \\\\).\\n\\nFurthermore, in numerical simulations, the time-dependent parameter should start at a finite value. We choose a sufficiently large but finite initial transverse field, so the initial conditions of Eqs. ( A1 ) and ( A2 ) can be expanded into a powers of \\\\( 1/g_i \\\\),\\n\\n\\\\[ u_q(t_i)^2 = 1 - \\\\frac{\\\\sin^2 q}{4g_i^2} + O(\\\\frac{1}{g_i}), \\\\]  \\n(A6)\\n\\\\[ v_q(t_i)^2 = 1 - u_q(t_i)^2, \\\\]  \\n(A7)\\n\\nBased on this approximation, the two constants, \\\\( C_1 \\\\) and \\\\( C_2 \\\\), can be expressed as\\n\\n\\\\[ |C_1|^2 = u_q(t_i)^2 e^{-\\\\frac{\\\\pi}{2} \\\\tau_Q \\\\sin^2 q \\\\tau_Q \\\\sin^2 q}, \\\\]  \\n(A8)\\n\\\\[ |C_2|^2 = 0, \\\\]  \\n(A9)\\n\\nfor \\\\( |z_i| \\\\gg 1 \\\\), and\\n\\n\\\\[ C_1 = \\\\frac{v_q(t_i)}{\\\\sqrt{2\\\\pi}} - \\\\frac{(-1)^{3/4} u_q(t_i)}{2} \\\\sqrt{\\\\tau_Q} \\\\sin q + O(\\\\tau_Q, \\\\tau_Q^2), \\\\]  \\n(A10)\\n\\\\[ C_2 = \\\\frac{v_q(t_i)}{\\\\sqrt{2\\\\pi}} + \\\\frac{(-1)^{3/4} u_q(t_i)}{2} \\\\sqrt{\\\\tau_Q} \\\\sin q + O(\\\\tau_Q, \\\\tau_Q^2), \\\\]  \\n(A11)\\n\\nfor \\\\( |z_i| \\\\ll 1 \\\\), where \\\\( z_i \\\\) is defined by Eq. ( 9 ).\\n\\n[1] T. W. B. Kibble, Journal of Physics A: Mathematical and General 9, 1387 (1976) . [2] T. W. B. Kibble, Physics Reports 67, 183 (1980) . [3] W. H. Zurek, Nature 317, 505 (1985) . [4] W. H. Zurek, Acta physica polonica. B 24, 1301 (1993) . [5] W. Zurek, Physics Reports 276, 177 (1996) . [6] I. Chuang, R. Durrer, N. Turok, and B. Yurke, Science 251, 1336 (1991) . [7] M. J. Bowick, L. Chandar, E. A. Schiff, and A. M. Srivastava, Science 263, 943 (1994) . [8] V. M. H. Ruutu, V. B. Eltsov, A. J. Gill, T. W. B. Kibble, M. Krusius, Y. G. Maklin, B. Pla\u00e7ais, G. E. Volovik, and W. Xu, Nature 382, 334 (1996) . [9] C. B\u00e4uerle, Y. M. Bunkov, S. N. Fisher, H. Godfrin, and G. R. Pickett, Nature 382, 332 (1996) . [10] R. Monaco, J. Mygind, and R. J. Rivers, Phys. Rev. Lett. 89, 080603 (2002) . [11] A. Maniv, E. Polturak, and G. Koren, Phys. Rev. Lett. 91, 197001 (2003) . [12] L. E. Sadler, J. M. Higbie, S. R. Leslie, M. Vengalattore, and D. M. Stamper-Kurn, Nature 443, 312 (2006) . [13] C. N. Weiler, T. W. Neely, D. R. Scherer, A. S. Bradley, M. J. Davis, and B. P. Anderson, Nature 455, 948 (2008) . [14] D. Golubchik, E. Polturak, and G. Koren, Phys. Rev. Lett. 104, 247002 (2010) . [15] G. D. Chiara, A. del Campo, G. Morigi, M. B. Plenio, and A. Retzker, New Journal of Physics 12, 115003 (2010) . [16] S. M. Griffin, M. Lilienblum, K. T. Delaney, Y. Kumagai, M. Fiebig, and N. A. Spaldin, Phys. Rev. X 2, 041022 (2012) .\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Proposition 2. Given integers $l$ and $k$ where $l, k \\\\geq 2$, the shortest negative cycle of $\\\\overline{BQ}(\\\\ell, 2k - 1)$ is of length $\\\\min\\\\{2l, 2k\\\\}$.\\n\\nProof. We first present two natural choices for a negative cycle, one of length $2k$ and another of length $2l$. The first is a negative cycle on the first two layers. Take a positive edge and connect its two ends with one of the two paths using only the negative edges that connect the two layers. This would result in a negative cycle of length $2k$. The second negative cycle we consider is by taking a positive edge and connecting each of its ends to the vertex $u$ by a shortest path (all edges negative). One of these paths will be of length $l$ and the other would be of length $l - 1$. Together with the first chosen edge itself then, they form a negative cycle of length $2l$.\\n\\nIt remains to show that the shortest of these two types of cycles gives us the negative girth. To that end, we will first show that a shortest negative cycle can only use one positive edge of $\\\\overline{BQ}(\\\\ell, 2k - 1)$. Towards a contradiction, let $C$ be a negative cycle with more than two positive edges. We aim to present a negative cycle $C'$ whose length is at most $|C| - 2$. We take two positive edges of $C$ that come consecutively on the cyclic order. Assume $xy$ and $x'y'$ are these two edges and that $x'$ is followed by $y$ in the cyclic order of $C$ (that is to say, there is no positive edge in the $x' - y$ path in $C$). We remove the two positive edges $xy$ and $x'y'$ and the $x'y$ path connecting them in $C$, but then we add a $xy'$ copy of this path (which also has no positive edge). The result is a closed walk whose sign is the same as that of $C$, and whose length is $|C| - 2$. But then this closed walk must contain a negative cycle, whose length then is also at most $|C| - 2$, a contradiction.\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Proposition 2. Given integers $l$ and $k$ where $l, k \\\\geq 2$, the shortest negative cycle of $\\\\overline{BQ}(\\\\ell, 2k - 1)$ is of length $\\\\min\\\\{2l, 2k\\\\}$.\\n\\nProof. We first present two natural choices for a negative cycle, one of length $2k$ and another of length $2l$. The first is a negative cycle on the first two layers. Take a positive edge and connect its two ends with one of the two paths using only the negative edges that connect the two layers. This would result in a negative cycle of length $2k$. The second negative cycle we consider is by taking a positive edge and connecting each of its ends to the vertex $u$ by a shortest path (all edges negative). One of these paths will be of length $l$ and the other would be of length $l - 1$. Together with the first chosen edge itself then, they form a negative cycle of length $2l$.\\n\\nIt remains to show that the shortest of these two types of cycles gives us the negative girth. To that end, we will first show that a shortest negative cycle can only use one positive edge of $\\\\overline{BQ}(\\\\ell, 2k - 1)$. Towards a contradiction, let $C$ be a negative cycle with more than two positive edges. We aim to present a negative cycle $C'$ whose length is at most $|C| - 2$. We take two positive edges of $C$ that come consecutively on the cyclic order. Assume $xy$ and $x'y'$ are these two edges and that $x'$ is followed by $y$ in the cyclic order of $C$ (that is to say, there is no positive edge in the $x' - y$ path in $C$). We remove the two positive edges $xy$ and $x'y'$ and the $x'y$ path connecting them in $C$, but then we add a $xy'$ copy of this path (which also has no positive edge). The result is a closed walk whose sign is the same as that of $C$, and whose length is $|C| - 2$. But then this closed walk must contain a negative cycle, whose length then is also at most $|C| - 2$, a contradiction.\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For a set $E$, $\\\\overset{\\\\circ}{E}$ denotes its interior and $\\\\overline{E}$ its closure.\\n\\nFor $E \\\\subseteq d$, $C_{\\\\overline{E}}[0, \\\\infty)$ denotes the set of continuous functions from $[0, \\\\infty)$ to $\\\\overline{E}$ and $D_{\\\\overline{E}}[0, \\\\infty)$ denotes the set of cadlag functions from $[0, \\\\infty)$ to $\\\\overline{E}$.\\n\\nFor $E \\\\subseteq d$, $\\\\mathcal{P}(\\\\overline{E})$ denotes the set of probability measures on $\\\\overline{E}$. For a stochastic process $Z$, $\\\\{\\\\mathcal{F}_t^Z\\\\}$ denotes the filtration generated by $Z$, i.e. $\\\\mathcal{F}_t^Z := \\\\sigma\\\\{Z(s), s \\\\leq t\\\\}$.\\n\\n## 2 Formulation of the problem and preliminaries\\n\\nLet $\\\\mathcal{W} \\\\subseteq d$ be a piecewise smooth cone with vertex at the origin i.e.\\n\\n$$\\\\mathcal{W} := \\\\bigcap_{j=1}^{m} \\\\mathcal{W}_j, \\\\quad \\\\mathcal{W}_j := \\\\{x = rz, z \\\\in S_j, r > 0\\\\},$$\\n\\nwhere $S_j$ is a nonempty domain in the unit sphere $S^{d-1}$ with $C^2$ boundary. Clearly\\n\\n$$\\\\mathcal{W} = \\\\{x = rz, z \\\\in S, r > 0\\\\}, \\\\quad \\\\text{where } S := \\\\bigcap_{j=1}^{m} S_j.$$  \\\\hspace{1cm} (2)\\n\\nIt is supposed that\\n\\n$$\\\\overline{S} = \\\\bigcap_{j=1}^{m} \\\\overline{S}_j.$$  \\\\hspace{1cm} (3)\\n\\nThe object of this work is the semimartingale obliquely reflecting Brownian motion (ORBM) in $\\\\overline{W}$ with drift $b$, dispersion matrix $\\\\sigma$ and radially constant direction of reflection $g^j$ on each face, i.e. for $x \\\\in \\\\partial \\\\mathcal{W}_j - \\\\{0\\\\}$, $x = rz$, $z \\\\in \\\\partial S_j$,\\n\\n$$g^j(x) = g^j(z),$$\\n\\nfor some unit vector field $g^j$ defined on $\\\\partial S_j$. This process can be defined as the solution of the following stochastic differential equation with reflection:\\n\\n$$X(t) = X(0) + bt + \\\\sigma W(t) + \\\\int_0^t \\\\gamma(s) d\\\\lambda(s), \\\\quad t \\\\geq 0,$$\\n\\n$$\\\\gamma(t) \\\\in G(X(t)), \\\\quad |\\\\gamma(t)| = 1, \\\\quad d\\\\lambda - a.e., \\\\quad t \\\\geq 0,$$\\n\\n$$X(t) \\\\in \\\\overline{W}, \\\\quad \\\\lambda(t) = \\\\int_0^t 1_{\\\\partial \\\\mathcal{W}}(X(s)) d\\\\lambda(s), \\\\quad t \\\\geq 0,$$\\n\\nwhere\\n\\n$$G(x) := \\\\{g : g = \\\\sum_{j \\\\in J(x)} u_j g^j(x), u_j \\\\geq 0\\\\}, \\\\quad J(x) := \\\\{j : x \\\\in \\\\partial \\\\mathcal{W}_j\\\\}, \\\\quad x \\\\in \\\\partial \\\\mathcal{W} - 0,$$\\n\\n$$G(0) := \\\\text{the closed convex cone generated by } \\\\{g^j(x), x \\\\in (\\\\partial \\\\mathcal{W}_j \\\\cap \\\\overline{W}) - \\\\{0\\\\}, j = 1, \\\\cdots, m\\\\}.$$  \\\\hspace{1cm} (5)\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For a set $E$, $\\\\overset{\\\\circ}{E}$ denotes its interior and $\\\\overline{E}$ its closure.\\n\\nFor $E \\\\subseteq d$, $C_{\\\\overline{E}}[0, \\\\infty)$ denotes the set of probability measures on $\\\\overline{E}$. For a stochastic process $Z$, $\\\\{\\\\mathcal{F}_t^Z\\\\}$ denotes the filtration generated by $Z$, i.e. $\\\\mathcal{F}_t^Z := \\\\sigma\\\\{Z(s), s \\\\leq t\\\\}$.\\n\\n## 2 Formulation of the problem and preliminaries\\n\\nLet $\\\\mathcal{W} \\\\subseteq d$ be a piecewise smooth cone with vertex at the origin i.e. \\n\\n$$\\\\mathcal{W} := \\\\bigcap_{j=1}^m \\\\mathcal{W}_j, \\\\quad \\\\mathcal{W}_j := \\\\{x = rz, z \\\\in S_j, r > 0\\\\},$$\\n\\nwhere $S_j$ is a nonempty domain in the unit sphere $S^{d-1}$ with $C^2$ boundary. Clearly\\n\\n$$\\\\mathcal{W} = \\\\{x = rz, z \\\\in S, r > 0\\\\}, \\\\quad \\\\text{where } S := \\\\bigcap_{j=1}^m S_j.$$  \\n\\nIt is supposed that\\n\\n$$\\\\overline{S} = \\\\bigcap_{j=1}^m \\\\overline{S}_j.$$  \\n\\nThe object of this work is the semimartingale obliquely reflecting Brownian motion (ORBM) in $\\\\overline{W}$ with drift $b$, dispersion matrix $\\\\sigma$ and radially constant direction of reflection $g^j$ on each face, i.e. for $x \\\\in \\\\partial \\\\mathcal{W}_j - \\\\{0\\\\}$, $x = rz$, $z \\\\in \\\\partial S_j$,\\n\\n$$g^j(x) = g^j(z),$$\\n\\nfor some unit vector field $g^j$ defined on $\\\\partial S_j$. This process can be defined as the solution of the following stochastic differential equation with reflection: \\n\\n$$X(t) = X(0) + bt + \\\\sigma W(t) + \\\\int_0^t \\\\gamma(s) d\\\\lambda(s), \\\\quad t \\\\geq 0,$$\\n\\n$$\\\\gamma(t) \\\\in G(X(t)), \\\\quad |\\\\gamma(t)| = 1, \\\\quad d\\\\lambda - a.e., \\\\quad t \\\\geq 0,$$\\n\\n$$X(t) \\\\in \\\\overline{W}, \\\\quad \\\\lambda(t) = \\\\int_0^t 1_{\\\\partial \\\\mathcal{W}}(X(s)) d\\\\lambda(s), \\\\quad t \\\\geq 0,$$\\n\\nwhere\\n\\n$$G(x) := \\\\{g : g = \\\\sum_{j \\\\in J(x)} u_j g^j(x), u_j \\\\geq 0\\\\}, \\\\quad J(x) := \\\\{j : x \\\\in \\\\partial \\\\mathcal{W}_j\\\\}, \\\\quad x \\\\in \\\\partial \\\\mathcal{W} - 0,$$\\n\\n$$G(0) := the closed convex cone generated by \\\\{g^j(x), x \\\\in (\\\\partial \\\\mathcal{W}_j \\\\cap \\\\overline{W}) - \\\\{0\\\\}, j = 1, \\\\cdots, m\\\\}.$$\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lemma 1.1. [0] If $G$ is a graph such that for $u, v \\\\in V(G)$, $uv \\\\notin E(G)$, then $\\\\rho(G) < \\\\rho(G + uv)$.\\n\\nLet $A$ be a real symmetric matrix whose rows and columns are indexed by $V = \\\\{1, 2, \\\\cdots, n\\\\}$. Let $\\\\{V_1, V_2, \\\\cdots, V_k\\\\}$ be a partition of $V$ such that the block partition of the matrix $A$ according to $\\\\{V_1, V_2, \\\\cdots, V_k\\\\}$ can be expressed as\\n\\n$$A = \\\\begin{bmatrix}\\nA_{11} & A_{12} & \\\\cdots & A_{1k} \\\\\\\\\\nA_{21} & A_{22} & \\\\cdots & A_{2k} \\\\\\\\\\n\\\\vdots & \\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\\\nA_{k1} & A_{k2} & \\\\cdots & A_{kk}\\n\\\\end{bmatrix},$$\\n\\nwhere $A_{ij}$ denotes the block formed by intersection of the rows in $V_i$ and the columns in $V_j$. Let $q_{ij}(A)$ denote the average row sum of $A_{ij}$. Then, the quotient matrix of $A$ with respect to the partition $\\\\{V_1, V_2, \\\\cdots, V_k\\\\}$ is given by\\n\\n$$Q(A) = [q_{ij}(A)].$$\\n\\nMoreover, if the row sum of each block $A_{ij}$ is constant then we say that the partition is equitable and $Q(A)$ is called an equitable quotient matrix of $A$. There is a nice relation between the spectrum of $A$ and that of $Q(A)$, which is stated now as a theorem.\\n\\nTheorem 1.2. [0] Let $A$ be a real symmetric matrix such that it has an equitable quotient matrix $Q(A)$, then, $\\\\sigma(Q(A)) \\\\subset \\\\sigma(Q(A))$. Moreover, if $A$ is nonnegative, then $\\\\rho(A) = \\\\rho(Q(A))$, i.e., the spectral radius of $Q(A)$ is actually the spectral radius of $A$.\\n\\nA vertex $v$ of a connected graph $G$ is a cut vertex of $G$ if $G - v$ is disconnected. A block of the graph $G$ is a maximal connected subgraph of $G$ that has no cut-vertex. Given two blocks $F$ and $H$ of graph $G$ are said to be adjacent if they are connected via a cut-vertex. We denote $F \\\\odot H$, to represent the induced subgraph on the vertex set of two adjacent blocks $F$ and $H$.\\n\\nA complete graph is a graph where each vertex is adjacent to every other vertex. A complete graph on $n$ vertices is denoted by $K_n$. A connected graph is called a clique tree if each of its blocks is a clique. Let $G$ be a clique tree with $d_1$ blocks of $K_{n_1}$, $d_2$ blocks of $K_{n_2}$, so on up to $d_k$ blocks of $K_{n_k}$, then we write $G$ with blocks $K_{n_1}^{(d_1)} \\\\odot K_{n_2}^{(d_2)} \\\\cdots \\\\odot K_{n_k}^{(d_k)}$ (for example see Figure 1). Here the above notation only gives information about the blocks of $G$, but not about the structure of the graph. But for a special case, if $G$ has a central cut vertex, then all the blocks are adjacent via the central cut vertex and we denote it by\\n\\n$$G = K_{n_1}^{(d_1)} \\\\odot K_{n_2}^{(d_2)} \\\\cdots \\\\odot K_{n_k}^{(d_k)}.$$\\n\\nA block $H$ of a clique tree $G$ is a pendent block of $G$ if $H$ has exactly one cut-vertex of $G$. Let $v$ be a cut-vertex of $G$. If $G - v$ consists of two disjoint graphs $W_1$ and $W_2$ and let $G_i (i = 1, 2)$ be the subgraph of $G$ induced by $\\\\{v\\\\} \\\\cup V(W_i)$, then $G$ is called the vertex-sum at $v$ of the two graphs $G_1$ and $G_2$, and denoted by $G = G_1 \\\\oplus_v G_2$. For a vertex $v$ in a graph $G$, the block index of $v$ is the number of blocks which share the vertex $v$ and is denoted by $b_G(v)$.\\n\\nThe spectral radius of a graph has been extensively studied subject to various graph theoretic constraints. In spectral graph theory, the maximization and minimization of the spectral radius of a given class of a graph and the problem of determining the extremal graphs find a particular interest among researchers. One such result due to Brualdi and Solheid in [0] is an important motivation for our study. In this article, the authors obtained the graphs that maximize the\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lemma 1.1. [0] If $G$ is a graph such that for $u, v \\\\in V(G)$, $uv \\\\notin E(G)$, then $\\\\rho(G) < \\\\rho(G + uv)$.\\n\\nLet $A$ be a real symmetric matrix whose rows and columns are indexed by $V = \\\\{1, 2, \\\\ldots, n\\\\}$. Let $\\\\{V_1, V_2, \\\\ldots, V_k\\\\}$ be a partition of $V$ such that the block partition of the matrix $A$ according to $\\\\{V_1, V_2, \\\\ldots, V_k\\\\}$ can be expressed as\\n\\n$$A = \\\\begin{bmatrix}\\nA_{11} & A_{12} & \\\\cdots & A_{1k} \\\\\\\\\\nA_{21} & A_{22} & \\\\cdots & A_{2k} \\\\\\\\\\n\\\\vdots & \\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\\\nA_{k1} & A_{k2} & \\\\cdots & A_{kk}\\n\\\\end{bmatrix},$$\\n\\nwhere $A_{ij}$ denotes the block formed by intersection of the rows in $V_i$ and the columns in $V_j$. Let $q_{ij}(A)$ denote the average row sum of $A_{ij}$. Then, the quotient matrix of $A$ with respect to the partition $\\\\{V_1, V_2, \\\\ldots, V_k\\\\}$ is given by\\n\\n$$Q(A) = [q_{ij}(A)].$$\\n\\nMoreover, if the row sum of each block $A_{ij}$ is constant then we say that the partition is equitable and $Q(A)$ is called an equitable quotient matrix of $A$. There is a nice relation between the spectrum of $A$ and that of $Q(A)$, which is stated now as a theorem. \\n\\nTheorem 1.2. [ 0 ] Let $A$ be a real symmetric matrix such that it has an equitable quotient matrix $Q(A)$, then, $\\\\sigma(Q(A)) \\\\subset \\\\sigma(Q(A))$. Moreover, if $A$ is nonnegative, then $\\\\rho(A) = \\\\rho(Q(A))$, i.e., the spectral radius of $Q(A)$ is actually the spectral radius of $A$.\\n\\nA vertex $v$ of a connected graph $G$ is a cut vertex of $G$ if $G - v$ is disconnected. A block of the graph $G$ is a maximal connected subgraph of G that has no cut-vertex. Given two blocks $F$ and $H$ of graph $G$ are said to be adjacent if they are connected via a cut-vertex. We denote $F \\\\odot H$, to represent the induced subgraph on the vertex set of two adjacent blocks $F$ and $H$.\\n\\nA complete graph is a graph where each vertex is adjacent to every other vertex. A complete graph on $n$ vertices is denoted by $K_n$. A connected graph is called a clique tree if each of its blocks is a clique. Let $G$ be a clique tree with $d_1$ blocks of $K_{n_1}$, $d_2$ blocks of $K_{n_2}$, so on up to $d_k$ blocks of $K_{n_k}$, then we write $G$ with blocks $K_{n_1}^{(d_1)} \\\\odot K_{n_2}^{(d_2)} \\\\odot \\\\cdots \\\\odot K_{n_k}^{(d_k)}$ (for example see Figure 1 ). Here the above notation only gives information about the blocks of $G$, but not about the structure of the graph. But for a special case, if $G$ has a central cut vertex, then all the blocks are adjacent via the central cut vertex and we denote it by\\n\\n$$G = K_{n_1}^{(d_1)} \\\\odot K_{n_2}^{(d_2)} \\\\odot \\\\cdots \\\\odot K_{n_k}^{(d_k)}.$$\\n\\nA block $H$ of a clique tree $G$ is a pendent block of $G$ if $H$ has exactly one cut-vertex of $G$. Let $v$ be a cut-vertex of $G$. If $G - v$ consists of two disjoint graphs $W_1$ and $W_2$ and let $G_i (i = 1, 2)$ be the subgraph of $G$ induced by $\\\\{v\\\\} \\\\cup V(W_i)$, then $G$ is called the vertex-sum at $v$ of the two graphs $G_1$ and $G_2$, and denoted by $G = G_1 \\\\oplus_v G_2$. For a vertex $v$ in a graph $G$, the block index of $v$ is the number of blocks which share the vertex $v$ and is denoted by $b_G(v)$.\\n\\nThe spectral radius of a graph has been extensively studied subject to various graph theoretic constraints. In spectral graph theory, the maximization and minimization of the spectral radius of a given class of a graph and the problem of determining the extremal graphs find a particular interest among researchers. One such result due to Brualdi and Solheid in [0] is an important motivation for our study. In this article, the authors obtained the graphs that maximize the\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where \\\\( \\\\rho_w \\\\) is the density of water and \\\\( C \\\\) the ocean function that equals one where water is present and zero otherwise. In the case of ice loading, the direct term is given by\\n\\n\\\\[\\n\\\\zeta = \\\\rho_i (1 - C) \\\\Delta I,\\n\\\\]\\n\\n(2.12)\\n\\nwhere \\\\( \\\\rho_i \\\\) is the density of ice, and \\\\( \\\\Delta I \\\\) the change in ice thickness. The factor, \\\\( 1 - C \\\\), within this expression accounts for the possibility of floating ice (e.g. Crawford et al. 2018, equations 31\u201334).\\n\\nConsider again a pair of solutions \\\\((u, \\\\phi)\\\\) and \\\\((u^\\\\dagger, \\\\phi^\\\\dagger)\\\\) of the loading problem associated, respectively, to with loads \\\\( \\\\sigma \\\\) and \\\\( \\\\sigma^\\\\dagger \\\\). Here, however, we assume that these loads are decomposed as in eq.(2.11) into water and direct terms that share a common ocean function. From the above expression for sea level change we can write\\n\\n\\\\[\\nu \\\\cdot \\\\nabla \\\\Phi + \\\\phi = -g \\\\Delta SL + \\\\Phi_g, \\\\quad u^\\\\dagger \\\\cdot \\\\nabla \\\\Phi + \\\\phi^\\\\dagger = -g \\\\Delta SL^\\\\dagger + \\\\Phi_g^\\\\dagger,\\n\\\\]\\n\\n(2.13)\\n\\nand hence eq.(2.8) becomes\\n\\n\\\\[\\n\\\\int_{\\\\partial M} (-g \\\\Delta SL^\\\\dagger + \\\\Phi_g^\\\\dagger) \\\\sigma \\\\, dS = \\\\int_{\\\\partial M} (-g \\\\Delta SL + \\\\Phi_g) \\\\sigma^\\\\dagger \\\\, dS.\\n\\\\]\\n\\n(2.14)\\n\\nThe terms involving the constants \\\\( \\\\Phi_g \\\\) and \\\\( \\\\Phi_g^\\\\dagger \\\\) vanish due to conservation of mass, and so the identity simplifies to\\n\\n\\\\[\\n\\\\int_{\\\\partial M} \\\\Delta SL^\\\\dagger \\\\sigma \\\\, dS = \\\\int_{\\\\partial M} \\\\Delta SL \\\\sigma^\\\\dagger \\\\, dS.\\n\\\\]\\n\\n(2.15)\\n\\nIf we now substitute into the equality the decompositions of the loads, \\\\( \\\\sigma \\\\) and \\\\( \\\\sigma^\\\\dagger \\\\), we find\\n\\n\\\\[\\n\\\\int_{\\\\partial M} \\\\Delta SL^\\\\dagger (\\\\rho_w C \\\\Delta SL + \\\\zeta) \\\\, dS = \\\\int_{\\\\partial M} \\\\Delta SL (\\\\rho_w C \\\\Delta SL^\\\\dagger + \\\\zeta^\\\\dagger) \\\\, dS,\\n\\\\]\\n\\n(2.16)\\n\\nand cancelling the terms symmetric in \\\\( \\\\Delta SL \\\\) and \\\\( \\\\Delta SL^\\\\dagger \\\\) we arrive at\\n\\n\\\\[\\n\\\\int_{\\\\partial M} \\\\Delta SL^\\\\dagger \\\\zeta \\\\, dS = \\\\int_{\\\\partial M} \\\\Delta SL \\\\zeta^\\\\dagger \\\\, dS.\\n\\\\]\\n\\n(2.17)\\n\\nAgain, this is a known result, being implied as a special case by the adjoint theory of Crawford et al. (2018) for sea level change in a viscoelastic earth model. What is new is the explicit statement as a reciprocity theorem along with the more elementary derivation that has been facilitated by restricting attention to the elastic fingerprint problem.\\n\\n### 2.4 Symmetry of the Green\u2019s function\\n\\nBecause the fingerprint problem is linear, its solution must take the form\\n\\n\\\\[\\n\\\\Delta SL(x) = \\\\int_{\\\\partial M} G(x, x') \\\\zeta(x') \\\\, dS_{x'},\\n\\\\]\\n\\n(2.18)\\n\\nfor an appropriate Green\u2019s function, where we have added a subscript to the surface element to make clear which variable it is defined with respect to. Suppose that, within eq.(2.17), we take\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where $\\\\rho_w$ is the density of water and $C$ the ocean function that equals one where water is present and zero otherwise. In the case of ice loading, the direct term is given by\\n\\n$$\\\\zeta = \\\\rho_i (1 - C) \\\\Delta I,$$\\n\\n(2.12)\\n\\nwhere $\\\\rho_i$ is the density of ice, and $\\\\Delta I$ the change in ice thickness. The factor, $1 - C$, within this expression accounts for the possibility of floating ice (e.g. Crawford et al. 2018, equations 31\u201334). Consider again a pair of solutions $(u, \\\\phi)$ and $(u^\\\\dagger, \\\\phi^\\\\dagger)$ of the loading problem associated, respectively, to with loads $\\\\sigma$ and $\\\\sigma^\\\\dagger$. Here, however, we assume that these loads are decomposed as in eq.(2.11) into water and direct terms that share a common ocean function. From the above expression for sea level change we can write\\n\\n$$u \\\\cdot \\\\nabla \\\\Phi + \\\\phi = -g \\\\Delta SL + \\\\Phi_g, \\\\quad u^\\\\dagger \\\\cdot \\\\nabla \\\\Phi + \\\\phi^\\\\dagger = -g \\\\Delta SL^\\\\dagger + \\\\Phi_g^\\\\dagger,$$\\n\\n(2.13)\\n\\nand hence eq.(2.8) becomes\\n\\n$$\\\\int_{\\\\partial M} (-g \\\\Delta SL^\\\\dagger + \\\\Phi_g^\\\\dagger) \\\\sigma \\\\, dS = \\\\int_{\\\\partial M} (-g \\\\Delta SL + \\\\Phi_g) \\\\sigma^\\\\dagger \\\\, dS.$$\\n\\n(2.14)\\n\\nThe terms involving the constants $\\\\Phi_g$ and $\\\\Phi_g^\\\\dagger$ vanish due to conservation of mass, and so the identity simplifies to\\n\\n$$\\\\int_{\\\\partial M} \\\\Delta SL^\\\\dagger \\\\sigma \\\\, dS = \\\\int_{\\\\partial M} \\\\Delta SL \\\\sigma^\\\\dagger \\\\, dS.$$\\n\\n(2.15)\\n\\nIf we now substitute into the equality the decompositions of the loads, $\\\\sigma$ and $\\\\sigma^\\\\dagger$, we find\\n\\n$$\\\\int_{\\\\partial M} \\\\Delta SL^\\\\dagger (\\\\rho_w C \\\\Delta SL + \\\\zeta) \\\\, dS = \\\\int_{\\\\partial M} \\\\Delta SL (\\\\rho_w C \\\\Delta SL^\\\\dagger + \\\\zeta^\\\\dagger) \\\\, dS,$$\\n\\n(2.16)\\n\\nand cancelling the terms symmetric in $\\\\Delta SL$ and $\\\\Delta SL^\\\\dagger$ we arrive at\\n\\n$$\\\\int_{\\\\partial M} \\\\Delta SL^\\\\dagger \\\\zeta \\\\, dS = \\\\int_{\\\\partial M} \\\\Delta SL \\\\zeta^\\\\dagger \\\\, dS.$$\\n\\n(2.17)\\n\\nAgain, this is a known result, being implied as a special case by the adjoint theory of Crawford et al. (2018) for sea level change in a viscoelastic earth model. What is new is the explicit statement as a reciprocity theorem along with the more elementary derivation that has been facilitated by restricting attention to the elastic fingerprint problem.  \\n\\n### 2.4 Symmetry of the Green\u2019s function\\n\\nBecause the fingerprint problem is linear, its solution must take the form\\n\\n$$\\\\Delta SL(x) = \\\\int_{\\\\partial M} G(x, x') \\\\zeta(x') \\\\, dS_{x'},$$\\n\\n(2.17), we take\\n\\n$$\\\\Delta SL(x) = \\\\int_{\\\\partial M} G(x, x') \\\\zeta(x') \\\\, dS_{x'},$$\\n\\n(2.18)\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"decline in activity on Stack Overflow.\\n\\nWe report the estimated effect of our difference-in-differences model in Table 1 and visualize the weekly estimates of the relative change in the Stack Overflow activity in Figure 2. Table 1 indicates that ChatGPT decreased posting activity on Stack Overflow by 15.6% ($1 - e^{-0.17}$). These results are robust to changes in the controls and starting point of the data time series. We also tested for heterogeneity in subsets of the data: considering only questions (rather than counting both questions and answers) and posts on weekdays. In both subsets our estimates did not deviate significantly from the main result: we estimate a 12% relative decrease in questions and 14% relative decrease in posts on weekdays.\\n\\n|                | (1) Number of posts | (2) Number of questions | (3) Weekday posts |\\n|----------------|---------------------|-------------------------|-------------------|\\n| Stack Overflow \u00d7 Post-GPT | -0.170** (0.0607) | -0.112+ (0.0619) | -0.149* (0.0636) |\\n| Observations   | 1,150               | 1,150                   | 1,150             |\\n| R-squared      | 0.995               | 0.994                   | 0.993             |\\n| R-squared within | 0.290               | 0.315                   | 0.232             |\\n| Outcome mean   | 16363               | 7273                    | 13191             |\\n| Outcome std. dev. | 29088               | 12661                   | 23685             |\\n\\nTable 1: Results of a difference-in-differences model, estimating the change in activity observed weekly on Stack Overflow following the release of ChatGPT, relative to activity on four other platforms less likely to have been impacted. All regressions comprise platform fixed effects, week fixed effects, and platform-specific linear time-trends. The standard-error of the estimate clustered on month is reported in parentheses. Significance codes: ***, $p < 0.001$, **, $p < 0.01$, *, $p < 0.05$, +, $p < 0.1$.\\n\\nFigure 2 shows that the impact of ChatGPT is increasing over time and is by the end of our study greater in magnitude than the average post-ChatGPT effect estimated in Table 1. By the end of April 2023, the estimated effect stabilizes at around 25%. Interestingly, ChatGPT use, in general, peaked around this time.\\\\(^3\\\\)\\n\\n\\\\(^3\\\\)[https://www.similarweb.com/blog/insights/ai-news/chatgpt-bard/](https://www.similarweb.com/blog/insights/ai-news/chatgpt-bard/)\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"decline in activity on Stack Overflow. We report the estimated effect of our difference-in-differences model in Table 1 and visualize the weekly estimates of the relative change in the Stack Overflow activity in Figure 2 . Table 1 indicates that ChatGPT decreased posting activity on Stack Overflow by 15.6% (1 \u2212 e^{\u22120.17}). These results are robust to changes in the controls and starting point of the data time series. We also tested for heterogeneity in subsets of the data: considering only questions (rather than counting both questions and answers) and posts on weekdays. In both subsets our estimates did not deviate significantly from the main result: we estimate a 12% relative decrease in questions and 14% relative decrease in posts on weekdays. \\n\\n|                | (1) Number of posts | (2) Number of questions | (3) Weekday posts |\\n|----------------|---------------------|-------------------------|-------------------|\\n| Stack Overflow \u00d7 Post-GPT | -0.170** (0.0607) | -0.112+ (0.0619) | -0.149* (0.0636) |\\n| Observations   | 1,150               | 1,150                   | 1,150             |\\n| R-squared      | 0.995               | 0.994                   | 0.993             |\\n| R-squared within | 0.290               | 0.315                   | 0.232             |\\n| Outcome mean   | 16363               | 7273                    | 13191             |\\n| Outcome std. dev. | 29088               | 12661                   | 23685             |\\n\\nTable 1: Results of a difference-in-differences model, estimating the change in activity observed weekly on Stack Overflow following the release of ChatGPT is increasing over time and is by the end of our study greater in magnitude than the average post-ChatGPT effect estimated in Table 1 . By the end of April 2023, the estimated effect stabilizes at around 25%. Interestingly, ChatGPT use, in general, peaked around this time.\u00b3\\n\\n\u00b3[https://www.similarweb.com/blog/insights/ai-news/chatgpt-bard/](https://www.similarweb.com/blog/insights/ai-news/chatgpt-bard/)\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A large topographic feature on the surface of the trans-Neptunian object (307261) 2002 MS\u2084 measured from stellar occultations\\n\\nF. L. Rommel\u00b9,\u00b2,\u00b3, F. Braga-Ribas\u00b3,\u00b9,\u00b2, J. L. Ortiz\u2074, B. Sicardy\u2075, P. Santos-Sanz\u2076, J. Desmars\u2076,\u2077, J. I. B. Camargo\u00b9,\u00b2, R. Vieira-Martins\u00b9,\u00b2, M. Assafin\u2078,\u00b2, B. E. Morgado\u2078,\u00b2,\u00b9, R. C. Bouleffre\u00b9,\u00b2, G. Benedetti-Rossi\u2079,\u00b2, A. R. Gomes-J\u00fanior\u00b9\u2070,\u2079,\u00b2, E. Fern\u00e1ndez-Valenzuela\u00b9\u00b9, B. J. Hollett\u00b9\u00b2, D. Souami\u00b9\u00b3,\u00b9\u2074, R. Duffard\u2074, G. Margot\u00b3,\u00b2, M. Vara-Lubiano\u2074, J. Lecacheux\u2075, J. L. Plouvier\u00b9\u2075, N. Morales\u2074, A. Maury\u00b9\u2076, J. Fabrega\u00b9\u2077, P. Ceravolo\u00b9\u2078, E. Jehin\u00b9\u2079, D. Albanese\u00b2\u2070, H. Mariey\u00b2\u00b9, S. Cikota\u00b2\u00b2,\u00b2\u00b3, D. Ru\u017edjak\u00b2\u2074, A. Cikota\u00b2\u2075, R. Szak\u00e1ts\u00b2\u2076,\u00b2\u2077, D. Baba Aissa\u00b2\u2078, Z. Gringhacene\u00b2\u2079, V. Kashuba\u00b2\u2079, N. Koshkin\u00b2\u2079, V. Zhukov\u00b3\u2070, S. Fi\u0161ek\u00b3\u00b9,\u00b3\u00b2, O. \u010cukr\u00b3\u00b3,\u00b3\u2074, S. \u00d6zer\u00b3\u2075,\u00b3\u2076, C. Schnabel\u00b3\u2077,\u00b3\u2078, M. Schnabel\u00b3\u2078, F. Signore\u00b3\u2079, L. Morrone\u2074\u2070,\u2074\u00b9, T. Santana-Ros\u2074\u00b2,\u2074\u00b3, C. L. Pereira\u00b9,\u00b2,\u00b3, M. Emilio\u2074\u2074,\u00b9,\u00b3, A. Y. Burdanov\u2074\u2075, J. de Wit\u2074\u2076, K. Barkaoui\u2074\u2077,\u2074\u2078, M. Gillon\u2074\u2079, G. Leto\u2075\u2070, A. Frasca\u2075\u00b9, G. Catanzaro\u2075\u00b2, R. Zannar Sanchez\u2075\u00b3, U. Tagliaferri\u2075\u2074, M. Di Sora\u2075\u2075, G. Isopi\u2075\u2076, Y. Krugly\u2075\u2077,\u2075\u2078, I. Slysusarev\u2075\u2079, V. Chiorn\u2075\u2070, H. Mikuz\u2075\u00b2,\u2075\u00b3, P. Bacci\u2075\u2074, M. Maestripieri\u2075\u2074, M. D. Grazia\u2075\u2074, I. de la Cueva\u2075\u2075, M. Yuste-Moreno\u2075\u2076, F. Ciabattari\u2075\u2076, O. M. Kozhukhov\u2075\u2077, M. Serra-Ricart\u2075\u2078,\u2075\u2079, M. R. Alarc\u00f3n\u2076\u2070,\u2076\u00b9, J. Licandro\u2076\u00b2,\u2076\u00b3, G. Masi\u2076\u2074, R. Bacci\u2076\u2075, J. M. Bosch\u2076\u2076, R. Behem\u2076\u2077, J.-P. Prost\u2076\u2078, S. Renner\u2076\u2079, M. Conjet\u2077\u2070, M. Bachin\u2077\u00b9, G.ucci\u2077\u00b2, L. Stoian\u2077\u00b3, A. Juravle\u2077\u2074, D. Carosati\u2077\u2075, G. Zheleznyak\u2077\u2076, N. Montigiani\u2077\u2077, C. R. Foster\u2077\u2078, M. Mannucci\u2077\u2079, N. Ruocco\u2078\u2070, F. Cuevas\u2078\u00b9, P. Di Marcantonio\u2078\u00b2, I. Coretti\u2078\u00b3, G. Irafrate\u2078\u2074, V. Baldini\u2078\u2075, M. Collins\u2078\u2076, A. P\u00e1l\u2078\u2077, B. Cs\u00e1k\u2078\u2078, E. Fern\u00e1ndez-Garc\u00eda\u2078\u2079, A. J. Castro-Tirado\u2079\u2070, L. Hudin\u2079\u00b9, J. M. Madiedo\u2079\u00b2, R. M. Anghel\u2079\u00b3, J. F. Calvo-Fern\u00e1ndez\u2079\u2074, A. Valvasori\u2079\u2075,\u2077\u2079, E. Guido\u2079\u2076, R. M. Gherase\u2079\u2077, S. Kamoun\u2079\u2078, R. Fafe\u2079\u2079, M. S\u00e1nchez-Gonz\u00e1lez\u2079\u2075,\u2078\u2075, L. Curelaru\u2079\u2076, C. D. Vintedevara\u2079\u2077, C. A. Danescu\u2079\u2078, J.-F. Gout\u2079\u2079, C. J. Schmitz\u2079\u2070, A. Sota\u2079\u00b9, I. Belskaya\u2079\u00b2,\u2079\u00b3, M. Rodr\u00edguez-Marco\u2079\u2074, K. Kilic\u2079\u2075,\u2079\u2076, E. Frappa\u2079\u2077, A. Klotz\u2079\u2078, M. Lavayssi\u00e8re\u2079\u2079, J. Marques Oliveira\u2079\u2075, M. Popescu\u2079\u2078, L. A. Mammana\u2079\u2079, E. Fern\u00e1ndez-Lajus\u2079\u2078,\u2079\u2079, M. Schmid\u2079\u2079, U. Hopp\u2079\u2079, R. Kom\u017eik\u00b9\u2070\u2070, T. Pribilla\u00b9\u2070\u00b9, D. Tomko\u00b9\u2070\u00b2, L. Hus\u00e1rik\u00b9\u2070\u00b3, O. Erec\u2079\u00b3,\u2079\u2074, S. Eryilmaz\u2079\u00b3, L. Buzzi\u00b9\u2070\u00b2, B. G\u00e4hrken\u00b9\u2070\u2075, D. Nardiello\u00b9\u2070\u2074,\u00b9\u2070\u2075, K. Hornoch\u00b9\u2070\u2076, E. Sonbas\u00b9\u2070\u2077,\u00b9\u2070\u2078, H. Ez\u00b9\u2070\u2079, V. Burwitz\u00b9\u00b9\u2070, P. Waldemar Sybilski\u00b9\u00b9\u00b9, W. Bykowski\u00b9\u00b9\u00b2, T. G. M\u00fcller\u00b9\u00b9\u00b3, W. Ogloza\u00b9\u00b9\u2074, R. Go\u00f1alves\u00b9\u00b9\u2075, J. F. Ferreira\u00b9\u00b9\u2076, M. Ferreira\u00b9\u00b9\u2077, M. Bento\u00b9\u00b9\u2078, S. Meister\u00b9\u00b9\u2079,\u00b9\u00b2\u2070, M. N. Bagrian\u00b9\u00b2\u00b9, M. Teke\u0161\u00b9\u00b2\u00b2,\u00b9\u00b2\u00b3, A. Marciak\u00b9\u00b2\u2074, M. Zoravec\u00b9\u00b2\u2075, P. Delin\u010d\u00e1k\u00b9\u00b2\u2076, G. Giannini\u00b9\u00b2\u2077, G. B. Casalnuovo\u00b9\u00b2\u2078, M. Boute\u2070\u00b9\u00b2\u2079, J. Sanchez\u00b9\u00b3\u2070, B. Klement\u00b9\u00b3\u00b9, N. W\u00fcnsche\u00b9\u00b3\u00b2, W. Burzynski\u00b9\u00b3\u00b3, M. Borkowski\u00b9\u00b3\u2074, M. Serrau\u00b9\u00b3\u00b9, G. Dangl\u00b9\u00b3\u00b2, O. Kl\u00f6ss\u00b9\u00b3\u00b3, C. Weber\u00b9\u00b3\u2074, M. Urbanek\u00b9\u00b3\u2075, J. Kub\u00e1nek\u00b9\u00b3\u2076, J. Kubi\u0161\u00b9\u00b3\u2077, J. Kubi\u0161\u00b9\u00b3\u2078, J. Spagnotto\u00b9\u00b3\u2079, A. A. Sickafoose\u00b9\u2074\u2070, R. Hueso\u00b9\u2074\u00b9, A. S\u00e1nchez-Lavega\u00b9\u2074\u00b2, R. S. Fisher\u00b9\u2074\u00b3, A. W. Rengstorff\u00b9\u2074\u2074, C. Perello\u00b9\u00b3\u2078,\u00b3\u2077, M. Dascalu\u00b9\u2074\u2074, M. Altan\u00b9\u2074\u2075, K. Gazeas\u00b9\u2074\u2076, T. de Santana\u00b9\u2075,\u00b9\u2074\u2077, R. Sfar\u00b9\u2074\u2078, O. C. Winter\u00b9\u2074\u2079, S. Kalkan\u00b9\u2075\u2070, O. Canales-Moreno\u00b9\u2075\u00b9, J. M. Trigo-Rodr\u00edguez\u00b9\u2075\u00b2, V.\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A large topographic feature on the surface of the trans-Neptunian object (307261) 2002 MS\u2084 measured from stellar occultations\\n\\nF. L. Rommel 3, 1, 2, 3, F. Braga-Ribas , 2, 3, 1, 2, J. L. Ortiz 4, B. Sicardy 5, P. Santos-Sanz 4, J. Desmars 6 , 7, J. I. B. Camargo 2 , R. Vieira-Martins 2, 1, M. Assafin 8 , 2, B. E. Morgado , 1, 2, 8, R. C. Boufleur 1, 2, G. Benedetti-Rossi 9, 5, 2.\\n\\nA. R. Gomes-J\u00fanior , 2, 10, 9, 2, E. Fern\u00e1ndez-Valenzuela 11, B. J. Holler 12, D. Souami 3, 13, 14, R. Duffard 4, G. Margoti 3, 2, M. Vara-Lubiano 3, J. Lecacheux 5, J. L. Plouvier 15, N. Morales 4, A. Maury 16, J. Fabrega 17, P. Ceravolo 18, E. Jehin 19, D. Albanese 20, H. Mariey 21, S. Cikota 22, 23, D. Ru\u017edjak 24, A. Cikota 25, R. Szak\u00e1ts 26, 27, D. Baba Aissa 28, Z. Gringahcene 29, V. Kashuba 29, N. Koshkin 29, V. Zhukov 30, S. Fi\u00b8sek 31, 32, O. \u010cakir 33, 34, S. \u00d6zer 35, 36, C. Schnabel 37, 38, M. Schnabel 38, F. Signore 39, L. Morrone 40, 41, T. Santana-Ros , O. \u00c7ak\u0131r 33, 34, S. \u00d6zer 35, 36, C. Schnabel 37, 38, M. Schnabel 38, F. Signore 39, L. Morrone 40, 41, T. Santana-Ros, 42, 43, C. L. Pereira 1, 2, 3, M. Emilio 44, 1, 3, A. Y. Burdanov 45, J. de Wit 45, K. Barkaoui 46, 47, 45, M. Gillon 46, G. Leto 48, A. Frasca 48, G. Catanzaro 48, R. Zanmar Sanchez 48, U. Tagliaferri 49, M. Di Sora 49, G. Isopi 49, Y. Krugly 50, 51, I. Slysusarev 50, V. Chiorn 50, H. Mikuz 52, 53, P. Bacci 54, M. Maestrip , H. Miku\u017e 52, 53, P. Bacci 54, M. Maestripieri 54, M. D. Grazia 54, I. de la Cueva 55, M. Yuste-Moreno 55, F. Ciabattari 56, O. M. Kozhukhov 57, M. Serra-Ricart 47, 58, M. R. Alarcon 47, 58, J. Licandro 57, 58, G. Masi 59, R. Bacci 60, J. M. Bosch 61, R. Behem 62, J.-P. Prost 62, S. Renner 7, 63, M. Conjet 21, M. Bachin 64, G.ucci 64, L. Stoian 65, A. Juravel 65, D. Carosati 66, B. Gowe 67, J. Carrillo 68, A. P. Zheleznyak 50, N. Montigiani 69, C. R. Foster 70, M. Mannucci 69, N. Ruocco 71, F. Cuevas 72, P. Di Marcantonio 73, I. Coretti 73, G. Iafrate 73, V. Baldini 73, M. Collins 74, A. P\u00e1l 76, B. Cs\u00e1k 76, E. Fern\u00e1ndez-Garcia 4, A. J. Castro-Tirado 76, L. Hudin 75, J. M. Madiedo 76, R. M. Anghel 76, J. F. Calvo-Fern\u00e1ndez 77, A. Valvasori 78, 79, E. Guido 80, R. M. Gherase 81, S. Kamoun 82, 83, R. Fafe 82, M. S\u00e1nchez-Gonz\u00e1lez 84, 85, L. Curelaru 86, C. D. Vintedev 87, C. A. Danescu 88, J.-F. Gout 89, C. J. Schmitz 90, A. Sota 91, I. Belskaya 92, 93, M. Rodr\u00edguez-Marco 94, K. Kilic 94, E. Frappa 94, A. Klotz 95, M. Lavayssi\u00e8re 96, J. Marques Oliveira 95, M. Popescu 98, L. A. Mammana 99, 97, 98, E. Fern\u00e1ndez-Lajus 99, 98, 99, M. Schmid 100, U. Hopp 100, R. Kom\u017eik 101, T. Pribilla 101, D. Tomko 101, M. Hus\u00e1rik 101, O. Erec 93, 92, S. Eryilmaz 93, L. Buzzi 102, B. G\u00e4hrken 103, D. Nardiello 104, 105, K. Hornoch 106, E. Sonbas 107, 108, H. Ert 109, V. Burwitz 110, P. Waldemar Sybilski 111, W. Bykowski 111, T. G. M\u00fcller 110, W. Ogloza 112, R. Go\u00f1alves 113, J. F. Ferreira 114, M. Ferreira 115, M. Bento 115, S. Meister 116, 117, M. N. Bagrian 118, M. Teke\u015f 119, 120, A. Marciak 121, M. Zoravec 122, P. Delin\u010d\u00e1k 122, G. Giann 123, G. B. Casalnuovo 124, M. Boute 125, J. Sanchez 126, B. Klement 127, N. W\u00fcnsche 127, W. Burzynski 128, 129, 37, M. Borkowski 130, M. Serrau 131, G. Dangl 37, O. Kl\u00f6 37, C. Weber 37, M. Urbanik 132, L. Rousselot 131, J. Kub\u00e1nek 137, 133, 134, P. Andre 135, C. Colazo 136, 137, 138, J. Spagnotto 139, A. A. Sickafoose 140, R. Hueso 141, A. S\u00e1nchez-Lavega 141, R. S. Fisher 1 "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Proposition 5.1 characterizes the slow dynamics of an $N$-spike quasi-equilibrium solution on the long $O\\\\left(\\\\epsilon^{-3}\\\\right)$ time-scale. We remark that this time-scale is longer than the $O\\\\left(\\\\epsilon^{-2}\\\\right)$ time-scale of slow spike dynamics for the GM and Gray-Scott models ([23], [8], [12]), where there are no chemotactic effects.\\n\\nIn Appendix H, we show that $\\\\beta_j$, as given in (5.17), can be calculated asymptotically by retaining only the contribution from the sub-inner solution. In particular, in Appendix H we provide the leading order estimate\\n\\n$$\\\\beta_j \\\\sim \\\\frac{2}{v_{\\\\text{max},j}}, \\\\quad \\\\text{for } v_{\\\\text{max},j} \\\\gg 1.$$  \\\\hspace{1cm} (5.26)\\n\\nMoreover, in Appendix H we show at the steady-state spike locations that $\\\\beta_j = \\\\beta_0 \\\\forall j$, with $\\\\beta_0$ given in (4.28).\\n\\nTo illustrate our results, we now compare the dynamics computed from the DAE system (5.24) with corresponding numerical results computed from the full PDE system (1.2) using FLEXPDE7 [14]. In our comparison, we computed the integrals defining $\\\\beta_j$ numerically from (5.17). The results for a one- and two-spike dynamics are shown in Figure 8 for the parameter values in the figure caption. In Figure 8a, where we chose the initial condition $x_1(0) = -0.1$, the asymptotic and numerical spike trajectories are favorably compared for a one-spike quasi-equilibrium pattern. In Figure 8b a similar favorable comparison is shown for the case of two-spike dynamics starting from the initial condition $x_1(0) = -0.6$ and $x_2(0) = 0.6$.\\n\\n![Figure 8:](image)\\n\\n**5.1 Computation of Jacobian Matrix for Balancing Conditions**\\n\\nIn this subsection, and as remarked in \u00a74, we show that when $d_1 \\\\in T_e$ the matrix $M$ in (4.29) arises from the linearization of the DAE dynamics (5.24) in Proposition 5.1 about the steady-state spike locations. Our approach below is inspired by a related analysis for the GM model in [58].\\n\\nTo this end, we use the Green\u2019s function in (2.24) together with its decomposition in (5.20) to define\\n\\n$$\\\\partial_{x_j} G(x_j; x_k) := \\\\begin{cases} \\\\frac{\\\\partial R}{\\\\partial x}(x; x_j)|_{x=x_j}, & j = k, \\\\\\\\ \\\\frac{\\\\partial G}{\\\\partial x}(x; x_k)|_{x=x_j}, & j \\\\neq k, \\\\end{cases} \\\\quad \\\\partial_{x_j} \\\\partial_{x_k} G(x_j; x_k) = \\\\begin{cases} \\\\frac{\\\\partial}{\\\\partial x}|_{x=x_j} \\\\frac{\\\\partial}{\\\\partial y}|_{y=x_k} R(x; y), & j = k, \\\\\\\\ \\\\partial_{x_j} \\\\partial_{x_k} G(x_j; x_k), & j \\\\neq k. \\\\end{cases}$$  \\\\hspace{1cm} (5.27)\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Proposition 5.1 characterizes the slow dynamics of an $N$-spike quasi-equilibrium solution on the long $O\\\\left(\\\\epsilon^{-3}\\\\right)$ time-scale. We remark that this time-scale is longer than the $O\\\\left(\\\\epsilon^{-2}\\\\right)$ time-scale of slow spike dynamics for the GM and Gray-Scott models ([ 23 ], [ 8 ], [ 12 ]), where there are no chemotactic e ff ects. \\n\\nIn Appendix H, we show that $\\\\beta_j$, as given in ( 5.17 ), can be calculated asymptotically by retaining only the contribution from the sub-inner solution. In particular, in Appendix we provide the leading order estimate\\n\\n$$\\\\beta_j \\\\sim \\\\frac{2}{v_{\\\\text{max},j}}, \\\\quad \\\\text{for } v_{\\\\text{max},j} \\\\gg 1.$$  \\n\\nMoreover, in Appendix H we show at the steady-state spike locations that $\\\\beta_j = \\\\beta_0 \\\\forall j$, with $\\\\beta_0$ given in ( 4.28 ).\\n\\nTo illustrate our results, we now compare the dynamics computed from the DAE system ( 5.24 ) with corresponding numerical results computed from the full PDE system ( 1.2 ) using FLEXPDE7 [ 14 ]. In our comparison, we computed the integrals defining $\\\\beta_j$ numerically from ( 5.17 ). The results for a oneand two-spike dynamics are shown in Figure 8 for the parameter values in the figure caption. In Figure 8a , where we chose the initial condition $x_1(0) = -0.1$, the asymptotic and numerical spike trajectories are favorably compared for a one-spike quasi-equilibrium pattern. In Figure 8b a similar favorable comparison is shown for the case of two-spike dynamics starting from the initial condition $x_1(0) = -0.6$ and $x_2(0) = 0.6$.\\n\\n![Figure 8:](image)\\n\\n5.1 Computation of Jacobian Matrix for Balancing Conditions\\n\\nIn this subsection, and as remarked in \u00a7 4 , we show that when $d_1 \\\\in T_e$ the matrix $M$ in ( 4.29 ) arises from the linearization of the DAE dynamics ( 5.24 ) in Proposition 5.1 about the steady-state spike locations. Our approach below is inspired by a related analysis for the GM model in [ 58 ].\\n\\nTo this end, we use the Green\u2019s function in ( 2.24 ) together with its decomposition in ( 5.20 ) to define\\n\\n$$\\\\partial_{x_j} G(x_j; x_k) := \\\\begin{cases} \\\\frac{\\\\partial R}{\\\\partial x}(x; x_j)|_{x=x_j}, & j = k, \\\\\\\\ \\\\frac{\\\\partial G}{\\\\partial x}(x; x_k)|_{x=x_j}, & j \\\\neq k, \\\\end{cases} \\\\quad \\\\partial_{x_j} \\\\partial_{x_k} G(x_j; x_k) = \\\\begin{cases} \\\\frac{\\\\partial}{\\\\partial x}|_{x=x_j} \\\\frac{\\\\partial}{\\\\partial y}|_{y=x_k} R(x; y), & j = k, \\\\\\\\ \\\\partial_{x_j} \\\\partial_{x_k} G(x_j; x_k), & j \\\\neq k. \\\\end{cases}$$  \\n\\n( 5.27 )\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where $S_n(u) = e^{-B_S \\\\psi_{s_n}(u)}$. To get the normalization constant we first consider the asymptotic expansion of the normalizable solution, $S_n(u)$, close to the boundary, $S_n(u) = N_{s_n} u^3 + \\\\cdots$, where $N_{s_n}$ is the normalization constant, which is obtained by plugging $S_n(u)$ in (4.3). Then decay constants of the scalar mesons are given by the following dictionary [40]\\n\\n$$F_{s_n} = \\\\zeta u e^{3A_s - \\\\Phi} \\\\partial_u S_n \\\\bigg|_{u=\\\\epsilon} = \\\\frac{3\\\\sqrt{N_c}}{2\\\\pi} N_{s_n}. \\\\quad (4.4)$$\\n\\nWe first investigate the behavior of the scalar meson decay constants as a function of the parameter $a_0$ in the chiral limit with the other parameters fixed as in model A. Our numerical results are displayed on the left panel of Fig. 14. As can be seen from the plot, the results show a smooth behavior of the decay constants in the region of interest, i.e., $a_0 < a_0^0$, where $a_0^0 \\\\approx 0.0974$. However, the behavior of the decay constant increases close to $a_0^0$ for the ground state while it decreases for the scalar resonances.\\n\\n![Figure 14](image)\\n\\n**Figure 14.** Left panel: The decay constants of the scalar mesons as a function of $a_0$ in the chiral limit. Right panel: The decay constants of the scalar mesons as a function of the quark mass for $\\\\lambda = 27$, $b_0 = 1.7$ and $a_0 = 0.02$.\\n\\nIn addition, we calculate the scalar meson decay constants as a function of the quark mass. Our numerical results are displayed on the right panel of Fig. 14. As can be seen from the plot, the decay constant increases in the region of small quark mass until it reaches some maximum value and then decreases when the quark mass grows. Finally, we calculate the decay constant using the final set of parameters for models A and B displayed in Table 1.\\n\\n| Models A and B | SW [13] | Experimental [78] |\\n|----------------|---------|-------------------|\\n| $F_{V_0}^{1/2}$ | 276     | 346.2 \u00b1 1.4       |\\n| $F_{V_1}^{1/2}$ | 341     | 433 \u00b1 13          |\\n| $F_{V_2}^{1/2}$ | 384     |                   |\\n\\nTable 6. The decay constants of the vector mesons (in MeV) obtained in our work (same result for models A and B), compared against the result obtained using the soft wall model (SW) [13]. In order to compare with experimental results of [78], we need to identify $F_V$ with $g_\\\\rho$. \\n\\n\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where $S_n(u) = e^{-B_S \\\\psi_{s_n}(u)}$. To get the normalization constant we first consider the asymptotic expansion of the normalizable solution, $S_n(u)$, close to the boundary, $S_n(u) = N_{s_n} u^3 + \\\\cdots$, where $N_{s_n}$ is the normalization constant, which is obtained by plugging $S_n(u)$ in (4.3). Then decay constants of the scalar mesons are given by the following dictionary [ 40 ]\\n\\n$$F_{s_n} = \\\\zeta u e^{3A_s - \\\\Phi} \\\\partial_u S_n \\\\bigg|_{u=\\\\epsilon} = \\\\frac{3\\\\sqrt{N_c}}{2\\\\pi} N_{s_n}. \\\\quad (4.4)$$\\n\\nWe first investigate the behavior of the scalar meson decay constants as a function of the parameter $a_0$ in the chiral limit with the other parameters fixed as in model A. Our numerical results are displayed on the left panel of Fig. 14 . As can be seen from the plot, the results show a smooth behavior of the decay constants in the region of interest, i.e., $a_0 < a_0^0$, where $a_0^0 \\\\approx 0.0974$. However, the behavior of the decay constant increases close to $a_0^0$ for the ground state while it decreases for the scalar resonances. \\n\\n![Figure 14 . As can be seen from the plot, the decay constant increases in the region of small quark mass until it reaches some maximum value and then decreases when the quark mass grows. Finally, we calculate the decay constant using the final set of parameters for models A and B displayed in Table 1 .](image)\\n\\n| Models A and B | SW [13] | Experimental [78] |\\n|----------------|---------|-------------------|\\n| $F_{V_0}^{1/2}$ | 276     | 346.2 \u00b1 1.4       |\\n| $F_{V_1}^{1/2}$ | 341     | 433 \u00b1 13          |\\n| $F_{V_2}^{1/2}$ | 384     |                   |\\n\\nTable 6. The decay constants of the vector mesons (in MeV) obtained in our work (same result for models A and B), compared against the result obtained using the soft wall model (SW) [13]. In order to compare with experimental results of [78], we need to identify $F_V$ with $g_\\\\rho$. \\n\\nIn addition, we calculate the scalar meson decay constants as a function of the quark mass. Our numerical results are displayed on the right panel of Fig. 14. As can be seen from the plot, the decay constant increases in the region of small quark mass until it reaches some maximum value and then decreases when the quark mass grows. Finally, we calculate the decay constant using the final set of parameters for models A and B displayed in Table 1.\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We construct a special ridge stream which utilizes this unique features, our method can avoid overfitting to certain frequency typical CNNs can learn certain frequency information from the features extracted from the ridges and generation artifacts in the frequency spectrum. To simultaneously utilize the simple yet effective convolutional neural network to learn more analyze the generation artifacts in these spectrum, and construct a input fingerprint image into different frequency domains, an- by [13], which discovers the obvious generation artifacts in the gerprint characteristic. In the generation artifact stream, inspired exploiting the unique features of the fingerprint images and gen- eration artifacts in frequency domain. Considering the impact of sweat on grayscale variations along the ridges of fingerprints [10], we construct a special ridge stream which utilizes this unique finger- print characteristic. In the generation artifact stream, inspired by [13], which discovers the obvious generation artifacts in the Discrete Cosine Transform (DCT) frequency domain, we transform the input fingerprint image into different frequency domains, an- alyze the generation artifacts in these spectrum, and construct a simple yet effective convolutional neural network to learn more robust generation artifacts between the generated and real images from the FFT frequency spectrum. To simultaneously utilize the features extracted from the ridges and generation artifacts in the final prediction, we build a simple yet effective fusion module. Since typical CNNs can learn certain frequency information from the input image or its frequency transformed spectrum, by utilizing the unique 1D ridge features jointly with the 2D generation artifact features, our method can avoid overfitting to certain frequency information, which can improve the robustness of our method.\\n\\nOur main contributions are summarized as follows:\\n\\n- We propose the first deep forgery detection method for GAN-generated fingerprint images, by jointly exploiting the unique 1D ridge features and 2D generation artifact features via a lightweight two-stream neural network to ensure the robustness and efficiency of the proposed work.\\n- We propose to exploit fingerprint-related characteristic and construct a ridge stream, which exploits the grayscale variations along the ridges. With this ridge stream, our method can avoid overfitting to certain frequency information, which can be easily interfered by existing anti-forensic method [11] and thus improves the overall robustness.\\n\\n- We analyze the frequency spectrum of the real and generated fingerprint images and construct a simple yet effective generation artifact stream, i.e., a shallow convolutional neural network, to extract frequency-domain inconsistencies.\\n- Comprehensive experiments demonstrate that our method is effective, efficient, and robust to the anti-forensic method [11].\\n\\n2 RELATED WORK\\n\\n2.1 Fingerprint Image Synthesis\\n\\nSeveral recent methods have been proposed for generating realistic fingerprint images automatically [3, 4, 12, 17, 35, 39, 44]. [3] and [35] combine a convolution autoencoder (CAE) and a GAN-based method (e.g., DCGAN [36], WGAN [18]) directly. [4] presents a GAN-based pipeline followed by a stochastic search algorithm over the latent variable space, to search for suitable latent variable and generate DeepMasterPrints which are synthetic fingerprints and can be matched against a large number of fingerprints. These techniques can be considered as single-staged architectures, whose input is a random vector and output is a generated image.\\n\\nTo make the synthetic results more realistic and obtain multiple impressions for a virtual identity, many multi-staged methods [12, 17, 39, 44] firstly synthesize a binary masterprint which defines a ridge structure and represents a new identity. After non-linear distortion and cropping, which simulate various pressures and different contact regions between the finger and the fingerprint sensor, the distorted masterprint is fed into another generative neural network to generate a realistic fingerprint image. The differences between these multi-staged methods lie in the way of the masterprint generation scheme, the distortion simulation method, and the final impression (fingerprint) generation approach. [12] utilizes the GAN-based methods (e.g., BigGAN [5]) in these three steps. [17] attempts to employ conventional rotation instead of the deep learning method in the distortion module. [39] adopts StyleGAN2 [22] to synthesize the fingerprint skeletons and CycleGAN [46] to generate photorealistic fingerprint images. [44] also proposes a CycleGAN-based L3 synthetic fingerprint generation (L3-SF) approach, which firstly uses traditional methods (e.g., Anguli [1], Gabor filter) to generate masterprints with pores and scratch. Besides, [44] releases a synthetic fingerprint database.\\n\\n2.2 Deep Forgery Detection\\n\\nResearchers have proposed a series of deep forgery detection approaches to detect the generated images [2, 7, 13, 23, 26\u201328, 30, 31, 33, 43]. [31] reveals that the texture statistics is an important detection clue and global texture features can improve the robustness of face forgery detection. [13] observes that GAN-generated images contain distinguishable artifacts in the frequency domain due to the upsampling operations in the generator of GAN architectures, and these artifacts can be utilized for forgery detection. [28] attempts to explore the discrepancies in Learned Noise Patterns (LNP) to detect forgery. [30] concatenates spatial images and phase spectrums to emphasize the artifacts and proposes a Spatial-Phase Shallow Learning method. [26] captures frequency-aware features and merge them with the extracted spatial domain\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We construct a special ridge stream which utilizes this unique features, our method can avoid overfitting to certain frequency typical CNNs can learn certain frequency information from the features extracted from the ridges and generation artifacts in the frequency spectrum. To simultaneously utilize the simple yet effective convolutional neural network to learn more analyze the generation artifacts in these spectrum, and construct a input fingerprint image into different frequency domains, an- by [13], which discovers the obvious generation artifacts in the gerprint characteristic. In the generation artifact stream, inspired exploiting the unique features of the fingerprint images and gen- eration artifacts in frequency domain. Considering the impact of sweat on grayscale variations along the ridges of fingerprints [10], we construct a special ridge stream which utilizes this unique finger- print characteristic. In the generation artifact stream, inspired by [13], which discovers the obvious generation artifacts in the Discrete Cosine Transform (DCT) frequency domain, we transform the input fingerprint image into different frequency domains, an- alyze the generation artifacts in these spectrum, and construct a simple yet effective convolutional neural network to learn more robust generation artifacts between the generated and real images from the FFT frequency spectrum. To simultaneously utilize the features extracted from the ridges and generation artifacts in the final prediction, we build a simple yet effective fusion module. Since typical CNNs can learn certain frequency information from the input image or its frequency transformed spectrum, by utilizing the unique 1D ridge features jointly with the 2D generation artifact features, our method can avoid overfitting to certain frequency information, which can improve the robustness of our method.\\n\\nOur main contributions are summarized as follows:\\n\\n- We propose the first deep forgery detection method for GAN-generated fingerprint images, by jointly exploiting the unique 1D ridge features and 2D generation artifact features via a lightweight two-stream neural network to ensure the robustness and efficiency of the proposed work.\\n- We propose to exploit fingerprint-related characteristic and construct a ridge stream, which exploits the grayscale variations along the ridges. With this ridge stream, our method can avoid overfitting to certain frequency information, which can be easily interfered by existing anti-forensic method [11] and thus improves the overall robustness.\\n\\n- We analyze the frequency spectrum of the real and generated fingerprint images and construct a simple yet effective generation artifact stream, i.e., a shallow convolutional neural network, to extract frequency-domain inconsistencies.\\n- Comprehensive experiments demonstrate that our method is effective, efficient, and robust to the anti-forensic method [11].\\n\\n2 RELATED WORK\\n\\n2.1 Fingerprint Image Synthesis\\n\\nSeveral recent methods have been proposed for generating realistic fingerprint images automatically [3, 4, 12, 17, 35, 39, 44]. [3] and [35] combine a convolution autoencoder (CAE) and a GAN-based method (e.g., DCGAN [36], WGAN [18]) directly. [4] presents a GAN-based pipeline followed by a stochastic search algorithm over the latent variable space, to search for suitable latent variable and generate DeepMasterPrints which are synthetic fingerprints and can be matched against a large number of fingerprints. These techniques can be considered as single-staged architectures, whose input is a random vector and output is a generated image.\\n\\nTo make the synthetic results more realistic and obtain multiple impressions for a virtual identity, many multi-staged methods [12, 17, 39, 44] firstly synthesize a binary masterprint which defines a ridge structure and represents a new identity. After non-linear distortion and cropping, which simulate various pressures and different contact regions between the finger and the fingerprint sensor, the distorted masterprint is fed into another generative neural network to generate a realistic fingerprint image. The differences between these multi-staged methods lie in the way of the masterprint generation scheme, the distortion simulation method, and the final impression (fingerprint) generation approach. [12] utilizes the GAN-based methods (e.g., BigGAN [5]) in these three steps. [17] attempts to employ conventional rotation instead of the deep learning method in the distortion module. [39] adopts StyleGAN2 [22] to synthesize the fingerprint skeletons and CycleGAN [46] to generate photorealistic fingerprint images. [44] also proposes a CycleGAN-based L3 synthetic fingerprint generation (L3-SF) approach, which firstly uses traditional methods (e.g., Anguli [1], Gabor filter) to generate masterprints with pores and scratch. Besides, [44] releases a synthetic fingerprint database.\\n\\n2.2 Deep Forgery Detection\\n\\nResearchers have proposed a series of deep forgery detection approaches to detect the generated images [2, 7, 13, 23, 26\u201328, 30, 31, 33, 43]. [31] reveals that the texture statistics is an important detection clue and global texture features can improve the robustness of face forgery detection. [13] observes that GAN-generated images contain distinguishable artifacts in the frequency domain due to the upsampling operations in the generator of GAN architectures, and these artifacts can be utilized for forgery detection. [28] attempts to explore the discrepancies in Learned Noise Patterns (LNP) to detect forgery. [30] concatenates spatial images and phase spectrums to emphasize the artifacts and proposes a Spatial-Phase Shallow Learning method. [26] captures frequency-aware features and merge them with the extracted spatial domain\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Remark 2.1.\\n\\n(1) It should be noted that for any potential $u$, the eigenvalues $(\\\\nu_n)$ of $L_u$ cannot be all simple. For instance, take $u(x) = e^{ix}$, one can easily check that for $L_u = D - T_u T_u^*$,\\n\\n$$L_u 1 = L_u e^{ix} = 0.$$ \\n\\n(2) Inequality (2.7) implies that as $n \\\\gg 0$, the lower bound of the distance between two consecutive eigenvalues $\\\\nu_n$ gets closer to 1.\\n\\nProof. All the presented inequalities are a direct consequence of the max\u2013min principle\\n\\n$$\\\\lambda_n = \\\\max_{F \\\\subseteq L^2_+} \\\\min_{\\\\dim F \\\\leq n} \\\\left\\\\{ \\\\langle \\\\tilde{L}_u h \\\\mid h \\\\rangle ; h \\\\in F^\\\\perp \\\\cap H^\\\\frac{1}{2}_+ (\\\\mathbb{T}) , \\\\|h\\\\|_{L^2} = 1 \\\\right\\\\}.$$ \\n\\n$$\\\\nu_n = \\\\max_{F \\\\subseteq L^2_+} \\\\min_{\\\\dim F \\\\leq n} \\\\left\\\\{ \\\\langle L_u h \\\\mid h \\\\rangle ; h \\\\in F^\\\\perp \\\\cap H^\\\\frac{1}{2}_+ (\\\\mathbb{T}) , \\\\|h\\\\|_{L^2} = 1 \\\\right\\\\}.$$ \\n\\nSpectrum of $\\\\tilde{L}_u$. Let $F$ be any subspace of $L^2_+ (\\\\mathbb{T})$ of dimension $n$, and consider $E := \\\\mathbb{C}1 \\\\oplus S(F)$, where $S$ is the shift operator, then\\n\\n$$\\\\lambda_{n+1} \\\\geq \\\\min \\\\{ \\\\langle \\\\tilde{L}_u h \\\\mid h \\\\rangle ; \\\\|h\\\\|_{L^2} = 1 , h \\\\in E^\\\\perp \\\\cap H^\\\\frac{1}{2}_+ \\\\}.$$ \\n\\nObserve that $E^\\\\perp = S \\\\left( F^\\\\perp \\\\right)$, thus by (2.2),\\n\\n$$\\\\lambda_{n+1} \\\\geq \\\\min \\\\left\\\\{ \\\\langle \\\\tilde{L}_u g \\\\mid g \\\\rangle + 1 + |\\\\langle Sg \\\\mid u \\\\rangle|^2 ; \\\\|g\\\\|_{L^2} = 1 , g \\\\in F^\\\\perp \\\\cap H^\\\\frac{1}{2}_+ \\\\right\\\\}.$$ \\n\\nIn addition, since $|\\\\langle Sg \\\\mid u \\\\rangle|^2 \\\\geq 0$, we infer for all $n \\\\in \\\\mathbb{N}_{\\\\geq 0}$,\\n\\n$$\\\\lambda_{n+1} \\\\geq \\\\lambda_n + 1.$$ \\n\\nSpectrum of $L_u$\u2013Inequality (2.6). Let $F$ be any subspace of $L^2_+ (\\\\mathbb{T})$ of dimension $n$, and take $G := \\\\mathbb{C}1 \\\\oplus S(F) + \\\\mathbb{C}u$. Then,\\n\\n$$\\\\nu_{n+2} (u) \\\\geq \\\\min \\\\{ \\\\langle L_u h \\\\mid h \\\\rangle ; \\\\|h\\\\|_{L^2} = 1 , h \\\\in G^\\\\perp \\\\cap H^\\\\frac{1}{2}_+ \\\\}.$$ \\n\\nSince $G^\\\\perp = S \\\\left( F^\\\\perp \\\\cap (S^* u)^\\\\perp \\\\right)$, then\\n\\n$$\\\\nu_{n+2} \\\\geq \\\\min \\\\left\\\\{ \\\\langle L_u Sg \\\\mid Sg \\\\rangle ; \\\\|g\\\\|_{L^2} = 1 , g \\\\in F^\\\\perp \\\\cap (S^* u)^\\\\perp \\\\cap H^\\\\frac{1}{2}_+ (\\\\mathbb{T}) \\\\right\\\\}.$$ \\n\\nNote that $g \\\\perp S^* u$, then by (2.2),\\n\\n$$\\\\nu_{n+2} \\\\geq \\\\min \\\\left\\\\{ \\\\langle L_u g \\\\mid g \\\\rangle + 1 ; \\\\|g\\\\|_{L^2} = 1 , g \\\\in F^\\\\perp \\\\cap (S^* u)^\\\\perp \\\\cap H^\\\\frac{1}{2}_+ (\\\\mathbb{T}) \\\\right\\\\},$$\\n\\nleading to\\n\\n$$\\\\nu_{n+2} \\\\geq \\\\nu_n + 1.$$\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Remark 2.1.\\n\\n(1) It should be noted that RANA BADREDDINE for any potential $u$, the eigenvalues $(\\\\nu_n)$ of $L_u$ cannot be all simple. For instance, take $u(x) = e^{ix}$, one can easily check that for $L_u = D - T_u T_u^*$, \\n\\n$$L_u 1 = L_u e^{ix} = 0.$$ \\n\\n(2) Inequality ( 2.7 ) implies that as $n \\\\gg 0$, the lower bound of the distance between two consecutive eigenvalues $\\\\nu_n$ gets closer to 1.\\n\\nProof. All the presented inequalities are a direct consequence of the max\u2013min principle\\n\\n$$\\\\lambda_n = \\\\max_{F \\\\subseteq L^2_+} \\\\min_{\\\\dim F \\\\leq n} \\\\left\\\\{ \\\\langle \\\\tilde{L}_u h \\\\mid h \\\\rangle ; h \\\\in F^\\\\perp \\\\cap H^\\\\frac{1}{2}_+ (\\\\mathbb{T}) , \\\\|h\\\\|_{L^2} = 1 \\\\right\\\\} .$$\\n\\n$$\\\\nu_n = \\\\max_{F \\\\subseteq L^2_+} \\\\min_{\\\\dim F \\\\leq n} \\\\left\\\\{ \\\\langle L_u h \\\\mid h \\\\rangle ; h \\\\in F^\\\\perp \\\\cap H^\\\\frac{1}{2}_+ (\\\\mathbb{T}) , \\\\|h\\\\|_{L^2} = 1 \\\\right\\\\} .$$\\n\\nSpectrum of $\\\\tilde{L}_u$. Let $F$ be any subspace of $L^2_+ (\\\\mathbb{T})$ of dimension $n$, and consider $E := \\\\mathbb{C}1 \\\\oplus S(F)$, where $S$ is the shift operator, then\\n\\n$$\\\\lambda_{n+1} \\\\geq \\\\min \\\\{ \\\\langle \\\\tilde{L}_u h \\\\mid h \\\\rangle ; \\\\|h\\\\|_{L^2} = 1 , h \\\\in E^\\\\perp \\\\cap H^\\\\frac{1}{2}_+ \\\\} .$$\\n\\nObserve that $E^\\\\perp = S \\\\left( F^\\\\perp \\\\right)$, thus by ( 2.2 ) ,\\n\\n$$\\\\lambda_{n+1} \\\\geq \\\\min \\\\left\\\\{ \\\\langle \\\\tilde{L}_u g \\\\mid g \\\\rangle + 1 + |\\\\langle Sg \\\\mid u \\\\rangle|^2 ; \\\\|g\\\\|_{L^2} = 1 , g \\\\in F^\\\\perp \\\\cap H^\\\\frac{1}{2}_+ \\\\right\\\\} .$$\\n\\nIn addition, since $|\\\\langle Sg \\\\mid u \\\\rangle|^2 \\\\geq 0$, we infer for all $n \\\\in \\\\mathbb{N}_{\\\\geq 0}$,\\n\\n$$\\\\lambda_{n+1} \\\\geq \\\\lambda_n + 1 .$$\\n\\nSpectrum of $L_u$\u2013Inequality ( 2.2 ), let $F$ be any subspace of $L^2_+ (\\\\mathbb{T})$ of dimension $n$, and take $G := \\\\mathbb{C}1 \\\\oplus S(F) + \\\\mathbb{C}u$ . Then,\\n\\n$$\\\\nu_{n+2} (u) \\\\geq \\\\min \\\\{ \\\\langle L_u h \\\\mid h \\\\rangle ; \\\\|h\\\\|_{L^2} = 1 , h \\\\in G^\\\\perp \\\\cap H^\\\\frac{1}{2}_+ \\\\} .$$\\n\\nSince $G^\\\\perp = S \\\\left( F^\\\\perp \\\\cap (S^* u)^\\\\perp \\\\right)$, then\\n\\n$$\\\\nu_{n+2} \\\\geq \\\\min \\\\left\\\\{ \\\\langle L_u Sg \\\\mid Sg \\\\rangle ; \\\\|g\\\\|_{L^2} = 1 , g \\\\in F^\\\\perp \\\\cap (S^* u)^\\\\perp \\\\cap H^\\\\frac{1}{2}_+ (\\\\mathbb{T}) \\\\right\\\\} .$$\\n\\nNote that $g \\\\perp S^* u$, then by ( 2.2 ),\\n\\n$$\\\\nu_{n+2} \\\\geq \\\\min \\\\left\\\\{ \\\\langle L_u g \\\\mid g \\\\rangle + 1 ; \\\\|g\\\\|_{L^2} = 1 , g \\\\in F^\\\\perp \\\\cap (S^* u)^\\\\perp \\\\cap H^\\\\frac{1}{2}_+ (\\\\mathbb{T}) \\\\right\\\\} ,$$\\n\\nleading to\\n\\n$$\\\\nu_{n+2} \\\\geq \\\\nu_n + 1 .$$\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"From embedding $L^{\\\\frac{2n}{n-2}}(\\\\Omega) \\\\hookrightarrow L^{\\\\frac{2n}{n-2(1+\\\\sigma)}}(\\\\Omega)$ and $D(A^{\\\\frac{1-\\\\sigma}{2}}) \\\\hookrightarrow L^{\\\\frac{2n}{n-2(1+\\\\sigma)}}(\\\\Omega)$, (1.10) and the Young inequality, we conclude\\n\\n\\\\[\\n(f_1(v_1(t)), A^\\\\sigma v_2(t)) \\\\leq C \\\\int_\\\\Omega (1 + |v_1(t)|^\\\\gamma) |A^\\\\sigma v_2(t)| \\\\, dx\\n\\\\]\\n\\n\\\\[\\n\\\\leq C \\\\left(1 + \\\\|v_1(t)\\\\|_{L^{\\\\frac{2n}{n+2-2\\\\sigma}}(\\\\Omega)}^\\\\gamma \\\\right) \\\\|A^\\\\sigma v_2(t)\\\\|_{L^{\\\\frac{2n}{n-2+2\\\\sigma}}(\\\\Omega)}\\n\\\\]\\n\\n\\\\[\\n\\\\leq C \\\\left(1 + \\\\|v_1(t)\\\\|_{L^{\\\\frac{2n}{n-2}}(\\\\Omega)}^\\\\gamma \\\\right) \\\\|A^\\\\sigma v_2(t)\\\\|_{L^{\\\\frac{2n}{n-2}}(\\\\Omega)}\\n\\\\]\\n\\n\\\\[\\n\\\\leq C \\\\left(1 + \\\\|v_1(t)\\\\|_{L^{\\\\frac{2n}{n-2}}(\\\\Omega)}^\\\\gamma \\\\right) \\\\|A^{\\\\frac{1+\\\\sigma}{2}} v_2(t)\\\\|\\n\\\\]\\n\\n\\\\[\\n\\\\leq C \\\\left(1 + \\\\|v_1(t)\\\\|_{L^{\\\\frac{2n}{n-2}}(\\\\Omega)}^\\\\gamma \\\\right) + \\\\frac{1}{16} \\\\|A^{\\\\frac{1+\\\\sigma}{2}} v_2(t)\\\\|^2.\\n\\\\]  \\n\\n(4.72)\\n\\nUsing (1.7), we derive\\n\\n\\\\[\\n|(f(u(t)) - f(v_1(t)), A^\\\\sigma v_2(t))| \\\\leq C \\\\int_\\\\Omega |(f'((1 - \\\\mu)u(t)) + \\\\mu v_1(t))| |u(t) - v_1(t)| |A^\\\\sigma v_2(t)| \\\\, dx\\n\\\\]\\n\\n\\\\[\\n\\\\leq C \\\\int_\\\\Omega \\\\left(1 + |u(t)|^{\\\\frac{4}{n-2}} + |v_1(t)|^{\\\\frac{4}{n-2}} \\\\right) |v_2(t)| |A^\\\\sigma v_2(t)| \\\\, dx,\\n\\\\]  \\n\\n(4.73)\\n\\nwhere $0 < \\\\mu < 1$.\\n\\nNoticing $u(t) = v_1(t) + v_2(t)$, it follows that\\n\\n\\\\[\\n\\\\int_\\\\Omega |v_1(t)|^{\\\\frac{4}{n-2}} |v_2(t)| |A^\\\\sigma v_2(t)| \\\\, dx \\\\leq C \\\\int_\\\\Omega \\\\left(1 + |v_1(t)|^{\\\\frac{4}{n-2}} + |v_2(t)|^{\\\\frac{4}{n-2}} \\\\right) |v_2(t)| |A^\\\\sigma v_2(t)| \\\\, dx.\\n\\\\]  \\n\\n(4.74)\\n\\nHence, from Lemma 2.11, the Cauchy and Young inequalities, we obtain there exists positive constants $\\\\tilde{C}_4$ and $\\\\tilde{C}_5$ such that\\n\\n\\\\[\\n\\\\int_\\\\Omega |v_2(t)| |A^\\\\sigma v_2(t)| \\\\, dx \\\\leq \\\\tilde{C}_4 + \\\\tilde{C}_5 \\\\|A^{\\\\frac{1+\\\\sigma}{2}} v_2(t)\\\\|^2.\\n\\\\]  \\n\\n(4.75)\\n\\nConducting similar calculations to (4.72), then by Corollary 4.8, we deduce\\n\\n\\\\[\\n\\\\int_\\\\Omega |v_1(t)|^{\\\\frac{4}{n-2}} |v_2(t)| |A^\\\\sigma v_2(t)| \\\\, dx \\\\leq C \\\\|v_1(t)\\\\|_{L^{\\\\frac{2n}{n-2}}(\\\\Omega)}^{\\\\frac{4}{n-2}} \\\\|v_2(t)\\\\|_{L^{\\\\frac{2n}{n-2(1+\\\\sigma)}}(\\\\Omega)} \\\\|A^\\\\sigma v_2(t)\\\\|_{L^{\\\\frac{2n}{n-2(1+\\\\sigma)}}(\\\\Omega)}\\n\\\\]\\n\\n\\\\[\\n\\\\leq C \\\\|A^{\\\\frac{1}{2}} v_1(t)\\\\|^{\\\\frac{4}{n-2}} \\\\|A^{\\\\frac{1+\\\\sigma}{2}} v_2(t)\\\\|^2\\n\\\\]\\n\\n\\\\[\\n\\\\leq \\\\frac{1}{16} \\\\|A^{\\\\frac{1+\\\\sigma}{2}} v_2(t)\\\\|^2 + C \\\\tilde{C}_3 \\\\|\\\\nabla v_1(t)\\\\|^2 \\\\|A^{\\\\frac{1+\\\\sigma}{2}} v_2(t)\\\\|^2.\\n\\\\]  \\n\\n(4.76)\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"From embedding $L^{\\\\frac{2n}{n-2}}(\\\\Omega) \\\\hookrightarrow L^{\\\\frac{2n}{n-2(1+\\\\sigma)}}(\\\\Omega)$ and $D(A^{\\\\frac{1-\\\\sigma}{2}}) \\\\hookrightarrow L^{\\\\frac{2n}{n-2(1+\\\\sigma)}}(\\\\Omega)$, (1.10) and the Young inequality, we conclude\\n\\n\\\\[\\n(f_1(v_1(t)), A^\\\\sigma v_2(t)) \\\\leq C \\\\int_\\\\Omega (1 + |v_1(t)|^\\\\gamma) |A^\\\\sigma v_2(t)| \\\\, dx\\n\\\\]\\n\\n\\\\[\\n\\\\leq C \\\\left(1 + \\\\|v_1(t)\\\\|_{L^{\\\\frac{2n}{n+2-2\\\\sigma}}(\\\\Omega)}^\\\\gamma\\\\right) \\\\|A^\\\\sigma v_2(t)\\\\|_{L^{\\\\frac{2n}{n-2+2\\\\sigma}}(\\\\Omega)}\\n\\\\]\\n\\n\\\\[\\n\\\\leq C \\\\left(1 + \\\\|v_1(t)\\\\|_{L^{\\\\frac{2n}{n-2}}(\\\\Omega)}^\\\\gamma\\\\right) \\\\|A^\\\\sigma v_2(t)\\\\|_{L^{\\\\frac{2n}{n-2+2\\\\sigma}}(\\\\Omega)}\\n\\\\]\\n\\n\\\\[\\n\\\\leq C \\\\left(1 + \\\\|v_1(t)\\\\|_{L^{\\\\frac{2n}{n-2}}(\\\\Omega)}^\\\\gamma\\\\right) \\\\|A^{\\\\frac{1+\\\\sigma}{2}} v_2(t)\\\\|\\n\\\\]\\n\\n\\\\[\\n\\\\leq C \\\\left(1 + \\\\|v_1(t)\\\\|_{L^{\\\\frac{2n}{n-2}}(\\\\Omega)}^\\\\gamma\\\\right) + \\\\frac{1}{16} \\\\|A^{\\\\frac{1+\\\\sigma}{2}} v_2(t)\\\\|^2.\\n\\\\]  \\n\\n(4.72)\\n\\nUsing (1.7 ), we derive\\n\\n\\\\[\\n|(f(u(t)) - f(v_1(t)), A^\\\\sigma v_2(t))| \\\\leq C \\\\int_\\\\Omega |(f'((1 - \\\\mu)u(t)) + \\\\mu v_1(t))| |u(t) - v_1(t)| |A^\\\\sigma v_2(t)| \\\\, dx\\n\\\\]\\n\\n\\\\[\\n\\\\leq C \\\\int_\\\\Omega \\\\left(1 + |u(t)|^{\\\\frac{4}{n-2}} + |v_1(t)|^{\\\\frac{4}{n-2}}\\\\right) |v_2(t)| |A^\\\\sigma v_2(t)| \\\\, dx,\\n\\\\]  \\n\\n(4.73)\\n\\nwhere $0 < \\\\mu < 1$.\\n\\nNoticing $u(t) = v_1(t) + v_2(t)$, it follows that\\n\\n\\\\[\\n\\\\int_\\\\Omega |v_1(t)|^{\\\\frac{4}{n-2}} |v_2(t)| |A^\\\\sigma v_2(t)| \\\\, dx \\\\leq C \\\\int_\\\\Omega \\\\left(1 + |v_1(t)|^{\\\\frac{4}{n-2}} + |v_2(t)|^{\\\\frac{4}{n-2}}\\\\right) |v_2(t)| |A^\\\\sigma v_2(t)| \\\\, dx.\\n\\\\]  \\n\\n(4.74)\\n\\nHence, from Lemma 2.11 , the Cauchy and Young inequalities, we obtain there exists positive constants $\\\\tilde{C}_4$ and $\\\\tilde{C}_5$ such that\\n\\n\\\\[\\n\\\\int_\\\\Omega |v_2(t)| |A^\\\\sigma v_2(t)| \\\\, dx \\\\leq \\\\tilde{C}_4 + \\\\tilde{C}_5 \\\\|A^{\\\\frac{1+\\\\sigma}{2}} v_2(t)\\\\|^2.\\n\\\\]  \\n\\n(4.72 ), then by Corollary 4.8 , we deduce\\n\\n\\\\[\\n\\\\int_\\\\Omega |v_1(t)|^{\\\\frac{4}{n-2}} |v_2(t)| |A^\\\\sigma v_2(t)| \\\\, dx \\\\leq C \\\\|v_1(t)\\\\|_{L^{\\\\frac{2n}{n-2}}(\\\\Omega)}^{\\\\frac{4}{n-2}} \\\\|v_2(t)\\\\|_{L^{\\\\frac{2n}{n-2(1+\\\\sigma)}}(\\\\Omega)} \\\\|A^\\\\sigma v_2(t)\\\\|_{L^{\\\\frac{2n}{n-2(1+\\\\sigma)}}(\\\\Omega)}\\n\\\\]\\n\\n\\\\[\\n\\\\leq C \\\\|A^{\\\\frac{1}{2}} v_1(t)\\\\|^{\\\\frac{4}{n-2}} \\\\|A^{\\\\frac{1+\\\\sigma}{2}} v_2(t)\\\\|^2\\n\\\\]\\n\\n\\\\[\\n\\\\leq \\\\frac{1}{16} \\\\|A^{\\\\frac{1+\\\\sigma}{2}} v_2(t)\\\\|^2 + C \\\\tilde{C}_3 \\\\|\\\\nabla v_1(t)\\\\|^2 \\\\|A^{\\\\frac{1+\\\\sigma}{2}} v_2(t)\\\\|^2.\\n\\\\]  \\n\\n(4.76)\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"4. TinySiamese Network\\n\\nThe proposed TinySiamese neural network takes on a new look and a new way of working which is different from the standard Siamese network. The difference first appears in the input processing of the network. Instead of having images as input, the input was the output feature vector of a pre-trained CNN model. In other words, all input images would be transformed into feature vectors using a feature extractor (such as a pre-trained CNN model) as illustrated in Fig. 3. Then, the Tiny-Siamese encoded the features in a small set of layers and finally calculated the distance between two encoded feature vectors and generated similarity score. Using this score, the model was trained from scratch with the Adam optimization algorithm and binary cross-entropy loss function.\\n\\n![Figure 3: The Proposed Architecture Based on TinySiamese Network for Verification.](image)\\n\\n4.1. Architecture\\n\\nUnlike the standard Siamese, the input of the TinySiamese was the encoded image as a feature vector. The backbone layers first aimed to extract relevant features using a linear fully-connected layer and a ReLU layer and then amplify them using another linear fully-connected layer and Sigmoid layer. The output size of the first linear layer had the half size of the input \\\\((n, n/2)\\\\) and was followed by a non-linear ReLU layer. The second linear layer took \\\\(n/2\\\\) features in input and came back to the same first input size in output \\\\((n/2, n)\\\\). This layer was followed by a non-linear Sigmoid layer. The outputs of the TinySiamese sub-networks were encoded into an \\\\(n\\\\)-dimensional vector using inputs of a size equal to \\\\(n\\\\). Siamese networks are usually trained...\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4. TinySiamese Network\\n\\nThe proposed TinySiamese neural network takes on a new look and a new way of working which is different from the standard Siamese network. The difference first appears in the input processing of the network. Instead of having images as input, the input was the output feature vector of a pretrained CNN model. In other words, all input images would be transformed into feature vectors using a feature extractor (such as a pre-trained CNN model) as illustrated in Fig. 3. Then, the Tiny-Siamese encoded the features in a small set of layers and finally calculated the distance between two encoded feature vectors and generated similarity score. Using this score, the model was trained from scratch with the Adam optimization algorithm and binary cross-entropy loss function. \\n\\n![Figure 3: The Proposed Architecture Based on TinySiamese Network for Verification.](image)\\n\\n4.1. Architecture\\n\\nUnlike the standard Siamese, the input of the TinySiamese was the encoded image as a feature vector. The backbone layers first aimed to extract relevant features using a linear fully-connected layer and a ReLU layer and then amplify them using another linear fully-connected layer and Sigmoid layer. The output size of the first linear layer had the half size of the input (n, n/2) and was followed by a non-linear ReLU layer. The second linear layer took n/2 features in input and came back to the same first input size in output (n/2, n). This layer was followed by a non-linear Sigmoid layer. The outputs of the TinySiamese sub-networks were encoded into an n-dimensional vector using inputs of a size equal to n. Siamese networks are usually trained\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Natural Language is All a Graph Needs\\n\\nRuosong Ye  \\nRutgers University  \\nruosong.ye@rutgers.edu\\n\\nCaiqi Zhang  \\nUniversity of Cambridge  \\ncz391@cam.ac.uk\\n\\nRunhui Wang  \\nRutgers University  \\nrunhui.wang@rutgers.edu\\n\\nShuyuan Xu  \\nRutgers University  \\nshuyuan.xu@rutgers.edu\\n\\nYongfeng Zhang  \\nRutgers University  \\nyongfeng.zhang@rutgers.edu\\n\\nAbstract\\n\\nThe emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scalable prompts based on natural language instructions, and use natural language to describe the geometric structure and node features of the graph for instruction tuning an LLM to perform learning and inference on graphs in a generative manner. Our method exceeds all competitive GNN baselines on ogbn-arxiv, Cora and PubMed datasets, which demonstrates the effectiveness of our method and sheds light on generative large language models as the foundation model for graph machine learning.\\n\\n1 Introduction\\n\\nBefore the advent of Transformers [1], various artificial intelligence domains with different inductive biases had diverse foundational model architectures. For instance, CNNs [2, 3] were designed with considerations for spatial invariance in images, leading to superior performance in computer vision tasks [4, 5]. Memory-enhanced models like RNNs [6] and LSTM [7, 8] were widely used for handling sequential data such as natural language [9] and audio [10]. Graph Neural Networks (GNNs) excel in capturing topological information by employing message passing and aggregation mechanisms, making them a preferred choice in the field of graph learning for a long time [11\u201313].\\n\\nIn recent years, the AI community has witnessed the emergence of numerous powerful pre-trained Large Language Models (LLMs) [14\u201318], which are driving huge advancements and lead to the pursuit of possible Artificial General Intelligence (AGI) [19]. Under this background, there is a trend towards unification in model architectures across different domains. Specifically, pre-trained Transformers have demonstrated remarkable performance on various modalities, such as images [20] and videos [21] in computer vision, text in natural language processing [22], structured data in graph machine learning [23], decision sequences in reinforcement learning [24], and visual-text pairs in multimodal tasks [25]. There has even been Transformers capable of handling twelve modalities [26].\\n\\nBesides model architecture, the unification of processing method in handling multimodal data is also a significant trend worth attention. T5 [15] established a text-to-text framework, unifying all NLP\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Natural Language is All a Graph Needs\\n\\nRuosong Ye  \\nRutgers University  \\nruosong.ye@rutgers.edu\\n\\nCaiqi Zhang  \\nUniversity of Cambridge  \\ncz391@cam.ac.uk\\n\\nRunhui Wang  \\nRutgers University  \\nrunhui.wang@ cz391@cam.ac.uk\\n\\nShuyuan Xu  \\nRutgers University  \\nshuyuan.xu@ runhui.wang@rutgers.edu\\n\\nYongfeng Zhang  \\nRutgers University  \\nyongfeng.zhang shuyuan.xu@rutgers.edu\\n\\nAbstract\\n\\nThe emergence of large-scale pre yongfeng.zhang@rutgers.edu\\n\\n1 Introduction\\n\\nBefore the advent of Transformers [1], various artificial intelligence domains with different inductive biases had diverse foundational model architectures. For instance, CNNs [2, 3] were designed with considerations for spatial invariance in images, leading to superior performance in computer vision tasks [4, 5]. Memory-enhanced models like RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scalable prompts based on natural language instructions, and use natural language to describe the geometric structure and node features of the graph for instruction tuning an LLM to perform learning and inference on graphs in a generative manner. Our method exceeds all competitive GNN baselines on ogbn-arxiv, Cora and PubMed datasets, which demonstrates the effectiveness of our method and sheds light on generative large language models as the foundation model for graph machine learning. Besides model architecture, the unification of processing method in handling multimodal data is also a significant trend worth attention. T5 [ 15 ] established a text-to-text framework, unifying all NLP\\n\\nPreprint. Preliminary work.\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"each root subgroup may be expressed in terms of commutators of other root subgroups. If some commutative unital ring $K$ acts on the root subgroups in a natural way, then the resulting odd form ring $(R, \\\\Delta)$ is an augmented odd form $K$-algebra and in the last claim of the theorem the maps of the root subgroups are isomorphisms.\\n\\nThe Chevalley commutator formula is\\n\\n$$[G_\\\\alpha, G_\\\\beta] \\\\leq \\\\prod_{i, j \\\\in \\\\{1, 2, \\\\ldots\\\\}} G_{i\\\\alpha + j\\\\beta},$$\\n\\nwe also assume that $G_{2\\\\alpha} \\\\leq G_\\\\alpha$ are 2-step nilpotent filtrations for any ultrashort root $\\\\alpha$. In other words, $G$ is a group with $BC_\\\\ell$-commutator relations in the sense of [8]. Alternatively, we may assume only that $G$ contains groups indexed by a root system of type $B_\\\\ell$ and\\n\\n$$[G_\\\\alpha, G_\\\\beta] \\\\leq \\\\prod_{i, j \\\\in \\\\mathbb{R}^+} G_{i\\\\alpha + j\\\\beta}.$$\\n\\nThese formulas turn out to be equivalent (modulo other natural conditions) up to a choice of the nilpotent filtrations $G_{2\\\\alpha} \\\\leq G_\\\\alpha$.\\n\\nThe paper is organized as follows. In section 2 we recall the definitions of odd form rings and associated unitary groups. In sections 3 and 4 we list the precise conditions on $G$ and its subgroups. Namely, the conditions are (C1)\u2013(C5) without using a commutative unital ring $K$ or (C1)\u2013(C8) involving $K$. In section 4 we also discuss the case $\\\\ell = 3$: under additional \u201cassociativity conditions\u201d (A1)\u2013(A4) the main results still hold, otherwise there are counterexamples (e.g. Chevalley groups of types $E_6$ and $E_7$). These associativity conditions always hold for $\\\\ell \\\\geq 4$ by theorem 1. Sections 5 and 6 contain the proof of the main theorem 2 for groups satisfying (C1)\u2013(C8). In the last section 7 we prove theorem 3 for groups satisfying only (C1)\u2013(C5).\\n\\n## 2 Odd form groups and odd form rings\\n\\nIn this paper we build a lot of 2-step nilpotent groups with various operations, so it is useful to develop some technique to simplify such constructions. The group operation of 2-step nilpotent groups is usually denoted by $\\\\cdot$. All lemmas in this section may be checked directly or using the machinery of polyquadratic maps [14, \u00a71.3].\\n\\nWe say that $(M, H)$ is a hermitian group if $M$ and $H$ are abelian groups, there is an automorphism\\n\\n$$(-): H \\\\to H$$\\n\\nof order at most 2, and there is a biadditive pairing $\\\\langle -, = \\\\rangle: M \\\\times M \\\\to H$ such that\\n\\n$$\\\\langle m, m' \\\\rangle = \\\\langle m', m \\\\rangle$$\\n\\nfor all $m, m' \\\\in M$. \\n\\n\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"each root subgroup may be expressed in terms of commutators of other root subgroups. If some commutative unital ring $K$ acts on the root subgroups in a natural way, then the resulting odd form ring $(R, \\\\Delta)$ is an augmented odd form $K$-algebra and in the last claim of the theorem the maps of the root subgroups are isomorphisms. The Chevalley commutator formula is\\n\\n$$[G_\\\\alpha, G_\\\\beta] \\\\leq \\\\prod_{i, j \\\\in \\\\{1, 2, \\\\ldots\\\\}} G_{i\\\\alpha + j\\\\beta},$$\\n\\nwe also assume that $G_{2\\\\alpha} \\\\leq G_\\\\alpha$ are 2-step nilpotent filtrations for any ultrashort root $\\\\alpha$. In other words, $G$ is a group with $BC_\\\\ell$-commutator relations in the sense of [8]. Alternatively, we may assume only that $G$ contains groups indexed by a root system of type $B_\\\\ell$ and\\n\\n$$[G_\\\\alpha, G_\\\\beta] \\\\leq \\\\prod_{i, j \\\\in \\\\mathbb{R}^+} G_{i\\\\alpha + j\\\\beta}.$$\\n\\nThese formulas turn out to be equivalent (modulo other natural conditions) up to a choice of the nilpotent filtrations $G_{2\\\\alpha} \\\\leq G_\\\\alpha$.\\n\\nThe paper is organized as follows. In section 2 we recall the definitions of odd form rings and associated unitary groups. In sections 3 and 4 we list the precise conditions on $G$ and its subgroups. Namely, the conditions are (C1)\u2013(C5) without using a commutative unital ring $K$ or (C1)\u2013(C8) involving $K$. In section 4 we also discuss the case $\\\\ell = 3$: under additional \u201cassociativity conditions\u201d (A1)\u2013(A4) the main results still hold, otherwise there are counterexamples (e.g. Chevalley groups of types $E_6$ and $E_7$). These associativity conditions always hold for $\\\\ell \\\\geq 4$ by theorem 1. Sections 5 and 6 contain the proof of the main theorem 7 we prove theorem 3 for groups satisfying only (C1)\u2013(C5).\\n\\n2 Odd form groups and odd form rings\\n\\nIn this paper we build a lot of 2-step nilpotent groups with various operations, so it is useful to develop some technique to simplify such constructions. The group operation of 2-step nilpotent groups is usually denoted by $\\\\cdot$. All lemmas in this section may be checked directly or using the machinery of polyquadratic maps [14, \u00a7 1.3].\\n\\nWe say that $(M, H)$ is a hermitian group if $M$ and $H$ are abelian groups, the group operation of 2-step nilpotent groups is usually denoted by $\\\\cdot$. All lemmas in this section may be checked directly or using the machinery of polyquadratic maps [14, \u00a7 1.3].\\n\\nWe say that $(M, H)$ is a hermitian group if $M$ and $H$ are abelian groups, there is an automorphism\\n\\n$$(-): H \\\\to H$$\\n\\nof order at most 2, and there is a biadditive pairing $\\\\langle -, = \\\\rangle: M \\\\times M \\\\to H$ such that\\n\\n$$\\\\langle m, m' \\\\rangle = \\\\langle m', m \\\\rangle$$\\n\\nfor all $m, m' \\\\in M$. \\n\\n3\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"the spectral coefficients. We can fit this dependency of the coefficients in order to obtain the spectral functions\\n\\n\\\\[ a(r_{\\\\text{ant}}, X_{\\\\text{slice}}, X_{\\\\text{max}}), \\\\]\\n\\\\[ b(r_{\\\\text{ant}}, X_{\\\\text{slice}}, X_{\\\\text{max}}), \\\\]\\n\\\\[ c(r_{\\\\text{ant}}, X_{\\\\text{slice}}, X_{\\\\text{max}}), \\\\]\\n\\nin every slice. This procedure is applied to each antenna independently, indicated by the explicit dependency on the antenna distance. Therefore in the current version of template synthesis \\\\( r_{\\\\text{ant}} \\\\), as well as \\\\( X_{\\\\text{slice}} \\\\), can only take values on a fixed grid. We note here that while we write the functions as both a function of \\\\( X_{\\\\text{slice}} \\\\) and \\\\( X_{\\\\text{max}} \\\\), a more accurate description should probably take some combination of the two that could serve as a proxy for shower age in the slice. We come back to this in Section VI.\\n\\nWe fit the spectral parameters as a function of \\\\( X_{\\\\text{max}} \\\\) using a parabola. In order to better deal with the large scatter for some slices, especially the very early and late ones where there are only a few particles, we opt to first bin the data points by \\\\( X_{\\\\text{max}} \\\\). In each bin we calculate the mean value of the spectral parameter and the corresponding standard deviation. If a bin contains less than two data points, which can occur because not all showers might contain particles in that slice, we do not consider it for the parabolic fit. All the other bins are then fed to a least-squares fitting routine. We end up with a spectral function describing the spectral parameter value as function of the shower \\\\( X_{\\\\text{max}} \\\\) in a given slice, for a particular antenna,\\n\\n\\\\[\\n\\\\begin{align*}\\n    a(r_{\\\\text{ant}}, X_{\\\\text{slice}}, X_{\\\\text{max}}) &= p_0^a + p_1^a \\\\cdot X_{\\\\text{max}} + p_2^a \\\\cdot X_{\\\\text{max}}^2 \\\\\\\\\\n    b(r_{\\\\text{ant}}, X_{\\\\text{slice}}, X_{\\\\text{max}}) &= p_0^b + p_1^b \\\\cdot X_{\\\\text{max}} + p_2^b \\\\cdot X_{\\\\text{max}}^2 \\\\\\\\\\n    c(r_{\\\\text{ant}}, X_{\\\\text{slice}}, X_{\\\\text{max}}) &= p_0^c + p_1^c \\\\cdot X_{\\\\text{max}} + p_2^c \\\\cdot X_{\\\\text{max}}^2.\\n\\\\end{align*}\\n\\\\]\\n\\nIn Figure 3 we present a schematic overview of how we extract the spectral functions. These need to be determined only once for the air shower geometry under consideration. Generalising them to arbitrary geometries will be the subject of future work.\\n\\nC. Construction of the template\\n\\nThe final ingredient of template synthesis, is the template itself. With this object and the spectral functions, we have all the necessary information to synthesise the emission from an air shower with arbitrary longitudinal profile.\\n\\nIn order to construct our template, we use a single microscopic simulation called the origin shower. The origin is a microscopic simulation, sliced using the same procedure as the simulation set that was used to extract the spectral functions. From the origin shower we calculate the amplitude frequency spectrum \\\\( A_{\\\\text{origin}} \\\\) and phase frequency spectrum \\\\( \\\\phi_{\\\\text{origin}} \\\\) in each antenna and every slice, as shown in Figure 4. Using the spectral functions with the \\\\( X_{\\\\text{max}} \\\\) of the origin shower, we can normalise these to obtain the spectra of the template,\\n\\n\\\\[\\nA_{\\\\text{template}}(r_{\\\\text{ant}}, f, X_{\\\\text{slice}}) = A_{\\\\text{origin}}(r_{\\\\text{ant}}, f, X_{\\\\text{slice}}) \\\\cdot \\\\left[ \\\\tilde{A}(r_{\\\\text{ant}}, f, X_{\\\\text{max}}) \\\\right]^{-1} \\\\\\\\\\n\\\\phi_{\\\\text{template}}(r_{\\\\text{ant}}, f, X_{\\\\text{slice}}) = \\\\phi_{\\\\text{origin}}(r_{\\\\text{ant}}, f, X_{\\\\text{slice}}).\\n\\\\]\\n\\nIn this equation the parameterised amplitude frequency spectrum \\\\( \\\\tilde{A}(f) \\\\) also depends on \\\\( X_{\\\\text{max}} \\\\) through the spectral coefficients, which are calculated using the fitted spectral functions. As a result, in the following the frequency \\\\( f \\\\) is restricted to the range used to fit the spectral functions.\\n\\nThe template thus contains a normalised version of the origin shower. It acquires its phase frequency spectrum as is, but the origin amplitude frequency spectrum is corrected for the longitudinal evolution (particle number in\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the spectral coefficients. We can fit this dependency of the coefficients in order to obtain the spectral functions\\n\\n\\\\[ a(r_{\\\\text{ant}}, X_{\\\\text{slice}}, X_{\\\\text{max}}), \\\\]\\n\\\\[ b(r_{\\\\text{ant}}, X_{\\\\text{slice}}, X_{\\\\text{max}}), \\\\]\\n\\\\[ c(r_{\\\\text{ant}}, X_{\\\\text{slice}}, X_{\\\\text{max}}), \\\\]\\n\\nin every slice. This procedure is applied to each antenna independently, indicated by the explicit dependency on the antenna distance. Therefore in the current version of template synthesis \\\\( r_{\\\\text{ant}} \\\\), as well as \\\\( X_{\\\\text{slice}} \\\\), can only take values on a fixed grid. We note here that while we write the functions as both a function of \\\\( X_{\\\\text{slice}} \\\\) and \\\\( X_{\\\\text{max}} \\\\), a more accurate description should probably take some combination of the two that could serve as a proxy for shower age in the slice. We come back to this in Section VI. We fit the spectral parameters as a function of \\\\( X_{\\\\text{max}} \\\\) using a parabola. In order to better deal with the large scatter for some slices, especially the very early and late ones where there are only a few particles, we opt to first bin the data points by \\\\( X_{\\\\text{max}} \\\\). In each bin we calculate the mean value of the spectral parameter and the corresponding standard deviation. If a bin contains less than two data points, which can occur because not all showers might contain particles in that slice, we do not consider it for the parabolic fit. All the other bins are then fed to a least-squares fitting routine. We end up with a spectral function describing the spectral parameter value as function of the shower \\\\( X_{\\\\text{max}} \\\\) in a given slice, for a particular antenna, \\\\( a(r_{\\\\text{ant}}, X_{\\\\text{slice}}, X_{\\\\text{max}}) = p_0^a + p_1^a \\\\cdot X_{\\\\text{max}} + p_2^a \\\\cdot X_{\\\\text{max}}^2 \\\\)\\n\\\\[ b(r_{\\\\text{ant}}, X_{\\\\text{slice}}, X_{\\\\text{max}}) = p_0^b + p_1^b \\\\cdot X_{\\\\text{max}} + p_2^b \\\\cdot X_{\\\\text{max}}^2 \\\\]\\n\\\\[ c(r_{\\\\text{ant}}, X_{\\\\text{slice}}, X_{\\\\text{max}}) = p_0^c + p_1^c \\\\cdot X_{\\\\text{max}} + p_2^c \\\\cdot X_{\\\\text{max}}^2 \\\\].\\n\\nIn Figure 3 we present a schematic overview of how we extract the spectral functions. These need to be determined only once for the air shower geometry under consideration. Generalising them to arbitrary geometries will be the subject of future work. \\n\\nC. Construction of the template\\n\\nThe final ingredient of template synthesis, is the template itself. With this object and the spectral functions, we have all the necessary information to synthesise the emission from an air shower with arbitrary longitudinal profile. In order to construct our template, we use a single microscopic simulation called the origin shower we calculate the amplitude frequency spectrum \\\\( A_{\\\\text{origin}} \\\\) and phase frequency spectrum \\\\( \\\\phi_{\\\\text{origin}} \\\\) in each antenna and every slice, as shown in Figure 4. Using the spectral functions with the \\\\( X_{\\\\text{max}} \\\\) of the origin shower, we can normalise these to obtain the spectra of the template, \\\\( A_{\\\\text{template}}(r_{\\\\text{ant}}, f, X_{\\\\text{slice}}) = A_{\\\\text{origin}}(r_{\\\\text{ant}}, f, X_{\\\\text{slice}}) \\\\cdot \\\\left[ \\\\tilde{A}(r_{\\\\text{ant}}, f, X_{\\\\text{slice}}) \\\\right]^{-1} \\\\)\\n\\\\[ \\\\phi_{\\\\text{template}}(r_{\\\\text{ant}}, f, X_{\\\\text{slice}}) = \\\\phi_{\\\\text{origin}}(r_{\\\\text{ant}}, f, X_{\\\\text{slice}}). \\\\]\\n\\nIn this equation the parameterised amplitude frequency spectrum  \\\\( \\\\tilde{A}(f) \\\\) also depends on \\\\( X_{\\\\text{max}} \\\\) through the spectral coefficients, which are calculated using the fitted spectral functions. As a result, in the following the frequency \\\\( f \\\\) is restricted to the range used to fit the spectral functions. The template thus contains a normalised version of the origin shower. It acquires its phase frequency spectrum as is, but the origin amplitude frequency spectrum is corrected for the longitudinal evolution (particle number in\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.3 Effectiveness of different features in the physical world\\n\\n4.3.1 Color transfer and texture blurring\\n\\nFigure 4: Patch with different feature changes. (left: original patch; middle: color transfer patch; right: texture blurring patch)\\n\\nIn physical-world deployments, attack patches are often affected by color transfer due to lighting conditions or blurring caused by camera focus or smudging. To analyze if these changes will affect the performance of the attack patches, we compare the performance of an original patch, a color-adjusted patch, and a local texture adjusted patch. The color-adjusted patch is applied by adding a value ($\\\\delta$) to all values in RGB channels and making sure $\\\\delta$ will not lead to an overflow. This color transfer will not change the texture information of the patch. The local texture adjustment is applied by using a 3x3 Gaussian blur. Figure 4 demonstrates the two types of feature adjustments applying to a patch with brightness range=0.24.\\n\\n| Brightness range | Original | Color transfer | Gaussian blur |\\n|------------------|----------|----------------|---------------|\\n| 1 (AdvPatch)     | 89.4%    | 90.8%          | 47.8%         |\\n| 0.35             | 89.5%    | 87.9%          | 22.7%         |\\n| 0.24             | 74.2%    | 75.3%          | 10.1%         |\\n\\nThe performances of different patches are shown in Table 1. Regardless of the lightness restriction, the color transfer patch achieves almost the same success rate as the original patch. This performance shows that the patch does not need to maintain a specific color to deceive the target network. On the other hand, the blurred patch exhibits a significant decrease in success rate, suggesting that local texture is the key feature in deceiving target networks. Using these findings, we can apply the proposed hue mapping method to adjust the color of the patch and enhance its integration with the target environment, resulting in further reduced visibility. This process does not require any learning and can be quickly applied when deploying the patch in the physical world.\\n\\n4.3.2 Random color variations\\n\\nWhen printing an attack patch, it is important to consider that normal printers are not able to produce a patch with precisely the same color as the digital version. Therefore, the patch\u2019s robustness to random color variations must be evaluated.\\n\\nTo replicate the color drift that commonly occurs during printing, we generate random noise within a restricted range that corresponds to a percentage of the original value. This approach allows us to simulate different levels of drift, and the results are shown in Table 2.\\n\\n| Brightness range | Original | 10% drift | 15% drift | 20% drift |\\n|------------------|----------|-----------|-----------|-----------|\\n| 1 (AdvPatch)     | 89.4%    | 87.6%     | 85.9%     | 83.3%     |\\n| 0.35             | 89.5%    | 84.2%     | 77.6%     | 67.2%     |\\n| 0.24             | 74.2%    | 68.6%     | 43.2%     | 27.3%     |\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.3 Effectiveness of different features in the physical world\\n\\n4.3.1 Color transfer and texture blurring\\n\\nFigure 4: Patch with different feature changes. (left: original patch; middle: color transfer patch; right: texture blurring patch)\\n\\nIn physical-world deployments, attack patches are often affected by color transfer due to lighting conditions or blurring caused by camera focus or smudging. To analyze if these changes will affect the performance of the attack patches, we compare the performance of an original patch, a color-adjusted patch, and a local texture adjusted patch. The color-adjusted patch is applied by adding a value (\u03b4) to all values in RGB channels and making sure \u03b4 will not lead to an overflow. This color transfer will not change the texture information of the patch. The local texture adjustment is applied by using a 3x3 Gaussian blur. Figure 4 demonstrates the two types of feature adjustments applying to a patch with brightness range=0.24. \\n\\n| Brightness range | Original | Color transfer | Gaussian blur |\\n|------------------|----------|----------------|---------------|\\n| 1 (AdvPatch)     | 89.4%    | 90.8%          | 47.8%         |\\n| 0.35             | 89.5%    | 87.9%          | 22.7%         |\\n| 0.24             | 74.2%    | 75.3%          | 10.1%         |\\n\\nThe performances of different patches are shown in Table 1. Regardless of the lightness restriction, the color transfer patch achieves almost the same success rate as the original patch. This performance shows that the patch does not need to maintain a specific color to deceive the target network. On the other hand, the blurred patch exhibits a significant decrease in success rate, suggesting that local texture is the key feature in deceiving target networks. Using these findings, we can apply the proposed hue mapping method to adjust the color of the patch and enhance its integration with the target environment, resulting in further reduced visibility. This process does not require any learning and can be quickly applied when deploying the patch in the physical world. \\n\\n4.3.2 Random color variations\\n\\nWhen printing an attack patch, it is important to consider that normal printers are not able to produce a patch with precisely the same color as the digital version. Therefore, the patch\u2019s robustness to random color variations must be evaluated. \\n\\nTo replicate the color drift that commonly occurs during printing, we generate random noise within a restricted range that corresponds to a percentage of the original value. This approach allows us to simulate different levels of drift, and the results are shown in Table 2. \\n\\n| Brightness range | Original | 10% drift | 15% drift | 20% drift |\\n|------------------|----------|-----------|-----------|-----------|\\n| 1 (AdvPatch)     | 89.4%    | 87.6%     | 85.9%     | 83.3%     |\\n| 0.35             | 89.5%    | 84.2%     | 77.6%     | 67.2%     |\\n| 0.24             | 74.2%    | 68.6%     | 43.2%     | 27.3%     |\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The cyan dashed line represents the linear least-squares best fit performed on the logarithm of the points for high densities ($\\\\rho_{\\\\text{thr}} > 1.1 \\\\times 10^{-21}$ g cm$^{-3}$). The best fit of $\\\\kappa = 0.47 \\\\pm 0.03$ is consistent with the strong-field limit of $B \\\\propto \\\\rho^{0.5}$. We have already shown in the previous section (Section 5) that our structures are on average highly elongated, and magnetic fields clearly help to deform the shape of the forming structures. It is therefore not unexpected that we find a shallower scaling compared to the weak field limit ($\\\\kappa = 0.67$).\\n\\nWe see that, while there is no clear transition from the sub- to the super-Alfv\u00e9nic regime, there is clearly a trend that higher Alfv\u00e9nic Mach numbers are preferentially obtained at the higher density end. This is confirmed by a Kolmogorov-Smirnov (KS) two-sample test, which compares if two distributions belong to the same population. In this case, we compare the $\\\\rho_{\\\\text{thr}}$-distributions of structures with $M_A > 1$ and $M_A \\\\leq 1$. We find the $p$-values$^3$ to be very low: $6 \\\\times 10^{-4}$ at 2 Myr and $5.2 \\\\times 10^{-15}$ at 3.5 Myr (see Table 6).\\n\\nCrutcher et al. (2010) found that the observed magnetic field distribution is rather flat at low density, in agreement with the idea that denser clouds are swept up along the magnetic field lines on large scales, while at higher density there is a power-law increase of the magnetic field strength. If spherical clouds start to collapse and the magnetic field is not strong enough to stop the collapse, one expects a power-law slope of $\\\\kappa = 0.5 - 0.67$ (see above).\\n\\nIn the case of our clouds, we find that the high-density end is well consistent with $\\\\kappa = 0.5$, and the lower-density end clearly shows a much shallower slope. Nonetheless, there does not seem to be a clear single density at which there is a sharp change in slope. Simulations by Li et al. (2015), Mocz et al. (2017), Girichidis et al. (2018), Zhang et al. (2019) find similarly the lack of a sharp transition density. Auddy et al. (2022) predict that the transition density depends on the fourth power of $M_A$. While of potential interest, this is unfortunately not demonstrable from the present analysis.\\n\\n### 6.2 Impact of magnetic fields on the energetics of sub-structures\\n\\nWe are also interested in assessing the energetic relevance of magnetic fields over different length scales in the MCs, especially with respect to potentially star-forming structures. For this purpose, we compute the volume term of the magnetic energy and compare it with the kinetic and potential energies. Similar work for the same simulations has been performed by Ganguly et al. (2022), who assess the virial balance of the cloud sub-structures. Here, we extend the range of our analysis to include the dynamics of lower-density gas (between $10^{-24}$ and $10^{-22}$ g cm$^{-3}$; low-den dendrogram analysis, see Table 2).\\n\\nThe magnetic energy of a given structure is computed as\\n\\n$$E_B = \\\\frac{1}{8\\\\pi} \\\\int \\\\rho |\\\\mathbf{B}|^2 d^3 r,$$\\n\\nwhere the integration is computed over the entire volume $V$ of the structure. The kinetic energy is computed using the following relation:\\n\\n$$E_{\\\\text{KE}} = \\\\frac{1}{2} \\\\int \\\\rho (\\\\mathbf{v} - \\\\mathbf{v}_0)^2 d^3 r.$$\\n\\nHere, $\\\\mathbf{v}_0$ is the centre of mass velocity computed from Eq. 17. The self-gravitating potential energy of a given structure is obtained using the following relation:\\n\\n$$E_{\\\\text{PE}} = -\\\\frac{1}{2} G \\\\int \\\\int \\\\frac{\\\\rho(r)\\\\rho(r')}{|\\\\mathbf{r} - \\\\mathbf{r}'|} d^3 r d^3 r',$$\\n\\nwhere $G$ is the gravitational constant. We compute the self-gravity of each dendrogram structure using a KD-tree algorithm (Bentley 1975) instead of an $O(N^2)$ direct computation.\\n\\nWe show the relative importance of magnetic fields with respect to potential and kinetic energy in the left and right panel of Fig. 7, respectively, for all MHD cloud structures at $t_{\\\\text{vol}} = 3.5$ Myr. For both plots, the $x$-axis represents the density threshold $\\\\rho_{\\\\text{thr}}$, and the $y$-axis represents $E_B/|E_{\\\\text{PE}}|$ (left) and $E_B/|E_{\\\\text{KE}}|$ (right), respectively. The colours of the points represent their morphologies. Here, for the purpose of understanding the dynamics of low-density gas, we also include the \\\"unclassified\\\" structures (i.e. structures with $>5\\\\%$ of their surface cells touching the edge of the analysis box, see Section 3). The side panels to the right and top of each plot show the marginal distributions of $N_{\\\\text{g}}/N_{\\\\text{tot}}$ for each morphology. Note that, since the definition of $N_{\\\\text{tot}}$ (Eq. 14) does not contain unclassified structures, the fractions in the two side panels add up to greater than unity. The filled symbols are molecular structures, while the open symbols are atomic. Typically, for low-density structures, which mostly consist of atomic gas, the magnetic energy is either comparable to or much larger than the potential energy (left panel of Fig. 7). The magnetic energy is\\n\\n| variable 1 | variable 2 | time [Myr] | p-value |\\n|------------|------------|------------|---------|\\n| $\\\\rho_{\\\\text{thr}} (M_A > 1)$ | $\\\\rho_{\\\\text{thr}} (M_A \\\\leq 1)$ | 2 | $6 \\\\times 10^{-4}$ |\\n| | | 3.5 | $5.2 \\\\times 10^{-15}$ |\\n\\nTable 6. The $p$-values of the 2-sample KS test for the density distribution of sub-Alfv\u00e9nic and super-Alfv\u00e9nic structures. We can see that the $p$-value is low for both 2 and 3.5 Myr, suggesting that sub-Alfv\u00e9nic and super-Alfv\u00e9nic structures (corresponding to bluish and reddish points in Fig. 6, respectively) have statistically significant differences in their density distributions.\\n\\n---\\n\\n$^3$ If the $p$-value is larger than a certain value (typically 0.05), this means that we cannot reject the null hypothesis that the sub-Alfv\u00e9nic and super-Alfv\u00e9nic structures have the same underlying density distribution.\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The cyan dashed line represents the linear least-squares best fit performed on the logarithm of the points for high densities (\\\\( \\\\rho_{\\\\text{thr}} > 1.1 \\\\times 10^{-21} \\\\text{ g cm}^{-3} \\\\)). The best fit of \\\\( \\\\kappa = 0.47 \\\\pm 0.03 \\\\) is consistent with the strong-field limit of \\\\( B \\\\propto \\\\rho^{0.5} \\\\). We have already shown in the previous section (Section 5 ) that our structures are on average highly elongated, and magnetic fields clearly help to deform the shape of the forming structures. It is therefore not unexpected that we find a shallower scaling compared to the weak field limit (\\\\( \\\\kappa = 0.67 \\\\)).\\n\\nWe see that, while there is no clear transition from the subto the super-Alfv\u00e9nic regime, there is clearly a trend that higher Alfv\u00e9nic Mach numbers are preferentially obtained at the higher density end. This is confirmed by a Kolmogorov-Smirnov (KS) two-sample test, which compares if two distributions belong to the same population. In this case, we compare the \\\\( \\\\rho_{\\\\text{thr}} \\\\)-distributions of structures with \\\\( M_A > 1 \\\\) and \\\\( M_A \\\\leq 1 \\\\). We find the \\\\( p \\\\)-values\\\\(^3\\\\) to be very low: \\\\( 6 \\\\times 10^{-4} \\\\) at 2 Myr -value is larger than a certain value (typically 0.05), this means that we cannot reject the null hypothesis that the sub-Alfv\u00e9nic and super-Alfv\u00e9nic structures have the same underlying density distribution.\\n\\n\\\\(^3\\\\) If the \\\\( p \\\\)-value is larger than a certain value (typically 0.05), this means that we cannot reject the null hypothesis that the sub-Alfv\u00e9nic and super-Alfv\u00e9nic structures have the same underlying density distribution.\\n\\n\\\\[ E_B = \\\\frac{1}{8\\\\pi} \\\\int |\\\\mathbf{B}|^2 d^3 r, \\\\]\\n\\nwhere the integration is computed over the entire volume \\\\( V \\\\) of the structure. The kinetic energy is computed using the following relation:\\n\\n\\\\[ E_{\\\\text{KE}} = \\\\frac{1}{2} \\\\int \\\\rho (v - v_0)^2 d^3 r. \\\\]\\n\\nHere, \\\\( v_0 \\\\) is the centre of mass velocity computed from Eq. 17. The self-gravitating potential energy of a given structure is obtained using the following relation:\\n\\n\\\\[ E_{\\\\text{PE}} = -\\\\frac{1}{2} G \\\\int \\\\int \\\\frac{\\\\rho(r)\\\\rho(r')}{|r - r'|} d^3 r d^3 r', \\\\]\\n\\nwhere \\\\( G \\\\) is the gravitational constant. We compute the self-gravity of each dendrogram structure using a KD-tree algorithm (Bentley 1975) instead of an \\\\( O(N^2) \\\\) direct computation.\\n\\nWe show the relative importance of magnetic fields with respect to potential and kinetic energy in the left and right panel of Fig. 7, respectively, for all MHD cloud structures at \\\\( t_{\\\\text{vol}} = 3.5 \\\\) Myr. For both plots, the \\\\( x \\\\)-axis represents the density threshold \\\\( \\\\rho_{\\\\text{thr}} \\\\), and the \\\\( y \\\\)-axis represents \\\\( E_B/|E_{\\\\text{PE}}| \\\\) (left) and \\\\( E_B/|E_{\\\\text{KE}}| \\\\) (right), respectively. The colours of the points represent their morphologies. Here, for the purpose of understanding the dynamics of low-density gas, we also include the \\\"unclassified\\\" structures (i.e. structures with >5% of their surface cells touching the edge of the analysis box, see Section 3). The side panels to the right and top of each plot show the marginal distributions of \\\\( N_{\\\\text{g}}/N_{\\\\text{tot}} \\\\) for each morphology. Note that, since the definition of \\\\( N_{\\\\text{tot}} \\\\) (Eq. 14) does not contain unclassified structures, the fractions in the two side panels add up to greater than unity. The filled symbols are molecular structures, while the open symbols are atomic. Typically, for low-density structures, which mostly consist of atomic gas, the magnetic energy is either comparable to or much larger than the potential energy (left panel of Fig. 7). The magnetic energy is\\n\\n| variable 1 | variable 2 | time [Myr] | p-value |\\n|------------|------------|------------|---------|\\n| \\\\( \\\\rho_{\\\\text{thr}} (M_A > 1) \\\\) | \\\\( \\\\rho_{\\\\text{thr}} (M_A \\\\leq 1) \\\\) | 2 | \\\\( 6 \\\\times 10^{-4} \\\\) |\\n| | | 3.5 | \\\\( 5.2 \\\\times 10^{-15} \\\\) |\\n\\nTable 6. The \\\\( p \\\\)-values of the 2-sample KS test for the density distribution of sub-Alfv\u00e9nic and super-Alfv\u00e9nic structures. We can see that the \\\\( p \\\\)-value is low for both 2 and 3.5 My at 2 Myr and 3.5 Myr (see Table 6 ).\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"instructions given in the prompt. Past research has shown the importance of stating the actions a model can take, such as outputting \u201cI don\u2019t know.\u201d (Zhou et al., 2023). Similarly, how strongly the prompt encourages a model to incorporate feedback can favor overoptimization.\\n\\n**Introducing Errors** Finally, effective feedback may communicate information on where the learner is failing, requiring an understanding of the possible error modes for a given task, and which ones the learner is likely in. For example, guessing and committing systematic reasoning mistakes are reflections of differing understandings. Exploring the error space and identifying the mistakes made by a learner is an important extension to the base framework directly derived from pedagogical and psychology of education research.\\n\\n### 4.3 Feedback Integration\\n\\nThe method used to transmit the feedback to the model influences how it is subsequently processed. Fernandes et al. (2023) identify three common feedback integration mechanisms: feedback-based imitation learning, joint-feedback modeling, and reinforcement learning. In addition to this, we also consider feedback use in in-context learning (Brown et al., 2020). The training objective will necessarily influence how the model is processing and incorporating feedback. Typically, the training relies upon either scalar feedback (a single number encoding how much the model should be rewarded for its output) or a ranking (how well a given output did in relation to other candidate answers). However, this is simple information, and does not leverage the rich and complex information encoded in natural language feedback. Section 5 therefore comprehensively explores the different types of information that can be encoded in feedback.\\n\\n### 5 Feedback Content Taxonomy\\n\\nIn Section 4, we presented an overview of the complex ecosystem of feedback, including an expansion specifically for LLMs (i.e., FELT) that connects various background elements (e.g., the learner, the task, the error types) to the actual feedback that must be given. In this section, we expand on our analysis of the content dimension of feedback in FELT. Specifically, we present a taxonomy of feedback content under two different forms: a set of 10 broad axes along which feedback can vary, and a more concrete set of nine emergent categories for feedback topic. Figure 4 presents an overview of the two different presentations of this taxonomy, and the mapping between them.\\n\\nWe motivate this taxonomy to finely categorize current approaches to textual feedback that implicitly formulate feedback solely for utility (i.e., how useful is the feedback for guiding a model toward a suitable response). However, they do not categorize its content, leaving a conceptual gap about what makes feedback useful. Our taxonomy stratifies the feedback space, allowing a deliberate and systematic study of feedback content.\\n\\n#### 5.1 General Taxonomy\\n\\nWe break down feedback content along ten dimensions that influence how feedback is formulated:\\n\\n1. **length**, an indication of how much feedback feedback is given, possibly measured by counting its number of tokens,\\n\\n2. **granularity**, a measure of the level of detail with which the feedback addresses the original answer \u2014 it is not a measure of how much of the answer is being considered, but rather of the level of detail with which it is being considered,\\\\(^8\\\\)\\n\\n3. **applicability of instructions**, expressing both whether the feedback contains instructions, as well as how applicable those instructions are for the learner and their current understanding and approach to solving the task,\\n\\n4. **answer coverage**, which registers how much of the learner\u2019s answer is considered to generate the given feedback. The feedback could be independent of the answer, or only relate to parts of the answer (e.g., focusing on a particular mistake), or the feedback might take the complete answer into consideration,\\n\\n5. **criteria**, denoting which criteria the answer is being evaluated on: global evaluation, specific dimensions (e.g., fluency, engagement, etc.), or, alternatively, no dimensions (the answer is not being evaluated),\\n\\n6. **information novelty**, indicating the degree to which learner already had access to the information provided in the feedback, ranging from all information being previously known\\n\\n\\\\(^8\\\\)For an open-answer example task, feedback might range from global learning meta-feedback, to global but task-specific, to paragraph-level, to sentence-level, to word-level, to token-level feedback.\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"instructions given in the prompt. Past research has shown the importance of stating the actions a model can take, such as outputting \u201cI don\u2019t know.\u201d ( Zhou et al. , 2023 ). Similarly, how strongly the prompt encourages a model to incorporate feedback can favor overoptimization. \\n\\n**Introducing Errors** Finally, effective feedback may communicate information on where the learner is failing, requiring an understanding of the possible error modes for a given task, and which ones the learner is likely in. For example, guessing and committing systematic reasoning mistakes are reflections of differing understandings. Exploring the error space and identifying the mistakes made by a learner is an important extension to the base framework directly derived from pedagogical and psychology of education research. \\n\\n### 4.3 Feedback Integration\\n\\nThe method used to transmit the feedback to the model influences how it is subsequently processed. Fernandes et al. ( 2023 ) identify three common feedback integration mechanisms: feedback-based imitation learning, joint-feedback modeling, and reinforcement learning. In addition to this, we also consider feedback use in in-context learning ( Brown et al. , 2020 ). The training objective will necessarily influence how the model is processing and incorporating feedback. Typically, the training relies upon either scalar feedback (a single number encoding how much the model should be rewarded for its output) or a ranking (how well a given output did in relation to other candidate answers). However, this is simple information, and does not leverage the rich and complex information encoded in natural language feedback. Section 5 therefore comprehensively explores the different types of information that can be encoded in feedback. \\n\\n### 5 Feedback Content Taxonomy\\n\\nIn Section 4 , we presented an overview of the complex ecosystem of feedback, including an expansion specifically for LLMs (i.e., FELT) that connects various background elements (e.g.,the learner, the task, the error types) to the actual feedback that must be given. In this section, we expand on our analysis of the content dimension of feedback in FELT. Specifically, we present a taxonomy of feedback content under two different forms: a set of 10 broad axes along which feedback can vary, and a more concrete set of nine emergent categories for feedback topic. Figure 4 presents an overview of the two different presentations of this taxonomy, and the mapping between them. \\n\\nWe motivate this taxonomy to finely categorize current approaches to textual feedback that implicitly formulate feedback solely for utility (i.e.,how useful is the feedback for guiding a model toward a suitable response). However, they do not categorize its content, leaving a conceptual gap about what makes feedback useful. Our taxonomy stratifies the feedback space, allowing a deliberate and systematic study of feedback content. \\n\\n#### 5.1 General Taxonomy\\n\\nWe break down feedback content along ten dimensions that influence how feedback is formulated: 1. length , an indication of how much feedback feedback is given, possibly measured by counting its number of tokens, 2. granularity , a measure of the level of detail with which the feedback addresses the original answer \u2014 it is not a measure of how much of the answer is being considered, but rather of the level of detail with which it is being considered, 8 For an open-answer example task, feedback might range from global learning meta-feedback, to global but taskspecific, to paragraph-level, to sentence-level, to word-level, to token-level feedback. \\n\\n3. applicability of instructions , expressing both whether the feedback contains instructions, as well as how applicable those instructions are for the learner and their current understanding and approach to solving the task, 4. answer coverage , which registers how much of the learner\u2019s answer is considered to generate the given feedback. The feedback could be independent of the answer, or only relate to parts of the answer (e.g.,, focusing on a particular mistake), or the feedback might take the complete answer into consideration, 5. criteria , denoting which criteria the answer is being evaluated on: global evaluation, specific dimensions (e.g., fluency, engagement, etc.), or, alternatively, no dimensions (the answer is not being evaluated), 6. information novelty , indicating the degree to which learner already had access to the information provided in the feedback, ranging from all information being previously known\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"are likely to be inaccurate. Indeed, the unconstrained estimate $f_{\\\\text{esc}}$ predicts high escape fraction values in the high stellar mass range and hence does not match our findings in fig. 4. We therefore artificially set the $f_{\\\\text{esc}}$ value to 0 for galaxies with $x_\\\\star > 8.5$, obtaining:\\n\\n$$f_{\\\\text{esc}} = \\\\begin{cases} 0 & f_{\\\\text{esc}} < 0 \\\\text{ or } M_\\\\star,\\\\log < 8.5 \\\\\\\\ 1 & f_{\\\\text{esc}} > 1 \\\\\\\\ f_{\\\\text{esc}} & \\\\text{otherwise.} \\\\end{cases}$$\\n\\n(14)\\n\\nIn order to determine the accuracy of the model in predicting individual escape fractions, we select a subsample of galaxies not used for the fitting in order to avoid problems due to over-fitting. As a measure for this accuracy we use the average relative deviation\\n\\n$$r = \\\\frac{1}{N_{\\\\text{test}}} \\\\sum_{i=1}^{N_{\\\\text{test}}} \\\\frac{|f_{\\\\text{esc, pred},i} - f_{\\\\text{esc},i}|}{f_{\\\\text{esc},i}},$$\\n\\n(15)\\n\\nwith $f_{\\\\text{esc, pred},i}$ and $f_{\\\\text{esc},i}$ being the predicted and modelled escape fraction of the $i$-th galaxy respectively, and $N_{\\\\text{test}}$ the number of test galaxies. We only used galaxies with $f_{\\\\text{esc}} > 0.01$, as this measure is not useful for $f_{\\\\text{esc}}$ approaching 0. We find a value of $r \\\\approx 1.2$, i.e. the average estimation error is of the order of a factor of 2, and as such the accuracy of predicting the escape fraction of a single halo is limited. However, for large scale studies where the statistical distribution of the escape fraction is more important, this model performs significantly better. Indeed, the average escape fraction obtained with the fitting formula is $f_{\\\\text{esc, pred},i} = 0.121 \\\\pm 0.086$, with the modelled escape fraction being $f_{\\\\text{esc},i} = 0.117 \\\\pm 0.133$.\\n\\nIn fig. 9 we show how well the fitting formula is able to reproduce the behaviour of the escape fraction in relation to the stellar mass and redshift that we examined in fig. 2. We see that the evolution of the escape fraction with redshift is successfully reproduced. However, the large gradients in $\\\\langle f_{\\\\text{esc}} \\\\rangle$ that are seen in fig. 8 are smoothed out. The reason for this likely lies in the optimization process used to find the fitting formula, as the mean squared error was used for optimization, and thus large gradients in the fitting function were disfavored because they led to large errors for the outer mass ranges.\\n\\nFig. 10 shows that the fitting formula is able to successfully predict the bimodality in the escape fraction, as seen in fig. 4. However the boundary between the two modes is less pronounced. This is likely caused by the smoothing effect of the optimization process of the fitting function discussed above.\\n\\nFinally, by comparing fig. 11 to fig. 3, we see that the fitting formula is able to reproduce all important trends, namely, the decrease in peak escape fraction with redshift and the approximate locations and values of the peaks. We also reproduce both the minima and maxima in the dependence of $f_{\\\\text{esc}}$ on $M_{\\\\text{gas}}$.\\n\\nAs mentioned earlier, it is important to emphasize that our modeling aims to capture the overall trends of LyC escape with galactic properties. Considering the inherent limitations in resolution and simplifications involved in estimating the LyC flux, it is crucial to scale the absolute value predicted by the fitting formula using a free parameter, which should be determined based on the specific ionizing photon budget required for reionization. We intend to investigate the large scale implication of these results and to determine scaling parameters in subsequent work.\\n\\n5 DISCUSSION AND CONCLUSIONS\\n\\nTo gain a better understanding of the correlation of LyC escape with galactic properties, we have applied the physically motivated model for the LyC escape fraction developed in F23 to $\\\\approx 600,000$ galaxies extracted from the TNG50 simulation (Nelson et al. 2019; Pillepich et al. 2019) in the range $5.2 < z < 20$.\\n\\nGiven the large uncertainties in the subgrid modeling of LyC escape, attempting a quantitative comparison of our results to those of previous studies would be impractical. Therefore, we focus our discussion on qualitative results. Numerous previous numerical studies, such as the First Billion Year project (Paardekooper et al. 2015), CODA-II (Lewis et al. 2020), FIRE-II (Ma et al. 2020), THE-SAN (Yeh et al. 2022), SPHINX (Rosdahl et al. 2022) and TNG50...\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"are likely to be inaccurate. Indeed, the unconstrained estimate $f_{\\\\text{esc}}$ predicts high escape fraction values in the high stellar mass range and hence does not match our findings in fig. 4 . We therefore artificially set the $f_{\\\\text{esc}}$ value to 0 for galaxies with $x_\\\\star > 8.5$, obtaining:\\n\\n$$f_{\\\\text{esc}} = \\\\begin{cases} 0 & f_{\\\\text{esc}} < 0 \\\\text{ or } M_\\\\star,\\\\log < 8.5 \\\\\\\\ 1 & f_{\\\\text{esc}} > 1 \\\\\\\\ f_{\\\\text{esc}} & \\\\text{otherwise.} \\\\end{cases}$$\\n\\n(14)\\n\\nIn order to determine the accuracy of the model in predicting individual escape fractions, we select a subsample of galaxies not used for the fitting in order to avoid problems due to over-fitting. As a measure for this accuracy we use the average relative deviation\\n\\n$$r = \\\\frac{1}{N_{\\\\text{test}}} \\\\sum_{i=1}^{N_{\\\\text{test}}} \\\\frac{|f_{\\\\text{esc, pred},i} - f_{\\\\text{esc},i}|}{f_{\\\\text{esc},i}},$$\\n\\n(15)\\n\\nwith $f_{\\\\text{esc, pred},i}$ and $f_{\\\\text{esc},i}$ being the predicted and modelled escape fraction of the $i$-th galaxy respectively, and $N_{\\\\text{test}}$ the number of test galaxies. We only used galaxies with $f_{\\\\text{esc}} > 0.01$, as this measure is not useful for $f_{\\\\text{esc}}$ approaching 0. We find a value of $r \\\\approx 1.2$, i.e. the average estimation error is of the order of a factor of 2, and as such the accuracy of predicting the escape fraction of a single halo is limited. However, for large scale studies where the statistical distribution of the escape fraction is more important, this model performs significantly better. Indeed, the average escape fraction obtained with the fitting formula is $f_{\\\\text{esc, pred},i} = 0.121 \\\\pm 0.086$, with the modelled escape fraction being $f_{\\\\text{esc},i} = 0.117 \\\\pm 0.133$.\\n\\nIn fig. 9 we show how well the fitting formula is able to reproduce the behaviour of the escape fraction in relation to the stellar mass and redshift that we examined in fig. 2 . We see that the evolution of the escape fraction with redshift is successfully reproduced. However, the large gradients in $\\\\langle f_{\\\\text{esc}} \\\\rangle$ that are seen in fig. 8 are smoothed out. The reason for this likely lies in the optimization process used to find the fitting formula, as the mean squared error was used for optimization, and thus large gradients in the fitting function were disfavored because they led to large errors for the outer mass ranges. Fig. 10 shows that the fitting formula is able to successfully predict the bimodality in the escape fraction, as seen in fig. 4 . However the boundary between the two modes is less pronounced. This is likely caused by the smoothing effect of the optimization process of the fitting function discussed above. Finally, by comparing fig. 11 to fig. 3 , we see that the fitting formula is able to reproduce all important trends, namely, the decrease in peak escape fraction with redshift and the approximate locations and values of the peaks. We also reproduce both the minima and maxima in the dependence of $f_{\\\\text{esc}}$ on $M_{\\\\text{gas}}$.\\n\\nAs mentioned earlier, it is important to emphasize that our modeling aims to capture the overall trends of LyC escape with galactic properties. Considering the inherent limitations in resolution and simplifications involved in estimating the LyC flux, it is crucial to scale the absolute value predicted by the fitting formula using a free parameter, which should be determined based on the specific ionizing photon budget required for reionization. We intend to investigate the large scale implication of these results and to determine scaling parameters in subsequent work.\\n\\n5 DISCUSSION AND CONCLUSIONS\\n\\nTo gain a better understanding of the correlation of LyC escape with galactic properties, we have applied the physically motivated model for the LyC escape fraction developed in F23 to $\\\\approx 600,000$ galaxies extracted from the TNG50 simulation (Nelson et al. 2019; Pillepich et al. 2019) in the range $5.2 < z < 20$.\\n\\nGiven the large uncertainties in the subgrid modeling of LyC escape, attempting a quantitative comparison of our results to those of previous studies would be impractical. Therefore, we focus our discussion on qualitative results. Numerous previous numerical studies, such as the First Billion Year project (Paardekooper et al. 2015), CODA-II (Lewis et al. 2020), FIRE-II (Ma et al. 2020), THE-SAN (Yeh et al. 2022), SPHINX (Rosdahl et al. 2022) and TNG50...\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"level accuracy, we can obtain the message that LeViLM is not capable of performing complicated (multi-hop) reasoning over the scene knowledge and producing accurate predictions. Besides, the prediction process is black-box and can not be explainable, which can be further studied in the future. The answer is that (i) The current baselines can only achieve strong results on easy or medium tasks and are unable to perform well on the hard task; (ii) The interpretability of the baselines is poor.\\n\\n4.5. Case Study\\n\\nTo further investigate the effects of knowledge, we perform qualitative analysis on four cases in the SK-VG dataset. Figure 5 shows the grounding results of four baselines on four referring expressions. It is observed that in the first case, all the baselines can ground the \u201ccane\u201d in the image even without the knowledge since there is only one cane presented. In the second case, the finetuned LeViLM can detect the target object even without knowledge, while it can not detect the \u201cBrandon\u2019s servant\u201d without knowledge in the third case. In the last case, all the baselines can not ground the referred object correctly, and the last three baselines all treat the \u201cSpider-Man\u201d as the \u201cenemy\u201d. This shows that the baseline models can not perform accurate reasoning in some complicated cases, demonstrating the challenges.\\n\\n5. Concluding Remarks\\n\\nThe visual grounding field has emerged as a prominent attractive research direction, where the models are required to reason over vision and language to ground the target objects. Yet, the language part of the existing VG benchmarks is only simple description texts, which can not evaluate the reasoning capability of the models comprehensively. To take a step in this direction, we propose a new benchmark dataset called SK-VG, which requires models to reason over the (image, scene knowledge, query) triples to perform accurate reasoning. We propose two approaches to perform this new task: Knowledge-embedded Vision-Language Interaction and Linguistic-enhanced Vision-Language Matching. Experimental results confirm the validity of the proposed approaches but also show that there is still substantial room for improvement, e.g., reasoning and interpretability.\\n\\nAcknowledgement\\n\\nThis work was supported in part by the Chinese Key-Area Research and Development Program of Guangdong Province (2020B0101350001), in part by the Guangdong Basic and Applied Basic Research Foundation (NO. 2020B1515020048), in part by the National Natural Science Foundation of China (NO. 61976250), in part by the Shenzhen Science and Technology Program (NO. JCYJ20220530141211024, NO. JCYJ20220818103001002), in part by the Fundamental Research Funds for the Central Universities under Grant 22lgqb25 and in part by the Guangdong Provincial Key Laboratory of Big Data Computing, The Chinese University of Hong Kong, Shenzhen. This work was also sponsored by Tencent CCF Open Fund (NO. RBFR2022009).\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"level accuracy, we can obtain the message that LeViLM is not capable of performing complicated (multi-hop) reasoning over the scene knowledge and producing accurate predictions. Besides, the prediction process is black-box and can not be explainable, which can be further studied in the future. The answer is that (i) The current baselines can only achieve strong results on easy or medium tasks and are unable to perform well on the hard task; (ii) The interpretability of the baselines is poor.  \\n\\n4.5. Case Study\\n\\nTo further investigate the effects of knowledge, we perform qualitative analysis on four cases in the SK-VG dataset. Figure 5 shows the grounding results of four baselines on four referring expressions. It is observed that in the first case, all the baselines can ground the \u201ccane\u201d in the image even without the knowledge since there is only one cane presented. In the second case, the finetuned LeViLM can detect the target object even without knowledge, while it can not detect the \u201cBrandon\u2019s servant\u201d without knowledge in the third case. In the last case, all the baselines can not ground the referred object correctly, and the last three baselines all treat the \u201cSpider-Man\u201d as the \u201cenemy\u201d. This shows that the baseline models can not perform accurate reasoning in some complicated cases, demonstrating the challenges.  \\n\\n5. Concluding Remarks\\n\\nThe visual grounding field has emerged as a prominent attractive research direction, where the models are required to reason over vision and language to ground the target objects. Yet, the language part of the existing VG benchmarks is only simple description texts, which can not evaluate the reasoning capability of the models comprehensively. To take a step in this direction, we propose a new benchmark dataset called SK-VG, which requires models to reason over the (image, scene knowledge, query) triples to perform accurate reasoning. We propose two approaches to perform this new task: Knowledge-embedded Vision-Language Interaction and Linguistic-enhanced Vision-Language Matching. Experimental results confirm the validity of the proposed approaches but also show that there is still substantial room for improvement, e.g., reasoning and interpretability.  \\n\\nAcknowledgement\\n\\nThis work was supported in part by the Chinese Key-Area Research and Development Program of Guangdong Province (2020B0101350001), in part by the Guangdong Basic and Applied Basic Research Foundation (NO. 2020B1515020048), in part by the National Natural Science Foundation of China (NO. 61976250), in part by the Shenzhen Science and Technology Program (NO. JCYJ20220530141211024, NO. JCYJ20220818103001002), in part by the Fundamental Research Funds for the Central Universities under Grant 22lgqb25 and in part by the Guangdong Provincial Key Laboratory of Big Data Computing, The Chinese University of Hong Kong, Shenzhen. This work was also sponsored by Tencent CCF Open Fund (NO. RBFR2022009).\"}\n\n "]}, {"edit": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unique continuation for an elliptic interface problem using unfitted isoparametric finite elements\\n\\nErik and Janosch\\n\\n1Department of Mathematics, University College London, Gower Street, London, WC1E 6BT, United Kingdom.\\n\\n*Corresponding author(s). E-mail(s): j.preuss@ucl.ac.uk; e.burman@ucl.ac.uk;\\n\\nAbstract\\n\\nWe study unique continuation over an interface using a stabilized unfitted finite element method tailored to the conditional stability of the problem. The interface is approximated using an isoparametric transformation of the background mesh and the corresponding geometrical error is included in our error analysis. To counter possible destabilizing effects caused by non-conformity of the discretization and cope with the interface conditions, we introduce adapted regularization terms. This allows to derive error estimates based on conditional stability. Numerical experiments suggest that the presence of an interface seems to be of minor importance for the continuation of the solution beyond the data domain. On the other hand, certain convexity properties of the geometry are crucial as has already been observed for many other problems without interfaces.\\n\\nKeywords: unfitted finite element method, unique continuation, interface problems, isoparametric finite element method, geometry errors, conditional H\u00f6lder stability\\n\\nMSC Classification: 35J15, 65N12, 65N20, 65N30, 86-08\\n\\n1 Introduction\\n\\n1.1 Motivation\"}\n\n "], "olmocr": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unique continuation for an elliptic interface problem using unfitted isoparametric finite elements\\n\\nErik and Janosch\\n\\n1Department of Mathematics, University College London, Gower Street, London, WC1E 6BT, United Kingdom.\\n\\n*Corresponding author(s). E-mail(s): j.preuss@ucl.ac.uk; e.burman@ucl.ac.uk;\\n\\nAbstract\\n\\nWe study unique continuation over an interface using a stabilized unfitted finite element method tailored to the conditional stability of the problem. The interface is approximated using an isoparametric transformation of the background mesh and the corresponding geometrical error is included in our error analysis. To counter possible destabilizing effects caused by non-conformity of the discretization and cope with the interface conditions, we introduce adapted regularization terms. This allows to derive error estimates based on conditional stability. Numerical experiments suggest that the presence of an interface seems to be of minor importance for the continuation of the solution beyond the data domain. On the other hand, certain convexity properties of the geometry are crucial as has already been observed for many other problems without interfaces. \\n\\nKeywords: unfitted finite element method, unique continuation, interface problems, isoparametric finite element method, geometry errors, conditional H\u00a8older stability\\n\\nMSC Classification: 35J15 , 65N12 , 65N20 , 65N30 , 86-08\\n\\n1 Introduction\\n\\n1.1 Motivation\"}\n\n "]}]