{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nougat:\n",
    "1. discard figure and put caption to the end of page\n",
    "2. keep table as latex code, put to the end of page\n",
    "3. table first, then figure captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.donut.modeling_donut_swin.DonutSwinModel'> is overwritten by shared encoder config: DonutSwinConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"depths\": [\n",
      "    2,\n",
      "    2,\n",
      "    14,\n",
      "    2\n",
      "  ],\n",
      "  \"drop_path_rate\": 0.1,\n",
      "  \"embed_dim\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"image_size\": [\n",
      "    896,\n",
      "    672\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"mlp_ratio\": 4.0,\n",
      "  \"model_type\": \"donut-swin\",\n",
      "  \"num_channels\": 3,\n",
      "  \"num_heads\": [\n",
      "    4,\n",
      "    8,\n",
      "    16,\n",
      "    32\n",
      "  ],\n",
      "  \"num_layers\": 4,\n",
      "  \"patch_size\": 4,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"use_absolute_embeddings\": false,\n",
      "  \"window_size\": 7\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.mbart.modeling_mbart.MBartForCausalLM'> is overwritten by shared decoder config: MBartConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 10,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.47.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import NougatProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import pymupdf\n",
    "import torch\n",
    "\n",
    "device = \"cuda:7\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"facebook/nougat-base\", device_map=device, torch_dtype=dtype)\n",
    "processor = NougatProcessor.from_pretrained(\"facebook/nougat-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Effectiveness of different features in the physical world\n",
      "\n",
      "#### 4.3.1 Color transfer and texture blurring\n",
      "\n",
      "\n",
      "\n",
      "In physical-world deployments, attack patches are often affected by color transfer due to lighting conditions or blurring caused by camera focus or smudging. To analyze if these changes will affect the performance of the attack patches, we compare the performance of an original patch, a color-adjusted patch, and a local texture adjusted patch. The color-adjusted patch is applied by adding a value (\\(\\delta\\)) to all values in RGB channels and making sure \\(\\delta\\) will not lead to an overflow. This color transfer will not change the texture information of the patch. The local texture adjustment is applied by using a 3x3 Gaussian blur. Figure 4 demonstrates the two types of feature adjustments applying to a patch with brightness range=0.24.\n",
      "\n",
      "\n",
      "\n",
      "The performances of different patches are shown in Table 1. Regardless of the lightness restriction, the color transfer patch achieves almost the same success rate as the original patch. This performance shows that the patch does not need to maintain a specific color to deceive the target network. On the other hand, the blurred patch exhibits a significant decrease in success rate, suggesting that local texture is the key feature in deceiving target networks. Using these findings, we can apply the proposed hue mapping method to adjust the color of the patch and enhance its integration with the target environment, resulting in further reduced visibility. This process does not require any learning and can be quickly applied when deploying the patch in the physical world.\n",
      "\n",
      "#### 4.3.2 Random color variations\n",
      "\n",
      "When printing an attack patch, it is important to consider that normal printers are not able to produce a patch with precisely the same color as the digital version. Therefore, the patch's robustness to random color variations must be evaluated.\n",
      "\n",
      "To replicate the color drift that commonly occurs during printing, we generate random noise within a restricted range that corresponds to a percentage of the original value. This approach allows us to simulate different levels of drift, and the results are shown in Table 2.\n",
      "\n",
      "\n",
      "\n",
      "\\begin{table} \\begin{tabular}{c c c c} \\hline \\hline Brightness range & Original & Color transfer & Gaussian blur \\\\ \\hline \\(1\\) (AdvPatch) & 89.4\\% & 90.8\\% & 47.8\\% \\\\ \\(0.35\\) & 89.5\\% & 87.9\\% & 22.7\\% \\\\ \\(0.24\\) & 74.2\\% & 75.3\\% & 10.1\\% \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 1: Performance with color transfer and Gaussian blur\n",
      "\n",
      "\\begin{table} \\begin{tabular}{c c c c c} \\hline \\hline Brightness range & Original & 10\\% drift & 15\\% drift & 20\\% drift \\\\ \\hline \\(1\\) (AdvPatch) & 89.4\\% & 87.6\\% & 85.9\\% & 83.3\\% \\\\ \\(0.35\\) & 89.5\\% & 84.2\\% & 77.6\\% & 67.2\\% \\\\ \\(0.24\\) & 74.2\\% & 68.6\\% & 43.2\\% & 27.3\\% \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 2: Performance with different color drift\n",
      "\n",
      "Figure 4: Patch with different feature changes. (left: original patch; middle: color transfer patch; right: texture blurring patch)\n"
     ]
    }
   ],
   "source": [
    "# sample image\n",
    "pixmap = pymupdf.open(\"./2307.00421.pdf\")[5].get_pixmap(dpi=800)\n",
    "mode = 'RGB'\n",
    "img = Image.frombytes(mode, [pixmap.width, pixmap.height], pixmap.samples)\n",
    "nougat_inputs = processor(img, return_tensors=\"pt\")\n",
    "outputs_nougat = model.generate(\n",
    "    nougat_inputs['pixel_values'].to(model.device, dtype=dtype),\n",
    "    min_length=1,\n",
    "    max_new_tokens=1024,\n",
    "    bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    ")\n",
    "generated_text = processor.batch_decode(outputs_nougat, skip_special_tokens=True)\n",
    "print(generated_text[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "copylookup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
