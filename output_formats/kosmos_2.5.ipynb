{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kosmos 2.5\n",
    "1. discard figure and put caption to the end\n",
    "2. put table at begining of page and convert to html (different to the kosmos-2.5 paper?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/duan/miniconda3/envs/copylookup/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Kosmos2_5TextForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [100260, 100265, 100266, 100267, 100268, 100269, 100270, 100271, 100272, 100273, 100274, 100275, 100276, 100277, 100278, 100279, 100281], your vocabulary could be corrupted !\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pymupdf\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, Kosmos2_5ForConditionalGeneration\n",
    "\n",
    "repo = \"microsoft/kosmos-2.5\"\n",
    "device = \"cuda:7\"\n",
    "dtype = torch.bfloat16\n",
    "model = Kosmos2_5ForConditionalGeneration.from_pretrained(repo, device_map=device, torch_dtype=dtype)\n",
    "processor = AutoProcessor.from_pretrained(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table>\n",
      "<thead>\n",
      "<tr>\n",
      "<th>\n",
      "Brightness range\n",
      "</th>\n",
      "<th>\n",
      "Original\n",
      "</th>\n",
      "<th>\n",
      "Color transfer\n",
      "</th>\n",
      "<th>\n",
      "Gaussian blur\n",
      "</th>\n",
      "</tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr>\n",
      "<td>\n",
      "1 (AdvPatch)\n",
      "</td>\n",
      "<td>\n",
      "89.4%\n",
      "</td>\n",
      "<td>\n",
      "90.8%\n",
      "</td>\n",
      "<td>\n",
      "47.8%\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "0.35\n",
      "</td>\n",
      "<td>\n",
      "89.5%\n",
      "</td>\n",
      "<td>\n",
      "87.9%\n",
      "</td>\n",
      "<td>\n",
      "22.7%\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "0.24\n",
      "</td>\n",
      "<td>\n",
      "74.2%\n",
      "</td>\n",
      "<td>\n",
      "75.3%\n",
      "</td>\n",
      "<td>\n",
      "10.1%\n",
      "</td>\n",
      "</tr>\n",
      "</tbody>\n",
      "</table>\n",
      "\n",
      "Table 1: Performance with color transfer and Gaussian blur\n",
      "\n",
      "<table>\n",
      "<thead>\n",
      "<tr>\n",
      "<th>\n",
      "Brightness range\n",
      "</th>\n",
      "<th>\n",
      "Original\n",
      "</th>\n",
      "<th>\n",
      "10% drift\n",
      "</th>\n",
      "<th>\n",
      "15% drift\n",
      "</th>\n",
      "<th>\n",
      "20% drift\n",
      "</th>\n",
      "</tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr>\n",
      "<td>\n",
      "1 (AdvPatch)\n",
      "</td>\n",
      "<td>\n",
      "89.4%\n",
      "</td>\n",
      "<td>\n",
      "87.6%\n",
      "</td>\n",
      "<td>\n",
      "85.9%\n",
      "</td>\n",
      "<td>\n",
      "83.3%\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "0.35\n",
      "</td>\n",
      "<td>\n",
      "89.5%\n",
      "</td>\n",
      "<td>\n",
      "84.2%\n",
      "</td>\n",
      "<td>\n",
      "77.6%\n",
      "</td>\n",
      "<td>\n",
      "67.2%\n",
      "</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "0.24\n",
      "</td>\n",
      "<td>\n",
      "74.2%\n",
      "</td>\n",
      "<td>\n",
      "68.0%\n",
      "</td>\n",
      "<td>\n",
      "43.2%\n",
      "</td>\n",
      "<td>\n",
      "27.3%\n",
      "</td>\n",
      "</tr>\n",
      "</tbody>\n",
      "</table>\n",
      "\n",
      "Table 2: Performance with different color drift\n",
      "\n",
      "### Effectiveness of different features in the physical world\n",
      "\n",
      "#### 4.3.1 Color transfer and texture blurring\n",
      "\n",
      "In physical-world deployments, attack patches are often affected by color transfer due to lighting conditions or blurring caused by camera focus or smudging. To analyze if these changes will affect the performance of the attack patches, we compare the performance of an original patch, a color-adjusted patch, and a local texture adjusted patch. The color-adjusted patch is applied by adding a value ( ) to all values in RGB channels and making sure will not lead to an overflow. This color transfer will not change the texture information of the patch. The local texture adjustment is applied by using a 3x3 Gaussian blur. Figure 4 demonstrates the two types of feature adjustments applying to a patch with brightness range=0.24.\n",
      "\n",
      "The performances of different patches are shown in Table 1. Regardless of the lightness restriction, the color transfer patch achieves almost the same success rate as the original patch. This performance shows that the patch does not need to maintain a specific color to deceive the target network. On the other hand, the blurred patch exhibits a significant decrease in success rate, suggesting that local texture is the key feature in deceiving target networks. Using these findings, we can apply the proposed hue mapping method to adjust the color of the patch and enhance its integration with the target environment, resulting in further reduced visibility. This process does not require any learning and can be quickly applied when deploying the patch in the physical world.\n",
      "\n",
      "#### 4.3.2 Random color variations\n",
      "\n",
      "When printing an attack patch, it is important to consider that normal printers are not able to produce a patch with precisely the same color as the digital version. Therefore, the patchâ€™s robustness to random color variations must be evaluated.\n",
      "\n",
      "To replicate the color drift that commonly occurs during printing, we generate random noise within a restricted range that corresponds to a percentage of the original value. This approach allows us to simulate different levels of drift, and the results are shown in Table 2.\n"
     ]
    }
   ],
   "source": [
    "# sample image\n",
    "pixmap = pymupdf.open(\"./2307.00421.pdf\")[5].get_pixmap(dpi=800)\n",
    "mode = 'RGB'\n",
    "img = Image.frombytes(mode, [pixmap.width, pixmap.height], pixmap.samples)\n",
    "\n",
    "prompt = \"<md>\"\n",
    "inputs = processor(text=prompt, images=img, return_tensors=\"pt\")\n",
    "\n",
    "height, width = inputs.pop(\"height\"), inputs.pop(\"width\")\n",
    "raw_width, raw_height = img.size\n",
    "scale_height = raw_height / height\n",
    "scale_width = raw_width / width\n",
    "\n",
    "inputs = {k: v.to(device) if v is not None else None for k, v in inputs.items()}\n",
    "inputs[\"flattened_patches\"] = inputs[\"flattened_patches\"].to(dtype)\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=1024,\n",
    ")\n",
    "\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(generated_text[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "copylookup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
