{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLMOCR:\n",
    "1. discard figure and keep caption.\n",
    "2. convert table to markdown.\n",
    "3. output format not really stable, sometime it ignores table caption, some time the caption of figure comes within square brackets.\n",
    "```\n",
    "![Figure 5: Patches with different color drift.](image)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/duan/miniconda3/envs/copylookup/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.49it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2VLForConditionalGeneration(\n",
       "  (visual): Qwen2VisionTransformerPretrainedModel(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "    )\n",
       "    (rotary_pos_emb): VisionRotaryEmbedding()\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x Qwen2VLVisionBlock(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): VisionSdpaAttention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp): VisionMlp(\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (act): QuickGELUActivation()\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (merger): PatchMerger(\n",
       "      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=3584, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (model): Qwen2VLModel(\n",
       "    (embed_tokens): Embedding(152064, 3584)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2VLDecoderLayer(\n",
       "        (self_attn): Qwen2VLSdpaAttention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "          (rotary_emb): Qwen2VLRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2VLRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "\n",
    "from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n",
    "\n",
    "from olmocr.prompts import build_finetuning_prompt\n",
    "from olmocr.prompts.anchor import get_anchor_text\n",
    "\n",
    "# Initialize the model\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\"allenai/olmOCR-7B-0225-preview\", torch_dtype=torch.bfloat16).eval()\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "from PIL import Image\n",
    "# Render page 1 to an image\n",
    "pixmap = pymupdf.open(\"./2307.00421.pdf\")[6].get_pixmap(dpi=800)\n",
    "mode = 'RGB'\n",
    "img = Image.frombytes(mode, [pixmap.width, pixmap.height], pixmap.samples)\n",
    "width, height = img.size\n",
    "target_longest_image_dim=1024\n",
    "if width > height:\n",
    "    new_width = target_longest_image_dim\n",
    "    new_height = int(height * target_longest_image_dim / width)\n",
    "else: \n",
    "    new_height = target_longest_image_dim\n",
    "    new_width = int(width * target_longest_image_dim / height)\n",
    "img = img.resize((new_width, new_height), Image.LANCZOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We notice that even with 10% random color drift, the attack patches can still maintain an attack rate as reliable as that for the original clean patches. This performance suggests that the patch is relatively robust and can withstand slight color variations during printing. Figure 5 demonstrates a set of attack patches with different noise levels. As the level of noise increases, the patch experiences certain color deviations. However, adjusting the color of the patch is distinct from this process because it uniformly alters all the pixels to maintain the local texture while achieving the desired color.\\\\n\\\\n![Figure 5: Patches with different color drift.](image)\\\\n\\\\n### 4.3.3 Patch scaling\\\\n\\\\nIn practical deployments, it is often the case that the environment can accommodate larger sized attack patches. As demonstrated in the previous sections, training larger sized patches can result in improved attack performance. However, in such scenarios, there may not be enough time to train a new patch with a custom size. Therefore, we are motivated to investigate whether we can enhance the performance of a pre-existing attack patch by using interpolation.\\\\n\\\\nWe use a patch with brightness range=0.24, and its initial attack success rate is 74.2%. Then we use bilinear interpolation to get patches with sizes 84x84, 98x98, and 112x112. The success rates of the different patches are shown in Table 3.\\\\n\\\\n| Patch size | 70x70 | 84x84 | 98x98 | 112x112 |\\\\n|------------|-------|-------|-------|---------|\\\\n| Success rate | 74.2% | 83.4% | 87.4% | 88.4% |\\\\n\\\\nWe notice that the attack success rate increases as the patch size grows. However, we also observe a diminishing marginal utility with each increment in size. In fact, we find that the success rate of the bilinear interpolation patch, even when it is the same size as the original patch, is lower. Some images with different scaled patches are shown in Figure 6.\\\\n\\\\n![Figure 6: Patch scaling. (left: original 70x70 patch; middle: scaled 98x98 patch; right: original 98x98 patch)](image)\\\\n\\\\n### 4.4 Physical-world attacks\\\\n\\\\nThe physical-world deployability of normal attack patches has been demonstrated in many works. However, a reasonable concern about the proposed BrPatch is whether the restricted brightness\"}'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#image_base64 = render_pdf_to_base64png(\"./paper.pdf\", 1, target_longest_image_dim=1024)\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "buffered = BytesIO()\n",
    "img.save(buffered, format=\"png\")\n",
    "image_base64 = base64.b64encode(buffered.getvalue())\n",
    "\n",
    "# Build the prompt, using document metadata\n",
    "anchor_text = get_anchor_text(\"./2307.00421.pdf\", 7, pdf_engine=\"pdfreport\", target_length=4000)\n",
    "prompt = build_finetuning_prompt(anchor_text)\n",
    "\n",
    "# Build the full prompt\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{image_base64}\"}},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Apply the chat template and processor\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "# main_image = Image.open(BytesIO(base64.b64decode(image_base64)))\n",
    "\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=[img],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = {key: value.to(device) for (key, value) in inputs.items()}\n",
    "\n",
    "\n",
    "# Generate the output\n",
    "output = model.generate(\n",
    "            **inputs,\n",
    "            #temperature=1,\n",
    "            max_new_tokens=4096,\n",
    "            #num_return_sequences=1,\n",
    "            #do_sample=True,\n",
    "        )\n",
    "\n",
    "# Decode the output\n",
    "prompt_length = inputs[\"input_ids\"].shape[1]\n",
    "new_tokens = output[:, prompt_length:]\n",
    "text_output = processor.tokenizer.batch_decode(\n",
    "    new_tokens, skip_special_tokens=True\n",
    ")\n",
    "\n",
    "text_output[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "copylookup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
